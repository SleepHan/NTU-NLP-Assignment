{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05ac734e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ.setdefault('TORCH_COMPILE_DISABLE', '1')\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "import torch\n",
    "\n",
    "# Method 2: Patch torch._dynamo.disable decorator after import\n",
    "try:\n",
    "    import torch._dynamo\n",
    "    # Patch the disable function to ignore the 'wrapping' parameter\n",
    "    if hasattr(torch._dynamo, 'disable'):\n",
    "        def patched_disable(fn=None, *args, **kwargs):\n",
    "            # Remove problematic 'wrapping' parameter if present\n",
    "            if 'wrapping' in kwargs:\n",
    "                kwargs.pop('wrapping')\n",
    "            if fn is None:\n",
    "                # Decorator usage: @disable\n",
    "                return lambda f: f\n",
    "            # Function usage: disable(fn) or disable(fn, **kwargs)\n",
    "            # Simply return the function unwrapped to avoid recursion\n",
    "            # The original disable was causing issues, so we bypass it entirely\n",
    "            return fn\n",
    "        torch._dynamo.disable = patched_disable\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not patch torch._dynamo: {e}\")\n",
    "    pass  # If patching fails, continue anyway\n",
    "\n",
    "import random, string\n",
    "\n",
    "from torchtext import data , datasets\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "\n",
    "os.environ['GENSIM_DATA_DIR'] = os.path.join(os.getcwd(), 'gensim-data')\n",
    "\n",
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.fasttext import load_facebook_model\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import time, copy\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44e533a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Prepping Data...\n",
      "[+] Test set formed!\n",
      "[+] Train and Validation sets formed!\n",
      "[+] Data prepped successfully!\n",
      "[*] Retrieving pretrained word embeddings...\n",
      "[*] Loading fasttext model...\n",
      "[+] Model loaded!\n",
      "[*] Forming embedding matrix...\n",
      "[+] Embedding matrix formed!\n",
      "[+] Embeddings retrieved successfully!\n",
      "\n",
      "Label distribution in training set:\n",
      "- ABBR: 69 samples (1.58%)\n",
      "- DESC: 930 samples (21.32%)\n",
      "- ENTY: 1000 samples (22.93%)\n",
      "- HUM: 978 samples (22.42%)\n",
      "- LOC: 668 samples (15.31%)\n",
      "- NUM: 717 samples (16.44%)\n",
      "Total samples: 4362, Sum of percentages: 100.00%\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "train_data, validation_data, test_data, LABEL, TEXT, pretrained_embed = data_prep(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c176434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86a01357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of classes: 6\n",
      "Classes: ['ENTY', 'HUM', 'DESC', 'NUM', 'LOC', 'ABBR']\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary for labels\n",
    "LABEL.build_vocab(train_data)\n",
    "num_classes = len(LABEL.vocab)\n",
    "print(f\"\\nNumber of classes: {num_classes}\")\n",
    "print(f\"Classes: {LABEL.vocab.itos}\")\n",
    "\n",
    "# Get pretrained embeddings from Part 1 (frozen embeddings)\n",
    "# TODO: Check if this step is redundant\n",
    "pretrained_embeddings = pretrained_embed.weight.data\n",
    "\n",
    "# Get embedding dimension and vocab size from the fasttext embedding layer\n",
    "embedding_dim = pretrained_embed.weight.shape[1]\n",
    "embedding_vocab_size = pretrained_embed.weight.shape[0]  # Vocab size from saved embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2597781b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT Vocab Size: 8153\n"
     ]
    }
   ],
   "source": [
    "print(f'TEXT Vocab Size: {len(TEXT.vocab.stoi)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aad6c8e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PART 3.4: TARGETED IMPROVEMENT FOR WEAK TOPICS\n",
      "================================================================================\n",
      "\n",
      "Strategies:\n",
      "  1. Data Augmentation for imbalanced classes (especially ABBR)\n",
      "  2. Positional Embeddings in attention layer\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PART 3.4: Targeted Improvement for Weak Topics\n",
    "# Strategy: Data Augmentation, Positional Embeddings\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 3.4: TARGETED IMPROVEMENT FOR WEAK TOPICS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nStrategies:\")\n",
    "print(\"  1. Data Augmentation for imbalanced classes (especially ABBR)\")\n",
    "print(\"  2. Positional Embeddings in attention layer\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Import required libraries for augmentation\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "try:\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c2db80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Step 2: Implementing Data Augmentation Functions...\n",
      "    ✓ Data augmentation functions ready\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Step 2: Data Augmentation Functions for Imbalanced Classes\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n>>> Step 2: Implementing Data Augmentation Functions...\")\n",
    "\n",
    "def get_synonyms(word):\n",
    "    \"\"\"Get synonyms for a word using WordNet\"\"\"\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonym = lemma.name().replace('_', ' ').lower()\n",
    "            if synonym != word and synonym.isalpha():\n",
    "                synonyms.add(synonym)\n",
    "    return list(synonyms)\n",
    "\n",
    "def synonym_replacement(tokens, n=1):\n",
    "    \"\"\"Replace n random words with their synonyms\"\"\"\n",
    "    new_tokens = tokens.copy()\n",
    "    words_to_replace = [i for i, word in enumerate(tokens) if word.isalpha() and len(word) > 2]\n",
    "    \n",
    "    if len(words_to_replace) == 0:\n",
    "        return tokens\n",
    "    \n",
    "    num_replacements = min(n, len(words_to_replace))\n",
    "    indices_to_replace = random.sample(words_to_replace, num_replacements)\n",
    "    \n",
    "    for idx in indices_to_replace:\n",
    "        synonyms = get_synonyms(tokens[idx])\n",
    "        if synonyms:\n",
    "            new_tokens[idx] = random.choice(synonyms)\n",
    "    \n",
    "    return new_tokens\n",
    "\n",
    "def random_insertion(tokens, n=1):\n",
    "    \"\"\"Randomly insert synonyms of n words\"\"\"\n",
    "    new_tokens = tokens.copy()\n",
    "    \n",
    "    for _ in range(n):\n",
    "        if len(new_tokens) == 0:\n",
    "            break\n",
    "        word = random.choice(new_tokens)\n",
    "        synonyms = get_synonyms(word)\n",
    "        if synonyms:\n",
    "            synonym = random.choice(synonyms)\n",
    "            insert_pos = random.randint(0, len(new_tokens))\n",
    "            new_tokens.insert(insert_pos, synonym)\n",
    "    \n",
    "    return new_tokens\n",
    "\n",
    "def random_deletion(tokens, p=0.1):\n",
    "    \"\"\"Randomly delete words with probability p\"\"\"\n",
    "    if len(tokens) == 1:\n",
    "        return tokens\n",
    "    \n",
    "    new_tokens = []\n",
    "    for token in tokens:\n",
    "        if random.random() > p:\n",
    "            new_tokens.append(token)\n",
    "    \n",
    "    if len(new_tokens) == 0:\n",
    "        return tokens[:1]\n",
    "    \n",
    "    return new_tokens\n",
    "\n",
    "def random_swap(tokens, n=1):\n",
    "    \"\"\"Randomly swap n pairs of words\"\"\"\n",
    "    new_tokens = tokens.copy()\n",
    "    \n",
    "    for _ in range(n):\n",
    "        if len(new_tokens) < 2:\n",
    "            break\n",
    "        idx1, idx2 = random.sample(range(len(new_tokens)), 2)\n",
    "        new_tokens[idx1], new_tokens[idx2] = new_tokens[idx2], new_tokens[idx1]\n",
    "    \n",
    "    return new_tokens\n",
    "\n",
    "def augment_text(text, augmentation_techniques=['synonym', 'insertion', 'deletion', 'swap'], \n",
    "                 num_augmentations=3):\n",
    "    \"\"\"Apply data augmentation to text\"\"\"\n",
    "    augmented_texts = []\n",
    "    \n",
    "    for _ in range(num_augmentations):\n",
    "        aug_text = text.copy()\n",
    "        technique = random.choice(augmentation_techniques)\n",
    "        \n",
    "        if technique == 'synonym' and len(aug_text) > 0:\n",
    "            aug_text = synonym_replacement(aug_text, n=random.randint(1, 2))\n",
    "        elif technique == 'insertion' and len(aug_text) > 0:\n",
    "            aug_text = random_insertion(aug_text, n=random.randint(1, 2))\n",
    "        elif technique == 'deletion' and len(aug_text) > 1:\n",
    "            aug_text = random_deletion(aug_text, p=0.1)\n",
    "        elif technique == 'swap' and len(aug_text) > 1:\n",
    "            aug_text = random_swap(aug_text, n=1)\n",
    "        \n",
    "        augmented_texts.append(aug_text)\n",
    "    \n",
    "    return augmented_texts\n",
    "\n",
    "print(\"    ✓ Data augmentation functions ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8f8ddcf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Step 3: Applying Data Augmentation for Imbalanced Classes...\n",
      "\n",
      "Original label distribution:\n",
      "  ABBR: 69 samples (1.58%)\n",
      "  DESC: 930 samples (21.32%)\n",
      "  ENTY: 1000 samples (22.93%)\n",
      "  HUM: 978 samples (22.42%)\n",
      "  LOC: 668 samples (15.31%)\n",
      "  NUM: 717 samples (16.44%)\n",
      "\n",
      "  Augmenting ABBR: 69 -> 900 samples\n",
      "    Generating 831 additional samples...\n",
      "    ✓ Generated 831 augmented samples\n",
      "\n",
      "  Augmenting NUM: 717 -> 850 samples\n",
      "    Generating 133 additional samples...\n",
      "    ✓ Generated 133 augmented samples\n",
      "\n",
      "  Augmenting HUM: 978 -> 1200 samples\n",
      "    Generating 222 additional samples...\n",
      "    ✓ Generated 222 augmented samples\n",
      "\n",
      "  Augmenting ENTY: 1000 -> 1300 samples\n",
      "    Generating 300 additional samples...\n",
      "    ✓ Generated 300 augmented samples\n",
      "\n",
      "  Augmenting LOC: 668 -> 800 samples\n",
      "    Generating 132 additional samples...\n",
      "    ✓ Generated 132 augmented samples\n",
      "\n",
      "Augmented label distribution:\n",
      "  ABBR: 900 samples (15.05%)\n",
      "  DESC: 930 samples (15.55%)\n",
      "  ENTY: 1300 samples (21.74%)\n",
      "  HUM: 1200 samples (20.07%)\n",
      "  LOC: 800 samples (13.38%)\n",
      "  NUM: 850 samples (14.21%)\n",
      "\n",
      "  Total samples: 4362 -> 5980\n",
      "  ✓ Data augmentation complete\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# Step 3: Apply Data Augmentation for Imbalanced Classes\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n>>> Step 3: Applying Data Augmentation for Imbalanced Classes...\")\n",
    "\n",
    "# Count current label distribution\n",
    "label_counts_p34 = Counter([ex.label for ex in train_data.examples])\n",
    "print(f\"\\nOriginal label distribution:\")\n",
    "for label, count in sorted(label_counts_p34.items()):\n",
    "    print(f\"  {label}: {count} samples ({count/len(train_data.examples)*100:.2f}%)\")\n",
    "\n",
    "# Augmentation targets (boost weaker topics more aggressively)\n",
    "target_counts_p34 = {\n",
    "    'ABBR': 900,   # heavy boost (~13x) to improve weakest class\n",
    "    'DESC': 930,   # keep strong class unchanged\n",
    "    'ENTY': 1300,  # moderate boost (~1.3x)\n",
    "    'HUM': 1200,   # boost (~1.23x)\n",
    "    'LOC': 800,    # modest boost (~1.2x)\n",
    "    'NUM': 850     # modest boost (~1.2x)\n",
    "}\n",
    "\n",
    "# Create augmented examples\n",
    "augmented_examples = list(train_data.examples)  # Start with all original examples\n",
    "\n",
    "for label in label_counts_p34.keys():\n",
    "    current_count = label_counts_p34[label]\n",
    "    target_count = target_counts_p34[label]\n",
    "    \n",
    "    if current_count < target_count:\n",
    "        label_examples = [ex for ex in train_data.examples if ex.label == label]\n",
    "        num_augmentations_needed = target_count - current_count\n",
    "        \n",
    "        print(f\"\\n  Augmenting {label}: {current_count} -> {target_count} samples\")\n",
    "        print(f\"    Generating {num_augmentations_needed} additional samples...\")\n",
    "        \n",
    "        augmented_count = 0\n",
    "        while augmented_count < num_augmentations_needed:\n",
    "            original_ex = random.choice(label_examples)\n",
    "            aug_texts = augment_text(original_ex.text, num_augmentations=1)\n",
    "            \n",
    "            for aug_text in aug_texts:\n",
    "                if augmented_count >= num_augmentations_needed:\n",
    "                    break\n",
    "                \n",
    "                new_ex = data.Example.fromlist([aug_text, label], \n",
    "                                               fields=[('text', TEXT), ('label', LABEL)])\n",
    "                augmented_examples.append(new_ex)\n",
    "                augmented_count += 1\n",
    "        \n",
    "        print(f\"    ✓ Generated {augmented_count} augmented samples\")\n",
    "\n",
    "# Create augmented dataset with proper field structure\n",
    "augmented_train_data = data.Dataset(augmented_examples, fields=[('text', TEXT), ('label', LABEL)])\n",
    "\n",
    "# Verify augmented distribution\n",
    "new_label_counts = Counter([ex.label for ex in augmented_examples])\n",
    "print(f\"\\nAugmented label distribution:\")\n",
    "for label, count in sorted(new_label_counts.items()):\n",
    "    print(f\"  {label}: {count} samples ({count/len(augmented_examples)*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\n  Total samples: {len(train_data.examples)} -> {len(augmented_examples)}\")\n",
    "print(f\"  ✓ Data augmentation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9ec4bc0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# augmented_train_data.examples[0].label\n",
    "[(ex.text, ex.label) for ex in augmented_train_data if ex.label not in ['ABBR','DESC','ENTY','HUM','LOC','NUM']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ce04616c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(None,\n",
       "            {'ENTY': 0, 'HUM': 1, 'DESC': 2, 'NUM': 3, 'LOC': 4, 'ABBR': 5})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABEL.vocab.stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a9afaf5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What', 'does', 'IOC', 'stand', '?', 'for']\n",
      "ABBR\n",
      "['channel', 'does', 'the', 'What', 'ESPN', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'Motors', 'abbreviation', 'of', 'the', 'company', 'name', '`', 'General', 'the', \"'\", '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'for', 'micro', '?']\n",
      "ABBR\n",
      "['What', 'does', 'CPR', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'abbreviation', 'IOC', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'BPH', 'is', '?']\n",
      "ABBR\n",
      "['What', 'set', 'CNN', 'stomach', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviated', 'term', 'National', 'for', 'the', 'used', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NN', 'O', '`', 'used', 'mean', 'when', '`', 'as', 'a', 'prefix', 'in', 'Irish', 'surnames', '?']\n",
      "ABBR\n",
      "['CNN', 'is', 'the', 'abbreviation', 'for', 'what', '?']\n",
      "ABBR\n",
      "['What', 'does', 'answer', 'IQ', 'stand', 'for', 'bear', '?']\n",
      "ABBR\n",
      "['What', 'does', 'CPR', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'MSG', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'snafu', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['CNN', 'is', 'the', 'abbreviation', 'for', 'what', '?']\n",
      "ABBR\n",
      "['What', '?', 'snafu', 'stand', 'for', 'does']\n",
      "ABBR\n",
      "['What', 'does', 'BTU', '?']\n",
      "ABBR\n",
      "['What', 'is', 'p.m.', 'an', 'abbreviation', 'for', ',', 'as', 'in', '5', '?']\n",
      "ABBR\n",
      "['What', 'does', 'SIDS', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['for', 'does', 'SHIELD', 'stand', 'What', '?']\n",
      "ABBR\n",
      "['What', 'does', 'VCR', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'S.O.S.', 'stand', '?']\n",
      "ABBR\n",
      "['What', 'is', '?', 'DTMF']\n",
      "ABBR\n",
      "['What', 'is', 'HTML', '?']\n",
      "ABBR\n",
      "['CNN', 'is', 'an', 'acronym', 'for', 'what', '?']\n",
      "ABBR\n",
      "['arrange', 'What', 'coiffe', 'does', 'S.O.S.', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['AFS', 'is', 'What', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'number', 'along', '`', '`', '5', \"''\", 'stand', 'for', 'on', 'FUBU', 'clothing', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'of', 'the', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', '?', 'DTMF', 'is']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'for', 'micro', '?']\n",
      "ABBR\n",
      "['When', 'reading', 'classified', 'ads', ',', 'what', 'does', ':', 'other', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'NASA', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'CNN', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'a', '`', '`', 'USB', \"''\", 'port', 'on', 'computer', '?']\n",
      "ABBR\n",
      "['What', 'the', 'does', 'channel', 'ESPN', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'make', 'msg', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'of', 'the', 'external', 'Olympic', 'Committee', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'abbreviation', 'OAS', 'for', 'stand', '?']\n",
      "ABBR\n",
      "['What', 'indiana', 'is', 'RAM', 'personify', 'in', 'the', 'computer', '?']\n",
      "ABBR\n",
      "['CNN', 'for', 'an', 'acronym', 'is', 'what', '?']\n",
      "ABBR\n",
      "['What', 'R.E.M.', 'does', 'stand', 'for', ',', 'as', 'in', 'the', 'rock', 'group', 'R.E.M.', '?']\n",
      "ABBR\n",
      "['What', 'is', '?', 'BPH']\n",
      "ABBR\n",
      "['What', 'does', 'stand', 'SHIELD', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'for', 'AIDS', 'stand', 'abbreviation', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'acronym', 'for', 'the', 'National', 'chest', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['CNN', 'is', 'an', 'acronym', 'follow', 'for', 'what', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NASDAQ', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['?', 'does', 'the', 'abbreviation', 'AIDS', 'stand', 'for', 'What']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'expression', 'for', 'the', 'Bureau', 'of', 'Investigation']\n",
      "ABBR\n",
      "['What', 'does', 'suffice', 'BTU', 'mean', '?', 'do']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'for', 'micro', '?']\n",
      "ABBR\n",
      "['What', 'answer', 'the', '`', '`', 'blue', 'ribbon', \"''\", 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'doe', 'CNN', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'JESSICA', 'intend', '?']\n",
      "ABBR\n",
      "['CNN', 'acronym', 'an', 'is', 'for', 'what', '?']\n",
      "ABBR\n",
      "['What', 'does', 'G.M.T.', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'RCD', '?', 'constitute']\n",
      "ABBR\n",
      "['What', 'does', '?', 'channel', 'ESPN', 'stand', 'for', 'the']\n",
      "ABBR\n",
      "['What', 'does', 'IQ', 'bandstand', 'for', '?']\n",
      "ABBR\n",
      "['?', 'does', 'NASDAQ', 'stand', 'for', 'What']\n",
      "ABBR\n",
      "['What', 'for', 'the', 'abbreviation', 'AIDS', 'stand', 'does', '?']\n",
      "ABBR\n",
      "['What', 'is', 'acronym', 'for', 'the', 'National', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['does', 'the', 'abbreviation', 'IOC', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'endure', 'the', 'abbreviation', 'OAS', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['what', 'is', 'an', 'acronym', 'for', 'CNN', '?']\n",
      "ABBR\n",
      "['What', 'is', 'p.m.', 'an', 'abbreviation', 'for', ',', '5', 'in', 'as', 'p.m.', '?']\n",
      "ABBR\n",
      "['What', 'is', 'RCD', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'of', 'the', 'International', 'Olympic', 'Committee', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NASA', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'CPR', '?', 'for', 'stand']\n",
      "ABBR\n",
      "['What', 'does', 'U.S.S.R.', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['for', 'does', 'the', 'abbreviation', 'OAS', 'stand', 'What', '?']\n",
      "ABBR\n",
      "['What', 'is', 'abbreviated', 'expression', 'for', 'the', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'channel', 'ESPN', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'pH', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'for', 'NASA', 'stand', 'does', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'count', '`', '`', '5', \"''\", 'stand', 'for', 'on', 'FUBU', 'clothing', '?']\n",
      "ABBR\n",
      "['What', 'is', 'RAM', 'in', 'the', 'figurer', '?']\n",
      "ABBR\n",
      "['What', 'does', 'e.g.', 'stand', 'act', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'JESSICA', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'execute', 'R.E.M.', 'stand', 'for', ',', 'as', 'in', 'the', 'rock', 'group', 'R.E.M.', '?']\n",
      "ABBR\n",
      "['CNN', 'is', 'an', 'acronym', 'for', 'what', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'for', 'Original', 'Equipment', 'Manufacturer', '?']\n",
      "ABBR\n",
      "['What', 'does', 'IQ', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'G.M.T.', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'do', 'the', 'letter', 'D.C.', 'stand', 'for', 'in', 'Washington', ',', 'D.C.', '?']\n",
      "ABBR\n",
      "['What', 'do', 'the', 'letters', 'D.C.', 'stand', 'for', 'in', 'Washington', ',', 'D.C.', '?']\n",
      "ABBR\n",
      "['What', 'does', 'BTU', 'mean']\n",
      "ABBR\n",
      "['What', 'does', 'the', '`', 'c', \"''\", 'stand', 'the', 'equation', 'E', '=', 'mc2', '?']\n",
      "ABBR\n",
      "['What', 'practise', 'S.O.S.', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'R.E.M.', 'stand', 'for', ',', 'as', 'in', 'the', 'rock', 'group', 'R.E.M.', '?']\n",
      "ABBR\n",
      "['What', 'equally', 'does', 'R.E.M.', 'stand', 'for', ',', 'as', 'in', 'the', 'rock', 'group', 'R.E.M.', '?']\n",
      "ABBR\n",
      "['What', 'make', 'A&W', 'of', 'solution', 'beer', 'fame', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['follow', 'What', 'is', 'the', 'abbreviated', 'expression', 'for', 'the', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['?', 'is', 'the', 'abbreviation', 'for', 'Original', 'Equipment', 'Manufacturer', 'What']\n",
      "ABBR\n",
      "['What', 'does', 'NAFTA', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviated', 'expression', 'manifestation', 'for', 'the', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['observe', 'What', 'is', 'HTML', 'follow', '?']\n",
      "ABBR\n",
      "['What', 'arrange', 'SIDS', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'SHIELD', 'stand', 'for', '?', 'shield']\n",
      "ABBR\n",
      "['iq', 'What', 'does', 'IQ', 'stand', 'for', '?', 'make']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'of', 'the', 'company', 'name', '`', 'General', 'Motors', \"'\", '?']\n",
      "ABBR\n",
      "['When', 'reading', 'classified', 'ads', ',', 'what', 'does', 'EENTY', ':', 'other', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'do', 'BTU', 'mean', '?']\n",
      "ABBR\n",
      "['does', 'What', 'A&W', 'of', 'root', 'beer', 'fame', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'SVHS', '?']\n",
      "ABBR\n",
      "['CNN', 'is', 'an', 'acronym', 'for', 'what', '?']\n",
      "ABBR\n",
      "['stand', 'does', 'IOC', 'What', 'for', '?']\n",
      "ABBR\n",
      "['does', 'What', 'IOC', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'mean', 'BTU', 'does', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'acronym', 'for', 'the', 'rating', 'system', 'for', 'airwave', 'conditioner', 'efficiency', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'acronym', 'for', 'the', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviated', 'expression', 'for', 'the', 'National', 'bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['When', 'reading', 'classified', 'ads', ',', 'does', 'EENTY', 'other', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'the', 'company', 'name', '`', 'General']\n",
      "ABBR\n",
      "['What', 'does', 'SHIELD', 'stand', '?', 'for']\n",
      "ABBR\n",
      "['form', 'is', 'the', 'full', 'What', 'of', '.com', '?']\n",
      "ABBR\n",
      "['What', 'is', 'Mikhail', 'Gorbachev', \"'s\", 'middle', 'initial', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'for', 'Original', 'Equipment', 'Manufacturer', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'for', 'Original', 'Equipment', 'Manufacturer', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviated', 'term', 'Bureau', 'for', 'the', 'National', 'used', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', \"'s\", 'the', 'abbreviation', 'tnt', 'for', 'trinitrotoluene', '?']\n",
      "ABBR\n",
      "['What', 'S.O.S.', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'SIDS', 'stand', 'for', '?', 'sids']\n",
      "ABBR\n",
      "['What', 'perform', 'Ms.', ',', 'overlook', ',', 'and', 'Mrs.', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'word', 'LASER', '?']\n",
      "ABBR\n",
      "['What', 'does', 'IQ', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'full', 'form', 'of', '.com']\n",
      "ABBR\n",
      "['?', 'does', 'NECROSIS', 'mean', 'What']\n",
      "ABBR\n",
      "['for', 'does', 'MSG', 'stand', 'What', '?']\n",
      "ABBR\n",
      "['What', 'is', 'bph', '?']\n",
      "ABBR\n",
      "['What', 'does', 'BUD', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'decoration', 'does', 'the', '`', '`', 'blue', 'ribbon', \"''\", 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviated', 'terminus', 'used', 'for', 'the', 'interior', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'for', 'Original', 'Equipment', 'Manufacturer', '?', 'equipment']\n",
      "ABBR\n",
      "['What', 'do', 'the', 'letters', 'D.C.', 'stand', 'for', 'in', 'Washington', ',', 'D.C.', '?']\n",
      "ABBR\n",
      "['What', 'does', 'JESSICA', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'is', 'a', '`', '`', 'USB', \"''\", 'port', 'on', 'a', 'computer', '?']\n",
      "ABBR\n",
      "['What', 'is', 'an', 'IOC', 'abbreviation', 'of', '?']\n",
      "ABBR\n",
      "['What', 'is', 'IOC', 'an', 'of', 'abbreviation', '?']\n",
      "ABBR\n",
      "['What', 'does', 'G.M.T.', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', \"'s\", 'the', 'abbreviation', 'for', 'tnt', '?']\n",
      "ABBR\n",
      "['What', 'is', 'a', '`', '`', 'USB', \"''\", 'port', 'a', 'computer', '?']\n",
      "ABBR\n",
      "['What', 'for', 'U.S.S.R.', 'stand', 'does', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'of', 'the', 'companionship', 'name', '`', 'General', 'Motors', \"'\", '?']\n",
      "ABBR\n",
      "['What', 'perform', 'e.g.', 'support', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'abbreviation', 'OAS', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', '`', '`', 'USB', \"''\", 'port', 'on', 'a', 'computer', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'Original', 'Equipment', 'Manufacturer', '?']\n",
      "ABBR\n",
      "['EKG', 'does', 'What', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'BTU', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'does', 'IQ', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', '`', 'a', '`', 'is', 'USB', \"''\", 'port', 'on', 'a', 'computer', '?']\n",
      "ABBR\n",
      "['What', 'is', 'RAM', 'the', 'computer', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviated', 'term', 'used', 'for', 'the', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'cause', 'the', '`', '`', 'c', \"''\", 'stand', 'for', 'in', 'the', 'equation', 'E', '=', 'mc2', '?']\n",
      "ABBR\n",
      "['What', 'does', 'R.E.M.', 'stand', 'for', ',', 'as', 'in', 'the', 'rock', 'group', 'R.E.M.', '?']\n",
      "ABBR\n",
      "['What', 'does', 'JESSICA', 'fare', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'is', 'IOC', 'an', 'abbreviation', 'of', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'full', 'form', 'of', '.com', '?']\n",
      "ABBR\n",
      "['What', 'does', 'signify', 'BTU', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'does', 'R.E.M.', 'stand', 'for', ',', 'as', 'in', 'the', 'rock', 'indium', 'group', 'R.E.M.', '?']\n",
      "ABBR\n",
      "['CNN', 'is', 'an', 'acronym', 'for', 'what', '?']\n",
      "ABBR\n",
      "['What', 'does', 'for', 'stand', 'VCR', '?']\n",
      "ABBR\n",
      "['is', 'p.m.', 'an', 'abbreviation', 'for', ',', 'as', 'in', '5', 'p.m.', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NASDAQ', 'stand', 'serve', 'for', 'brook', '?']\n",
      "ABBR\n",
      "['What', 'does', 'snafu', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'U.S.S.R.', 'stand', 'for']\n",
      "ABBR\n",
      "['What', 'does', 'VCR', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'abbreviation', 'the', 'is', 'for', 'micro', '?']\n",
      "ABBR\n",
      "['CPR', 'is', 'the', 'abbreviation', 'for', 'what', '?']\n",
      "ABBR\n",
      "['What', 'is', 'DTMF', '?']\n",
      "ABBR\n",
      "['What', 'does', 'R.E.M.', 'fend', 'for', ',', 'as', 'in', 'the', 'rock', 'group', 'R.E.M.', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NN', 'O', '`', '`', 'mean', 'when', 'used', 'as', 'a', 'prefix', 'mingy', 'in', 'Irish', 'surnames', '?']\n",
      "ABBR\n",
      "['base', 'What', 'iq', 'does', 'IQ', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', '`', '`', 'c', \"''\", 'stand', 'for', 'in', 'the', 'equation', 'E', '=', 'mc2', '?']\n",
      "ABBR\n",
      "['does', 'What', 'NASDAQ', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['does', 'pH', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'R.E.M.', 'stand', 'for', ',', 'as', 'in', 'the', 'rock', 'group', 'R.E.M.', '?']\n",
      "ABBR\n",
      "['What', 'does', 'snafu', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['?', 'is', 'the', 'abbreviated', 'term', 'used', 'for', 'the', 'National', 'Bureau', 'of', 'Investigation', 'What']\n",
      "ABBR\n",
      "['What', 'dress', 'gangrene', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'does', 'IBM', 'abide', 'for', '?']\n",
      "ABBR\n",
      "['CNN', 'is', 'the', 'abbreviation', 'for', 'what', '?']\n",
      "ABBR\n",
      "['What', 'does', 'support', 'pH', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'for', 'CPR', 'stand', 'does', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'Bureau', 'for', 'the', 'National', 'acronym', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', \"'\", 'INRI', 'stand', 'for', 'when', 'used', 'on', 'Jesus', 'does', 'cross', '?']\n",
      "ABBR\n",
      "['What', 'is', 'abbreviation', 'for', 'micro', '?']\n",
      "ABBR\n",
      "['What', 'is', 'Mikhail', 'Gorbachev', \"'s\", 'middle', 'initial', 'gorbachev', '?']\n",
      "ABBR\n",
      "['What', 'is', '?', 'LMDS']\n",
      "ABBR\n",
      "['What', 'does', 'e.g.', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'DEET', '?']\n",
      "ABBR\n",
      "['What', 'is', 'SVHS', '?']\n",
      "ABBR\n",
      "['What', 'is', 'Mikhail', 'Gorbachev', \"'s\", 'midriff', 'initial', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NAFTA', 'act', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'arrange', 'the', '`', '`', 'gamy', 'ribbon', \"''\", 'stand', 'for', '?']\n",
      "ABBR\n",
      "['in', 'is', 'RAM', 'What', 'the', 'computer', '?']\n",
      "ABBR\n",
      "['What', 'does', 'IQ', 'bandstand', 'for', '?']\n",
      "ABBR\n",
      "['When', 'reading', 'classified', 'ads', ',', 'what', 'does', 'EENTY', ':', 'other', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'drive', 'RAM', 'in', 'the', 'computer', '?']\n",
      "ABBR\n",
      "['?', 'does', 'NECROSIS', 'mean', 'What']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'for', 'micro', '?']\n",
      "ABBR\n",
      "['the', 'is', 'CNN', 'abbreviation', 'for', 'what', '?']\n",
      "ABBR\n",
      "['What', 'is', 'of', 'abbreviated', 'expression', 'for', 'the', 'National', 'Bureau', 'the', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'does', 'BMW', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'IBM', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'necrosis', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'do', 'the', 'letters', 'D.C.', 'stand', 'for', 'in', 'Washington', ',', 'D.C.', '?']\n",
      "ABBR\n",
      "['What', 'does', 'IOC', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'JESSICA', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'for', 'Original', 'Equipment', 'Manufacturer', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'acronym', 'CPR', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'execute', 'U.S.S.R.', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'A&W', 'of', 'root', 'stand', 'fame', 'beer', 'for', '?']\n",
      "ABBR\n",
      "['CNN', 'is', 'an', 'acronym', 'for', 'what', '?']\n",
      "ABBR\n",
      "['What', 'does', 'R.E.M.', 'stand', 'for', ',', 'as', 'R.E.M.', 'the', 'rock', 'group', 'in', '?']\n",
      "ABBR\n",
      "['What', 'does', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'of', 'the', 'company', 'name', '`', 'General', 'Motors', 'identify', \"'\", '?']\n",
      "ABBR\n",
      "['What', 'is', 'LMDS', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'acronym', 'for', 'the', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'channel', 'stand', 'ESPN', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'bud', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'LMDS', '?']\n",
      "ABBR\n",
      "['abbreviation', 'is', 'p.m.', 'an', 'What', 'for', ',', 'as', 'in', '5', 'p.m.', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'of', 'the', 'company', 'name', '`', 'General', 'drive', \"'\", '?']\n",
      "ABBR\n",
      "['What', 'does', 'VCR', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'manage', 'CPR', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'abbreviation', 'OAS', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'CNN', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['CNN', 'is', 'the', 'for', 'what', '?']\n",
      "ABBR\n",
      "['CPR', 'is', 'the', 'abbreviation', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviated', 'term', 'used', 'for', 'cost', 'the', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'motor', 'of', 'the', 'company', 'name', '`', 'General', 'Motors', \"'\", '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'of', 'company', 'name', 'General', 'Motors', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NAFTA', 'dress', 'stand', 'for', 'fend', '?']\n",
      "ABBR\n",
      "['is', 'IOC', 'an', 'abbreviation', 'of', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'channel', 'ESPN', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'do', 'the', 'letter', 'D.C.', 'stand', 'for', 'in', 'Washington', ',', 'D.C.', '?']\n",
      "ABBR\n",
      "['is', 'What', 'HTML', '?']\n",
      "ABBR\n",
      "['What', 'does', 'IOC', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'a', '`', '`', 'USB', \"''\", 'port', 'on', 'a', 'computer', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', '`', '`', 'blue', 'ribbon', \"''\", 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'U.S.S.R.', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'pedestal', 'NAFTA', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['CNN', 'is', 'be', 'the', 'abbreviation', 'for', 'what', '?']\n",
      "ABBR\n",
      "['What', 'does', 'S.O.S.', 'for', 'stand', '?']\n",
      "ABBR\n",
      "['What', 'is', 'a', 'port', '`', 'USB', \"''\", '`', 'on', 'a', 'computer', '?']\n",
      "ABBR\n",
      "['What', 'is', 'LMDS']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'for', 'Original', 'Equipment', 'Manufacturer', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'of', 'the', 'international', 'Olympic', 'Committee', '?']\n",
      "ABBR\n",
      "['CNN', 'is', 'an', 'acronym', 'for', 'comprise', 'what', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NN', 'O', '`', '`', 'miserly', 'when', 'used', 'as', 'a', 'prefix', 'in', 'irish', 'surnames', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NASDAQ', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'acronym', 'CPR', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'for', 'micro', '?']\n",
      "ABBR\n",
      "['does', 'What', 'BTU', 'mean', '?']\n",
      "ABBR\n",
      "['does', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'full', 'form', 'of', '?', '.com']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'abbreviation', 'AIDS', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'IOC', 'an', 'abbreviation', 'of', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NASDAQ', 'base', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'abbreviation', 'OAS', 'stand', 'for', '?', 'pedestal']\n",
      "ABBR\n",
      "['What', 'arrange', 'NASDAQ', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'bud', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'number', '`', '`', '5', \"''\", 'stand', 'for', 'on', 'FUBU', 'clothing', '?']\n",
      "ABBR\n",
      "['?', 'is', 'the', 'abbreviation', 'for', 'what', 'CNN']\n",
      "ABBR\n",
      "['What', 'does', 'NASA', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'IBM', 'rack', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'of', 'the', 'National', 'Bureau', 'of']\n",
      "ABBR\n",
      "['What', 'do', 'the', 'letters', 'D.C.', 'stand', 'for', 'in', 'Washington', ',', 'D.C.', '?']\n",
      "ABBR\n",
      "['does', 'the', 'AIDS', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'acronym', 'for', 'the', 'National', 'of', 'Investigation']\n",
      "ABBR\n",
      "['CNN', 'is', 'the', 'abbreviation', 'for', 'what', '?']\n",
      "ABBR\n",
      "['What', 'SVHS', 'is', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'abbreviation', 'IOC', 'perform', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'es', 'the', '`', '`', 'c', \"''\", 'stand', 'for', 'in', 'the', 'equation', 'E', '=', 'mc2', '?']\n",
      "ABBR\n",
      "['What', 'e.g.', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'a', '`', '`', 'USB', \"''\", 'porthole', 'on', 'a', 'computer', '?']\n",
      "ABBR\n",
      "['What', 'do', 'the', 'letters', 'CE', 'stand', 'for', 'on', 'so', 'many', 'products', ',', 'particularly', 'electrical', ',', 'purchased', 'now', '?']\n",
      "ABBR\n",
      "['What', 'does', 'JESSICA', 'mean', 'exercise', '?']\n",
      "ABBR\n",
      "['What', 'trinitrotoluene', 'the', 'abbreviation', 'for', \"'s\", '?']\n",
      "ABBR\n",
      "['does', 'pH', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'serve', '`', 'PSI', \"'\", 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'U.S.S.R.', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'pH', 'for', 'stand', '?']\n",
      "ABBR\n",
      "['What', 'is', '?', 'DTMF']\n",
      "ABBR\n",
      "['What', 'does', 'NAFTA', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['CPR', 'is', 'abbreviation', 'the', 'for', 'what', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', '`', '`', 'c', \"''\", 'stand', 'for', 'in', 'the', 'equation', 'E', '=', 'mc2', '?']\n",
      "ABBR\n",
      "['What', 'is', 'Mikhail', 'Gorbachev', \"'s\", 'middle', 'initial', '?']\n",
      "ABBR\n",
      "['?', 'does', '`', 'PSI', \"'\", 'stand', 'for', 'What']\n",
      "ABBR\n",
      "['bastardly', 'What', 'does', 'NECROSIS', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'stand', 'pH', 'does', 'for', '?']\n",
      "ABBR\n",
      "['What', 'strain', 'is', 'the', 'acronym', 'for', 'the', 'rating', 'system', 'for', 'air', 'conditioner', 'efficiency', '?']\n",
      "ABBR\n",
      "['What', 'is', 'soma', 'the', 'full', 'form', 'of', '.com', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'the', 'of', 'abbreviation', 'International', 'Olympic', 'Committee', '?']\n",
      "ABBR\n",
      "['What', 'does', 'Ms.', ',', 'Miss', ',', 'and', 'Mrs.', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'abbreviation', 'AIDS', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'abbreviation', 'oas', 'brook', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'DEET', '?', 'cost']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviated', 'term', 'used', 'for', 'the', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'is', 'AFS', '?']\n",
      "ABBR\n",
      "['What', 'does', 'S.O.S.', 'stand', '?', 'for']\n",
      "ABBR\n",
      "['What', 'bud', 'does', 'BUD', 'stand', 'bud', 'for', '?']\n",
      "ABBR\n",
      "['IOC', 'an', 'abbreviation', 'of', '?']\n",
      "ABBR\n",
      "['What', 'for', 'the', 'abbreviation', 'OAS', 'stand', 'does', '?']\n",
      "ABBR\n",
      "['What', 'is', 'DTMF', '?']\n",
      "ABBR\n",
      "['What', 'does', 'BMW', 'standpoint', 'for', '?']\n",
      "ABBR\n",
      "['mean', 'does', 'JESSICA', 'What', '?']\n",
      "ABBR\n",
      "['What', 'do', 'the', 'letters', 'D.C.', 'bear', 'for', 'in', 'Washington', ',', 'D.C.', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'expression', 'for', 'the', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'does', 'decoration', 'the', '`', '`', 'blue', 'ribbon', \"''\", 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'p.m.', 'an', 'abbreviation', 'for', ',', 'as', 'in', '5', 'live', 'p.m.', '?']\n",
      "ABBR\n",
      "['What', 'does', 'SIDS', '?', 'for', 'stand']\n",
      "ABBR\n",
      "['What', 'is', 'AFS', '?']\n",
      "ABBR\n",
      "['What', 'do', 'the', 'letters', 'D.C.', 'stand', 'for', 'in', 'Washington', ',', 'D.C.', '?']\n",
      "ABBR\n",
      "['What', 'does', 'A&W', 'of', 'root', 'beer', 'fame', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NAFTA', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'HTML', 'constitute', 'html', '?']\n",
      "ABBR\n",
      "['for', 'does', 'the', 'abbreviation', 'OAS', 'stand', 'What', '?']\n",
      "ABBR\n",
      "['What', 'does', 'BMW', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'ESPN', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'channel', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'Mikhail', 'Gorbachev', \"'s\", 'middle', 'initial', '?']\n",
      "ABBR\n",
      "['What', 'is', 'AFS', '?']\n",
      "ABBR\n",
      "['the', 'is', 'What', 'abbreviation', 'for', 'Original', 'Equipment', 'Manufacturer', '?']\n",
      "ABBR\n",
      "['does', 'snafu', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviated', 'expression', 'for', 'the', 'National', 'Bureau', 'of', 'Investigation', '?', 'brief']\n",
      "ABBR\n",
      "['What', 'in', 'p.m.', 'an', 'abbreviation', 'for', ',', 'as', 'is', '5', 'p.m.', '?']\n",
      "ABBR\n",
      "['What', 'NN', 'O', '`', 'mean', 'when', 'used', 'a', 'prefix', 'in', 'Irish', 'surnames', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'abbreviation', 'IOC', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', '?', 'snafu', 'stand', 'for', 'does']\n",
      "ABBR\n",
      "['What', 'is', '?', 'DTMF']\n",
      "ABBR\n",
      "['CNN', 'is', 'an', 'acronym', 'for', 'what', '?']\n",
      "ABBR\n",
      "['is', 'What', 'RCD', '?']\n",
      "ABBR\n",
      "['What', 'do', 'the', 'letters', 'CE', 'purchased', 'for', 'on', 'so', 'many', 'products', ',', 'particularly', 'electrical', ',', 'stand', 'now', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'abbreviation', 'OAS', 'rack', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'A&W', 'of', 'root', 'beer', 'fame', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['CNN', 'is', 'an', 'acronym', 'for', 'what', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NASA', 'stand', 'energy', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NASA', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'do', 'the', 'letters', 'stand', 'D.C.', 'for', 'in', 'Washington', ',', 'D.C.', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'acronym', 'CPR', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'of', 'the', 'International', 'olympian', 'Committee', '?']\n",
      "ABBR\n",
      "['does', 'the', 'abbreviation', 'IOC', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'U.S.S.R.', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'term', 'the', 'abbreviated', 'is', 'used', 'for', 'the', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'does', 'SIDS', 'stand', 'for', 'sids', '?']\n",
      "ABBR\n",
      "['What', 'does', 'JESSICA', 'entail', '?']\n",
      "ABBR\n",
      "['What', 'does', 'R.E.M.', 'stand', 'for', ',', 'as', 'in', 'the', 'rock', 'group', 'R.E.M.', '?']\n",
      "ABBR\n",
      "['What', 'does', 'e.g.', 'stand', 'for']\n",
      "ABBR\n",
      "['What', 'is', 'DEET', '?']\n",
      "ABBR\n",
      "['What', 'cause', 'the', 'channel', 'ESPN', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'do', 'the', 'letters', 'D.C.', 'stand', 'for', 'in', 'Washington', ',', 'D.C.', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'for', '?', 'Equipment', 'Manufacturer', 'Original']\n",
      "ABBR\n",
      "['What', 'does', 'channel', 'ESPN', 'stand', 'for']\n",
      "ABBR\n",
      "['What', 'is', 'bph', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviated', 'term', 'used', 'for', 'the', 'National', 'Bureau', 'of', '?']\n",
      "ABBR\n",
      "['What', 'does', 'bud', 'stomach', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'DTMF', '?']\n",
      "ABBR\n",
      "['abbreviation', 'does', 'the', 'What', 'IOC', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'IQ', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'MSG', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'channel', 'ESPN', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', \"'s\", 'the', 'abbreviation', 'for', 'trinitrotoluene', '?']\n",
      "ABBR\n",
      "['What', 'does', 'psi', '`', 'PSI', \"'\", 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'IBM', 'for', '?']\n",
      "ABBR\n",
      "['stand', 'does', 'CNN', 'What', 'for', '?']\n",
      "ABBR\n",
      "['mean', 'does', 'BTU', 'What', '?']\n",
      "ABBR\n",
      "['What', 'does', 'A&W', '?', 'root', 'beer', 'fame', 'stand', 'for', 'of']\n",
      "ABBR\n",
      "['What', 'execute', 'R.E.M.', 'stand', 'for', ',', 'as', 'in', 'the', 'rock', 'group', 'R.E.M.', '?']\n",
      "ABBR\n",
      "['When', 'reading', 'classified', 'advertizement', ',', 'what', 'does', 'EENTY', ':', 'other', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'VCR', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'LMDS', '?']\n",
      "ABBR\n",
      "['What', 'as', 'R.E.M.', 'stand', 'for', ',', 'does', 'in', 'the', 'rock', 'group', 'R.E.M.', '?']\n",
      "ABBR\n",
      "['What', 'does', 'SHIELD', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['?', 'is', 'Mikhail', 'Gorbachev', \"'s\", 'middle', 'initial', 'What']\n",
      "ABBR\n",
      "['is', 'the', 'of', 'the', 'company', '`', 'General', 'Motors', \"'\", '?']\n",
      "ABBR\n",
      "['What', '?', 'NASA', 'stand', 'for', 'does']\n",
      "ABBR\n",
      "['When', 'reading', 'classified', 'ads', ',', 'what', 'does', 'EENTY', ':', 'other', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', '`', '`', 'blue', 'ribbon', \"''\", 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'BTU', 'mean']\n",
      "ABBR\n",
      "['is', 'the', 'acronym', 'for', 'the', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'does', 'MSG', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'JESSICA', '?', 'mean']\n",
      "ABBR\n",
      "['What', 'is', 'aries', 'in', 'the', 'reckoner', '?']\n",
      "ABBR\n",
      "['What', 'does', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'IOC', 'an', 'abbreviation', 'of', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'acronym', 'CPR', 'mean', 'intend', '?']\n",
      "ABBR\n",
      "['What', 'exercise', 'pH', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'RCD', '?']\n",
      "ABBR\n",
      "['cause', 'What', 'does', 'snafu', 'stand', 'for', 'stall', '?']\n",
      "ABBR\n",
      "['What', 'serve', 'IOC', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'set', 'the', 'acronym', 'CPR', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'act', 'U.S.S.R.', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'Olympic', 'of', 'the', 'International', 'abbreviation', 'Committee', '?']\n",
      "ABBR\n",
      "['What', 'D.C.', 'the', 'letters', 'do', 'stand', 'for', 'in', 'Washington', ',', 'D.C.', '?']\n",
      "ABBR\n",
      "['What', 'do', 'the', 'letters', 'CE', 'stand', 'for', 'on', 'so', 'many', ',', 'particularly', 'electrical', ',', 'purchased', 'now', '?']\n",
      "ABBR\n",
      "['What', 'does', 'fend', 'IQ', 'iq', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NASDAQ', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'msg', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'bph', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NASA', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['exercise', 'What', 'does', 'MSG', 'stand', 'for', 'msg', '?']\n",
      "ABBR\n",
      "['When', 'reading', 'classified', 'ads', ',', 'what', 'does', '?', ':', 'other', 'stand', 'for', 'EENTY']\n",
      "ABBR\n",
      "['What', 'does', 'U.S.S.R.', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'LMDS']\n",
      "ABBR\n",
      "['What', 'is', 'DEET', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NASA', 'hairdo', 'coiffure', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'html', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'of', 'the', 'International', 'Olympic', 'Committee', '?']\n",
      "ABBR\n",
      "['What', 'does', 'A&W', 'of', 'root', 'beer', 'fame', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'CNN', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['bandstand', 'What', 'does', 'the', 'abbreviation', 'IOC', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'R.E.M.', 'stand', 'for', ',', 'as', 'in', 'the', 'rock', 'group', 'R.E.M.', '?']\n",
      "ABBR\n",
      "['?', 'is', 'SVHS', 'What']\n",
      "ABBR\n",
      "['What', 'does', 'CPR', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['is', 'LMDS', '?']\n",
      "ABBR\n",
      "['What', 'is', 'SVHS', '?']\n",
      "ABBR\n",
      "['What', 'does', 'VCR', 'stand', '?', 'for']\n",
      "ABBR\n",
      "['What', 'does', 'R.E.M.', 'stand', 'for', ',', 'as', 'in', 'the', 'rock', 'group', 'R.E.M.', '?']\n",
      "ABBR\n",
      "['CNN', 'is', 'the', 'abbreviation', 'for', 'what', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', '?', 'LASER', 'mean', 'word']\n",
      "ABBR\n",
      "['What', \"'s\", 'the', 'abbreviation', 'trinitrotoluene', '?']\n",
      "ABBR\n",
      "['What', 'is', 'a', '`', '`', 'USB', \"''\", 'port', 'on', 'a', 'computer', '?']\n",
      "ABBR\n",
      "['is', 'the', 'acronym', 'for', 'the', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NN', 'O', '`', '`', 'mean', 'when', 'used', 'as', 'a', 'prefix', 'in', 'Irish', 'surnames', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'for', 'Original', 'Equipment', 'Manufacturer', '?']\n",
      "ABBR\n",
      "['What', 'does', 'VCR', 'stand', 'for']\n",
      "ABBR\n",
      "['What', 'does', 'BUD', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'SIDS', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'of', 'the', 'International', 'Olympic', 'Committee', '?']\n",
      "ABBR\n",
      "['What', 'is', 'Gorbachev', \"'s\", 'middle', 'initial', '?']\n",
      "ABBR\n",
      "['What', 'abide', 'does', 'MSG', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'NAFTA', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'for', 'BUD', 'stand', 'does', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'the', 'of', 'International', 'Olympic', 'Committee', '?']\n",
      "ABBR\n",
      "['What', 'do', 'the', 'letters', 'D.C.', '?', 'for', 'in', 'Washington', ',', 'D.C.', 'stand']\n",
      "ABBR\n",
      "['What', 'G.M.T.', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'acronym', 'for', 'the', 'rating', 'system', 'for', 'air', 'conditioner', 'efficiency', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviated', 'term', 'National', 'for', 'the', 'used', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'does', 'CPR', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'for', 'stand', 'MSG', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NN', 'O', '`', '`', 'mean', 'surnames', 'used', 'as', 'a', 'prefix', 'in', 'Irish', 'when', '?']\n",
      "ABBR\n",
      "['What', 'is', 'a', '`', '`', 'USB', \"''\", 'port', 'on', 'a', 'computer', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'of', 'the', 'National', 'chest', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['does', 'BMW', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', '?', 'VCR', 'stand', 'for', 'does']\n",
      "ABBR\n",
      "['What', 'does', 'nasdaq', 'NASDAQ', 'stand', 'for', 'cause', '?']\n",
      "ABBR\n",
      "['What', 'does', 'acronym', 'CPR', 'mean']\n",
      "ABBR\n",
      "['What', '?', 'BMW', 'stand', 'for', 'does']\n",
      "ABBR\n",
      "['What', 'does', 'BUD', 'stand', 'for', 'serve', '?']\n",
      "ABBR\n",
      "['What', 'is', 'SVHS', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'of', 'the', '?', 'Olympic', 'Committee', 'International']\n",
      "ABBR\n",
      "['What', 'does', 'doe', 'JESSICA', 'mean', 'energy', '?']\n",
      "ABBR\n",
      "['What', 'is', 'a', '`', '`', 'USB', \"''\", 'port', 'on', 'computer', '?']\n",
      "ABBR\n",
      "['the', 'expression', 'for', 'the', 'National', 'Bureau', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'is', 'SVHS', '?']\n",
      "ABBR\n",
      "['CNN', 'is', 'be', 'the', 'abbreviation', 'for', 'what', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviated', 'term', 'used', 'for', 'the', 'office', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'does', 'CNN', 'stand', 'for']\n",
      "ABBR\n",
      "['What', 'endure', 'does', 'BUD', 'stand', 'for', '?', 'suffer']\n",
      "ABBR\n",
      "['What', 'does', 'BMW', 'stand', 'for']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviated', 'expression', 'for', 'the', 'National', 'Bureau', 'Investigation', 'of', '?']\n",
      "ABBR\n",
      "['CNN', 'is', 'the', 'abbreviation', 'for', 'what', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'channel', 'ESPN', 'stand']\n",
      "ABBR\n",
      "['What', 'does', 'CPR', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviated', 'expression', 'for', 'the', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'for', 'micro', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'acronym', 'for', 'the', 'rating', 'system', 'for', 'air', 'conditioner', 'efficiency', '?']\n",
      "ABBR\n",
      "['What', 'does', 'IOC', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'be', 'a', 'figurer', '`', '`', 'USB', \"''\", 'port', 'on', 'a', 'computer', '?']\n",
      "ABBR\n",
      "['does', 'MSG', 'stand', '?']\n",
      "ABBR\n",
      "['What', 'does', 'BTU', 'base', '?']\n",
      "ABBR\n",
      "['What', 'is', 'DTMF', 'live', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NECROSIS', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'do', 'the', 'letters', 'D.C.', 'stand', 'for', 'in', 'Washington', 'D.C.', ',', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'for', 'micro', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NAFTA', '?', 'for', 'stand']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'of', 'the', 'company', 'name', '`', 'General', 'general', 'Motors', \"'\", '?']\n",
      "ABBR\n",
      "['What', 'behave', 'NASDAQ', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', '`', '`', 'blue', 'ribbon', \"''\", 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'VCR', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'U.S.S.R.', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NASDAQ', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'execute', 'the', 'channel', 'ESPN', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', '?', 'the', '`', '`', 'c', \"''\", 'stand', 'for', 'in', 'the', 'equation', 'E', '=', 'mc2', 'does']\n",
      "ABBR\n",
      "['What', 'palm', 'does', 'the', '`', '`', 'blue', 'ribbon', \"''\", 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'BMW', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'A&W', 'of', 'root', 'beer', 'fame', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['investigation', 'What', 'is', 'the', 'abbreviated', 'term', 'used', 'for', 'the', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['nafta', 'What', 'does', 'nafta', 'NAFTA', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'NASDAQ', 'does', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abridge', 'term', 'used', 'for', 'the', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['is', 'What', 'DTMF', '?']\n",
      "ABBR\n",
      "['What', 'does', 'BMW', 'stand', 'suffice', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', '`', 'PSI', \"'\", 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'DEET', '?']\n",
      "ABBR\n",
      "['What', 'coiffe', 'INRI', 'stand', 'for', 'when', 'used', 'on', 'Jesus', \"'\", 'cross', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'for', 'archetype', 'Equipment', 'Manufacturer', '?']\n",
      "ABBR\n",
      "['What', 'does', 'IOC', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'SIDS', 'viewpoint', 'for', '?']\n",
      "ABBR\n",
      "['What', '?', 'IOC', 'stand', 'for', 'does']\n",
      "ABBR\n",
      "['What', 'does', 'VCR', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'word', 'LASER', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'OAS', 'the', 'abbreviation', 'does', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'INRI', 'stand', 'for', 'when', 'used', 'on', 'Jesus', \"'\", 'cross', '?']\n",
      "ABBR\n",
      "['What', \"'s\", 'the', 'trinitrotoluene', 'for', 'abbreviation', '?']\n",
      "ABBR\n",
      "['What', 'is', 'Original', 'abbreviation', 'for', 'the', 'Equipment', 'Manufacturer', '?']\n",
      "ABBR\n",
      "['What', 'does', 'BTU', '?', 'mean']\n",
      "ABBR\n",
      "['What', 'does', 'the', '?', 'IOC', 'stand', 'for', 'abbreviation']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'abbreviation', 'AIDS', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'LASER', 'word', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'does', 'INRI', 'stand', 'for', 'when', 'used', 'on', \"'\", 'cross', '?']\n",
      "ABBR\n",
      "['What', 'stand', 'IOC', 'does', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'probe', 'the', 'acronym', 'for', 'the', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', '?', 'NECROSIS', 'mean', 'does']\n",
      "ABBR\n",
      "['What', 'does', 'mean', 'NECROSIS', '?']\n",
      "ABBR\n",
      "['What', 'is', 'AFS', '?']\n",
      "ABBR\n",
      "['CNN', 'is', 'the', 'abbreviation', 'for', 'what', '?']\n",
      "ABBR\n",
      "['What', 'does', 'nafta', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'channel', 'ESPN', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'BMW', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['CNN', 'is', 'an', 'acronym', 'for', 'what', '?']\n",
      "ABBR\n",
      "['for', 'does', 'NASA', 'stand', 'What', '?']\n",
      "ABBR\n",
      "['What', 'exist', 'is', 'DEET', '?']\n",
      "ABBR\n",
      "['What', 'set', 'A&W', 'of', 'root', 'beer', 'fame', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'mean', 'JESSICA', 'does', '?']\n",
      "ABBR\n",
      "['What', 'does', 'S.O.S.', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', '?']\n",
      "ABBR\n",
      "['is', 'CNN', 'an', 'acronym', 'for', 'what', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'channel', 'ESPN', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['CPR', 'is', 'the', 'abbreviation', 'for', 'what', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NN', 'O', '`', '`', 'mean', 'when', 'axerophthol', 'used', 'as', 'a', 'prefix', 'in', 'expend', 'Irish', 'surnames', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'abbreviation', 'IOC', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'IOC', 'an', 'abbreviation', 'of', '?']\n",
      "ABBR\n",
      "['angstrom', 'What', 'does', 'R.E.M.', 'aggroup', 'stand', 'for', ',', 'as', 'in', 'the', 'rock', 'group', 'R.E.M.', '?']\n",
      "ABBR\n",
      "['What', 'does', 'R.E.M.', 'stand', 'for', ',', 'as', 'in', 'the', 'rock', 'R.E.M.', '?']\n",
      "ABBR\n",
      "['What', 'doe', '`', 'PSI', \"'\", 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'AFS', '?']\n",
      "ABBR\n",
      "['What', 'does', 'BMW', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'sids', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['NECROSIS', 'does', 'What', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'fare', 'CNN', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'Mikhail', 'Gorbachev', \"'s\", 'middle', 'equal', 'initial', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'for', 'personify', 'Original', 'Equipment', 'Manufacturer', '?']\n",
      "ABBR\n",
      "['What', 'is', 'be', 'BPH', '?', 'be']\n",
      "ABBR\n",
      "['What', 'is', 'Bureau', 'abbreviated', 'expression', 'for', 'the', 'National', 'the', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'exercise', 'EKG', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'think', 'does', 'the', 'acronym', 'CPR', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'does', 'BUD', 'energy', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'for', 'stand', 'IQ', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'full', '.com', 'of', 'form', '?']\n",
      "ABBR\n",
      "['What', 'html', 'is', 'HTML', '?']\n",
      "ABBR\n",
      "['What', 'does', 'BMW', 'stall', 'for', '?']\n",
      "ABBR\n",
      "['What', 'Investigation', 'the', 'abbreviated', 'expression', 'for', 'the', 'National', 'Bureau', 'of', 'is', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'of', 'the', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'word', 'LASER', 'average', '?']\n",
      "ABBR\n",
      "['What', 'does', 'stand', 'SHIELD', 'for', '?']\n",
      "ABBR\n",
      "['CNN', 'is', 'for', 'acronym', 'an', 'what', '?']\n",
      "ABBR\n",
      "['What', 'does', 'BUD', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'SHIELD', '?', 'for', 'stand']\n",
      "ABBR\n",
      "['What', 'does', 'U.S.S.R.', '?', 'for', 'stand']\n",
      "ABBR\n",
      "['interior', 'What', 'is', 'the', 'abbreviation', 'of', 'the', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'is', 'RAM', 'in', 'the', 'computer', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NECROSIS', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'stand', 'S.O.S.', 'does', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'snafu', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'BUD', 'for', 'stand', '?']\n",
      "ABBR\n",
      "['What', 'is', 'port', '`', '`', 'USB', \"''\", 'a', 'on', 'a', 'computer', '?']\n",
      "ABBR\n",
      "['What', 'the', 'abbreviated', 'expression', 'for', 'the', 'National', 'Bureau', 'Investigation', '?']\n",
      "ABBR\n",
      "['for', 'is', 'the', 'abbreviation', 'What', 'micro', '?']\n",
      "ABBR\n",
      "['What', 'is', 'IOC', 'an', 'abbreviation', 'of']\n",
      "ABBR\n",
      "['What', 'does', 'INRI', '?', 'for', 'when', 'used', 'on', 'Jesus', \"'\", 'cross', 'stand']\n",
      "ABBR\n",
      "['What', 'does', 'U.S.S.R.', 'stand', 'bandstand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'e.g.', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'standstill', 'G.M.T.', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'do', 'the', 'letter', 'CE', 'stand', 'for', 'on', 'so', 'many', 'products', ',', 'particularly', 'electrical', ',', 'purchase', 'now', '?']\n",
      "ABBR\n",
      "['What', 'is', '?', 'BPH']\n",
      "ABBR\n",
      "['What', 'manage', 'snafu', 'bandstand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'the', 'acronym', 'CPR', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'acronym', 'CPR', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'does', 'CNN', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'suffice', 'IBM', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'IQ', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'Mikhail', 'Gorbachev', \"'s\", 'middle', 'initial', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'of', 'abbreviation', 'the', 'company', 'name', '`', 'General', 'Motors', \"'\", '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'abbreviation', 'IOC', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'R.E.M.', 'digest', 'for', ',', 'as', 'in', 'the', 'shake', 'group', 'R.E.M.', '?']\n",
      "ABBR\n",
      "['What', 'does', 'U.S.S.R.', 'resist', 'for', '?']\n",
      "ABBR\n",
      "['What', \"'s\", 'the', 'abbreviation', 'for', 'trinitrotoluene', '?']\n",
      "ABBR\n",
      "['What', 'doe', 'BMW', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['When', 'reading', 'classified', 'ads', ',', 'interpretation', 'what', 'does', 'EENTY', ':', 'other', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'e.g.', 'bear', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'of', 'the', 'International', 'olympian', 'Committee', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'exercise', 'number', '`', '`', '5', \"''\", 'stand', 'for', 'on', 'FUBU', 'clothing', '?']\n",
      "ABBR\n",
      "['What', 'does', 'G.M.T.', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['?', 'does', 'MSG', 'stand', 'for', 'What']\n",
      "ABBR\n",
      "['What', 'does', 'execute', 'MSG', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'U.S.S.R.', '?', 'for', 'stand']\n",
      "ABBR\n",
      "['What', 'does', 'S.O.S.', 'stand', '?']\n",
      "ABBR\n",
      "['What', 'does', 'BTU', 'mean', 'btu', '?']\n",
      "ABBR\n",
      "['What', 'does', 'INRI', 'stand', 'for', 'when', 'used', 'on', 'stomach', 'Jesus', \"'\", 'cross', '?']\n",
      "ABBR\n",
      "['What', 'does', 'pH', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'U.S.S.R.', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'of', 'the', 'equal', 'International', 'Olympic', 'Committee', '?']\n",
      "ABBR\n",
      "['What', 'does', 'CPR', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['CNN', 'is', 'an', 'acronym', 'for', 'what', '?']\n",
      "ABBR\n",
      "['meanspirited', 'What', 'does', 'NN', 'O', '`', 'use', '`', 'mean', 'when', 'used', 'as', 'a', 'prefix', 'in', 'Irish', 'surnames', '?']\n",
      "ABBR\n",
      "['What', 'is', 'RCD', '?']\n",
      "ABBR\n",
      "['What', 'nasdaq', 'does', 'NASDAQ', 'stand', 'suffer', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'CPR', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NN', 'O', '`', '`', 'average', 'when', 'used', 'as', 'a', 'prefix', 'in', 'Irish', 'surnames', '?']\n",
      "ABBR\n",
      "['What', 'does', 'R.E.M.', 'stand', 'for', ',', 'as', 'in', 'the', 'rock', 'group', 'R.E.M.', '?']\n",
      "ABBR\n",
      "['What', 'does', 'INRI', 'stand', 'for', 'utilise', 'when', 'used', 'on', 'Jesus', \"'\", 'cross', '?']\n",
      "ABBR\n",
      "['What', 'SIDS', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'BTU', 'mingy', '?']\n",
      "ABBR\n",
      "['What', 'come', 'NAFTA', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['does', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'R.E.M.', 'stand', 'for', ',', 'as', 'in', 'the', 'rock', 'group', 'R.E.M.', '?']\n",
      "ABBR\n",
      "['What', 'is', \"''\", '`', '`', 'USB', 'a', 'port', 'on', 'a', 'computer', '?']\n",
      "ABBR\n",
      "['What', 'does', 'nafta', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'p.m.', 'an', 'abbreviation', 'for', ',', 'as', 'in', '5', 'p.m.', '?']\n",
      "ABBR\n",
      "['What', 'does', 'snafu', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'of', 'the', 'National', 'Bureau', 'of', 'probe', '?']\n",
      "ABBR\n",
      "['`', 'is', 'a', 'What', '`', 'USB', \"''\", 'port', 'on', 'a', 'computer', '?']\n",
      "ABBR\n",
      "['does', 'pH', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'acronym', 'CPR', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'SVHS', 'is', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'number', '`', '`', '5', 'stand', 'for', 'on', 'FUBU', 'clothing', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NASA', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NASDAQ', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['When', 'reading', 'classified', 'ads', ',', 'what', 'does', 'EENTY', ':', 'other', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['CPR', 'is', 'the', 'abbreviation', 'for', 'cpr', 'what', '?']\n",
      "ABBR\n",
      "['What', 'for', 'NASDAQ', 'stand', 'does', '?']\n",
      "ABBR\n",
      "['?', 'is', 'HTML', 'What']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'abbreviation', 'IOC', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'contract', 'term', 'used', 'for', 'the', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'G.M.T.', 'does', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['CNN', 'is', 'the', 'abbreviation', 'for', 'what', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'for', 'the', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'does', 'IOC', 'stand', 'for']\n",
      "ABBR\n",
      "['What', 'does', 'vcr', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', '?', 'LMDS']\n",
      "ABBR\n",
      "['CNN', 'is', 'the', 'abbreviation', 'what', 'for', '?']\n",
      "ABBR\n",
      "['is', 'What', 'HTML', '?']\n",
      "ABBR\n",
      "['What', 'does', 'stand', '`', '`', 'c', \"''\", 'the', 'for', 'in', 'the', 'equation', 'E', '=', 'mc2', '?']\n",
      "ABBR\n",
      "['What', 'is', 'LMDS', '?']\n",
      "ABBR\n",
      "['CNN', 'is', 'the', 'what', 'for', 'abbreviation', '?']\n",
      "ABBR\n",
      "['What', 'does', 'e.g.', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'for', 'the', 'abbreviation', 'is', 'micro', '?']\n",
      "ABBR\n",
      "['What', 'is', 'DEET', '?']\n",
      "ABBR\n",
      "['What', 'of', 'the', 'acronym', 'for', 'the', 'National', 'Bureau', 'is', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'is', '?', 'RCD']\n",
      "ABBR\n",
      "['What', 'does', 'IOC', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['When', 'reading', 'EENTY', 'ads', ',', 'what', 'does', 'classified', ':', 'other', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'IQ', 'iq', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', '`', '`', 'blue', 'ribbon', \"''\", 'stand', 'for', '?']\n",
      "ABBR\n",
      "['an', 'is', 'p.m.', 'What', 'abbreviation', 'for', ',', 'as', 'in', '5', 'p.m.', '?']\n",
      "ABBR\n",
      "['What', 'does', 'IOC', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['be', 'What', 'is', 'RCD', '?']\n",
      "ABBR\n",
      "['follow', 'CNN', 'is', 'the', 'abbreviation', 'for', 'what', '?']\n",
      "ABBR\n",
      "['What', 'does', 'A&W', 'of', 'root', 'fame', 'beer', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'S.O.S.', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'BMW', 'support', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'equal', 'the', 'abbreviated', 'term', 'used', 'for', 'the', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'number', '`', '`', '5', \"''\", 'stand', 'for', 'on', 'FUBU', 'clothing', '?']\n",
      "ABBR\n",
      "['What', 'does', 'INRI', 'stand', 'for', 'used', 'on', 'Jesus', \"'\", 'cross', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NAFTA', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['CNN', 'is', 'an', 'be', 'acronym', 'for', 'what', 'follow', '?']\n",
      "ABBR\n",
      "['What', 'an', 'p.m.', 'is', 'abbreviation', 'for', ',', 'as', 'in', '5', 'p.m.', '?']\n",
      "ABBR\n",
      "['What', '?', 'the', 'abbreviation', 'OAS', 'stand', 'for', 'does']\n",
      "ABBR\n",
      "['What', 'is', 'IOC', 'an', 'abbreviation', 'of', '?']\n",
      "ABBR\n",
      "['What', 'do', 'the', 'letters', 'CE', 'brook', 'for', 'on', 'so', 'many', 'products', ',', 'particularly', 'electrical', ',', 'purchased', 'now', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'word', 'LASER', 'mean', 'book', '?']\n",
      "ABBR\n",
      "['does', 'What', 'U.S.S.R.', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'INRI', 'stand', 'for', 'when', 'used', \"'\", 'Jesus', 'on', 'cross', '?']\n",
      "ABBR\n",
      "['What', 'does', 'MSG', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'DEET', '?']\n",
      "ABBR\n",
      "['What', 'is', 'SVHS', '?']\n",
      "ABBR\n",
      "['What', 'does', 'BMW', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'INRI', 'stand', 'for', 'when', 'used', 'on', 'Jesus', \"'\", 'cross', '?']\n",
      "ABBR\n",
      "['RAM', 'is', 'What', 'in', 'the', 'computer', '?']\n",
      "ABBR\n",
      "['What', 'does', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'do', 'the', 'letters', 'CE', 'stand', 'for', 'on', 'so', 'many', 'products', ',', 'particularly', 'electrical', ',', 'purchased', 'now', '?', 'ware']\n",
      "ABBR\n",
      "['What', 'is', 'HTML', '?']\n",
      "ABBR\n",
      "['What', 'does', 'IBM', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'IQ', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'U.S.S.R.', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'answer', 'the', 'abbreviation', 'OAS', 'abide', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'e.g.', 'set', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'for', 'stand', 'snafu', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'acronym', 'CPR', '?']\n",
      "ABBR\n",
      "['What', 'is', '?', 'LMDS']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'of', 'abbreviation', 'the', 'company', 'name', '`', 'General', 'Motors', \"'\", '?']\n",
      "ABBR\n",
      "['What', 'does', 'NN', 'O', '`', '`', 'mean', 'when', 'used', '?', 'a', 'prefix', 'in', 'Irish', 'surnames', 'as']\n",
      "ABBR\n",
      "['What', 'does', 'CPR', '?', 'for', 'stand']\n",
      "ABBR\n",
      "['What', 'doe', '`', 'PSI', \"'\", 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'word', 'LASER', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'of', 'the', 'company', 'name', '`', 'general', 'Motors', \"'\", '?']\n",
      "ABBR\n",
      "['What', 'does', 'IBM', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviated', 'expression', 'for', 'the', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'of', 'the', 'company', 'name', '`', 'General', 'Motors', \"'\", '?']\n",
      "ABBR\n",
      "['What', 'does', 'BTU', 'mean', '?']\n",
      "ABBR\n",
      "['?', 'does', 'snafu', 'stand', 'for', 'What']\n",
      "ABBR\n",
      "['is', 'What', 'AFS', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'abbreviation', 'IOC', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'of', 'the', 'company', 'name', '`', 'General', 'Motors', \"'\", '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', '?', 'micro', 'for']\n",
      "ABBR\n",
      "['What', '?', 'IOC', 'stand', 'for', 'does']\n",
      "ABBR\n",
      "['CNN', 'is', 'the', 'abbreviation', 'for', 'what', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NECROSIS', '?', 'mean']\n",
      "ABBR\n",
      "['What', 'does', 'S.O.S.', 'stand', 'for']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviated', 'term', 'used', 'for', 'Bureau', 'National', 'the', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'BTU', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'of', 'the', 'International', 'Olympic', 'Committee', '?']\n",
      "ABBR\n",
      "['What', 'SHIELD', 'does', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'IQ', 'stand', 'for', 'standstill', '?']\n",
      "ABBR\n",
      "['What', 'is', 'AFS', '?']\n",
      "ABBR\n",
      "['What', 'is', 'HTML', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'number', '`', '`', '5', \"''\", 'stand', 'for', 'on', 'FUBU', 'clothing', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'for', 'Original', 'Equipment', 'Manufacturer', '?']\n",
      "ABBR\n",
      "['does', 'R.E.M.', 'stand', 'for', ',', 'as', 'in', 'the', 'rock', 'group', 'R.E.M.', '?']\n",
      "ABBR\n",
      "['constitute', 'What', 'is', 'RCD', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'for', 'master', 'Equipment', 'Manufacturer', '?']\n",
      "ABBR\n",
      "['What', 'stand', 'pH', 'does', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'G.M.T.', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'come', 'BMW', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'INRI', 'savior', 'stand', 'for', 'when', 'used', 'on', 'Jesus', \"'\", 'cross', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NAFTA', 'for']\n",
      "ABBR\n",
      "['What', 'is', 'inch', 'RAM', 'in', 'the', 'computer', '?']\n",
      "ABBR\n",
      "['What', 'does', 'G.M.T.', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', \"'s\", 'the', 'abbreviation', 'for', 'trinitrotoluene', '?']\n",
      "ABBR\n",
      "['What', 'does', 'pH', 'for', 'stand', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NASDAQ', 'for', 'stand', '?']\n",
      "ABBR\n",
      "['When', 'reading', 'classified', 'ads', ',', 'what', 'does', 'EENTY', ':', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', '`', '`', 'blue', '?', \"''\", 'stand', 'for', 'ribbon']\n",
      "ABBR\n",
      "['is', 'an', 'acronym', 'for', 'what', '?']\n",
      "ABBR\n",
      "['What', 'is', 'RAM', 'in', 'the', 'computer', '?']\n",
      "ABBR\n",
      "['What', 'does', 'INRI', 'use', 'stand', 'for', 'when', 'used', 'on', 'Jesus', \"'\", 'cross', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'acronym', 'CPR', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'is', '?', 'HTML']\n",
      "ABBR\n",
      "['What', 'is', 'a', '`', '`', 'USB', \"''\", 'larboard', 'on', 'a', 'figurer', '?']\n",
      "ABBR\n",
      "['What', 'is', 'p.m.', 'an', 'abbreviation', 'for', ',', 'as', 'in', '5', 'p.m.', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'for', 'micro', '?']\n",
      "ABBR\n",
      "['does', 'the', 'acronym', 'CPR', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'HTML', '?']\n",
      "ABBR\n",
      "['What', 'does', 'A&W', 'of', 'root', 'beer', 'for', 'stand', 'fame', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', '`', '`', 'c', \"''\", 'stand', 'for', 'in', 'the', 'par', 'E', '=', 'mc2', '?']\n",
      "ABBR\n",
      "['DTMF', 'is', 'What', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'acronym', 'cpr', 'CPR', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NASDAQ', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['CNN', 'is', 'an', 'acronym', 'for', 'what', '?']\n",
      "ABBR\n",
      "['What', 'does', 'Ms.', 'bandstand', ',', 'Miss', ',', 'and', 'Mrs.', 'ms', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'EKG', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'letters', 'the', 'do', 'D.C.', 'stand', 'for', 'in', 'Washington', ',', 'D.C.', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'abbreviation', 'AIDS', 'stand', 'for', 'stall', 'behave', '?']\n",
      "ABBR\n",
      "['What', 'do', 'the', 'letters', 'D.C.', 'stand', 'for', 'in', 'Washington', ',', '?']\n",
      "ABBR\n",
      "['What', 'is', 'DTMF', '?']\n",
      "ABBR\n",
      "['CNN', 'is', 'an', 'acronym', 'for', 'what', '?']\n",
      "ABBR\n",
      "['CNN', 'is', 'the', 'abbreviation', 'for', 'what', '?']\n",
      "ABBR\n",
      "['What', 'BMW', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'DTMF', '?']\n",
      "ABBR\n",
      "['What', 'does', 'answer', 'base', 'NASDAQ', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['fend', 'What', 'does', 'e.g.', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'Mikhail', 'Gorbachev', \"'s\", 'middle', 'initial', '?']\n",
      "ABBR\n",
      "['does', 'What', 'IBM', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NN', 'O', '`', '`', 'mingy', 'when', 'used', 'as', 'a', 'prefix', 'in', 'Irish', 'surnames', '?']\n",
      "ABBR\n",
      "['What', 'does', 'IQ', 'stand', '?']\n",
      "ABBR\n",
      "['is', 'micro', '?']\n",
      "ABBR\n",
      "['NASDAQ', 'does', 'What', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'come', 'IQ', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'do', 'the', 'indium', 'letters', 'D.C.', 'stand', 'for', 'in', 'Washington', ',', 'D.C.', '?']\n",
      "ABBR\n",
      "['What', 'does', 'pH', 'digest', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', '?', 'mean', 'JESSICA']\n",
      "ABBR\n",
      "['What', 'stand', 'IQ', 'does', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'micro', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'of', 'represent', 'the', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'does', '`', 'PSI', \"'\", 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'CNN', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'BMW', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'IOC', 'stand', 'for', '?', 'support']\n",
      "ABBR\n",
      "['What', 'U.S.S.R.', 'does', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'IBM', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['is', 'What', 'LMDS', '?']\n",
      "ABBR\n",
      "['What', 'is', 'DTMF', '?', 'embody']\n",
      "ABBR\n",
      "['What', 'is', 'SVHS', '?']\n",
      "ABBR\n",
      "['What', 'does', 'word', 'LASER', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'is', 'DTMF', '?']\n",
      "ABBR\n",
      "['channel', 'does', 'the', 'What', 'ESPN', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['IQ', 'does', 'What', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'DTMF', '?']\n",
      "ABBR\n",
      "['What', 'does', 'Ms.', ',', 'Miss', ',', 'and', 'Mrs.', 'stand', '?', 'for']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'for', 'Original', 'Equipment', 'Manufacturer', '?']\n",
      "ABBR\n",
      "['What', 'come', 'does', 'EKG', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NASDAQ', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['`', 'is', 'the', 'abbreviation', 'of', 'the', 'company', 'name', 'What', 'General', 'Motors', \"'\", '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'for', 'micro', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'acronym', 'for', 'the', 'rating', 'for', 'air', 'conditioner', 'efficiency']\n",
      "ABBR\n",
      "['What', 'do', 'the', 'letters', 'D.C.', 'stand', 'dc', 'for', 'in', 'Washington', ',', 'D.C.', '?']\n",
      "ABBR\n",
      "['What', 'does', 'IBM', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['CNN', 'is', 'the', 'for', 'what', '?']\n",
      "ABBR\n",
      "['electrocardiogram', 'What', 'does', 'EKG', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'of', 'the', 'National', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'does', 'BUD', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NASA', 'stand', 'for', '?', 'execute']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'acronym', 'for', 'the', 'rating', 'system', 'for', 'air', 'conditioner', 'efficiency', '?']\n",
      "ABBR\n",
      "['What', 'is', 'html', '?']\n",
      "ABBR\n",
      "['What', 'do', 'the', 'letters', 'CE', 'for', 'stand', 'on', 'so', 'many', 'products', ',', 'particularly', 'electrical', ',', 'purchased', 'now', '?']\n",
      "ABBR\n",
      "['for', 'does', 'BUD', 'stand', 'What', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'for', 'Original', 'Equipment', '?', 'Manufacturer']\n",
      "ABBR\n",
      "['What', 'does', 'stand', '?']\n",
      "ABBR\n",
      "['What', 'does', 'INRI', 'stand', 'when', 'used', 'on', 'Jesus', \"'\", 'cross', '?']\n",
      "ABBR\n",
      "['What', 'does', 'e.g.', 'base', 'for', '?']\n",
      "ABBR\n",
      "['the', 'is', 'What', 'abbreviation', 'for', 'micro', '?']\n",
      "ABBR\n",
      "['What', 'set', 'cpr', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'of', 'the', 'company', '`', 'General', \"'\", '?']\n",
      "ABBR\n",
      "['CNN', 'is', 'the', 'abbreviation', 'for', 'what', '?']\n",
      "ABBR\n",
      "['What', 'is', 'Mikhail', 'Gorbachev', \"'s\", 'middle', 'initial', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'for', 'micro', '?']\n",
      "ABBR\n",
      "['?', 'is', 'the', 'abbreviation', 'for', 'what', 'CNN']\n",
      "ABBR\n",
      "['micro', 'is', 'the', 'abbreviation', 'for', 'What', '?']\n",
      "ABBR\n",
      "['What', 'is', '?']\n",
      "ABBR\n",
      "['What', 'does', 'EKG', 'stall', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'of', 'the', 'outside', 'Olympic', 'Committee', '?']\n",
      "ABBR\n",
      "['What', 'is', 'AFS', '?']\n",
      "ABBR\n",
      "['What', 'energy', 'CPR', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'do', 'the', 'letters', 'CE', 'stand', 'for', 'on', 'so', 'many', 'products', ',', 'particularly', 'electrical', ',', 'purchased', 'now', '?', 'forthwith']\n",
      "ABBR\n",
      "831\n"
     ]
    }
   ],
   "source": [
    "abbr_aug_ex = [ex for ex in augmented_train_data if ex.label == \"ABBR\"]\n",
    "abbr_ex = [ex for ex in train_data if ex.label == \"ABBR\"]\n",
    "\n",
    "count = 0\n",
    "for ex in abbr_aug_ex:\n",
    "    if ex not in abbr_ex:\n",
    "        print(ex.text)\n",
    "        print(ex.label)\n",
    "        count += 1\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "858eb2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label distribution in training set:\n",
      "- ABBR: 900 samples (15.05%)\n",
      "- DESC: 930 samples (15.55%)\n",
      "- ENTY: 1300 samples (21.74%)\n",
      "- HUM: 1200 samples (20.07%)\n",
      "- LOC: 800 samples (13.38%)\n",
      "- NUM: 850 samples (14.21%)\n"
     ]
    }
   ],
   "source": [
    "# Count how many samples per label in the train set\n",
    "label_counts_p34 = Counter([ex.label for ex in augmented_train_data.examples])\n",
    "total_examples_p34 = len(augmented_train_data)\n",
    "\n",
    "print(\"\\nLabel distribution in training set:\")\n",
    "for label, count in sorted(label_counts_p34.items()):\n",
    "    percentage = (count / total_examples_p34) * 100\n",
    "    print(f\"- {label}: {count} samples ({percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2926e83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PART 3.5: SAMPLING STRATEGIES VS DATA AUGMENTATION\n",
      "================================================================================\n",
      "Comparing text augmentation, weighted sampling, and their combination\n",
      "across the Simple RNN baseline (Part 2 best config) and the RNN + BERT hybrid.\n",
      "================================================================================\n",
      ">>> RNNWithAttentionClassifier ready (mean aggregation baseline)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PART 3.5: Sampling Strategies vs Data Augmentation\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 3.5: SAMPLING STRATEGIES VS DATA AUGMENTATION\")\n",
    "print(\"=\"*80)\n",
    "print(\"Comparing text augmentation, weighted sampling, and their combination\")\n",
    "print(\"across the Simple RNN baseline (Part 2 best config) and the RNN + BERT hybrid.\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Extended RNN Classifier with multiple aggregation methods\n",
    "class RNN_Classifier_Aggregation(nn.Module):\n",
    "    \"\"\"\n",
    "    RNN for topic classification with multiple aggregation strategies.\n",
    "    Uses pretrained embeddings (learnable/updated during training).\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, \n",
    "                 n_layers=1, dropout=0.0, padding_idx=0, pretrained_embeddings=None,\n",
    "                 aggregation='mean'):\n",
    "        super(RNN_Classifier_Aggregation, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.aggregation = aggregation  # 'last', 'mean', 'max'\n",
    "        \n",
    "        # Embedding layer initialized with pretrained embeddings\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "            # Make embeddings learnable (updated during training)\n",
    "            self.embedding.weight.requires_grad = True\n",
    "        \n",
    "        # RNN layer\n",
    "        self.rnn = nn.RNN(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=n_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.0  # No dropout in baseline\n",
    "        )\n",
    "                \n",
    "        # Fully connected output layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        # text: [batch_size, seq_len]\n",
    "        # text_lengths: [batch_size]\n",
    "        \n",
    "        # Embed the input\n",
    "        embedded = self.embedding(text)  # [batch_size, seq_len, embedding_dim]\n",
    "        \n",
    "        # Get dimensions\n",
    "        batch_size = embedded.size(0)\n",
    "        seq_len = embedded.size(1)\n",
    "        \n",
    "        # Handle text_lengths\n",
    "        text_lengths_flat = text_lengths.flatten().cpu().long()\n",
    "        if len(text_lengths_flat) != batch_size:\n",
    "            raise ValueError(\n",
    "                f\"text_lengths size mismatch: got {len(text_lengths_flat)} elements, \"\n",
    "                f\"expected {batch_size}\"\n",
    "            )\n",
    "        \n",
    "        # Clamp lengths\n",
    "        text_lengths_clamped = torch.clamp(text_lengths_flat, min=1, max=seq_len)\n",
    "        \n",
    "        text_lengths_clamped_device = text_lengths_clamped.to(text.device)\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, text_lengths_clamped, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        \n",
    "        # Pass through RNN\n",
    "        packed_output, hidden = self.rnn(packed_embedded)\n",
    "        # hidden: [n_layers, batch_size, hidden_dim]\n",
    "        \n",
    "        # Aggregate word representations to sentence representation\n",
    "        if self.aggregation == 'last':\n",
    "            sentence_repr = hidden[-1]  # [batch_size, hidden_dim]\n",
    "            \n",
    "        elif self.aggregation == 'mean':\n",
    "            output, output_lengths = nn.utils.rnn.pad_packed_sequence(\n",
    "                packed_output, batch_first=True\n",
    "            )\n",
    "            # output: [batch_size, seq_len, hidden_dim]\n",
    "            \n",
    "            # Create mask for padding\n",
    "            mask = torch.arange(seq_len, device=text.device).unsqueeze(0) < text_lengths_clamped_device.unsqueeze(1)\n",
    "            mask = mask.unsqueeze(2).float()  # [batch_size, seq_len, 1]\n",
    "            \n",
    "            # Apply mask and compute mean\n",
    "            masked_output = output * mask\n",
    "            sum_output = masked_output.sum(dim=1)  # [batch_size, hidden_dim]\n",
    "            sentence_repr = sum_output / text_lengths_clamped_device.unsqueeze(1).float()\n",
    "            \n",
    "        elif self.aggregation == 'max':\n",
    "            # Max pooling over all outputs\n",
    "            output, output_lengths = nn.utils.rnn.pad_packed_sequence(\n",
    "                packed_output, batch_first=True\n",
    "            )\n",
    "            # output: [batch_size, seq_len, hidden_dim]\n",
    "            \n",
    "            mask = torch.arange(seq_len, device=text.device).unsqueeze(0) < text_lengths_clamped_device.unsqueeze(1)\n",
    "            mask = mask.unsqueeze(2).float()  # [batch_size, seq_len, 1]\n",
    "            \n",
    "            masked_output = output * mask + (1 - mask) * float('-inf')\n",
    "            sentence_repr, _ = torch.max(masked_output, dim=1)\n",
    "        \n",
    "        # Pass through fully connected layer\n",
    "        output = self.fc(sentence_repr)  # [batch_size, output_dim]\n",
    "        \n",
    "        return output\n",
    "\n",
    "print(\">>> RNNWithAttentionClassifier ready (mean aggregation baseline)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a7904aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Transformers library available\n",
      ">>> RNNBertClassifier ready (Part 3.3 best model)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Hybrid Model: RNN + BERT with Attention (Part 3.3 best model)\n",
    "# ============================================================================\n",
    "\n",
    "try:\n",
    "    from transformers import BertModel, BertTokenizer\n",
    "    BERT_AVAILABLE = True\n",
    "    print(\">>> Transformers library available\")\n",
    "except ImportError:\n",
    "    BERT_AVAILABLE = False\n",
    "    print(\">>> Warning: transformers library not found. Install `transformers` to run BERT experiments.\")\n",
    "\n",
    "\n",
    "class RNNBertClassifier(nn.Module):\n",
    "    \"\"\"Hybrid model that feeds BERT embeddings into a BiLSTM + attention head.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        output_dim,\n",
    "        hidden_dim=256,\n",
    "        n_layers=2,\n",
    "        dropout=0.5,\n",
    "        bert_model_name='bert-base-uncased',\n",
    "        freeze_bert=False,\n",
    "    ):\n",
    "        super(RNNBertClassifier, self).__init__()\n",
    "\n",
    "        if not BERT_AVAILABLE:\n",
    "            raise RuntimeError(\"Transformers library is required for RNN+BERT experiments.\")\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.freeze_bert = freeze_bert\n",
    "\n",
    "        # Load pretrained BERT model and tokenizer\n",
    "        self.bert_tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "        self.bert_model = BertModel.from_pretrained(bert_model_name)\n",
    "\n",
    "        if freeze_bert:\n",
    "            for param in self.bert_model.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        bert_output_dim = self.bert_model.config.hidden_size  # 768 for bert-base-uncased\n",
    "\n",
    "        # BiLSTM over contextualised embeddings\n",
    "        self.bilstm = nn.LSTM(\n",
    "            bert_output_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=n_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout if n_layers > 1 else 0,\n",
    "        )\n",
    "\n",
    "        # Bahdanau-style attention\n",
    "        self.attention_linear1 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.attention_linear2 = nn.Linear(hidden_dim, 1)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "    def forward(self, text, text_lengths, text_vocab=None):\n",
    "        device = text.device\n",
    "        batch_size, seq_len = text.size()\n",
    "\n",
    "        sentences = []\n",
    "        for i in range(batch_size):\n",
    "            actual_len = text_lengths[i].item() if isinstance(text_lengths[i], torch.Tensor) else text_lengths[i]\n",
    "            tokens = []\n",
    "            for j in range(min(actual_len, seq_len)):\n",
    "                token_idx = text[i, j].item()\n",
    "                if text_vocab and token_idx < len(text_vocab):\n",
    "                    token = text_vocab[token_idx]\n",
    "                    if token not in ['<pad>', '<unk>', '<sos>', '<eos>']:\n",
    "                        tokens.append(token)\n",
    "                else:\n",
    "                    tokens.append(str(token_idx))\n",
    "            sentences.append(\" \".join(tokens))\n",
    "\n",
    "        encoded = self.bert_tokenizer(\n",
    "            sentences,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors='pt'\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.set_grad_enabled(not self.freeze_bert):\n",
    "            bert_outputs = self.bert_model(**encoded)\n",
    "            bert_embeddings = bert_outputs.last_hidden_state  # [batch, seq_len, hidden]\n",
    "\n",
    "        bert_lengths = encoded['attention_mask'].sum(dim=1)\n",
    "        max_len = bert_embeddings.size(1)\n",
    "        bert_lengths = bert_lengths.clamp(max=max_len).cpu()\n",
    "\n",
    "        packed_bert = nn.utils.rnn.pack_padded_sequence(\n",
    "            bert_embeddings, bert_lengths, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        packed_output, _ = self.bilstm(packed_bert)\n",
    "        bilstm_output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
    "\n",
    "        attention_scores = self.attention_linear1(bilstm_output)\n",
    "        attention_scores = self.tanh(attention_scores)\n",
    "        attention_scores = self.attention_linear2(attention_scores).squeeze(2)\n",
    "\n",
    "        seq_len_attn = bilstm_output.size(1)\n",
    "        mask = torch.arange(seq_len_attn, device=device).unsqueeze(0) < bert_lengths.unsqueeze(1).to(device)\n",
    "        attention_scores = attention_scores.masked_fill(~mask, float('-inf'))\n",
    "\n",
    "        attention_weights = torch.softmax(attention_scores, dim=1).unsqueeze(2)\n",
    "        context_vector = torch.sum(attention_weights * bilstm_output, dim=1)\n",
    "\n",
    "        context_vector = self.dropout(context_vector)\n",
    "        logits = self.fc(context_vector)\n",
    "        return logits\n",
    "\n",
    "print(\">>> RNNBertClassifier ready (Part 3.3 best model)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dcda8508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================\n",
    "# Helper: Topic-wise evaluation\n",
    "# =========================================================================\n",
    "\n",
    "def evaluate_per_topic_p35(model, iterator, device):\n",
    "    \"\"\"Evaluate accuracy per topic on the provided iterator.\"\"\"\n",
    "    model.eval()\n",
    "    topic_correct = defaultdict(int)\n",
    "    topic_total = defaultdict(int)\n",
    "    idx_to_label = LABEL.vocab.itos\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "            predictions = model(text, text_lengths)\n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            for pred, label in zip(preds.cpu().numpy(), labels.cpu().numpy()):\n",
    "                topic_name = idx_to_label[label]\n",
    "                topic_total[topic_name] += 1\n",
    "                if pred == label:\n",
    "                    topic_correct[topic_name] += 1\n",
    "\n",
    "    topic_metrics = {}\n",
    "    for topic in sorted(topic_total.keys()):\n",
    "        total = topic_total[topic]\n",
    "        correct = topic_correct[topic]\n",
    "        accuracy = correct / total if total > 0 else 0.0\n",
    "        topic_metrics[topic] = {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"correct\": correct,\n",
    "            \"total\": total,\n",
    "        }\n",
    "    return topic_metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0220a7b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Preparing dataset variants for Part 3.5 experiments...\n",
      "  - Original train: 4362 samples\n",
      "      ABBR: 69 (1.58%)\n",
      "      DESC: 930 (21.32%)\n",
      "      ENTY: 1000 (22.93%)\n",
      "      HUM: 978 (22.42%)\n",
      "      LOC: 668 (15.31%)\n",
      "      NUM: 717 (16.44%)\n",
      "  - Augmented train: 5980 samples\n",
      "      ABBR: 900 (15.05%)\n",
      "      DESC: 930 (15.55%)\n",
      "      ENTY: 1300 (21.74%)\n",
      "      HUM: 1200 (20.07%)\n",
      "      LOC: 800 (13.38%)\n",
      "      NUM: 850 (14.21%)\n",
      "  - Weighted-sampled train: 4362 samples\n",
      "      ABBR: 798 (18.29%)\n",
      "      DESC: 533 (12.22%)\n",
      "      ENTY: 918 (21.05%)\n",
      "      HUM: 720 (16.51%)\n",
      "      LOC: 668 (15.31%)\n",
      "      NUM: 725 (16.62%)\n",
      "  - Augmented + weighted train: 5980 samples\n",
      "      ABBR: 1126 (18.83%)\n",
      "      DESC: 761 (12.73%)\n",
      "      ENTY: 1218 (20.37%)\n",
      "      HUM: 935 (15.64%)\n",
      "      LOC: 985 (16.47%)\n",
      "      NUM: 955 (15.97%)\n",
      "\n",
      ">>> Dataset variants ready. Criterion initialised for upcoming runs.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Dataset Variants & Utilities for Experiments\n",
    "# ============================================================================\n",
    "\n",
    "def describe_dataset(name, dataset):\n",
    "    counts = Counter(ex.label for ex in dataset.examples)\n",
    "    total = len(dataset.examples)\n",
    "    print(f\"  - {name}: {total} samples\")\n",
    "    for label, count in sorted(counts.items()):\n",
    "        print(f\"      {label}: {count} ({count/total*100:.2f}%)\")\n",
    "    return counts\n",
    "\n",
    "\n",
    "# Topic-wise accuracy from latest weighted-sampler evaluation (used to boost weak classes)\n",
    "P35_TOPIC_ACCURACY = {\n",
    "    \"ABBR\": 0.7778,\n",
    "    \"DESC\": 0.9855,\n",
    "    \"ENTY\": 0.7128,\n",
    "    \"HUM\": 0.8769,\n",
    "    \"LOC\": 0.8889,\n",
    "    \"NUM\": 0.8584,\n",
    "}\n",
    "# Convert to difficulty scores (higher when accuracy is lower)\n",
    "P35_TOPIC_DIFFICULTY = {label: max(0.0, 1.0 - acc) for label, acc in P35_TOPIC_ACCURACY.items()}\n",
    "# Global multiplier for difficulty adjustment; tweak to emphasise weak topics more/less\n",
    "P35_DIFFICULTY_SCALE = 2.0\n",
    "\n",
    "\n",
    "def create_weighted_dataset(source_dataset, target_size=None, seed=SEED, difficulty_scale=P35_DIFFICULTY_SCALE):\n",
    "    \"\"\"Mimic WeightedRandomSampler by sampling examples according to class weights, with extra boosts for weak topics.\"\"\"\n",
    "    rng = random.Random(seed)\n",
    "    counts = Counter(ex.label for ex in source_dataset.examples)\n",
    "    total = sum(counts.values())\n",
    "    base_class_weights = {label: total / count for label, count in counts.items()}\n",
    "\n",
    "    class_boosts = {\n",
    "        label: 1.0 + difficulty_scale * P35_TOPIC_DIFFICULTY.get(label, 0.0)\n",
    "        for label in counts.keys()\n",
    "    }\n",
    "\n",
    "    weights = [base_class_weights[ex.label] * class_boosts.get(ex.label, 1.0) for ex in source_dataset.examples]\n",
    "    sample_size = target_size or len(source_dataset.examples)\n",
    "\n",
    "    sampled_examples = rng.choices(source_dataset.examples, weights=weights, k=sample_size)\n",
    "    fields = [('text', TEXT), ('label', LABEL)]\n",
    "    return data.Dataset(sampled_examples, fields=fields)\n",
    "\n",
    "\n",
    "print(\"\\n>>> Preparing dataset variants for Part 3.5 experiments...\")\n",
    "base_counts = describe_dataset(\"Original train\", train_data)\n",
    "aug_counts = describe_dataset(\"Augmented train\", augmented_train_data)\n",
    "\n",
    "weighted_train_data = create_weighted_dataset(train_data)\n",
    "weighted_counts = describe_dataset(\"Weighted-sampled train\", weighted_train_data)\n",
    "\n",
    "augmented_weighted_train_data = create_weighted_dataset(augmented_train_data)\n",
    "aug_weighted_counts = describe_dataset(\"Augmented + weighted train\", augmented_weighted_train_data)\n",
    "\n",
    "p35_datasets = {\n",
    "    \"original\": train_data,\n",
    "    \"augmented\": augmented_train_data,\n",
    "    \"weighted\": weighted_train_data,\n",
    "    \"augmented_weighted\": augmented_weighted_train_data,\n",
    "}\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "p35_results = {\n",
    "    \"simple_rnn_baseline\": {},\n",
    "    \"rnn_bert\": {}\n",
    "}\n",
    "\n",
    "print(\"\\n>>> Dataset variants ready. Criterion initialised for upcoming runs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bda6694d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Simple RNN (mean pooling) experiment runner\n",
    "# ============================================================================\n",
    "\n",
    "def reset_random_seeds(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def build_iterator(dataset, batch_size, shuffle):\n",
    "    return data.BucketIterator(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        sort_key=lambda x: len(x.text),\n",
    "        sort_within_batch=True,\n",
    "        shuffle=shuffle,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "\n",
    "RNN_BASE_HIDDEN_DIM = 512\n",
    "RNN_BASE_N_LAYERS = 1\n",
    "RNN_BASE_DROPOUT = 0.5\n",
    "RNN_BASE_BATCH_SIZE = 32\n",
    "RNN_BASE_LEARNING_RATE = 1e-4\n",
    "RNN_BASE_WEIGHT_DECAY = 1e-5\n",
    "RNN_BASE_L1_LAMBDA = 1e-6\n",
    "RNN_BASE_N_EPOCHS = 100\n",
    "RNN_BASE_PATIENCE = 10\n",
    "RNN_BASE_AGGREGATION = \"mean\"\n",
    "RNN_BASE_SAVE_MODEL = True\n",
    "\n",
    "\n",
    "def _criterion_with_l1(base_criterion, model, l1_lambda):\n",
    "    if l1_lambda <= 0:\n",
    "        return base_criterion\n",
    "\n",
    "    def wrapped(predictions, labels):\n",
    "        loss = base_criterion(predictions, labels)\n",
    "        l1_term = 0.0\n",
    "        for param in model.parameters():\n",
    "            l1_term += param.abs().sum()\n",
    "        return loss + l1_lambda * l1_term\n",
    "\n",
    "    return wrapped\n",
    "\n",
    "\n",
    "def run_simple_rnn_experiment(dataset_key, description, save_suffix):\n",
    "    if dataset_key not in p35_datasets:\n",
    "        raise ValueError(f\"Unknown dataset key: {dataset_key}\")\n",
    "\n",
    "    reset_random_seeds(SEED)\n",
    "    train_dataset = p35_datasets[dataset_key]\n",
    "\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(f\"Running Simple RNN (mean pooling) experiment: {description}\")\n",
    "    print(\"-\"*80)\n",
    "\n",
    "    train_iter = build_iterator(train_dataset, RNN_BASE_BATCH_SIZE, shuffle=True)\n",
    "    val_iter = build_iterator(validation_data, RNN_BASE_BATCH_SIZE, shuffle=False)\n",
    "    test_iter = build_iterator(test_data, RNN_BASE_BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    model = RNN_Classifier_Aggregation(\n",
    "        vocab_size=len(TEXT.vocab),\n",
    "        embedding_dim=embedding_dim,\n",
    "        hidden_dim=RNN_BASE_HIDDEN_DIM,\n",
    "        output_dim=num_classes,\n",
    "        n_layers=RNN_BASE_N_LAYERS,\n",
    "        dropout=RNN_BASE_DROPOUT,\n",
    "        padding_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "        pretrained_embeddings=pretrained_embeddings,\n",
    "        aggregation=RNN_BASE_AGGREGATION,\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = optim.RMSprop(\n",
    "        model.parameters(),\n",
    "        lr=RNN_BASE_LEARNING_RATE,\n",
    "        weight_decay=RNN_BASE_WEIGHT_DECAY,\n",
    "    )\n",
    "\n",
    "    criterion_with_l1 = _criterion_with_l1(criterion, model, RNN_BASE_L1_LAMBDA)\n",
    "\n",
    "    model, history = train_model_with_history(\n",
    "        model,\n",
    "        train_iter,\n",
    "        val_iter,\n",
    "        optimizer,\n",
    "        criterion_with_l1,\n",
    "        RNN_BASE_N_EPOCHS,\n",
    "        device,\n",
    "        num_classes,\n",
    "        patience=RNN_BASE_PATIENCE,\n",
    "        model_name=f\"Simple RNN ({description})\",\n",
    "    )\n",
    "\n",
    "    test_loss, test_acc, test_f1, test_auc = evaluate_model(\n",
    "        model,\n",
    "        test_iter,\n",
    "        criterion,\n",
    "        device,\n",
    "        f\"Simple RNN ({description})\",\n",
    "        num_classes,\n",
    "    )\n",
    "\n",
    "    topic_metrics = evaluate_per_topic_p35(model, test_iter, device)\n",
    "\n",
    "    model_path = f\"weights/part35_simple_rnn_{save_suffix}.pt\"\n",
    "    if RNN_BASE_SAVE_MODEL:\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    p35_results[\"simple_rnn_baseline\"][save_suffix] = {\n",
    "        \"description\": description,\n",
    "        \"dataset_key\": dataset_key,\n",
    "        \"history\": history,\n",
    "        \"test_metrics\": {\n",
    "            \"loss\": test_loss,\n",
    "            \"accuracy\": test_acc,\n",
    "            \"f1\": test_f1,\n",
    "            \"auc\": test_auc,\n",
    "        },\n",
    "        \"topic_metrics\": topic_metrics,\n",
    "        \"model_path\": model_path if RNN_BASE_SAVE_MODEL else None,\n",
    "        \"model\": model,\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"history\": history,\n",
    "        \"topic_metrics\": topic_metrics,\n",
    "        \"test_metrics\": {\n",
    "            \"loss\": test_loss,\n",
    "            \"accuracy\": test_acc,\n",
    "            \"f1\": test_f1,\n",
    "            \"auc\": test_auc,\n",
    "        },\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8bff8e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Executing Simple RNN experiments...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Running Simple RNN (mean pooling) experiment: Text Augmentation\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      ">>> Training Simple RNN (Text Augmentation)\n",
      "    Parameters: 2,865,746\n",
      "    Max epochs: 100, Patience: 10\n",
      "Epoch: 01/100 | Time: 0m 3s\n",
      "\tTrain Loss: 1.4115 | Train Acc: 48.01%\n",
      "\tVal Loss: 1.1884 | Val Acc: 53.12% | Val F1: 0.5118 | Val AUC: 0.8492\n",
      "Epoch: 02/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.9923 | Train Acc: 68.01%\n",
      "\tVal Loss: 1.0153 | Val Acc: 63.94% | Val F1: 0.6485 | Val AUC: 0.8867\n",
      "Epoch: 03/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.7699 | Train Acc: 77.39%\n",
      "\tVal Loss: 0.8230 | Val Acc: 73.12% | Val F1: 0.7394 | Val AUC: 0.9246\n",
      "Epoch: 04/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.6082 | Train Acc: 83.60%\n",
      "\tVal Loss: 0.8520 | Val Acc: 74.31% | Val F1: 0.7536 | Val AUC: 0.9292\n",
      "Epoch: 05/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.5075 | Train Acc: 87.58%\n",
      "\tVal Loss: 0.8348 | Val Acc: 73.12% | Val F1: 0.7413 | Val AUC: 0.9377\n",
      "Epoch: 06/100 | Time: 0m 4s\n",
      "\tTrain Loss: 0.4317 | Train Acc: 89.70%\n",
      "\tVal Loss: 1.0560 | Val Acc: 68.44% | Val F1: 0.7223 | Val AUC: 0.9236\n",
      "Epoch: 07/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.3786 | Train Acc: 90.87%\n",
      "\tVal Loss: 0.7653 | Val Acc: 77.16% | Val F1: 0.7764 | Val AUC: 0.9462\n",
      "Epoch: 08/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.3277 | Train Acc: 92.47%\n",
      "\tVal Loss: 0.7877 | Val Acc: 75.60% | Val F1: 0.7671 | Val AUC: 0.9433\n",
      "Epoch: 09/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.2998 | Train Acc: 93.08%\n",
      "\tVal Loss: 0.6651 | Val Acc: 83.85% | Val F1: 0.8417 | Val AUC: 0.9533\n",
      "Epoch: 10/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.2681 | Train Acc: 93.98%\n",
      "\tVal Loss: 0.7242 | Val Acc: 80.92% | Val F1: 0.8115 | Val AUC: 0.9480\n",
      "Epoch: 11/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.2546 | Train Acc: 94.62%\n",
      "\tVal Loss: 0.6553 | Val Acc: 82.29% | Val F1: 0.8256 | Val AUC: 0.9548\n",
      "Epoch: 12/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.2268 | Train Acc: 95.10%\n",
      "\tVal Loss: 0.6455 | Val Acc: 82.29% | Val F1: 0.8238 | Val AUC: 0.9572\n",
      "Epoch: 13/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.2192 | Train Acc: 95.75%\n",
      "\tVal Loss: 0.6426 | Val Acc: 82.94% | Val F1: 0.8322 | Val AUC: 0.9573\n",
      "Epoch: 14/100 | Time: 0m 4s\n",
      "\tTrain Loss: 0.1810 | Train Acc: 96.32%\n",
      "\tVal Loss: 0.7497 | Val Acc: 81.28% | Val F1: 0.8138 | Val AUC: 0.9473\n",
      "Epoch: 15/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.1558 | Train Acc: 96.91%\n",
      "\tVal Loss: 0.8363 | Val Acc: 81.01% | Val F1: 0.8130 | Val AUC: 0.9561\n",
      "Epoch: 16/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.1345 | Train Acc: 97.22%\n",
      "\tVal Loss: 0.8017 | Val Acc: 81.56% | Val F1: 0.8165 | Val AUC: 0.9566\n",
      "Epoch: 17/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.1265 | Train Acc: 97.61%\n",
      "\tVal Loss: 0.7452 | Val Acc: 82.75% | Val F1: 0.8274 | Val AUC: 0.9578\n",
      "Epoch: 18/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.1252 | Train Acc: 97.53%\n",
      "\tVal Loss: 1.0393 | Val Acc: 78.99% | Val F1: 0.7902 | Val AUC: 0.9499\n",
      "Epoch: 19/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.1103 | Train Acc: 97.93%\n",
      "\tVal Loss: 0.7675 | Val Acc: 83.58% | Val F1: 0.8373 | Val AUC: 0.9570\n",
      "\t>>> Early stopping at epoch 19, best val acc: 83.85%\n",
      "\n",
      ">>> Training completed! Best validation accuracy: 83.85%\n",
      "    Best validation F1: 0.8417\n",
      "    Best validation AUC-ROC: 0.9533\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Running Simple RNN (mean pooling) experiment: Weighted Sampling\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      ">>> Training Simple RNN (Weighted Sampling)\n",
      "    Parameters: 2,865,746\n",
      "    Max epochs: 100, Patience: 10\n",
      "Epoch: 01/100 | Time: 0m 2s\n",
      "\tTrain Loss: 1.4684 | Train Acc: 44.59%\n",
      "\tVal Loss: 1.3063 | Val Acc: 47.89% | Val F1: 0.4357 | Val AUC: 0.8287\n",
      "Epoch: 02/100 | Time: 0m 2s\n",
      "\tTrain Loss: 1.0399 | Train Acc: 65.91%\n",
      "\tVal Loss: 1.1638 | Val Acc: 57.89% | Val F1: 0.5894 | Val AUC: 0.8562\n",
      "Epoch: 03/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.7839 | Train Acc: 76.96%\n",
      "\tVal Loss: 0.9023 | Val Acc: 67.71% | Val F1: 0.6776 | Val AUC: 0.9141\n",
      "Epoch: 04/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.6118 | Train Acc: 81.73%\n",
      "\tVal Loss: 0.8611 | Val Acc: 69.45% | Val F1: 0.6906 | Val AUC: 0.9240\n",
      "Epoch: 05/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.5226 | Train Acc: 85.79%\n",
      "\tVal Loss: 0.8545 | Val Acc: 73.39% | Val F1: 0.7446 | Val AUC: 0.9220\n",
      "Epoch: 06/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.4619 | Train Acc: 87.32%\n",
      "\tVal Loss: 0.7905 | Val Acc: 74.50% | Val F1: 0.7533 | Val AUC: 0.9333\n",
      "Epoch: 07/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.3811 | Train Acc: 90.26%\n",
      "\tVal Loss: 1.5438 | Val Acc: 60.92% | Val F1: 0.5830 | Val AUC: 0.8604\n",
      "Epoch: 08/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.3899 | Train Acc: 90.49%\n",
      "\tVal Loss: 0.7870 | Val Acc: 74.40% | Val F1: 0.7559 | Val AUC: 0.9344\n",
      "Epoch: 09/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.3242 | Train Acc: 92.25%\n",
      "\tVal Loss: 0.7663 | Val Acc: 76.79% | Val F1: 0.7760 | Val AUC: 0.9391\n",
      "Epoch: 10/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.2738 | Train Acc: 93.67%\n",
      "\tVal Loss: 0.8062 | Val Acc: 76.24% | Val F1: 0.7704 | Val AUC: 0.9353\n",
      "Epoch: 11/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.2308 | Train Acc: 95.48%\n",
      "\tVal Loss: 0.7762 | Val Acc: 77.52% | Val F1: 0.7825 | Val AUC: 0.9402\n",
      "Epoch: 12/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.2162 | Train Acc: 95.55%\n",
      "\tVal Loss: 0.8489 | Val Acc: 79.08% | Val F1: 0.7939 | Val AUC: 0.9387\n",
      "Epoch: 13/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.1899 | Train Acc: 96.70%\n",
      "\tVal Loss: 0.7955 | Val Acc: 79.63% | Val F1: 0.8007 | Val AUC: 0.9422\n",
      "Epoch: 14/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.1544 | Train Acc: 97.73%\n",
      "\tVal Loss: 0.7951 | Val Acc: 81.28% | Val F1: 0.8137 | Val AUC: 0.9439\n",
      "Epoch: 15/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.1742 | Train Acc: 97.20%\n",
      "\tVal Loss: 1.5792 | Val Acc: 66.61% | Val F1: 0.6624 | Val AUC: 0.9079\n",
      "Epoch: 16/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.1353 | Train Acc: 98.10%\n",
      "\tVal Loss: 0.8255 | Val Acc: 80.09% | Val F1: 0.8026 | Val AUC: 0.9431\n",
      "Epoch: 17/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.1360 | Train Acc: 97.91%\n",
      "\tVal Loss: 0.8751 | Val Acc: 79.36% | Val F1: 0.7968 | Val AUC: 0.9404\n",
      "Epoch: 18/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0985 | Train Acc: 98.85%\n",
      "\tVal Loss: 0.8272 | Val Acc: 81.10% | Val F1: 0.8117 | Val AUC: 0.9448\n",
      "Epoch: 19/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.1329 | Train Acc: 97.85%\n",
      "\tVal Loss: 0.8087 | Val Acc: 82.29% | Val F1: 0.8244 | Val AUC: 0.9450\n",
      "Epoch: 20/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0833 | Train Acc: 98.97%\n",
      "\tVal Loss: 0.9795 | Val Acc: 77.89% | Val F1: 0.7766 | Val AUC: 0.9367\n",
      "Epoch: 21/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0823 | Train Acc: 99.01%\n",
      "\tVal Loss: 0.9265 | Val Acc: 80.28% | Val F1: 0.8037 | Val AUC: 0.9397\n",
      "Epoch: 22/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0958 | Train Acc: 98.49%\n",
      "\tVal Loss: 1.0048 | Val Acc: 80.09% | Val F1: 0.7990 | Val AUC: 0.9361\n",
      "Epoch: 23/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0684 | Train Acc: 99.15%\n",
      "\tVal Loss: 1.0061 | Val Acc: 79.72% | Val F1: 0.7977 | Val AUC: 0.9386\n",
      "Epoch: 24/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.1013 | Train Acc: 97.98%\n",
      "\tVal Loss: 0.9257 | Val Acc: 81.28% | Val F1: 0.8134 | Val AUC: 0.9459\n",
      "Epoch: 25/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0743 | Train Acc: 99.06%\n",
      "\tVal Loss: 1.1987 | Val Acc: 76.97% | Val F1: 0.7673 | Val AUC: 0.9295\n",
      "Epoch: 26/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0585 | Train Acc: 99.61%\n",
      "\tVal Loss: 0.9298 | Val Acc: 81.28% | Val F1: 0.8136 | Val AUC: 0.9418\n",
      "Epoch: 27/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0717 | Train Acc: 99.17%\n",
      "\tVal Loss: 0.8899 | Val Acc: 81.10% | Val F1: 0.8109 | Val AUC: 0.9445\n",
      "Epoch: 28/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0722 | Train Acc: 99.17%\n",
      "\tVal Loss: 1.0858 | Val Acc: 78.62% | Val F1: 0.7877 | Val AUC: 0.9363\n",
      "Epoch: 29/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.1154 | Train Acc: 98.19%\n",
      "\tVal Loss: 1.0316 | Val Acc: 80.46% | Val F1: 0.8065 | Val AUC: 0.9387\n",
      "\t>>> Early stopping at epoch 29, best val acc: 82.29%\n",
      "\n",
      ">>> Training completed! Best validation accuracy: 82.29%\n",
      "    Best validation F1: 0.8244\n",
      "    Best validation AUC-ROC: 0.9450\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Running Simple RNN (mean pooling) experiment: Augmentation + Weighted Sampling\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      ">>> Training Simple RNN (Augmentation + Weighted Sampling)\n",
      "    Parameters: 2,865,746\n",
      "    Max epochs: 100, Patience: 10\n",
      "Epoch: 01/100 | Time: 0m 3s\n",
      "\tTrain Loss: 1.4216 | Train Acc: 47.36%\n",
      "\tVal Loss: 1.2507 | Val Acc: 49.82% | Val F1: 0.4406 | Val AUC: 0.8437\n",
      "Epoch: 02/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.9685 | Train Acc: 69.05%\n",
      "\tVal Loss: 1.0678 | Val Acc: 60.55% | Val F1: 0.6144 | Val AUC: 0.8905\n",
      "Epoch: 03/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.7618 | Train Acc: 77.86%\n",
      "\tVal Loss: 0.8785 | Val Acc: 70.18% | Val F1: 0.7261 | Val AUC: 0.9190\n",
      "Epoch: 04/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.5666 | Train Acc: 84.88%\n",
      "\tVal Loss: 0.9462 | Val Acc: 69.72% | Val F1: 0.7214 | Val AUC: 0.9201\n",
      "Epoch: 05/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.4538 | Train Acc: 88.73%\n",
      "\tVal Loss: 0.7309 | Val Acc: 77.16% | Val F1: 0.7817 | Val AUC: 0.9440\n",
      "Epoch: 06/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.3963 | Train Acc: 91.29%\n",
      "\tVal Loss: 0.8079 | Val Acc: 74.95% | Val F1: 0.7654 | Val AUC: 0.9338\n",
      "Epoch: 07/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.3228 | Train Acc: 93.09%\n",
      "\tVal Loss: 0.7299 | Val Acc: 77.89% | Val F1: 0.7906 | Val AUC: 0.9476\n",
      "Epoch: 08/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.2833 | Train Acc: 94.50%\n",
      "\tVal Loss: 0.6938 | Val Acc: 80.73% | Val F1: 0.8080 | Val AUC: 0.9502\n",
      "Epoch: 09/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.2653 | Train Acc: 95.03%\n",
      "\tVal Loss: 0.6809 | Val Acc: 79.36% | Val F1: 0.7949 | Val AUC: 0.9495\n",
      "Epoch: 10/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.2006 | Train Acc: 96.39%\n",
      "\tVal Loss: 0.6865 | Val Acc: 79.36% | Val F1: 0.7972 | Val AUC: 0.9500\n",
      "Epoch: 11/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.2124 | Train Acc: 95.85%\n",
      "\tVal Loss: 0.7092 | Val Acc: 78.72% | Val F1: 0.7900 | Val AUC: 0.9481\n",
      "Epoch: 12/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.1569 | Train Acc: 97.22%\n",
      "\tVal Loss: 0.8558 | Val Acc: 78.99% | Val F1: 0.7949 | Val AUC: 0.9468\n",
      "Epoch: 13/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.1626 | Train Acc: 96.86%\n",
      "\tVal Loss: 0.8203 | Val Acc: 78.17% | Val F1: 0.7842 | Val AUC: 0.9478\n",
      "Epoch: 14/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.1129 | Train Acc: 98.19%\n",
      "\tVal Loss: 0.8991 | Val Acc: 80.55% | Val F1: 0.8081 | Val AUC: 0.9474\n",
      "Epoch: 15/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.1342 | Train Acc: 97.37%\n",
      "\tVal Loss: 1.1360 | Val Acc: 75.32% | Val F1: 0.7765 | Val AUC: 0.9328\n",
      "Epoch: 16/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.1058 | Train Acc: 98.34%\n",
      "\tVal Loss: 0.7909 | Val Acc: 80.18% | Val F1: 0.8043 | Val AUC: 0.9508\n",
      "Epoch: 17/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.1041 | Train Acc: 98.26%\n",
      "\tVal Loss: 0.9064 | Val Acc: 81.47% | Val F1: 0.8173 | Val AUC: 0.9507\n",
      "Epoch: 18/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.0697 | Train Acc: 99.05%\n",
      "\tVal Loss: 0.9537 | Val Acc: 81.28% | Val F1: 0.8127 | Val AUC: 0.9522\n",
      "Epoch: 19/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.1041 | Train Acc: 98.18%\n",
      "\tVal Loss: 0.7482 | Val Acc: 80.73% | Val F1: 0.8138 | Val AUC: 0.9531\n",
      "Epoch: 20/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.0670 | Train Acc: 99.08%\n",
      "\tVal Loss: 1.0113 | Val Acc: 81.19% | Val F1: 0.8159 | Val AUC: 0.9502\n",
      "Epoch: 21/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.1079 | Train Acc: 98.28%\n",
      "\tVal Loss: 0.8799 | Val Acc: 79.45% | Val F1: 0.8000 | Val AUC: 0.9518\n",
      "Epoch: 22/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.0588 | Train Acc: 99.33%\n",
      "\tVal Loss: 0.8964 | Val Acc: 80.18% | Val F1: 0.8087 | Val AUC: 0.9519\n",
      "Epoch: 23/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.0580 | Train Acc: 99.31%\n",
      "\tVal Loss: 0.9547 | Val Acc: 79.54% | Val F1: 0.8022 | Val AUC: 0.9501\n",
      "Epoch: 24/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.0598 | Train Acc: 99.21%\n",
      "\tVal Loss: 1.1438 | Val Acc: 79.82% | Val F1: 0.8036 | Val AUC: 0.9484\n",
      "Epoch: 25/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.0964 | Train Acc: 98.70%\n",
      "\tVal Loss: 1.0501 | Val Acc: 80.09% | Val F1: 0.8050 | Val AUC: 0.9516\n",
      "Epoch: 26/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.0450 | Train Acc: 99.46%\n",
      "\tVal Loss: 1.1595 | Val Acc: 78.44% | Val F1: 0.7959 | Val AUC: 0.9491\n",
      "Epoch: 27/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.0646 | Train Acc: 99.15%\n",
      "\tVal Loss: 1.2152 | Val Acc: 76.70% | Val F1: 0.7671 | Val AUC: 0.9483\n",
      "\t>>> Early stopping at epoch 27, best val acc: 81.47%\n",
      "\n",
      ">>> Training completed! Best validation accuracy: 81.47%\n",
      "    Best validation F1: 0.8173\n",
      "    Best validation AUC-ROC: 0.9507\n",
      "\n",
      ">>> Simple RNN experiments queued. Run the cells to execute training if needed.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Run Simple RNN + Attention experiments\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n>>> Executing Simple RNN experiments...\")\n",
    "\n",
    "rnn_results_text_aug = run_simple_rnn_experiment(\n",
    "    dataset_key=\"augmented\",\n",
    "    description=\"Text Augmentation\",\n",
    "    save_suffix=\"text_aug\",\n",
    ")\n",
    "\n",
    "rnn_results_weighted = run_simple_rnn_experiment(\n",
    "    dataset_key=\"weighted\",\n",
    "    description=\"Weighted Sampling\",\n",
    "    save_suffix=\"weighted_sampler\",\n",
    ")\n",
    "\n",
    "rnn_results_aug_weighted = run_simple_rnn_experiment(\n",
    "    dataset_key=\"augmented_weighted\",\n",
    "    description=\"Augmentation + Weighted Sampling\",\n",
    "    save_suffix=\"text_aug_weighted\",\n",
    ")\n",
    "\n",
    "print(\"\\n>>> Simple RNN experiments queued. Run the cells to execute training if needed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "426bd88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Topic-wise accuracy for Simple RNN variants\n",
      "\n",
      "Simple RNN (Text Augmentation)\n",
      "------------------------------\n",
      "Topic      Accuracy %   Correct    Total     \n",
      "---------------------------------------------\n",
      "ABBR       77.78        7          9         \n",
      "DESC       97.83        135        138       \n",
      "ENTY       64.89        61         94        \n",
      "HUM        86.15        56         65        \n",
      "LOC        86.42        70         81        \n",
      "NUM        89.38        101        113       \n",
      "\n",
      "Simple RNN (Weighted Sampling)\n",
      "------------------------------\n",
      "Topic      Accuracy %   Correct    Total     \n",
      "---------------------------------------------\n",
      "ABBR       77.78        7          9         \n",
      "DESC       97.83        135        138       \n",
      "ENTY       72.34        68         94        \n",
      "HUM        86.15        56         65        \n",
      "LOC        87.65        71         81        \n",
      "NUM        88.50        100        113       \n",
      "\n",
      "Simple RNN (Augmentation + Weighted Sampling)\n",
      "---------------------------------------------\n",
      "Topic      Accuracy %   Correct    Total     \n",
      "---------------------------------------------\n",
      "ABBR       88.89        8          9         \n",
      "DESC       42.75        59         138       \n",
      "ENTY       54.26        51         94        \n",
      "HUM        89.23        58         65        \n",
      "LOC        83.95        68         81        \n",
      "NUM        85.84        97         113       \n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Topic-wise accuracy summary for Simple RNN experiments\n",
    "# ============================================================================\n",
    "\n",
    "def display_topic_metrics(title, metrics_dict):\n",
    "    print(f\"\\n{title}\")\n",
    "    print(\"-\" * len(title))\n",
    "    header = f\"{'Topic':<10} {'Accuracy %':<12} {'Correct':<10} {'Total':<10}\"\n",
    "    print(header)\n",
    "    print(\"-\" * len(header))\n",
    "    for topic in sorted(metrics_dict.keys()):\n",
    "        stats = metrics_dict[topic]\n",
    "        acc_pct = stats['accuracy'] * 100\n",
    "        print(f\"{topic:<10} {acc_pct:<12.2f} {stats['correct']:<10} {stats['total']:<10}\")\n",
    "\n",
    "print(\"\\n>>> Topic-wise accuracy for Simple RNN variants\")\n",
    "for run_key, info in p35_results[\"simple_rnn_baseline\"].items():\n",
    "    topic_metrics = info.get(\"topic_metrics\")\n",
    "    if not topic_metrics:\n",
    "        continue\n",
    "    title = f\"Simple RNN ({info['description']})\"\n",
    "    display_topic_metrics(title, topic_metrics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e096065b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# RNN + BERT experiment runner\n",
    "# ============================================================================\n",
    "\n",
    "BERT_HIDDEN_DIM = 256\n",
    "BERT_N_LAYERS = 2\n",
    "BERT_DROPOUT = 0.5\n",
    "BERT_BATCH_SIZE = 32\n",
    "BERT_LEARNING_RATE = 2e-5\n",
    "BERT_OTHER_LR = BERT_LEARNING_RATE * 10\n",
    "BERT_N_EPOCHS = 50\n",
    "BERT_PATIENCE = 7\n",
    "BERT_MODEL_NAME = 'bert-base-uncased'\n",
    "BERT_FREEZE = False\n",
    "BERT_SAVE_MODEL = True\n",
    "\n",
    "\n",
    "def run_rnn_bert_experiment(dataset_key, description, save_suffix, freeze_bert=BERT_FREEZE):\n",
    "    if not BERT_AVAILABLE:\n",
    "        raise RuntimeError(\"Transformers library is unavailable; cannot run RNN+BERT experiments.\")\n",
    "    if dataset_key not in p35_datasets:\n",
    "        raise ValueError(f\"Unknown dataset key: {dataset_key}\")\n",
    "\n",
    "    reset_random_seeds(SEED)\n",
    "    train_dataset = p35_datasets[dataset_key]\n",
    "\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(f\"Running RNN + BERT experiment: {description}\")\n",
    "    print(\"-\"*80)\n",
    "\n",
    "    train_iter = build_iterator(train_dataset, BERT_BATCH_SIZE, shuffle=True)\n",
    "    val_iter = build_iterator(validation_data, BERT_BATCH_SIZE, shuffle=False)\n",
    "    test_iter = build_iterator(test_data, BERT_BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    model = RNNBertClassifier(\n",
    "        output_dim=num_classes,\n",
    "        hidden_dim=BERT_HIDDEN_DIM,\n",
    "        n_layers=BERT_N_LAYERS,\n",
    "        dropout=BERT_DROPOUT,\n",
    "        bert_model_name=BERT_MODEL_NAME,\n",
    "        freeze_bert=freeze_bert,\n",
    "    ).to(device)\n",
    "\n",
    "    bert_params = [p for p in model.bert_model.parameters() if p.requires_grad]\n",
    "    other_params = [p for n, p in model.named_parameters() if 'bert_model' not in n]\n",
    "\n",
    "    optimizer_grouped_parameters = []\n",
    "    if bert_params:\n",
    "        optimizer_grouped_parameters.append({'params': bert_params, 'lr': BERT_LEARNING_RATE})\n",
    "    if other_params:\n",
    "        optimizer_grouped_parameters.append({'params': other_params, 'lr': BERT_OTHER_LR})\n",
    "\n",
    "    optimizer = optim.AdamW(optimizer_grouped_parameters, weight_decay=0.01)\n",
    "\n",
    "    model, history = train_model_with_history_bert(\n",
    "        model,\n",
    "        train_iter,\n",
    "        val_iter,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        BERT_N_EPOCHS,\n",
    "        device,\n",
    "        num_classes,\n",
    "        patience=BERT_PATIENCE,\n",
    "        model_name=f\"RNN+BERT ({description})\",\n",
    "        text_vocab=TEXT.vocab.itos,\n",
    "    )\n",
    "\n",
    "    test_loss, test_acc, test_f1, test_auc = evaluate_model_bert(\n",
    "        model,\n",
    "        test_iter,\n",
    "        criterion,\n",
    "        device,\n",
    "        f\"RNN+BERT ({description})\",\n",
    "        num_classes,\n",
    "        text_vocab=TEXT.vocab.itos,\n",
    "    )\n",
    "\n",
    "    topic_metrics = evaluate_per_topic_p35(model, test_iter, device)\n",
    "\n",
    "    model_path = f\"weights/part35_rnn_bert_{save_suffix}.pt\"\n",
    "    if BERT_SAVE_MODEL:\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    p35_results[\"rnn_bert\"][save_suffix] = {\n",
    "        \"description\": description,\n",
    "        \"dataset_key\": dataset_key,\n",
    "        \"history\": history,\n",
    "        \"test_metrics\": {\n",
    "            \"loss\": test_loss,\n",
    "            \"accuracy\": test_acc,\n",
    "            \"f1\": test_f1,\n",
    "            \"auc\": test_auc,\n",
    "        },\n",
    "        \"topic_metrics\": topic_metrics,\n",
    "        \"model_path\": model_path if BERT_SAVE_MODEL else None,\n",
    "        \"freeze_bert\": freeze_bert,\n",
    "        \"model\": model,\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"history\": history,\n",
    "        \"topic_metrics\": topic_metrics,\n",
    "        \"test_metrics\": {\n",
    "            \"loss\": test_loss,\n",
    "            \"accuracy\": test_acc,\n",
    "            \"f1\": test_f1,\n",
    "            \"auc\": test_auc,\n",
    "        },\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c9059c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Executing RNN + BERT experiments...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Running RNN + BERT experiment: Text Augmentation\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      ">>> Training RNN+BERT (Text Augmentation)\n",
      "    Parameters: 113,295,111\n",
      "    Max epochs: 50, Patience: 7\n",
      "Epoch: 01/50 | Time: 0m 54s\n",
      "\tTrain Loss: 0.6090 | Train Acc: 78.66%\n",
      "\tVal Loss: 0.4462 | Val Acc: 87.98% | Val F1: 0.8856 | Val AUC: 0.9826\n",
      "Epoch: 02/50 | Time: 0m 54s\n",
      "\tTrain Loss: 0.1534 | Train Acc: 95.62%\n",
      "\tVal Loss: 0.4987 | Val Acc: 88.99% | Val F1: 0.8977 | Val AUC: 0.9797\n",
      "Epoch: 03/50 | Time: 0m 53s\n",
      "\tTrain Loss: 0.0850 | Train Acc: 97.86%\n",
      "\tVal Loss: 0.5744 | Val Acc: 88.81% | Val F1: 0.8970 | Val AUC: 0.9745\n",
      "Epoch: 04/50 | Time: 0m 54s\n",
      "\tTrain Loss: 0.0392 | Train Acc: 99.03%\n",
      "\tVal Loss: 0.5874 | Val Acc: 89.54% | Val F1: 0.9003 | Val AUC: 0.9850\n",
      "Epoch: 05/50 | Time: 0m 54s\n",
      "\tTrain Loss: 0.0357 | Train Acc: 99.21%\n",
      "\tVal Loss: 0.7902 | Val Acc: 89.17% | Val F1: 0.8981 | Val AUC: 0.9831\n",
      "Epoch: 06/50 | Time: 0m 59s\n",
      "\tTrain Loss: 0.0154 | Train Acc: 99.67%\n",
      "\tVal Loss: 0.7844 | Val Acc: 88.35% | Val F1: 0.8926 | Val AUC: 0.9817\n",
      "Epoch: 07/50 | Time: 1m 3s\n",
      "\tTrain Loss: 0.0194 | Train Acc: 99.55%\n",
      "\tVal Loss: 0.7861 | Val Acc: 88.81% | Val F1: 0.8925 | Val AUC: 0.9816\n",
      "Epoch: 08/50 | Time: 0m 56s\n",
      "\tTrain Loss: 0.0148 | Train Acc: 99.73%\n",
      "\tVal Loss: 0.7990 | Val Acc: 89.82% | Val F1: 0.9020 | Val AUC: 0.9809\n",
      "Epoch: 09/50 | Time: 0m 55s\n",
      "\tTrain Loss: 0.0250 | Train Acc: 99.60%\n",
      "\tVal Loss: 0.8129 | Val Acc: 89.63% | Val F1: 0.9017 | Val AUC: 0.9798\n",
      "Epoch: 10/50 | Time: 0m 55s\n",
      "\tTrain Loss: 0.0032 | Train Acc: 99.92%\n",
      "\tVal Loss: 0.8136 | Val Acc: 90.28% | Val F1: 0.9062 | Val AUC: 0.9836\n",
      "Epoch: 11/50 | Time: 0m 54s\n",
      "\tTrain Loss: 0.0101 | Train Acc: 99.83%\n",
      "\tVal Loss: 0.8424 | Val Acc: 89.45% | Val F1: 0.8994 | Val AUC: 0.9828\n",
      "Epoch: 12/50 | Time: 0m 54s\n",
      "\tTrain Loss: 0.0129 | Train Acc: 99.78%\n",
      "\tVal Loss: 0.9155 | Val Acc: 89.08% | Val F1: 0.8972 | Val AUC: 0.9760\n",
      "Epoch: 13/50 | Time: 0m 55s\n",
      "\tTrain Loss: 0.0063 | Train Acc: 99.88%\n",
      "\tVal Loss: 0.8508 | Val Acc: 89.91% | Val F1: 0.9037 | Val AUC: 0.9794\n",
      "Epoch: 14/50 | Time: 0m 57s\n",
      "\tTrain Loss: 0.0069 | Train Acc: 99.88%\n",
      "\tVal Loss: 1.1116 | Val Acc: 88.81% | Val F1: 0.8932 | Val AUC: 0.9723\n",
      "Epoch: 15/50 | Time: 0m 57s\n",
      "\tTrain Loss: 0.0081 | Train Acc: 99.83%\n",
      "\tVal Loss: 0.9379 | Val Acc: 90.09% | Val F1: 0.9072 | Val AUC: 0.9793\n",
      "Epoch: 16/50 | Time: 0m 54s\n",
      "\tTrain Loss: 0.0055 | Train Acc: 99.88%\n",
      "\tVal Loss: 1.0324 | Val Acc: 89.08% | Val F1: 0.8979 | Val AUC: 0.9769\n",
      "Epoch: 17/50 | Time: 0m 54s\n",
      "\tTrain Loss: 0.0071 | Train Acc: 99.90%\n",
      "\tVal Loss: 1.0108 | Val Acc: 89.63% | Val F1: 0.9022 | Val AUC: 0.9809\n",
      "\t>>> Early stopping at epoch 17, best val acc: 90.28%\n",
      "\n",
      ">>> Training completed! Best validation accuracy: 90.28%\n",
      "    Best validation F1: 0.9062\n",
      "    Best validation AUC-ROC: 0.9836\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Running RNN + BERT experiment: Weighted Sampling\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      ">>> Training RNN+BERT (Weighted Sampling)\n",
      "    Parameters: 113,295,111\n",
      "    Max epochs: 50, Patience: 7\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 3.21 GiB is allocated by PyTorch, and 257.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m>>> Executing RNN + BERT experiments...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m bert_results_text_aug = run_rnn_bert_experiment(\n\u001b[32m      9\u001b[39m     dataset_key=\u001b[33m\"\u001b[39m\u001b[33maugmented\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     10\u001b[39m     description=\u001b[33m\"\u001b[39m\u001b[33mText Augmentation\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     11\u001b[39m     save_suffix=\u001b[33m\"\u001b[39m\u001b[33mtext_aug\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     12\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m bert_results_weighted = \u001b[43mrun_rnn_bert_experiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset_key\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweighted\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWeighted Sampling\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_suffix\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweighted_sampler\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m bert_results_aug_weighted = run_rnn_bert_experiment(\n\u001b[32m     21\u001b[39m     dataset_key=\u001b[33m\"\u001b[39m\u001b[33maugmented_weighted\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     22\u001b[39m     description=\u001b[33m\"\u001b[39m\u001b[33mAugmentation + Weighted Sampling\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     23\u001b[39m     save_suffix=\u001b[33m\"\u001b[39m\u001b[33mtext_aug_weighted\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     24\u001b[39m )\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m>>> RNN + BERT experiments queued. Run the cells to execute training if needed.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 55\u001b[39m, in \u001b[36mrun_rnn_bert_experiment\u001b[39m\u001b[34m(dataset_key, description, save_suffix, freeze_bert)\u001b[39m\n\u001b[32m     51\u001b[39m     optimizer_grouped_parameters.append({\u001b[33m'\u001b[39m\u001b[33mparams\u001b[39m\u001b[33m'\u001b[39m: other_params, \u001b[33m'\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m'\u001b[39m: BERT_OTHER_LR})\n\u001b[32m     53\u001b[39m optimizer = optim.AdamW(optimizer_grouped_parameters, weight_decay=\u001b[32m0.01\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m model, history = \u001b[43mtrain_model_with_history_bert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m    \u001b[49m\u001b[43mBERT_N_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBERT_PATIENCE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mRNN+BERT (\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdescription\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m)\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext_vocab\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTEXT\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m test_loss, test_acc, test_f1, test_auc = evaluate_model_bert(\n\u001b[32m     70\u001b[39m     model,\n\u001b[32m     71\u001b[39m     test_iter,\n\u001b[32m   (...)\u001b[39m\u001b[32m     76\u001b[39m     text_vocab=TEXT.vocab.itos,\n\u001b[32m     77\u001b[39m )\n\u001b[32m     79\u001b[39m topic_metrics = evaluate_per_topic_p35(model, test_iter, device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\riri\\Documents\\GitHub\\NTU-NLP-Assignment\\utils.py:459\u001b[39m, in \u001b[36mtrain_model_with_history_bert\u001b[39m\u001b[34m(model, train_iterator, val_iterator, optimizer, criterion, n_epochs, device, num_classes, patience, model_name, text_vocab)\u001b[39m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m val_iterator:\n\u001b[32m    458\u001b[39m     text, text_lengths, labels, _ = process_batch_bert(batch, text_vocab, device, debug=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m459\u001b[39m     predictions = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_lengths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_vocab\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_vocab\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    460\u001b[39m     loss = criterion(predictions, labels)\n\u001b[32m    461\u001b[39m     val_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\riri\\Documents\\GitHub\\NTU-NLP-Assignment\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\riri\\Documents\\GitHub\\NTU-NLP-Assignment\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 100\u001b[39m, in \u001b[36mRNNBertClassifier.forward\u001b[39m\u001b[34m(self, text, text_lengths, text_vocab)\u001b[39m\n\u001b[32m     95\u001b[39m bert_lengths = bert_lengths.clamp(\u001b[38;5;28mmax\u001b[39m=max_len).cpu()\n\u001b[32m     97\u001b[39m packed_bert = nn.utils.rnn.pack_padded_sequence(\n\u001b[32m     98\u001b[39m     bert_embeddings, bert_lengths, batch_first=\u001b[38;5;28;01mTrue\u001b[39;00m, enforce_sorted=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     99\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m packed_output, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbilstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpacked_bert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    101\u001b[39m bilstm_output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    103\u001b[39m attention_scores = \u001b[38;5;28mself\u001b[39m.attention_linear1(bilstm_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\riri\\Documents\\GitHub\\NTU-NLP-Assignment\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\riri\\Documents\\GitHub\\NTU-NLP-Assignment\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\riri\\Documents\\GitHub\\NTU-NLP-Assignment\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:1136\u001b[39m, in \u001b[36mLSTM.forward\u001b[39m\u001b[34m(self, input, hx)\u001b[39m\n\u001b[32m   1124\u001b[39m     result = _VF.lstm(\n\u001b[32m   1125\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   1126\u001b[39m         hx,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1133\u001b[39m         \u001b[38;5;28mself\u001b[39m.batch_first,\n\u001b[32m   1134\u001b[39m     )\n\u001b[32m   1135\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1136\u001b[39m     result = \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1137\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1138\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1139\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1140\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1141\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1142\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1143\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1144\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1145\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1146\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1147\u001b[39m output = result[\u001b[32m0\u001b[39m]\n\u001b[32m   1148\u001b[39m hidden = result[\u001b[32m1\u001b[39m:]\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 3.21 GiB is allocated by PyTorch, and 257.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Run RNN + BERT experiments\n",
    "# ============================================================================\n",
    "\n",
    "if BERT_AVAILABLE:\n",
    "    print(\"\\n>>> Executing RNN + BERT experiments...\")\n",
    "\n",
    "    bert_results_text_aug = run_rnn_bert_experiment(\n",
    "        dataset_key=\"augmented\",\n",
    "        description=\"Text Augmentation\",\n",
    "        save_suffix=\"text_aug\",\n",
    "    )\n",
    "\n",
    "    bert_results_weighted = run_rnn_bert_experiment(\n",
    "        dataset_key=\"weighted\",\n",
    "        description=\"Weighted Sampling\",\n",
    "        save_suffix=\"weighted_sampler\",\n",
    "    )\n",
    "\n",
    "    bert_results_aug_weighted = run_rnn_bert_experiment(\n",
    "        dataset_key=\"augmented_weighted\",\n",
    "        description=\"Augmentation + Weighted Sampling\",\n",
    "        save_suffix=\"text_aug_weighted\",\n",
    "    )\n",
    "\n",
    "    print(\"\\n>>> RNN + BERT experiments queued. Run the cells to execute training if needed.\")\n",
    "else:\n",
    "    print(\"\\n>>> Skipping RNN + BERT experiments (transformers library unavailable).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ce0d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================\n",
    "# Topic-wise accuracy summary for RNN + BERT experiments\n",
    "# =========================================================================\n",
    "\n",
    "if p35_results[\"rnn_bert\"]:\n",
    "    print(\"\\n>>> Topic-wise accuracy for RNN + BERT variants\")\n",
    "    for key, info in p35_results[\"rnn_bert\"].items():\n",
    "        topic_metrics = info.get(\"topic_metrics\")\n",
    "        if not topic_metrics:\n",
    "            continue\n",
    "        title = f\"RNN + BERT ({info['description']})\"\n",
    "        display_topic_metrics(title, topic_metrics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9b5ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PART 3.5 SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 3.5: TEXT AUGMENTATION VS WEIGHTED SAMPLING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n>>> Dataset Variants Used:\")\n",
    "for key in [\"original\", \"augmented\", \"weighted\", \"augmented_weighted\"]:\n",
    "    dataset = p35_datasets[key]\n",
    "    print(f\"  - {key}: {len(dataset.examples)} samples\")\n",
    "\n",
    "print(\"\\n>>> Simple RNN (mean pooling) Experiments:\")\n",
    "if p35_results[\"simple_rnn_baseline\"]:\n",
    "    for key, info in p35_results[\"simple_rnn_baseline\"].items():\n",
    "        metrics = info.get(\"test_metrics\", {})\n",
    "        print(f\"  {info['description']} [{key}]\")\n",
    "        if metrics:\n",
    "            print(f\"    - Test Accuracy: {metrics.get('accuracy', 0)*100:.2f}%\")\n",
    "            print(f\"    - Test F1: {metrics.get('f1', 0):.4f}\")\n",
    "            print(f\"    - Test AUC: {metrics.get('auc', 0):.4f}\")\n",
    "            print(f\"    - Test Loss: {metrics.get('loss', 0):.4f}\")\n",
    "        topic_metrics = info.get(\"topic_metrics\")\n",
    "        if topic_metrics:\n",
    "            weakest = min(topic_metrics.items(), key=lambda kv: kv[1]['accuracy'])\n",
    "            print(f\"    - Weakest Topic: {weakest[0]} ({weakest[1]['accuracy']*100:.2f}%)\")\n",
    "        print(f\"    - Model saved to: {info['model_path']}\")\n",
    "else:\n",
    "    print(\"  - Pending (run the experiment cells above)\")\n",
    "\n",
    "print(\"\\n>>> RNN + BERT Experiments:\")\n",
    "if p35_results[\"rnn_bert\"]:\n",
    "    for key, info in p35_results[\"rnn_bert\"].items():\n",
    "        metrics = info.get(\"test_metrics\", {})\n",
    "        print(f\"  {info['description']} [{key}]\")\n",
    "        if metrics:\n",
    "            print(f\"    - Test Accuracy: {metrics.get('accuracy', 0)*100:.2f}%\")\n",
    "            print(f\"    - Test F1: {metrics.get('f1', 0):.4f}\")\n",
    "            print(f\"    - Test AUC: {metrics.get('auc', 0):.4f}\")\n",
    "            print(f\"    - Test Loss: {metrics.get('loss', 0):.4f}\")\n",
    "        topic_metrics = info.get(\"topic_metrics\")\n",
    "        if topic_metrics:\n",
    "            weakest = min(topic_metrics.items(), key=lambda kv: kv[1]['accuracy'])\n",
    "            print(f\"    - Weakest Topic: {weakest[0]} ({weakest[1]['accuracy']*100:.2f}%)\")\n",
    "        print(f\"    - Model saved to: {info['model_path']}\")\n",
    "else:\n",
    "    print(\"  - Pending (run the experiment cells above)\")\n",
    "\n",
    "print(\"\\n>>> Next Steps:\")\n",
    "print(\"  - Execute the experiment cells sequentially (they can be time-consuming).\")\n",
    "print(\"  - Optionally add plotting/analysis cells to compare validation curves across runs.\")\n",
    "print(\"  - Update the final report with observations once metrics are populated.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 3.5 SETUP COMPLETE\")\n",
    "print(\"=\"*80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
