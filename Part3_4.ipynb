{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05ac734e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ.setdefault('TORCH_COMPILE_DISABLE', '1')\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "import torch\n",
    "\n",
    "# Method 2: Patch torch._dynamo.disable decorator after import\n",
    "try:\n",
    "    import torch._dynamo\n",
    "    # Patch the disable function to ignore the 'wrapping' parameter\n",
    "    if hasattr(torch._dynamo, 'disable'):\n",
    "        def patched_disable(fn=None, *args, **kwargs):\n",
    "            # Remove problematic 'wrapping' parameter if present\n",
    "            if 'wrapping' in kwargs:\n",
    "                kwargs.pop('wrapping')\n",
    "            if fn is None:\n",
    "                # Decorator usage: @disable\n",
    "                return lambda f: f\n",
    "            # Function usage: disable(fn) or disable(fn, **kwargs)\n",
    "            # Simply return the function unwrapped to avoid recursion\n",
    "            # The original disable was causing issues, so we bypass it entirely\n",
    "            return fn\n",
    "        torch._dynamo.disable = patched_disable\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not patch torch._dynamo: {e}\")\n",
    "    pass  # If patching fails, continue anyway\n",
    "\n",
    "import random, string\n",
    "\n",
    "from torchtext import data , datasets\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "\n",
    "os.environ['GENSIM_DATA_DIR'] = os.path.join(os.getcwd(), 'gensim-data')\n",
    "\n",
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.fasttext import load_facebook_model\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import time, copy\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44e533a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Prepping Data...\n",
      "[+] Test set formed!\n",
      "[+] Train and Validation sets formed!\n",
      "[+] Data prepped successfully!\n",
      "[*] Retrieving pretrained word embeddings...\n",
      "[*] Loading fasttext model...\n",
      "[+] Model loaded!\n",
      "[*] Forming embedding matrix...\n",
      "[+] Embedding matrix formed!\n",
      "[+] Embeddings retrieved successfully!\n",
      "\n",
      "Label distribution in training set:\n",
      "- ABBR: 69 samples (1.58%)\n",
      "- DESC: 930 samples (21.32%)\n",
      "- ENTY: 1000 samples (22.93%)\n",
      "- HUM: 978 samples (22.42%)\n",
      "- LOC: 668 samples (15.31%)\n",
      "- NUM: 717 samples (16.44%)\n",
      "Total samples: 4362, Sum of percentages: 100.00%\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "train_data, validation_data, test_data, LABEL, TEXT, pretrained_embed = data_prep(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c176434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86a01357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of classes: 6\n",
      "Classes: ['ENTY', 'HUM', 'DESC', 'NUM', 'LOC', 'ABBR']\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary for labels\n",
    "LABEL.build_vocab(train_data)\n",
    "num_classes = len(LABEL.vocab)\n",
    "print(f\"\\nNumber of classes: {num_classes}\")\n",
    "print(f\"Classes: {LABEL.vocab.itos}\")\n",
    "\n",
    "# Get pretrained embeddings from Part 1 (frozen embeddings)\n",
    "pretrained_embeddings = pretrained_embed.weight.data\n",
    "\n",
    "# Get embedding dimension and vocab size from the fasttext embedding layer\n",
    "embedding_dim = pretrained_embed.weight.shape[1]\n",
    "embedding_vocab_size = pretrained_embed.weight.shape[0]  # Vocab size from saved embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2597781b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT Vocab Size: 8102\n"
     ]
    }
   ],
   "source": [
    "print(f'TEXT Vocab Size: {len(TEXT.vocab.stoi)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aad6c8e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PART 3.4: TARGETED IMPROVEMENT FOR WEAK TOPICS\n",
      "================================================================================\n",
      "\n",
      "Strategies:\n",
      "  1. Data Augmentation for imbalanced classes (especially ABBR)\n",
      "  2. Positional Embeddings in attention layer\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PART 3.4: Targeted Improvement for Weak Topics\n",
    "# Strategy: Data Augmentation, Positional Embeddings\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 3.4: TARGETED IMPROVEMENT FOR WEAK TOPICS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nStrategies:\")\n",
    "print(\"  1. Data Augmentation for imbalanced classes (especially ABBR)\")\n",
    "print(\"  2. Positional Embeddings in attention layer\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Import required libraries for augmentation\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "try:\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c2db80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Step 2: Implementing Data Augmentation Functions...\n",
      "    ✓ Data augmentation functions ready\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Step 2: Data Augmentation Functions for Imbalanced Classes\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n>>> Step 2: Implementing Data Augmentation Functions...\")\n",
    "\n",
    "def get_synonyms(word):\n",
    "    \"\"\"Get synonyms for a word using WordNet\"\"\"\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonym = lemma.name().replace('_', ' ').lower()\n",
    "            if synonym != word and synonym.isalpha():\n",
    "                synonyms.add(synonym)\n",
    "    return list(synonyms)\n",
    "\n",
    "def synonym_replacement(tokens, n=1):\n",
    "    \"\"\"Replace n random words with their synonyms\"\"\"\n",
    "    new_tokens = tokens.copy()\n",
    "    words_to_replace = [i for i, word in enumerate(tokens) if word.isalpha() and len(word) > 2]\n",
    "    \n",
    "    if len(words_to_replace) == 0:\n",
    "        return tokens\n",
    "    \n",
    "    num_replacements = min(n, len(words_to_replace))\n",
    "    indices_to_replace = random.sample(words_to_replace, num_replacements)\n",
    "    \n",
    "    for idx in indices_to_replace:\n",
    "        synonyms = get_synonyms(tokens[idx])\n",
    "        if synonyms:\n",
    "            new_tokens[idx] = random.choice(synonyms)\n",
    "    \n",
    "    return new_tokens\n",
    "\n",
    "def random_insertion(tokens, n=1):\n",
    "    \"\"\"Randomly insert synonyms of n words\"\"\"\n",
    "    new_tokens = tokens.copy()\n",
    "    \n",
    "    for _ in range(n):\n",
    "        if len(new_tokens) == 0:\n",
    "            break\n",
    "        word = random.choice(new_tokens)\n",
    "        synonyms = get_synonyms(word)\n",
    "        if synonyms:\n",
    "            synonym = random.choice(synonyms)\n",
    "            insert_pos = random.randint(0, len(new_tokens))\n",
    "            new_tokens.insert(insert_pos, synonym)\n",
    "    \n",
    "    return new_tokens\n",
    "\n",
    "def random_deletion(tokens, p=0.1):\n",
    "    \"\"\"Randomly delete words with probability p\"\"\"\n",
    "    if len(tokens) == 1:\n",
    "        return tokens\n",
    "    \n",
    "    new_tokens = []\n",
    "    for token in tokens:\n",
    "        if random.random() > p:\n",
    "            new_tokens.append(token)\n",
    "    \n",
    "    if len(new_tokens) == 0:\n",
    "        return tokens[:1]\n",
    "    \n",
    "    return new_tokens\n",
    "\n",
    "def random_swap(tokens, n=1):\n",
    "    \"\"\"Randomly swap n pairs of words\"\"\"\n",
    "    new_tokens = tokens.copy()\n",
    "    \n",
    "    for _ in range(n):\n",
    "        if len(new_tokens) < 2:\n",
    "            break\n",
    "        idx1, idx2 = random.sample(range(len(new_tokens)), 2)\n",
    "        new_tokens[idx1], new_tokens[idx2] = new_tokens[idx2], new_tokens[idx1]\n",
    "    \n",
    "    return new_tokens\n",
    "\n",
    "def augment_text(text, augmentation_techniques=['synonym', 'insertion', 'deletion', 'swap'], \n",
    "                 num_augmentations=3):\n",
    "    \"\"\"Apply data augmentation to text\"\"\"\n",
    "    augmented_texts = []\n",
    "    \n",
    "    for _ in range(num_augmentations):\n",
    "        aug_text = text.copy()\n",
    "        technique = random.choice(augmentation_techniques)\n",
    "        \n",
    "        if technique == 'synonym' and len(aug_text) > 0:\n",
    "            aug_text = synonym_replacement(aug_text, n=random.randint(1, 2))\n",
    "        elif technique == 'insertion' and len(aug_text) > 0:\n",
    "            aug_text = random_insertion(aug_text, n=random.randint(1, 2))\n",
    "        elif technique == 'deletion' and len(aug_text) > 1:\n",
    "            aug_text = random_deletion(aug_text, p=0.1)\n",
    "        elif technique == 'swap' and len(aug_text) > 1:\n",
    "            aug_text = random_swap(aug_text, n=1)\n",
    "        \n",
    "        augmented_texts.append(aug_text)\n",
    "    \n",
    "    return augmented_texts\n",
    "\n",
    "print(\"    ✓ Data augmentation functions ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f8ddcf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Step 3: Applying Data Augmentation for Imbalanced Classes...\n",
      "\n",
      "Original label distribution:\n",
      "  ABBR: 69 samples (1.58%)\n",
      "  DESC: 930 samples (21.32%)\n",
      "  ENTY: 1000 samples (22.93%)\n",
      "  HUM: 978 samples (22.42%)\n",
      "  LOC: 668 samples (15.31%)\n",
      "  NUM: 717 samples (16.44%)\n",
      "\n",
      "  Augmenting NUM: 717 -> 850 samples\n",
      "    Generating 133 additional samples...\n",
      "    ✓ Generated 133 augmented samples\n",
      "\n",
      "  Augmenting HUM: 978 -> 1200 samples\n",
      "    Generating 222 additional samples...\n",
      "    ✓ Generated 222 augmented samples\n",
      "\n",
      "  Augmenting ENTY: 1000 -> 1300 samples\n",
      "    Generating 300 additional samples...\n",
      "    ✓ Generated 300 augmented samples\n",
      "\n",
      "  Augmenting LOC: 668 -> 800 samples\n",
      "    Generating 132 additional samples...\n",
      "    ✓ Generated 132 augmented samples\n",
      "\n",
      "  Augmenting ABBR: 69 -> 900 samples\n",
      "    Generating 831 additional samples...\n",
      "    ✓ Generated 831 augmented samples\n",
      "\n",
      "Augmented label distribution:\n",
      "  ABBR: 900 samples (15.05%)\n",
      "  DESC: 930 samples (15.55%)\n",
      "  ENTY: 1300 samples (21.74%)\n",
      "  HUM: 1200 samples (20.07%)\n",
      "  LOC: 800 samples (13.38%)\n",
      "  NUM: 850 samples (14.21%)\n",
      "\n",
      "  Total samples: 4362 -> 5980\n",
      "  ✓ Data augmentation complete\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# Step 3: Apply Data Augmentation for Imbalanced Classes\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n>>> Step 3: Applying Data Augmentation for Imbalanced Classes...\")\n",
    "\n",
    "# Count current label distribution\n",
    "label_counts_p34 = Counter([ex.label for ex in train_data.examples])\n",
    "print(f\"\\nOriginal label distribution:\")\n",
    "for label, count in sorted(label_counts_p34.items()):\n",
    "    print(f\"  {label}: {count} samples ({count/len(train_data.examples)*100:.2f}%)\")\n",
    "\n",
    "# Augmentation targets (boost weaker topics more aggressively)\n",
    "target_counts_p34 = {\n",
    "    'ABBR': 900,   # heavy boost (~13x) to improve weakest class\n",
    "    'DESC': 930,   # keep strong class unchanged\n",
    "    'ENTY': 1300,  # moderate boost (~1.3x)\n",
    "    'HUM': 1200,   # boost (~1.23x)\n",
    "    'LOC': 800,    # modest boost (~1.2x)\n",
    "    'NUM': 850     # modest boost (~1.2x)\n",
    "}\n",
    "\n",
    "# Create augmented examples\n",
    "augmented_examples = list(train_data.examples)  # Start with all original examples\n",
    "\n",
    "for label in label_counts_p34.keys():\n",
    "    current_count = label_counts_p34[label]\n",
    "    target_count = target_counts_p34[label]\n",
    "    \n",
    "    if current_count < target_count:\n",
    "        label_examples = [ex for ex in train_data.examples if ex.label == label]\n",
    "        num_augmentations_needed = target_count - current_count\n",
    "        \n",
    "        print(f\"\\n  Augmenting {label}: {current_count} -> {target_count} samples\")\n",
    "        print(f\"    Generating {num_augmentations_needed} additional samples...\")\n",
    "        \n",
    "        augmented_count = 0\n",
    "        while augmented_count < num_augmentations_needed:\n",
    "            original_ex = random.choice(label_examples)\n",
    "            aug_texts = augment_text(original_ex.text, num_augmentations=1)\n",
    "            \n",
    "            for aug_text in aug_texts:\n",
    "                if augmented_count >= num_augmentations_needed:\n",
    "                    break\n",
    "                \n",
    "                new_ex = data.Example.fromlist([aug_text, label], \n",
    "                                               fields=[('text', TEXT), ('label', LABEL)])\n",
    "                augmented_examples.append(new_ex)\n",
    "                augmented_count += 1\n",
    "        \n",
    "        print(f\"    ✓ Generated {augmented_count} augmented samples\")\n",
    "\n",
    "# Create augmented dataset with proper field structure\n",
    "augmented_train_data = data.Dataset(augmented_examples, fields=[('text', TEXT), ('label', LABEL)])\n",
    "\n",
    "# Verify augmented distribution\n",
    "new_label_counts = Counter([ex.label for ex in augmented_examples])\n",
    "print(f\"\\nAugmented label distribution:\")\n",
    "for label, count in sorted(new_label_counts.items()):\n",
    "    print(f\"  {label}: {count} samples ({count/len(augmented_examples)*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\n  Total samples: {len(train_data.examples)} -> {len(augmented_examples)}\")\n",
    "print(f\"  ✓ Data augmentation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ec4bc0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# augmented_train_data.examples[0].label\n",
    "[(ex.text, ex.label) for ex in augmented_train_data if ex.label not in ['ABBR','DESC','ENTY','HUM','LOC','NUM']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce04616c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(None,\n",
       "            {'ENTY': 0, 'HUM': 1, 'DESC': 2, 'NUM': 3, 'LOC': 4, 'ABBR': 5})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABEL.vocab.stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9afaf5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "abbr_aug_ex = [ex for ex in augmented_train_data if ex.label == \"ABBR\"]\n",
    "abbr_ex = [ex for ex in train_data if ex.label == \"ABBR\"]\n",
    "\n",
    "count = 0\n",
    "# for ex in abbr_aug_ex:\n",
    "#     if ex not in abbr_ex:\n",
    "#         print(ex.text)\n",
    "#         print(ex.label)\n",
    "#         count += 1\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "858eb2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label distribution in training set:\n",
      "- ABBR: 900 samples (15.05%)\n",
      "- DESC: 930 samples (15.55%)\n",
      "- ENTY: 1300 samples (21.74%)\n",
      "- HUM: 1200 samples (20.07%)\n",
      "- LOC: 800 samples (13.38%)\n",
      "- NUM: 850 samples (14.21%)\n"
     ]
    }
   ],
   "source": [
    "# Count how many samples per label in the train set\n",
    "label_counts_p34 = Counter([ex.label for ex in augmented_train_data.examples])\n",
    "total_examples_p34 = len(augmented_train_data)\n",
    "\n",
    "print(\"\\nLabel distribution in training set:\")\n",
    "for label, count in sorted(label_counts_p34.items()):\n",
    "    percentage = (count / total_examples_p34) * 100\n",
    "    print(f\"- {label}: {count} samples ({percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2926e83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PART 3.4: SAMPLING STRATEGIES VS DATA AUGMENTATION\n",
      "================================================================================\n",
      "Comparing text augmentation, weighted sampling, and their combination\n",
      "across the Simple RNN baseline (Part 2 best config) and the RNN + BERT hybrid.\n",
      "================================================================================\n",
      ">>> Simple RNN ready (mean aggregation baseline)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PART 3.5: Sampling Strategies vs Data Augmentation\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 3.4: SAMPLING STRATEGIES VS DATA AUGMENTATION\")\n",
    "print(\"=\"*80)\n",
    "print(\"Comparing text augmentation, weighted sampling, and their combination\")\n",
    "print(\"across the Simple RNN baseline (Part 2 best config) and the RNN + BERT hybrid.\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Extended RNN Classifier with multiple aggregation methods\n",
    "class RNN_Classifier_Aggregation(nn.Module):\n",
    "    \"\"\"\n",
    "    RNN for topic classification with multiple aggregation strategies.\n",
    "    Uses pretrained embeddings (learnable/updated during training).\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, \n",
    "                 n_layers=1, dropout=0.0, padding_idx=0, pretrained_embeddings=None,\n",
    "                 aggregation='mean'):\n",
    "        super(RNN_Classifier_Aggregation, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.aggregation = aggregation  # 'last', 'mean', 'max'\n",
    "        \n",
    "        # Embedding layer initialized with pretrained embeddings\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "            self.embedding.weight.requires_grad = True\n",
    "        \n",
    "        # RNN layer\n",
    "        self.rnn = nn.RNN(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=n_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.0  # No dropout in baseline\n",
    "        )\n",
    "                \n",
    "        # Fully connected output layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, text, text_lengths):        \n",
    "        # Embed the input\n",
    "        embedded = self.embedding(text)  # [batch_size, seq_len, embedding_dim]\n",
    "        \n",
    "        # Get dimensions\n",
    "        batch_size = embedded.size(0)\n",
    "        seq_len = embedded.size(1)\n",
    "        \n",
    "        # Handle text_lengths\n",
    "        text_lengths_flat = text_lengths.flatten().cpu().long()\n",
    "        if len(text_lengths_flat) != batch_size:\n",
    "            raise ValueError(\n",
    "                f\"text_lengths size mismatch: got {len(text_lengths_flat)} elements, \"\n",
    "                f\"expected {batch_size}\"\n",
    "            )\n",
    "        \n",
    "        # Clamp lengths\n",
    "        text_lengths_clamped = torch.clamp(text_lengths_flat, min=1, max=seq_len)\n",
    "        \n",
    "        text_lengths_clamped_device = text_lengths_clamped.to(text.device)\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, text_lengths_clamped, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        \n",
    "        # Pass through RNN\n",
    "        packed_output, hidden = self.rnn(packed_embedded)\n",
    "        \n",
    "        # Aggregate word representations to sentence representation\n",
    "        if self.aggregation == 'last':\n",
    "            sentence_repr = hidden[-1]  # [batch_size, hidden_dim]\n",
    "            \n",
    "        elif self.aggregation == 'mean':\n",
    "            output, output_lengths = nn.utils.rnn.pad_packed_sequence(\n",
    "                packed_output, batch_first=True\n",
    "            )\n",
    "            # output: [batch_size, seq_len, hidden_dim]\n",
    "            \n",
    "            # Create mask for padding\n",
    "            mask = torch.arange(seq_len, device=text.device).unsqueeze(0) < text_lengths_clamped_device.unsqueeze(1)\n",
    "            mask = mask.unsqueeze(2).float()  # [batch_size, seq_len, 1]\n",
    "            \n",
    "            # Apply mask and compute mean\n",
    "            masked_output = output * mask\n",
    "            sum_output = masked_output.sum(dim=1)  # [batch_size, hidden_dim]\n",
    "            sentence_repr = sum_output / text_lengths_clamped_device.unsqueeze(1).float()\n",
    "            \n",
    "        elif self.aggregation == 'max':\n",
    "            # Max pooling over all outputs\n",
    "            output, output_lengths = nn.utils.rnn.pad_packed_sequence(\n",
    "                packed_output, batch_first=True\n",
    "            )\n",
    "            # output: [batch_size, seq_len, hidden_dim]\n",
    "            \n",
    "            mask = torch.arange(seq_len, device=text.device).unsqueeze(0) < text_lengths_clamped_device.unsqueeze(1)\n",
    "            mask = mask.unsqueeze(2).float()  # [batch_size, seq_len, 1]\n",
    "            \n",
    "            masked_output = output * mask + (1 - mask) * float('-inf')\n",
    "            sentence_repr, _ = torch.max(masked_output, dim=1)\n",
    "        \n",
    "        # Pass through fully connected layer\n",
    "        output = self.fc(sentence_repr)  # [batch_size, output_dim]\n",
    "        \n",
    "        return output\n",
    "\n",
    "print(\">>> Simple RNN ready (mean aggregation baseline)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02842886",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7904aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Transformers library available\n",
      ">>> RNNBertClassifier ready (Part 3.3 best model)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Hybrid Model: RNN + BERT with Attention (Part 3.3 best model)\n",
    "# ============================================================================\n",
    "\n",
    "try:\n",
    "    from transformers import BertModel, BertTokenizer\n",
    "    BERT_AVAILABLE = True\n",
    "    print(\">>> Transformers library available\")\n",
    "except ImportError:\n",
    "    BERT_AVAILABLE = False\n",
    "    print(\">>> Warning: transformers library not found. Install `transformers` to run BERT experiments.\")\n",
    "\n",
    "\n",
    "class RNNBertClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    RNN with Pretrained BERT embeddings\n",
    "    Uses BERT to get contextualized embeddings, then passes through BiLSTM with attention\n",
    "    \"\"\"\n",
    "    def __init__(self, output_dim, hidden_dim=256, n_layers=2, dropout=0.5,\n",
    "                 bert_model_name='distilbert-base-uncased', freeze_bert=False):\n",
    "        super(RNNBertClassifier, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.freeze_bert = freeze_bert\n",
    "        \n",
    "        # Load pretrained BERT model and tokenizer\n",
    "        self.bert_tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "        self.bert_model = BertModel.from_pretrained(bert_model_name)\n",
    "        \n",
    "        # Freeze BERT parameters if specified\n",
    "        if freeze_bert:\n",
    "            for param in self.bert_model.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # BERT output dimension (768 for bert-base-uncased)\n",
    "        bert_output_dim = self.bert_model.config.hidden_size\n",
    "        \n",
    "        # Bidirectional LSTM to process BERT embeddings\n",
    "        self.bilstm = nn.LSTM(\n",
    "            bert_output_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=n_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout if n_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Additive Attention Mechanism (Bahdanau-style)\n",
    "        self.attention_linear1 = nn.Linear(hidden_dim * 2, hidden_dim)  # *2 for bidirectional\n",
    "        self.attention_linear2 = nn.Linear(hidden_dim, 1)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Fully connected output layer\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        \n",
    "    def forward(self, text, text_lengths, text_vocab=None):        \n",
    "        batch_size = text.size(0)\n",
    "        seq_len = text.size(1)\n",
    "        device = text.device\n",
    "        \n",
    "        # Convert token indices back to text strings using vocab\n",
    "        text_list = []\n",
    "        for i in range(batch_size):\n",
    "            actual_len = text_lengths[i].item() if isinstance(text_lengths[i], torch.Tensor) else text_lengths[i]\n",
    "            tokens = []\n",
    "            for j in range(min(actual_len, seq_len)):\n",
    "                token_idx = text[i, j].item()\n",
    "                if text_vocab is not None and token_idx < len(text_vocab):\n",
    "                    token = text_vocab[token_idx]\n",
    "                    # Skip special tokens\n",
    "                    if token not in ['<pad>', '<unk>', '<sos>', '<eos>']:\n",
    "                        tokens.append(token)\n",
    "                else:\n",
    "                    # Fallback if vocab not provided\n",
    "                    tokens.append(str(token_idx))\n",
    "            # Join tokens to form sentence\n",
    "            sentence = \" \".join(tokens)\n",
    "            text_list.append(sentence)\n",
    "        \n",
    "        # Tokenize with BERT\n",
    "        encoded = self.bert_tokenizer(\n",
    "            text_list,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors='pt'\n",
    "        ).to(device)\n",
    "        \n",
    "        # Get BERT embeddings\n",
    "        with torch.set_grad_enabled(not self.freeze_bert):\n",
    "            bert_outputs = self.bert_model(**encoded)\n",
    "            bert_embeddings = bert_outputs.last_hidden_state  # [batch_size, seq_len, 768]\n",
    "        \n",
    "        # Get actual sequence lengths from BERT tokenizer\n",
    "        bert_lengths = encoded['attention_mask'].sum(dim=1).cpu()\n",
    "\n",
    "\n",
    "        # Get BERT embeddings\n",
    "        with torch.set_grad_enabled(not self.freeze_bert):\n",
    "            bert_outputs = self.bert_model(**encoded)\n",
    "            bert_embeddings = bert_outputs.last_hidden_state  # [batch_size, seq_len, 768]\n",
    "\n",
    "        # Align lengths\n",
    "        bert_lengths = encoded['attention_mask'].sum(dim=1)\n",
    "        max_len = bert_embeddings.size(1)\n",
    "        bert_lengths = bert_lengths.clamp(max=max_len).cpu()\n",
    "\n",
    "        # Pack safely\n",
    "        packed_bert = nn.utils.rnn.pack_padded_sequence(\n",
    "        bert_embeddings, bert_lengths, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "\n",
    "        \n",
    "        # Pack sequences for efficient RNN processing\n",
    "        packed_bert = nn.utils.rnn.pack_padded_sequence(\n",
    "            bert_embeddings, bert_lengths, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        \n",
    "        # Pass through bidirectional LSTM\n",
    "        packed_output, (hidden, cell) = self.bilstm(packed_bert)\n",
    "        \n",
    "        # Unpack sequences\n",
    "        bilstm_output, output_lengths = nn.utils.rnn.pad_packed_sequence(\n",
    "            packed_output, batch_first=True\n",
    "        )\n",
    "        # bilstm_output: [batch_size, seq_len, hidden_dim * 2]\n",
    "        \n",
    "        # Apply Attention Mechanism\n",
    "        attention_scores = self.attention_linear1(bilstm_output)  # [batch_size, seq_len, hidden_dim]\n",
    "        attention_scores = self.tanh(attention_scores)\n",
    "        attention_scores = self.attention_linear2(attention_scores).squeeze(2)  # [batch_size, seq_len]\n",
    "        \n",
    "        # Mask padding positions\n",
    "        batch_size_attn, seq_len_attn = bilstm_output.size(0), bilstm_output.size(1)\n",
    "        mask = torch.arange(seq_len_attn, device=device).unsqueeze(0) < bert_lengths.unsqueeze(1).to(device)\n",
    "        attention_scores = attention_scores.masked_fill(~mask, float('-inf'))\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        attention_weights = torch.softmax(attention_scores, dim=1).unsqueeze(2)  # [batch_size, seq_len, 1]\n",
    "        \n",
    "        # Compute weighted sum\n",
    "        context_vector = torch.sum(attention_weights * bilstm_output, dim=1)  # [batch_size, hidden_dim * 2]\n",
    "        \n",
    "        # Apply dropout\n",
    "        context_vector = self.dropout(context_vector)\n",
    "        \n",
    "        # Pass through fully connected layer\n",
    "        output = self.fc(context_vector)  # [batch_size, output_dim]\n",
    "        \n",
    "        return output\n",
    "\n",
    "print(\">>> RNNBertClassifier ready (Part 3.3 best model)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dcda8508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================\n",
    "# Helper: Topic-wise evaluation\n",
    "# =========================================================================\n",
    "\n",
    "def evaluate_per_topic_p35(model, iterator, device, text_vocab=None):\n",
    "    \"\"\"Evaluate accuracy per topic on the provided iterator.\"\"\"\n",
    "    model.eval()\n",
    "    topic_correct = defaultdict(int)\n",
    "    topic_total = defaultdict(int)\n",
    "    idx_to_label = LABEL.vocab.itos\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "            if text_vocab is not None:\n",
    "                predictions = model(text, text_lengths, text_vocab=text_vocab)\n",
    "            else:\n",
    "                predictions = model(text, text_lengths)\n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            for pred, label in zip(preds.cpu().numpy(), labels.cpu().numpy()):\n",
    "                topic_name = idx_to_label[label]\n",
    "                topic_total[topic_name] += 1\n",
    "                if pred == label:\n",
    "                    topic_correct[topic_name] += 1\n",
    "\n",
    "    topic_metrics = {}\n",
    "    for topic in sorted(topic_total.keys()):\n",
    "        total = topic_total[topic]\n",
    "        correct = topic_correct[topic]\n",
    "        accuracy = correct / total if total > 0 else 0.0\n",
    "        topic_metrics[topic] = {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"correct\": correct,\n",
    "            \"total\": total,\n",
    "        }\n",
    "    return topic_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0220a7b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Preparing dataset variants for Part 3.4 experiments...\n",
      "  - Original train: 4362 samples\n",
      "      ABBR: 69 (1.58%)\n",
      "      DESC: 930 (21.32%)\n",
      "      ENTY: 1000 (22.93%)\n",
      "      HUM: 978 (22.42%)\n",
      "      LOC: 668 (15.31%)\n",
      "      NUM: 717 (16.44%)\n",
      "  - Augmented train: 5980 samples\n",
      "      ABBR: 900 (15.05%)\n",
      "      DESC: 930 (15.55%)\n",
      "      ENTY: 1300 (21.74%)\n",
      "      HUM: 1200 (20.07%)\n",
      "      LOC: 800 (13.38%)\n",
      "      NUM: 850 (14.21%)\n",
      "  - Weighted-sampled train: 4362 samples\n",
      "      ABBR: 798 (18.29%)\n",
      "      DESC: 577 (13.23%)\n",
      "      ENTY: 845 (19.37%)\n",
      "      HUM: 715 (16.39%)\n",
      "      LOC: 724 (16.60%)\n",
      "      NUM: 703 (16.12%)\n",
      "  - Augmented + weighted train: 5980 samples\n",
      "      ABBR: 1092 (18.26%)\n",
      "      DESC: 804 (13.44%)\n",
      "      ENTY: 1222 (20.43%)\n",
      "      HUM: 919 (15.37%)\n",
      "      LOC: 960 (16.05%)\n",
      "      NUM: 983 (16.44%)\n",
      "\n",
      ">>> Dataset variants ready. Criterion initialised for upcoming runs.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Dataset Variants & Utilities for Experiments\n",
    "# ============================================================================\n",
    "\n",
    "def describe_dataset(name, dataset):\n",
    "    counts = Counter(ex.label for ex in dataset.examples)\n",
    "    total = len(dataset.examples)\n",
    "    print(f\"  - {name}: {total} samples\")\n",
    "    for label, count in sorted(counts.items()):\n",
    "        print(f\"      {label}: {count} ({count/total*100:.2f}%)\")\n",
    "    return counts\n",
    "\n",
    "\n",
    "# Topic-wise accuracy from latest weighted-sampler evaluation (used to boost weak classes)\n",
    "P35_TOPIC_ACCURACY = {\n",
    "    \"ABBR\": 0.7778,\n",
    "    \"DESC\": 0.9855,\n",
    "    \"ENTY\": 0.7128,\n",
    "    \"HUM\": 0.8769,\n",
    "    \"LOC\": 0.8889,\n",
    "    \"NUM\": 0.8584,\n",
    "}\n",
    "# Convert to difficulty scores (higher when accuracy is lower)\n",
    "P35_TOPIC_DIFFICULTY = {label: max(0.0, 1.0 - acc) for label, acc in P35_TOPIC_ACCURACY.items()}\n",
    "# Global multiplier for difficulty adjustment; tweak to emphasise weak topics more/less\n",
    "P35_DIFFICULTY_SCALE = 2.0\n",
    "\n",
    "\n",
    "def create_weighted_dataset(source_dataset, target_size=None, seed=SEED, difficulty_scale=P35_DIFFICULTY_SCALE):\n",
    "    \"\"\"Mimic WeightedRandomSampler by sampling examples according to class weights, with extra boosts for weak topics.\"\"\"\n",
    "    rng = random.Random(seed)\n",
    "    counts = Counter(ex.label for ex in source_dataset.examples)\n",
    "    total = sum(counts.values())\n",
    "    base_class_weights = {label: total / count for label, count in counts.items()}\n",
    "\n",
    "    class_boosts = {\n",
    "        label: 1.0 + difficulty_scale * P35_TOPIC_DIFFICULTY.get(label, 0.0)\n",
    "        for label in counts.keys()\n",
    "    }\n",
    "\n",
    "    weights = [base_class_weights[ex.label] * class_boosts.get(ex.label, 1.0) for ex in source_dataset.examples]\n",
    "    sample_size = target_size or len(source_dataset.examples)\n",
    "\n",
    "    sampled_examples = rng.choices(source_dataset.examples, weights=weights, k=sample_size)\n",
    "    fields = [('text', TEXT), ('label', LABEL)]\n",
    "    return data.Dataset(sampled_examples, fields=fields)\n",
    "\n",
    "\n",
    "print(\"\\n>>> Preparing dataset variants for Part 3.4 experiments...\")\n",
    "base_counts = describe_dataset(\"Original train\", train_data)\n",
    "aug_counts = describe_dataset(\"Augmented train\", augmented_train_data)\n",
    "\n",
    "weighted_train_data = create_weighted_dataset(train_data)\n",
    "weighted_counts = describe_dataset(\"Weighted-sampled train\", weighted_train_data)\n",
    "\n",
    "augmented_weighted_train_data = create_weighted_dataset(augmented_train_data)\n",
    "aug_weighted_counts = describe_dataset(\"Augmented + weighted train\", augmented_weighted_train_data)\n",
    "\n",
    "p35_datasets = {\n",
    "    \"original\": train_data,\n",
    "    \"augmented\": augmented_train_data,\n",
    "    \"weighted\": weighted_train_data,\n",
    "    \"augmented_weighted\": augmented_weighted_train_data,\n",
    "}\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "p35_results = {\n",
    "    \"simple_rnn_baseline\": {},\n",
    "    \"rnn_bert\": {}\n",
    "}\n",
    "\n",
    "print(\"\\n>>> Dataset variants ready. Criterion initialised for upcoming runs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda6694d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Simple RNN (mean pooling) experiment runner\n",
    "# ============================================================================\n",
    "\n",
    "def reset_random_seeds(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def build_iterator(dataset, batch_size, shuffle):\n",
    "    return data.BucketIterator(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        sort_key=lambda x: len(x.text),\n",
    "        sort_within_batch=True,\n",
    "        shuffle=shuffle,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "RNN_BASE_HYPERPARM = hyperparam_prep()\n",
    "RNN_BASE_HYPERPARM['HIDDEN_DIM'] *= 2\n",
    "RNN_BASE_HYPERPARM['N_LAYERS'] = 1\n",
    "RNN_BASE_HYPERPARM['SAVE_MODEL'] = True\n",
    "\n",
    "\n",
    "def run_simple_rnn_experiment(dataset_key, description, save_suffix):\n",
    "    if dataset_key not in p35_datasets:\n",
    "        raise ValueError(f\"Unknown dataset key: {dataset_key}\")\n",
    "\n",
    "    reset_random_seeds(SEED)\n",
    "    train_dataset = p35_datasets[dataset_key]\n",
    "\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(f\"Running Simple RNN (mean pooling) experiment: {description}\")\n",
    "    print(\"-\"*80)\n",
    "\n",
    "    train_iter = build_iterator(train_dataset, RNN_BASE_HYPERPARM['BATCH_SIZE'], shuffle=True)\n",
    "    val_iter = build_iterator(validation_data, RNN_BASE_HYPERPARM['BATCH_SIZE'], shuffle=False)\n",
    "    test_iter = build_iterator(test_data, RNN_BASE_HYPERPARM['BATCH_SIZE'], shuffle=False)\n",
    "\n",
    "    model = RNN_Classifier_Aggregation(\n",
    "        vocab_size=len(TEXT.vocab),\n",
    "        embedding_dim=embedding_dim,\n",
    "        hidden_dim=RNN_BASE_HYPERPARM['HIDDEN_DIM'],\n",
    "        output_dim=num_classes,\n",
    "        n_layers=RNN_BASE_HYPERPARM['N_LAYERS'],\n",
    "        dropout=RNN_BASE_HYPERPARM['DROPOUT'],\n",
    "        padding_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "        pretrained_embeddings=pretrained_embeddings,\n",
    "        aggregation=RNN_BASE_HYPERPARM['AGGREGATOR'],\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.RMSprop(\n",
    "        model.parameters(),\n",
    "        lr=RNN_BASE_HYPERPARM['LEARNING_RATE'],\n",
    "        weight_decay=RNN_BASE_HYPERPARM['L2_LAMBDA'],\n",
    "    )\n",
    "\n",
    "    model, history = train_model_with_history(\n",
    "        model,\n",
    "        train_iter,\n",
    "        val_iter,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        RNN_BASE_HYPERPARM['N_EPOCHS'],\n",
    "        device,\n",
    "        num_classes,\n",
    "        RNN_BASE_HYPERPARM['L1_LAMBDA'],\n",
    "        patience=RNN_BASE_HYPERPARM['PATIENCE'],\n",
    "        model_name=f\"Simple RNN ({description})\",\n",
    "    )\n",
    "\n",
    "    test_loss, test_acc, test_f1, test_auc = evaluate_model(\n",
    "        model,\n",
    "        test_iter,\n",
    "        criterion,\n",
    "        device,\n",
    "        f\"Simple RNN ({description})\",\n",
    "        num_classes,\n",
    "    )\n",
    "\n",
    "    topic_metrics = evaluate_per_topic_p35(model, test_iter, device)\n",
    "\n",
    "    model_path = f\"weights/part35_simple_rnn_{save_suffix}.pt\"\n",
    "    if RNN_BASE_HYPERPARM['SAVE_MODEL']:\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    p35_results[\"simple_rnn_baseline\"][save_suffix] = {\n",
    "        \"description\": description,\n",
    "        \"dataset_key\": dataset_key,\n",
    "        \"history\": history,\n",
    "        \"test_metrics\": {\n",
    "            \"loss\": test_loss,\n",
    "            \"accuracy\": test_acc,\n",
    "            \"f1\": test_f1,\n",
    "            \"auc\": test_auc,\n",
    "        },\n",
    "        \"topic_metrics\": topic_metrics,\n",
    "        \"model_path\": model_path if RNN_BASE_HYPERPARM['SAVE_MODEL'] else None,\n",
    "        \"model\": model,\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"history\": history,\n",
    "        \"topic_metrics\": topic_metrics,\n",
    "        \"test_metrics\": {\n",
    "            \"loss\": test_loss,\n",
    "            \"accuracy\": test_acc,\n",
    "            \"f1\": test_f1,\n",
    "            \"auc\": test_auc,\n",
    "        },\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8bff8e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Executing Simple RNN experiments...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Running Simple RNN (mean pooling) experiment: Text Augmentation\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      ">>> Training Simple RNN (Text Augmentation)\n",
      "    Parameters: 2,850,446\n",
      "    Max epochs: 100, Patience: 10\n",
      "Epoch: 01/100 | Time: 0m 3s\n",
      "\tTrain Loss: 1.4262 | Train Acc: 48.04%\n",
      "\tVal Loss: 1.2242 | Val Acc: 51.74% | Val F1: 0.5132 | Val AUC: 0.8446\n",
      "Epoch: 02/100 | Time: 0m 2s\n",
      "\tTrain Loss: 1.0686 | Train Acc: 64.80%\n",
      "\tVal Loss: 1.0033 | Val Acc: 64.77% | Val F1: 0.6561 | Val AUC: 0.8898\n",
      "Epoch: 03/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.8441 | Train Acc: 75.57%\n",
      "\tVal Loss: 0.8863 | Val Acc: 71.28% | Val F1: 0.7231 | Val AUC: 0.9143\n",
      "Epoch: 04/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.6761 | Train Acc: 82.02%\n",
      "\tVal Loss: 0.9673 | Val Acc: 67.61% | Val F1: 0.6837 | Val AUC: 0.8970\n",
      "Epoch: 05/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.5800 | Train Acc: 85.67%\n",
      "\tVal Loss: 0.7847 | Val Acc: 74.31% | Val F1: 0.7575 | Val AUC: 0.9321\n",
      "Epoch: 06/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.4907 | Train Acc: 87.79%\n",
      "\tVal Loss: 0.9829 | Val Acc: 68.26% | Val F1: 0.6966 | Val AUC: 0.9044\n",
      "Epoch: 07/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.4127 | Train Acc: 89.80%\n",
      "\tVal Loss: 0.7082 | Val Acc: 78.99% | Val F1: 0.8011 | Val AUC: 0.9508\n",
      "Epoch: 08/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.3642 | Train Acc: 91.99%\n",
      "\tVal Loss: 0.6336 | Val Acc: 81.19% | Val F1: 0.8222 | Val AUC: 0.9537\n",
      "Epoch: 09/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.3226 | Train Acc: 93.44%\n",
      "\tVal Loss: 0.6616 | Val Acc: 78.72% | Val F1: 0.7942 | Val AUC: 0.9510\n",
      "Epoch: 10/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.2681 | Train Acc: 94.60%\n",
      "\tVal Loss: 0.7176 | Val Acc: 79.63% | Val F1: 0.8012 | Val AUC: 0.9447\n",
      "Epoch: 11/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.2457 | Train Acc: 95.28%\n",
      "\tVal Loss: 0.6516 | Val Acc: 82.02% | Val F1: 0.8283 | Val AUC: 0.9552\n",
      "Epoch: 12/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.2582 | Train Acc: 94.62%\n",
      "\tVal Loss: 0.7469 | Val Acc: 79.17% | Val F1: 0.8004 | Val AUC: 0.9428\n",
      "Epoch: 13/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.1938 | Train Acc: 96.51%\n",
      "\tVal Loss: 0.6174 | Val Acc: 83.03% | Val F1: 0.8357 | Val AUC: 0.9583\n",
      "Epoch: 14/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.1780 | Train Acc: 96.67%\n",
      "\tVal Loss: 0.7556 | Val Acc: 79.45% | Val F1: 0.8078 | Val AUC: 0.9478\n",
      "Epoch: 15/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.1582 | Train Acc: 97.19%\n",
      "\tVal Loss: 0.6874 | Val Acc: 81.93% | Val F1: 0.8248 | Val AUC: 0.9581\n",
      "Epoch: 16/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.1778 | Train Acc: 96.45%\n",
      "\tVal Loss: 0.7544 | Val Acc: 82.66% | Val F1: 0.8331 | Val AUC: 0.9576\n",
      "Epoch: 17/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.1406 | Train Acc: 97.44%\n",
      "\tVal Loss: 0.6336 | Val Acc: 82.66% | Val F1: 0.8315 | Val AUC: 0.9589\n",
      "Epoch: 18/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.1217 | Train Acc: 97.86%\n",
      "\tVal Loss: 0.9645 | Val Acc: 78.26% | Val F1: 0.7949 | Val AUC: 0.9500\n",
      "Epoch: 19/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.1447 | Train Acc: 97.12%\n",
      "\tVal Loss: 0.7580 | Val Acc: 82.39% | Val F1: 0.8318 | Val AUC: 0.9569\n",
      "Epoch: 20/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.1080 | Train Acc: 98.08%\n",
      "\tVal Loss: 0.8343 | Val Acc: 82.29% | Val F1: 0.8265 | Val AUC: 0.9546\n",
      "Epoch: 21/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.1118 | Train Acc: 98.14%\n",
      "\tVal Loss: 0.7771 | Val Acc: 81.65% | Val F1: 0.8252 | Val AUC: 0.9577\n",
      "Epoch: 22/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.1034 | Train Acc: 98.04%\n",
      "\tVal Loss: 0.7517 | Val Acc: 81.83% | Val F1: 0.8260 | Val AUC: 0.9554\n",
      "Epoch: 23/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.1033 | Train Acc: 98.34%\n",
      "\tVal Loss: 0.7862 | Val Acc: 81.93% | Val F1: 0.8307 | Val AUC: 0.9586\n",
      "\t>>> Early stopping at epoch 23, best val acc: 83.03%\n",
      "\n",
      ">>> Training completed! Best validation accuracy: 83.03%\n",
      "    Best validation F1: 0.8357\n",
      "    Best validation AUC-ROC: 0.9583\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Running Simple RNN (mean pooling) experiment: Weighted Sampling\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      ">>> Training Simple RNN (Weighted Sampling)\n",
      "    Parameters: 2,850,446\n",
      "    Max epochs: 100, Patience: 10\n",
      "Epoch: 01/100 | Time: 0m 2s\n",
      "\tTrain Loss: 1.4607 | Train Acc: 48.65%\n",
      "\tVal Loss: 1.2245 | Val Acc: 55.78% | Val F1: 0.5511 | Val AUC: 0.8523\n",
      "Epoch: 02/100 | Time: 0m 2s\n",
      "\tTrain Loss: 1.0048 | Train Acc: 69.49%\n",
      "\tVal Loss: 1.0186 | Val Acc: 65.87% | Val F1: 0.6711 | Val AUC: 0.8944\n",
      "Epoch: 03/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.7964 | Train Acc: 76.87%\n",
      "\tVal Loss: 0.8852 | Val Acc: 69.82% | Val F1: 0.7118 | Val AUC: 0.9194\n",
      "Epoch: 04/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.6572 | Train Acc: 81.38%\n",
      "\tVal Loss: 0.8557 | Val Acc: 66.06% | Val F1: 0.6604 | Val AUC: 0.9250\n",
      "Epoch: 05/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.5590 | Train Acc: 85.53%\n",
      "\tVal Loss: 0.8744 | Val Acc: 67.98% | Val F1: 0.6890 | Val AUC: 0.9227\n",
      "Epoch: 06/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.4658 | Train Acc: 87.76%\n",
      "\tVal Loss: 0.7599 | Val Acc: 72.39% | Val F1: 0.7454 | Val AUC: 0.9354\n",
      "Epoch: 07/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.4139 | Train Acc: 90.07%\n",
      "\tVal Loss: 0.9900 | Val Acc: 67.34% | Val F1: 0.6916 | Val AUC: 0.9014\n",
      "Epoch: 08/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.3447 | Train Acc: 92.23%\n",
      "\tVal Loss: 0.7721 | Val Acc: 74.22% | Val F1: 0.7552 | Val AUC: 0.9414\n",
      "Epoch: 09/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.3492 | Train Acc: 91.70%\n",
      "\tVal Loss: 0.7516 | Val Acc: 76.79% | Val F1: 0.7820 | Val AUC: 0.9469\n",
      "Epoch: 10/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.2785 | Train Acc: 94.09%\n",
      "\tVal Loss: 0.7700 | Val Acc: 74.31% | Val F1: 0.7523 | Val AUC: 0.9427\n",
      "Epoch: 11/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.2266 | Train Acc: 95.51%\n",
      "\tVal Loss: 0.7457 | Val Acc: 76.33% | Val F1: 0.7759 | Val AUC: 0.9485\n",
      "Epoch: 12/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.2375 | Train Acc: 94.77%\n",
      "\tVal Loss: 0.9473 | Val Acc: 74.13% | Val F1: 0.7444 | Val AUC: 0.9356\n",
      "Epoch: 13/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.1886 | Train Acc: 96.17%\n",
      "\tVal Loss: 0.7164 | Val Acc: 77.34% | Val F1: 0.7809 | Val AUC: 0.9510\n",
      "Epoch: 14/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.1807 | Train Acc: 96.56%\n",
      "\tVal Loss: 0.9696 | Val Acc: 74.59% | Val F1: 0.7398 | Val AUC: 0.9243\n",
      "Epoch: 15/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.1549 | Train Acc: 96.95%\n",
      "\tVal Loss: 1.1857 | Val Acc: 70.00% | Val F1: 0.7303 | Val AUC: 0.9180\n",
      "Epoch: 16/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.1687 | Train Acc: 96.42%\n",
      "\tVal Loss: 0.9111 | Val Acc: 77.25% | Val F1: 0.7725 | Val AUC: 0.9415\n",
      "Epoch: 17/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.1478 | Train Acc: 97.13%\n",
      "\tVal Loss: 0.7670 | Val Acc: 78.90% | Val F1: 0.7944 | Val AUC: 0.9513\n",
      "Epoch: 18/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.1264 | Train Acc: 97.82%\n",
      "\tVal Loss: 0.8200 | Val Acc: 79.63% | Val F1: 0.7926 | Val AUC: 0.9516\n",
      "Epoch: 19/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0909 | Train Acc: 98.74%\n",
      "\tVal Loss: 1.0312 | Val Acc: 76.15% | Val F1: 0.7635 | Val AUC: 0.9382\n",
      "Epoch: 20/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.1506 | Train Acc: 97.25%\n",
      "\tVal Loss: 0.8655 | Val Acc: 80.18% | Val F1: 0.8038 | Val AUC: 0.9481\n",
      "Epoch: 21/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0722 | Train Acc: 99.11%\n",
      "\tVal Loss: 0.8542 | Val Acc: 80.73% | Val F1: 0.8088 | Val AUC: 0.9497\n",
      "Epoch: 22/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0870 | Train Acc: 98.81%\n",
      "\tVal Loss: 0.8222 | Val Acc: 80.92% | Val F1: 0.8102 | Val AUC: 0.9500\n",
      "Epoch: 23/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.1045 | Train Acc: 98.17%\n",
      "\tVal Loss: 0.8176 | Val Acc: 81.01% | Val F1: 0.8127 | Val AUC: 0.9507\n",
      "Epoch: 24/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0608 | Train Acc: 99.27%\n",
      "\tVal Loss: 0.8955 | Val Acc: 81.01% | Val F1: 0.8115 | Val AUC: 0.9493\n",
      "Epoch: 25/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0963 | Train Acc: 98.19%\n",
      "\tVal Loss: 0.9738 | Val Acc: 77.71% | Val F1: 0.7858 | Val AUC: 0.9421\n",
      "Epoch: 26/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0571 | Train Acc: 99.47%\n",
      "\tVal Loss: 1.1176 | Val Acc: 75.60% | Val F1: 0.7575 | Val AUC: 0.9358\n",
      "Epoch: 27/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.1129 | Train Acc: 98.40%\n",
      "\tVal Loss: 0.9175 | Val Acc: 81.65% | Val F1: 0.8184 | Val AUC: 0.9484\n",
      "Epoch: 28/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0436 | Train Acc: 99.77%\n",
      "\tVal Loss: 0.9236 | Val Acc: 81.65% | Val F1: 0.8180 | Val AUC: 0.9479\n",
      "Epoch: 29/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0994 | Train Acc: 98.14%\n",
      "\tVal Loss: 0.9017 | Val Acc: 81.93% | Val F1: 0.8200 | Val AUC: 0.9521\n",
      "Epoch: 30/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0640 | Train Acc: 99.13%\n",
      "\tVal Loss: 0.8829 | Val Acc: 79.54% | Val F1: 0.7960 | Val AUC: 0.9446\n",
      "Epoch: 31/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0398 | Train Acc: 99.75%\n",
      "\tVal Loss: 0.8893 | Val Acc: 81.93% | Val F1: 0.8203 | Val AUC: 0.9501\n",
      "Epoch: 32/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0800 | Train Acc: 98.83%\n",
      "\tVal Loss: 0.9891 | Val Acc: 80.37% | Val F1: 0.8045 | Val AUC: 0.9452\n",
      "Epoch: 33/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0395 | Train Acc: 99.72%\n",
      "\tVal Loss: 0.9489 | Val Acc: 81.56% | Val F1: 0.8167 | Val AUC: 0.9489\n",
      "Epoch: 34/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0603 | Train Acc: 99.36%\n",
      "\tVal Loss: 1.8268 | Val Acc: 68.07% | Val F1: 0.6572 | Val AUC: 0.9023\n",
      "Epoch: 35/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0998 | Train Acc: 98.03%\n",
      "\tVal Loss: 0.8964 | Val Acc: 80.46% | Val F1: 0.8069 | Val AUC: 0.9464\n",
      "Epoch: 36/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0334 | Train Acc: 99.82%\n",
      "\tVal Loss: 0.9563 | Val Acc: 81.74% | Val F1: 0.8191 | Val AUC: 0.9475\n",
      "Epoch: 37/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0340 | Train Acc: 99.77%\n",
      "\tVal Loss: 1.0685 | Val Acc: 80.83% | Val F1: 0.8091 | Val AUC: 0.9459\n",
      "Epoch: 38/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.1567 | Train Acc: 97.13%\n",
      "\tVal Loss: 0.9614 | Val Acc: 80.00% | Val F1: 0.8008 | Val AUC: 0.9448\n",
      "Epoch: 39/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0353 | Train Acc: 99.79%\n",
      "\tVal Loss: 1.0353 | Val Acc: 81.19% | Val F1: 0.8131 | Val AUC: 0.9461\n",
      "\t>>> Early stopping at epoch 39, best val acc: 81.93%\n",
      "\n",
      ">>> Training completed! Best validation accuracy: 81.93%\n",
      "    Best validation F1: 0.8200\n",
      "    Best validation AUC-ROC: 0.9521\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Running Simple RNN (mean pooling) experiment: Augmentation + Weighted Sampling\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      ">>> Training Simple RNN (Augmentation + Weighted Sampling)\n",
      "    Parameters: 2,850,446\n",
      "    Max epochs: 100, Patience: 10\n",
      "Epoch: 01/100 | Time: 0m 2s\n",
      "\tTrain Loss: 1.3587 | Train Acc: 51.30%\n",
      "\tVal Loss: 1.2089 | Val Acc: 53.30% | Val F1: 0.5759 | Val AUC: 0.8522\n",
      "Epoch: 02/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.8913 | Train Acc: 72.89%\n",
      "\tVal Loss: 1.0229 | Val Acc: 65.50% | Val F1: 0.6875 | Val AUC: 0.8950\n",
      "Epoch: 03/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.6815 | Train Acc: 80.72%\n",
      "\tVal Loss: 1.0014 | Val Acc: 67.52% | Val F1: 0.6946 | Val AUC: 0.9050\n",
      "Epoch: 04/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.5391 | Train Acc: 85.80%\n",
      "\tVal Loss: 1.0085 | Val Acc: 66.42% | Val F1: 0.6945 | Val AUC: 0.9172\n",
      "Epoch: 05/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.4597 | Train Acc: 88.34%\n",
      "\tVal Loss: 0.8407 | Val Acc: 73.49% | Val F1: 0.7536 | Val AUC: 0.9336\n",
      "Epoch: 06/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.3827 | Train Acc: 90.72%\n",
      "\tVal Loss: 0.8448 | Val Acc: 73.39% | Val F1: 0.7487 | Val AUC: 0.9289\n",
      "Epoch: 07/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.3268 | Train Acc: 92.98%\n",
      "\tVal Loss: 0.9333 | Val Acc: 71.38% | Val F1: 0.7319 | Val AUC: 0.9344\n",
      "Epoch: 08/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.2680 | Train Acc: 95.23%\n",
      "\tVal Loss: 0.8384 | Val Acc: 75.87% | Val F1: 0.7725 | Val AUC: 0.9396\n",
      "Epoch: 09/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.2458 | Train Acc: 95.00%\n",
      "\tVal Loss: 0.8277 | Val Acc: 76.24% | Val F1: 0.7647 | Val AUC: 0.9440\n",
      "Epoch: 10/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.2077 | Train Acc: 96.49%\n",
      "\tVal Loss: 0.8058 | Val Acc: 77.98% | Val F1: 0.7893 | Val AUC: 0.9441\n",
      "Epoch: 11/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.1947 | Train Acc: 96.56%\n",
      "\tVal Loss: 0.9076 | Val Acc: 74.22% | Val F1: 0.7545 | Val AUC: 0.9327\n",
      "Epoch: 12/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.1487 | Train Acc: 97.68%\n",
      "\tVal Loss: 0.9469 | Val Acc: 76.42% | Val F1: 0.7623 | Val AUC: 0.9442\n",
      "Epoch: 13/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.1485 | Train Acc: 97.36%\n",
      "\tVal Loss: 1.0968 | Val Acc: 75.05% | Val F1: 0.7690 | Val AUC: 0.9306\n",
      "Epoch: 14/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.1486 | Train Acc: 97.27%\n",
      "\tVal Loss: 0.8174 | Val Acc: 79.45% | Val F1: 0.8002 | Val AUC: 0.9465\n",
      "Epoch: 15/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.1186 | Train Acc: 97.94%\n",
      "\tVal Loss: 0.9880 | Val Acc: 77.16% | Val F1: 0.7798 | Val AUC: 0.9427\n",
      "Epoch: 16/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.1034 | Train Acc: 98.33%\n",
      "\tVal Loss: 0.9662 | Val Acc: 78.81% | Val F1: 0.7969 | Val AUC: 0.9458\n",
      "Epoch: 17/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.1221 | Train Acc: 97.83%\n",
      "\tVal Loss: 0.9040 | Val Acc: 80.00% | Val F1: 0.8019 | Val AUC: 0.9488\n",
      "Epoch: 18/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0835 | Train Acc: 98.85%\n",
      "\tVal Loss: 0.9804 | Val Acc: 78.72% | Val F1: 0.7947 | Val AUC: 0.9470\n",
      "Epoch: 19/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0797 | Train Acc: 98.68%\n",
      "\tVal Loss: 0.9775 | Val Acc: 79.17% | Val F1: 0.7990 | Val AUC: 0.9474\n",
      "Epoch: 20/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0843 | Train Acc: 98.58%\n",
      "\tVal Loss: 1.0046 | Val Acc: 77.98% | Val F1: 0.7874 | Val AUC: 0.9449\n",
      "Epoch: 21/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0869 | Train Acc: 98.60%\n",
      "\tVal Loss: 1.1270 | Val Acc: 76.88% | Val F1: 0.7744 | Val AUC: 0.9442\n",
      "Epoch: 22/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0820 | Train Acc: 98.51%\n",
      "\tVal Loss: 1.0832 | Val Acc: 76.61% | Val F1: 0.7808 | Val AUC: 0.9438\n",
      "Epoch: 23/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0738 | Train Acc: 98.96%\n",
      "\tVal Loss: 1.1118 | Val Acc: 78.62% | Val F1: 0.7942 | Val AUC: 0.9444\n",
      "Epoch: 24/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.1030 | Train Acc: 98.75%\n",
      "\tVal Loss: 1.1120 | Val Acc: 79.36% | Val F1: 0.7989 | Val AUC: 0.9430\n",
      "Epoch: 25/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0549 | Train Acc: 99.33%\n",
      "\tVal Loss: 1.1511 | Val Acc: 77.80% | Val F1: 0.7855 | Val AUC: 0.9426\n",
      "Epoch: 26/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0557 | Train Acc: 99.38%\n",
      "\tVal Loss: 1.1305 | Val Acc: 77.98% | Val F1: 0.7889 | Val AUC: 0.9431\n",
      "Epoch: 27/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0626 | Train Acc: 99.00%\n",
      "\tVal Loss: 1.1110 | Val Acc: 77.98% | Val F1: 0.7901 | Val AUC: 0.9448\n",
      "\t>>> Early stopping at epoch 27, best val acc: 80.00%\n",
      "\n",
      ">>> Training completed! Best validation accuracy: 80.00%\n",
      "    Best validation F1: 0.8019\n",
      "    Best validation AUC-ROC: 0.9488\n",
      "\n",
      ">>> Simple RNN experiments queued. Run the cells to execute training if needed.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Run Simple RNN + Attention experiments\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n>>> Executing Simple RNN experiments...\")\n",
    "\n",
    "rnn_results_text_aug = run_simple_rnn_experiment(\n",
    "    dataset_key=\"augmented\",\n",
    "    description=\"Text Augmentation\",\n",
    "    save_suffix=\"text_aug\",\n",
    ")\n",
    "\n",
    "rnn_results_weighted = run_simple_rnn_experiment(\n",
    "    dataset_key=\"weighted\",\n",
    "    description=\"Weighted Sampling\",\n",
    "    save_suffix=\"weighted_sampler\",\n",
    ")\n",
    "\n",
    "rnn_results_aug_weighted = run_simple_rnn_experiment(\n",
    "    dataset_key=\"augmented_weighted\",\n",
    "    description=\"Augmentation + Weighted Sampling\",\n",
    "    save_suffix=\"text_aug_weighted\",\n",
    ")\n",
    "\n",
    "print(\"\\n>>> Simple RNN experiments queued. Run the cells to execute training if needed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "426bd88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Topic-wise accuracy for Simple RNN variants\n",
      "\n",
      "Simple RNN (Text Augmentation)\n",
      "------------------------------\n",
      "Topic      Accuracy %   Correct    Total     \n",
      "---------------------------------------------\n",
      "ABBR       77.78        7          9         \n",
      "DESC       85.51        118        138       \n",
      "ENTY       61.70        58         94        \n",
      "HUM        86.15        56         65        \n",
      "LOC        90.12        73         81        \n",
      "NUM        86.73        98         113       \n",
      "---------------------------------------------\n",
      "Topic      0.82         410        500       \n",
      "\n",
      "Simple RNN (Weighted Sampling)\n",
      "------------------------------\n",
      "Topic      Accuracy %   Correct    Total     \n",
      "---------------------------------------------\n",
      "ABBR       77.78        7          9         \n",
      "DESC       96.38        133        138       \n",
      "ENTY       62.77        59         94        \n",
      "HUM        87.69        57         65        \n",
      "LOC        83.95        68         81        \n",
      "NUM        83.19        94         113       \n",
      "---------------------------------------------\n",
      "Topic      0.84         418        500       \n",
      "\n",
      "Simple RNN (Augmentation + Weighted Sampling)\n",
      "---------------------------------------------\n",
      "Topic      Accuracy %   Correct    Total     \n",
      "---------------------------------------------\n",
      "ABBR       88.89        8          9         \n",
      "DESC       44.20        61         138       \n",
      "ENTY       69.15        65         94        \n",
      "HUM        84.62        55         65        \n",
      "LOC        86.42        70         81        \n",
      "NUM        84.96        96         113       \n",
      "---------------------------------------------\n",
      "Topic      0.71         355        500       \n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Topic-wise accuracy summary for Simple RNN experiments\n",
    "# ============================================================================\n",
    "\n",
    "def display_topic_metrics(title, metrics_dict):\n",
    "    print(f\"\\n{title}\")\n",
    "    print(\"-\" * len(title))\n",
    "    header = f\"{'Topic':<10} {'Accuracy %':<12} {'Correct':<10} {'Total':<10}\"\n",
    "    print(header)\n",
    "    print(\"-\" * len(header))\n",
    "    for topic in sorted(metrics_dict.keys()):\n",
    "        stats = metrics_dict[topic]\n",
    "        acc_pct = stats['accuracy'] * 100\n",
    "        print(f\"{topic:<10} {acc_pct:<12.2f} {stats['correct']:<10} {stats['total']:<10}\")\n",
    "    print(\"-\" * len(header))\n",
    "    \n",
    "    total_cor = sum([metrics_dict[topic]['correct'] for topic in metrics_dict])\n",
    "    total_sam = sum([metrics_dict[topic]['total'] for topic in metrics_dict])\n",
    "    total_acc = total_cor / total_sam \n",
    "    print(f\"{'Topic':<10} {total_acc:<12.2f} {total_cor:<10} {total_sam:<10}\")\n",
    "\n",
    "print(\"\\n>>> Topic-wise accuracy for Simple RNN variants\")\n",
    "for run_key, info in p35_results[\"simple_rnn_baseline\"].items():\n",
    "    topic_metrics = info.get(\"topic_metrics\")\n",
    "    if not topic_metrics:\n",
    "        continue\n",
    "    title = f\"Simple RNN ({info['description']})\"\n",
    "    display_topic_metrics(title, topic_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e096065b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# RNN + BERT experiment runner\n",
    "# ============================================================================\n",
    "BERT_HYPERPARAM = hyperparam_prep()\n",
    "BERT_HYPERPARAM['LEARNING_RATE'] = 2e-5\n",
    "BERT_HYPERPARAM['OTHER_LR'] = BERT_HYPERPARAM['LEARNING_RATE'] * 10\n",
    "BERT_HYPERPARAM['N_EPOCHS'] //= 2\n",
    "BERT_HYPERPARAM['MODEL_NAME'] = 'bert-base-uncased'\n",
    "BERT_HYPERPARAM['FREEZE'] = False\n",
    "BERT_HYPERPARAM['SAVE_MODEL'] = True\n",
    "\n",
    "\n",
    "def run_rnn_bert_experiment(dataset_key, description, save_suffix, TEXT, freeze_bert=BERT_HYPERPARAM['FREEZE']):\n",
    "    if not BERT_AVAILABLE:\n",
    "        raise RuntimeError(\"Transformers library is unavailable; cannot run RNN+BERT experiments.\")\n",
    "    if dataset_key not in p35_datasets:\n",
    "        raise ValueError(f\"Unknown dataset key: {dataset_key}\")\n",
    "\n",
    "    reset_random_seeds(SEED)\n",
    "    train_dataset = p35_datasets[dataset_key]\n",
    "\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(f\"Running RNN + BERT experiment: {description}\")\n",
    "    print(\"-\"*80)\n",
    "\n",
    "    train_iter = build_iterator(train_dataset, BERT_HYPERPARAM['BATCH_SIZE'], shuffle=True)\n",
    "    val_iter = build_iterator(validation_data, BERT_HYPERPARAM['BATCH_SIZE'], shuffle=False)\n",
    "    test_iter = build_iterator(test_data, BERT_HYPERPARAM['BATCH_SIZE'], shuffle=False)\n",
    "\n",
    "    model = RNNBertClassifier(\n",
    "        output_dim=num_classes,\n",
    "        hidden_dim=BERT_HYPERPARAM['HIDDEN_DIM'],\n",
    "        n_layers=BERT_HYPERPARAM['N_LAYERS'],\n",
    "        dropout=BERT_HYPERPARAM['DROPOUT'],\n",
    "        bert_model_name=BERT_HYPERPARAM['MODEL_NAME'],\n",
    "        freeze_bert=freeze_bert,\n",
    "    ).to(device)\n",
    "\n",
    "    bert_params = [p for p in model.bert_model.parameters() if p.requires_grad]\n",
    "    other_params = [p for n, p in model.named_parameters() if 'bert_model' not in n]\n",
    "\n",
    "    optimizer_grouped_parameters = []\n",
    "    if bert_params:\n",
    "        optimizer_grouped_parameters.append({'params': bert_params, 'lr': BERT_HYPERPARAM['LEARNING_RATE']})\n",
    "    if other_params:\n",
    "        optimizer_grouped_parameters.append({'params': other_params, 'lr': BERT_HYPERPARAM['OTHER_LR']})\n",
    "\n",
    "    optimizer = optim.RMSprop(optimizer_grouped_parameters, weight_decay=BERT_HYPERPARAM['L2_LAMBDA'])\n",
    "\n",
    "    model, history = train_model_with_history_bert(\n",
    "        model,\n",
    "        train_iter,\n",
    "        val_iter,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        BERT_HYPERPARAM['N_EPOCHS'],\n",
    "        device,\n",
    "        num_classes,\n",
    "        BERT_HYPERPARAM['L1_LAMBDA'],\n",
    "        patience=BERT_HYPERPARAM['PATIENCE'],\n",
    "        model_name=f\"RNN+BERT ({description})\",\n",
    "        text_vocab=TEXT.vocab.itos,\n",
    "    )\n",
    "\n",
    "    test_loss, test_acc, test_f1, test_auc = evaluate_model_bert(\n",
    "        model,\n",
    "        test_iter,\n",
    "        criterion,\n",
    "        device,\n",
    "        f\"RNN+BERT ({description})\",\n",
    "        num_classes,\n",
    "        text_vocab=TEXT.vocab.itos,\n",
    "    )\n",
    "\n",
    "    topic_metrics = evaluate_per_topic_p35(model, test_iter, device, TEXT.vocab.itos)\n",
    "\n",
    "    model_path = f\"weights/part35_rnn_bert_{save_suffix}.pt\"\n",
    "    if BERT_HYPERPARAM['SAVE_MODEL']:\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    p35_results[\"rnn_bert\"][save_suffix] = {\n",
    "        \"description\": description,\n",
    "        \"dataset_key\": dataset_key,\n",
    "        \"history\": history,\n",
    "        \"test_metrics\": {\n",
    "            \"loss\": test_loss,\n",
    "            \"accuracy\": test_acc,\n",
    "            \"f1\": test_f1,\n",
    "            \"auc\": test_auc,\n",
    "        },\n",
    "        \"topic_metrics\": topic_metrics,\n",
    "        \"model_path\": model_path if BERT_HYPERPARAM['SAVE_MODEL'] else None,\n",
    "        \"freeze_bert\": freeze_bert,\n",
    "        \"model\": model,\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"history\": history,\n",
    "        \"topic_metrics\": topic_metrics,\n",
    "        \"test_metrics\": {\n",
    "            \"loss\": test_loss,\n",
    "            \"accuracy\": test_acc,\n",
    "            \"f1\": test_f1,\n",
    "            \"auc\": test_auc,\n",
    "        },\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c9059c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Executing RNN + BERT experiments...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Running RNN + BERT experiment: Text Augmentation\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      ">>> Training RNN+BERT (Text Augmentation)\n",
      "    Parameters: 113,295,111\n",
      "    Max epochs: 50, Patience: 7\n",
      "Epoch: 01/50 | Time: 1m 0s\n",
      "\tTrain Loss: 4.0338 | Train Acc: 86.32%\n",
      "\tVal Loss: 0.4932 | Val Acc: 87.06% | Val F1: 0.8847 | Val AUC: 0.9761\n",
      "Epoch: 02/50 | Time: 1m 1s\n",
      "\tTrain Loss: 3.6542 | Train Acc: 96.32%\n",
      "\tVal Loss: 0.4801 | Val Acc: 88.62% | Val F1: 0.8949 | Val AUC: 0.9809\n",
      "Epoch: 03/50 | Time: 1m 2s\n",
      "\tTrain Loss: 3.5057 | Train Acc: 98.24%\n",
      "\tVal Loss: 0.6472 | Val Acc: 88.17% | Val F1: 0.8933 | Val AUC: 0.9832\n",
      "Epoch: 04/50 | Time: 1m 1s\n",
      "\tTrain Loss: 3.3836 | Train Acc: 99.18%\n",
      "\tVal Loss: 0.7525 | Val Acc: 87.98% | Val F1: 0.8935 | Val AUC: 0.9804\n",
      "Epoch: 05/50 | Time: 1m 2s\n",
      "\tTrain Loss: 3.2876 | Train Acc: 99.21%\n",
      "\tVal Loss: 0.6121 | Val Acc: 88.72% | Val F1: 0.8942 | Val AUC: 0.9826\n",
      "Epoch: 06/50 | Time: 1m 0s\n",
      "\tTrain Loss: 3.1875 | Train Acc: 99.52%\n",
      "\tVal Loss: 0.6729 | Val Acc: 89.54% | Val F1: 0.9032 | Val AUC: 0.9831\n",
      "Epoch: 07/50 | Time: 1m 0s\n",
      "\tTrain Loss: 3.1005 | Train Acc: 99.55%\n",
      "\tVal Loss: 0.7183 | Val Acc: 88.81% | Val F1: 0.8972 | Val AUC: 0.9841\n",
      "Epoch: 08/50 | Time: 1m 0s\n",
      "\tTrain Loss: 3.0086 | Train Acc: 99.70%\n",
      "\tVal Loss: 0.7668 | Val Acc: 88.26% | Val F1: 0.8934 | Val AUC: 0.9822\n",
      "Epoch: 09/50 | Time: 1m 1s\n",
      "\tTrain Loss: 2.9297 | Train Acc: 99.65%\n",
      "\tVal Loss: 0.8838 | Val Acc: 87.80% | Val F1: 0.8893 | Val AUC: 0.9809\n",
      "Epoch: 10/50 | Time: 1m 2s\n",
      "\tTrain Loss: 2.8376 | Train Acc: 99.70%\n",
      "\tVal Loss: 0.9482 | Val Acc: 87.71% | Val F1: 0.8920 | Val AUC: 0.9759\n",
      "Epoch: 11/50 | Time: 1m 2s\n",
      "\tTrain Loss: 2.7572 | Train Acc: 99.63%\n",
      "\tVal Loss: 0.8394 | Val Acc: 88.44% | Val F1: 0.8936 | Val AUC: 0.9685\n",
      "Epoch: 12/50 | Time: 1m 2s\n",
      "\tTrain Loss: 2.6833 | Train Acc: 99.62%\n",
      "\tVal Loss: 0.7909 | Val Acc: 89.17% | Val F1: 0.9003 | Val AUC: 0.9760\n",
      "Epoch: 13/50 | Time: 1m 2s\n",
      "\tTrain Loss: 2.5974 | Train Acc: 99.85%\n",
      "\tVal Loss: 0.8562 | Val Acc: 88.35% | Val F1: 0.8930 | Val AUC: 0.9648\n",
      "\t>>> Early stopping at epoch 13, best val acc: 89.54%\n",
      "\n",
      ">>> Training completed! Best validation accuracy: 89.54%\n",
      "    Best validation F1: 0.9032\n",
      "    Best validation AUC-ROC: 0.9831\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Running RNN + BERT experiment: Weighted Sampling\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      ">>> Training RNN+BERT (Weighted Sampling)\n",
      "    Parameters: 113,295,111\n",
      "    Max epochs: 50, Patience: 7\n",
      "Epoch: 01/50 | Time: 0m 49s\n",
      "\tTrain Loss: 4.0607 | Train Acc: 85.12%\n",
      "\tVal Loss: 0.3431 | Val Acc: 90.64% | Val F1: 0.9069 | Val AUC: 0.9830\n",
      "Epoch: 02/50 | Time: 0m 51s\n",
      "\tTrain Loss: 3.6497 | Train Acc: 97.32%\n",
      "\tVal Loss: 0.4306 | Val Acc: 90.73% | Val F1: 0.9071 | Val AUC: 0.9831\n",
      "Epoch: 03/50 | Time: 0m 44s\n",
      "\tTrain Loss: 3.5108 | Train Acc: 99.01%\n",
      "\tVal Loss: 0.4997 | Val Acc: 91.47% | Val F1: 0.9151 | Val AUC: 0.9836\n",
      "Epoch: 04/50 | Time: 0m 44s\n",
      "\tTrain Loss: 3.4274 | Train Acc: 99.43%\n",
      "\tVal Loss: 0.5173 | Val Acc: 91.65% | Val F1: 0.9173 | Val AUC: 0.9843\n",
      "Epoch: 05/50 | Time: 0m 43s\n",
      "\tTrain Loss: 3.3418 | Train Acc: 99.61%\n",
      "\tVal Loss: 0.5754 | Val Acc: 90.92% | Val F1: 0.9097 | Val AUC: 0.9818\n",
      "Epoch: 06/50 | Time: 0m 41s\n",
      "\tTrain Loss: 3.2594 | Train Acc: 99.66%\n",
      "\tVal Loss: 0.5865 | Val Acc: 92.02% | Val F1: 0.9208 | Val AUC: 0.9819\n",
      "Epoch: 07/50 | Time: 0m 41s\n",
      "\tTrain Loss: 3.1728 | Train Acc: 99.91%\n",
      "\tVal Loss: 0.6196 | Val Acc: 91.10% | Val F1: 0.9118 | Val AUC: 0.9836\n",
      "Epoch: 08/50 | Time: 0m 41s\n",
      "\tTrain Loss: 3.1009 | Train Acc: 99.70%\n",
      "\tVal Loss: 0.6277 | Val Acc: 90.73% | Val F1: 0.9088 | Val AUC: 0.9787\n",
      "Epoch: 09/50 | Time: 0m 40s\n",
      "\tTrain Loss: 3.0171 | Train Acc: 99.79%\n",
      "\tVal Loss: 0.6717 | Val Acc: 90.64% | Val F1: 0.9068 | Val AUC: 0.9766\n",
      "Epoch: 10/50 | Time: 0m 41s\n",
      "\tTrain Loss: 2.9443 | Train Acc: 99.82%\n",
      "\tVal Loss: 0.6725 | Val Acc: 90.37% | Val F1: 0.9041 | Val AUC: 0.9821\n",
      "Epoch: 11/50 | Time: 0m 46s\n",
      "\tTrain Loss: 2.8750 | Train Acc: 99.77%\n",
      "\tVal Loss: 0.6927 | Val Acc: 90.46% | Val F1: 0.9049 | Val AUC: 0.9764\n",
      "Epoch: 12/50 | Time: 0m 45s\n",
      "\tTrain Loss: 2.8198 | Train Acc: 99.40%\n",
      "\tVal Loss: 0.6714 | Val Acc: 90.00% | Val F1: 0.9014 | Val AUC: 0.9766\n",
      "Epoch: 13/50 | Time: 0m 41s\n",
      "\tTrain Loss: 2.7459 | Train Acc: 99.75%\n",
      "\tVal Loss: 0.7671 | Val Acc: 89.17% | Val F1: 0.8940 | Val AUC: 0.9747\n",
      "\t>>> Early stopping at epoch 13, best val acc: 92.02%\n",
      "\n",
      ">>> Training completed! Best validation accuracy: 92.02%\n",
      "    Best validation F1: 0.9208\n",
      "    Best validation AUC-ROC: 0.9819\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Running RNN + BERT experiment: Augmentation + Weighted Sampling\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      ">>> Training RNN+BERT (Augmentation + Weighted Sampling)\n",
      "    Parameters: 113,295,111\n",
      "    Max epochs: 50, Patience: 7\n",
      "Epoch: 01/50 | Time: 0m 56s\n",
      "\tTrain Loss: 4.0112 | Train Acc: 87.04%\n",
      "\tVal Loss: 0.5071 | Val Acc: 87.43% | Val F1: 0.8844 | Val AUC: 0.9740\n",
      "Epoch: 02/50 | Time: 0m 55s\n",
      "\tTrain Loss: 3.6109 | Train Acc: 97.71%\n",
      "\tVal Loss: 0.6900 | Val Acc: 86.61% | Val F1: 0.8733 | Val AUC: 0.9736\n",
      "Epoch: 03/50 | Time: 0m 54s\n",
      "\tTrain Loss: 3.4615 | Train Acc: 99.15%\n",
      "\tVal Loss: 0.7018 | Val Acc: 88.17% | Val F1: 0.8896 | Val AUC: 0.9780\n",
      "Epoch: 04/50 | Time: 0m 55s\n",
      "\tTrain Loss: 3.3562 | Train Acc: 99.35%\n",
      "\tVal Loss: 0.7300 | Val Acc: 87.43% | Val F1: 0.8815 | Val AUC: 0.9783\n",
      "Epoch: 05/50 | Time: 0m 55s\n",
      "\tTrain Loss: 3.2465 | Train Acc: 99.55%\n",
      "\tVal Loss: 0.8597 | Val Acc: 87.16% | Val F1: 0.8798 | Val AUC: 0.9783\n",
      "Epoch: 06/50 | Time: 0m 55s\n",
      "\tTrain Loss: 3.1463 | Train Acc: 99.58%\n",
      "\tVal Loss: 0.9134 | Val Acc: 88.17% | Val F1: 0.8892 | Val AUC: 0.9787\n",
      "Epoch: 07/50 | Time: 0m 55s\n",
      "\tTrain Loss: 3.0493 | Train Acc: 99.68%\n",
      "\tVal Loss: 0.8096 | Val Acc: 87.98% | Val F1: 0.8875 | Val AUC: 0.9811\n",
      "Epoch: 08/50 | Time: 0m 55s\n",
      "\tTrain Loss: 2.9575 | Train Acc: 99.72%\n",
      "\tVal Loss: 0.8968 | Val Acc: 86.97% | Val F1: 0.8770 | Val AUC: 0.9776\n",
      "Epoch: 09/50 | Time: 0m 55s\n",
      "\tTrain Loss: 2.8804 | Train Acc: 99.67%\n",
      "\tVal Loss: 1.0269 | Val Acc: 85.05% | Val F1: 0.8633 | Val AUC: 0.9687\n",
      "Epoch: 10/50 | Time: 0m 59s\n",
      "\tTrain Loss: 2.7940 | Train Acc: 99.80%\n",
      "\tVal Loss: 0.9377 | Val Acc: 87.16% | Val F1: 0.8774 | Val AUC: 0.9760\n",
      "\t>>> Early stopping at epoch 10, best val acc: 88.17%\n",
      "\n",
      ">>> Training completed! Best validation accuracy: 88.17%\n",
      "    Best validation F1: 0.8896\n",
      "    Best validation AUC-ROC: 0.9780\n",
      "\n",
      ">>> RNN + BERT experiments queued. Run the cells to execute training if needed.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Run RNN + BERT experiments\n",
    "# ============================================================================\n",
    "\n",
    "if BERT_AVAILABLE:\n",
    "    print(\"\\n>>> Executing RNN + BERT experiments...\")\n",
    "\n",
    "    bert_results_text_aug = run_rnn_bert_experiment(\n",
    "        dataset_key=\"augmented\",\n",
    "        description=\"Text Augmentation\",\n",
    "        save_suffix=\"text_aug\",\n",
    "        TEXT=TEXT,\n",
    "    )\n",
    "\n",
    "    bert_results_weighted = run_rnn_bert_experiment(\n",
    "        dataset_key=\"weighted\",\n",
    "        description=\"Weighted Sampling\",\n",
    "        save_suffix=\"weighted_sampler\",\n",
    "        TEXT=TEXT,\n",
    "    )\n",
    "\n",
    "    bert_results_aug_weighted = run_rnn_bert_experiment(\n",
    "        dataset_key=\"augmented_weighted\",\n",
    "        description=\"Augmentation + Weighted Sampling\",\n",
    "        save_suffix=\"text_aug_weighted\",\n",
    "        TEXT=TEXT,\n",
    "    )\n",
    "\n",
    "    print(\"\\n>>> RNN + BERT experiments queued. Run the cells to execute training if needed.\")\n",
    "else:\n",
    "    print(\"\\n>>> Skipping RNN + BERT experiments (transformers library unavailable).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d1ce0d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Topic-wise accuracy for RNN + BERT variants\n",
      "\n",
      "RNN + BERT (Text Augmentation)\n",
      "------------------------------\n",
      "Topic      Accuracy %   Correct    Total     \n",
      "---------------------------------------------\n",
      "ABBR       100.00       9          9         \n",
      "DESC       30.43        42         138       \n",
      "ENTY       82.98        78         94        \n",
      "HUM        96.92        63         65        \n",
      "LOC        97.53        79         81        \n",
      "NUM        94.69        107        113       \n",
      "---------------------------------------------\n",
      "Topic      0.76         378        500       \n",
      "\n",
      "RNN + BERT (Weighted Sampling)\n",
      "------------------------------\n",
      "Topic      Accuracy %   Correct    Total     \n",
      "---------------------------------------------\n",
      "ABBR       88.89        8          9         \n",
      "DESC       87.68        121        138       \n",
      "ENTY       85.11        80         94        \n",
      "HUM        95.38        62         65        \n",
      "LOC        96.30        78         81        \n",
      "NUM        93.81        106        113       \n",
      "---------------------------------------------\n",
      "Topic      0.91         455        500       \n",
      "\n",
      "RNN + BERT (Augmentation + Weighted Sampling)\n",
      "---------------------------------------------\n",
      "Topic      Accuracy %   Correct    Total     \n",
      "---------------------------------------------\n",
      "ABBR       88.89        8          9         \n",
      "DESC       32.61        45         138       \n",
      "ENTY       92.55        87         94        \n",
      "HUM        95.38        62         65        \n",
      "LOC        97.53        79         81        \n",
      "NUM        93.81        106        113       \n",
      "---------------------------------------------\n",
      "Topic      0.77         387        500       \n"
     ]
    }
   ],
   "source": [
    "# =========================================================================\n",
    "# Topic-wise accuracy summary for RNN + BERT experiments\n",
    "# =========================================================================\n",
    "\n",
    "if p35_results[\"rnn_bert\"]:\n",
    "    print(\"\\n>>> Topic-wise accuracy for RNN + BERT variants\")\n",
    "    for key, info in p35_results[\"rnn_bert\"].items():\n",
    "        topic_metrics = info.get(\"topic_metrics\")\n",
    "        if not topic_metrics:\n",
    "            continue\n",
    "        title = f\"RNN + BERT ({info['description']})\"\n",
    "        display_topic_metrics(title, topic_metrics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1f9b5ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PART 3.4: TEXT AUGMENTATION VS WEIGHTED SAMPLING\n",
      "================================================================================\n",
      "\n",
      ">>> Dataset Variants Used:\n",
      "  - original: 4362 samples\n",
      "  - augmented: 5980 samples\n",
      "  - weighted: 4362 samples\n",
      "  - augmented_weighted: 5980 samples\n",
      "\n",
      ">>> Simple RNN (mean pooling) Experiments:\n",
      "  Text Augmentation [text_aug]\n",
      "    - Test Accuracy: 82.00%\n",
      "    - Test F1: 0.8309\n",
      "    - Test AUC: 0.9589\n",
      "    - Test Loss: 0.6303\n",
      "    - Weakest Topic: ENTY (61.70%)\n",
      "    - Model saved to: weights/part35_simple_rnn_text_aug.pt\n",
      "  Weighted Sampling [weighted_sampler]\n",
      "    - Test Accuracy: 83.60%\n",
      "    - Test F1: 0.8343\n",
      "    - Test AUC: 0.9434\n",
      "    - Test Loss: 0.6901\n",
      "    - Weakest Topic: ENTY (62.77%)\n",
      "    - Model saved to: weights/part35_simple_rnn_weighted_sampler.pt\n",
      "  Augmentation + Weighted Sampling [text_aug_weighted]\n",
      "    - Test Accuracy: 71.00%\n",
      "    - Test F1: 0.7468\n",
      "    - Test AUC: 0.9518\n",
      "    - Test Loss: 0.7942\n",
      "    - Weakest Topic: DESC (44.20%)\n",
      "    - Model saved to: weights/part35_simple_rnn_text_aug_weighted.pt\n",
      "\n",
      ">>> RNN + BERT Experiments:\n",
      "  Text Augmentation [text_aug]\n",
      "    - Test Accuracy: 75.60%\n",
      "    - Test F1: 0.7883\n",
      "    - Test AUC: 0.9075\n",
      "    - Test Loss: 2.0883\n",
      "    - Weakest Topic: DESC (30.43%)\n",
      "    - Model saved to: weights/part35_rnn_bert_text_aug.pt\n",
      "  Weighted Sampling [weighted_sampler]\n",
      "    - Test Accuracy: 91.00%\n",
      "    - Test F1: 0.9152\n",
      "    - Test AUC: 0.9651\n",
      "    - Test Loss: 0.7034\n",
      "    - Weakest Topic: ENTY (85.11%)\n",
      "    - Model saved to: weights/part35_rnn_bert_weighted_sampler.pt\n",
      "  Augmentation + Weighted Sampling [text_aug_weighted]\n",
      "    - Test Accuracy: 77.40%\n",
      "    - Test F1: 0.8055\n",
      "    - Test AUC: 0.9595\n",
      "    - Test Loss: 1.8749\n",
      "    - Weakest Topic: DESC (32.61%)\n",
      "    - Model saved to: weights/part35_rnn_bert_text_aug_weighted.pt\n",
      "\n",
      "================================================================================\n",
      "PART 3.4 SETUP COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PART 3.4 SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 3.4: TEXT AUGMENTATION VS WEIGHTED SAMPLING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n>>> Dataset Variants Used:\")\n",
    "for key in [\"original\", \"augmented\", \"weighted\", \"augmented_weighted\"]:\n",
    "    dataset = p35_datasets[key]\n",
    "    print(f\"  - {key}: {len(dataset.examples)} samples\")\n",
    "\n",
    "print(\"\\n>>> Simple RNN (mean pooling) Experiments:\")\n",
    "if p35_results[\"simple_rnn_baseline\"]:\n",
    "    for key, info in p35_results[\"simple_rnn_baseline\"].items():\n",
    "        metrics = info.get(\"test_metrics\", {})\n",
    "        print(f\"  {info['description']} [{key}]\")\n",
    "        if metrics:\n",
    "            print(f\"    - Test Accuracy: {metrics.get('accuracy', 0)*100:.2f}%\")\n",
    "            print(f\"    - Test F1: {metrics.get('f1', 0):.4f}\")\n",
    "            print(f\"    - Test AUC: {metrics.get('auc', 0):.4f}\")\n",
    "            print(f\"    - Test Loss: {metrics.get('loss', 0):.4f}\")\n",
    "        topic_metrics = info.get(\"topic_metrics\")\n",
    "        if topic_metrics:\n",
    "            weakest = min(topic_metrics.items(), key=lambda kv: kv[1]['accuracy'])\n",
    "            print(f\"    - Weakest Topic: {weakest[0]} ({weakest[1]['accuracy']*100:.2f}%)\")\n",
    "        print(f\"    - Model saved to: {info['model_path']}\")\n",
    "else:\n",
    "    print(\"  - Pending (run the experiment cells above)\")\n",
    "\n",
    "print(\"\\n>>> RNN + BERT Experiments:\")\n",
    "if p35_results[\"rnn_bert\"]:\n",
    "    for key, info in p35_results[\"rnn_bert\"].items():\n",
    "        metrics = info.get(\"test_metrics\", {})\n",
    "        print(f\"  {info['description']} [{key}]\")\n",
    "        if metrics:\n",
    "            print(f\"    - Test Accuracy: {metrics.get('accuracy', 0)*100:.2f}%\")\n",
    "            print(f\"    - Test F1: {metrics.get('f1', 0):.4f}\")\n",
    "            print(f\"    - Test AUC: {metrics.get('auc', 0):.4f}\")\n",
    "            print(f\"    - Test Loss: {metrics.get('loss', 0):.4f}\")\n",
    "        topic_metrics = info.get(\"topic_metrics\")\n",
    "        if topic_metrics:\n",
    "            weakest = min(topic_metrics.items(), key=lambda kv: kv[1]['accuracy'])\n",
    "            print(f\"    - Weakest Topic: {weakest[0]} ({weakest[1]['accuracy']*100:.2f}%)\")\n",
    "        print(f\"    - Model saved to: {info['model_path']}\")\n",
    "else:\n",
    "    print(\"  - Pending (run the experiment cells above)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 3.4 SETUP COMPLETE\")\n",
    "print(\"=\"*80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
