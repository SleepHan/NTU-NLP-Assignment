{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05ac734e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ.setdefault('TORCH_COMPILE_DISABLE', '1')\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "import torch\n",
    "\n",
    "# Method 2: Patch torch._dynamo.disable decorator after import\n",
    "try:\n",
    "    import torch._dynamo\n",
    "    # Patch the disable function to ignore the 'wrapping' parameter\n",
    "    if hasattr(torch._dynamo, 'disable'):\n",
    "        def patched_disable(fn=None, *args, **kwargs):\n",
    "            # Remove problematic 'wrapping' parameter if present\n",
    "            if 'wrapping' in kwargs:\n",
    "                kwargs.pop('wrapping')\n",
    "            if fn is None:\n",
    "                # Decorator usage: @disable\n",
    "                return lambda f: f\n",
    "            # Function usage: disable(fn) or disable(fn, **kwargs)\n",
    "            # Simply return the function unwrapped to avoid recursion\n",
    "            # The original disable was causing issues, so we bypass it entirely\n",
    "            return fn\n",
    "        torch._dynamo.disable = patched_disable\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not patch torch._dynamo: {e}\")\n",
    "    pass  # If patching fails, continue anyway\n",
    "\n",
    "import random, string\n",
    "\n",
    "from torchtext import data , datasets\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "\n",
    "os.environ['GENSIM_DATA_DIR'] = os.path.join(os.getcwd(), 'gensim-data')\n",
    "\n",
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.fasttext import load_facebook_model\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import time, copy\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44e533a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Prepping Data...\n",
      "[+] Test set formed!\n",
      "[+] Train and Validation sets formed!\n",
      "[+] Data prepped successfully!\n",
      "[*] Retrieving pretrained word embeddings...\n",
      "[*] Loading fasttext model...\n",
      "[+] Model loaded!\n",
      "[*] Forming embedding matrix...\n",
      "[+] Embedding matrix formed!\n",
      "[+] Embeddings retrieved successfully!\n",
      "\n",
      "Label distribution in training set:\n",
      "- ABBR: 69 samples (1.58%)\n",
      "- DESC: 930 samples (21.32%)\n",
      "- ENTY: 1000 samples (22.93%)\n",
      "- HUM: 978 samples (22.42%)\n",
      "- LOC: 668 samples (15.31%)\n",
      "- NUM: 717 samples (16.44%)\n",
      "Total samples: 4362, Sum of percentages: 100.00%\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "train_data, validation_data, test_data, LABEL, TEXT, pretrained_embed = data_prep(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c176434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86a01357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of classes: 6\n",
      "Classes: ['ENTY', 'HUM', 'DESC', 'NUM', 'LOC', 'ABBR']\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary for labels\n",
    "LABEL.build_vocab(train_data)\n",
    "num_classes = len(LABEL.vocab)\n",
    "print(f\"\\nNumber of classes: {num_classes}\")\n",
    "print(f\"Classes: {LABEL.vocab.itos}\")\n",
    "\n",
    "# Get pretrained embeddings from Part 1 (frozen embeddings)\n",
    "pretrained_embeddings = pretrained_embed.weight.data\n",
    "\n",
    "# Get embedding dimension and vocab size from the fasttext embedding layer\n",
    "embedding_dim = pretrained_embed.weight.shape[1]\n",
    "embedding_vocab_size = pretrained_embed.weight.shape[0]  # Vocab size from saved embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2597781b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT Vocab Size: 8165\n"
     ]
    }
   ],
   "source": [
    "print(f'TEXT Vocab Size: {len(TEXT.vocab.stoi)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aad6c8e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PART 3.4: TARGETED IMPROVEMENT FOR WEAK TOPICS\n",
      "================================================================================\n",
      "\n",
      "Strategies:\n",
      "  1. Data Augmentation for imbalanced classes (especially ABBR)\n",
      "  2. Positional Embeddings in attention layer\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PART 3.4: Targeted Improvement for Weak Topics\n",
    "# Strategy: Data Augmentation, Positional Embeddings\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 3.4: TARGETED IMPROVEMENT FOR WEAK TOPICS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nStrategies:\")\n",
    "print(\"  1. Data Augmentation for imbalanced classes (especially ABBR)\")\n",
    "print(\"  2. Positional Embeddings in attention layer\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Import required libraries for augmentation\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "try:\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c2db80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Step 2: Implementing Data Augmentation Functions...\n",
      "    ✓ Data augmentation functions ready\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Step 2: Data Augmentation Functions for Imbalanced Classes\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n>>> Step 2: Implementing Data Augmentation Functions...\")\n",
    "\n",
    "def get_synonyms(word):\n",
    "    \"\"\"Get synonyms for a word using WordNet\"\"\"\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonym = lemma.name().replace('_', ' ').lower()\n",
    "            if synonym != word and synonym.isalpha():\n",
    "                synonyms.add(synonym)\n",
    "    return list(synonyms)\n",
    "\n",
    "def synonym_replacement(tokens, n=1):\n",
    "    \"\"\"Replace n random words with their synonyms\"\"\"\n",
    "    new_tokens = tokens.copy()\n",
    "    words_to_replace = [i for i, word in enumerate(tokens) if word.isalpha() and len(word) > 2]\n",
    "    \n",
    "    if len(words_to_replace) == 0:\n",
    "        return tokens\n",
    "    \n",
    "    num_replacements = min(n, len(words_to_replace))\n",
    "    indices_to_replace = random.sample(words_to_replace, num_replacements)\n",
    "    \n",
    "    for idx in indices_to_replace:\n",
    "        synonyms = get_synonyms(tokens[idx])\n",
    "        if synonyms:\n",
    "            new_tokens[idx] = random.choice(synonyms)\n",
    "    \n",
    "    return new_tokens\n",
    "\n",
    "def random_insertion(tokens, n=1):\n",
    "    \"\"\"Randomly insert synonyms of n words\"\"\"\n",
    "    new_tokens = tokens.copy()\n",
    "    \n",
    "    for _ in range(n):\n",
    "        if len(new_tokens) == 0:\n",
    "            break\n",
    "        word = random.choice(new_tokens)\n",
    "        synonyms = get_synonyms(word)\n",
    "        if synonyms:\n",
    "            synonym = random.choice(synonyms)\n",
    "            insert_pos = random.randint(0, len(new_tokens))\n",
    "            new_tokens.insert(insert_pos, synonym)\n",
    "    \n",
    "    return new_tokens\n",
    "\n",
    "def random_deletion(tokens, p=0.1):\n",
    "    \"\"\"Randomly delete words with probability p\"\"\"\n",
    "    if len(tokens) == 1:\n",
    "        return tokens\n",
    "    \n",
    "    new_tokens = []\n",
    "    for token in tokens:\n",
    "        if random.random() > p:\n",
    "            new_tokens.append(token)\n",
    "    \n",
    "    if len(new_tokens) == 0:\n",
    "        return tokens[:1]\n",
    "    \n",
    "    return new_tokens\n",
    "\n",
    "def random_swap(tokens, n=1):\n",
    "    \"\"\"Randomly swap n pairs of words\"\"\"\n",
    "    new_tokens = tokens.copy()\n",
    "    \n",
    "    for _ in range(n):\n",
    "        if len(new_tokens) < 2:\n",
    "            break\n",
    "        idx1, idx2 = random.sample(range(len(new_tokens)), 2)\n",
    "        new_tokens[idx1], new_tokens[idx2] = new_tokens[idx2], new_tokens[idx1]\n",
    "    \n",
    "    return new_tokens\n",
    "\n",
    "def augment_text(text, augmentation_techniques=['synonym', 'insertion', 'deletion', 'swap'], \n",
    "                 num_augmentations=3):\n",
    "    \"\"\"Apply data augmentation to text\"\"\"\n",
    "    augmented_texts = []\n",
    "    \n",
    "    for _ in range(num_augmentations):\n",
    "        aug_text = text.copy()\n",
    "        technique = random.choice(augmentation_techniques)\n",
    "        \n",
    "        if technique == 'synonym' and len(aug_text) > 0:\n",
    "            aug_text = synonym_replacement(aug_text, n=random.randint(1, 2))\n",
    "        elif technique == 'insertion' and len(aug_text) > 0:\n",
    "            aug_text = random_insertion(aug_text, n=random.randint(1, 2))\n",
    "        elif technique == 'deletion' and len(aug_text) > 1:\n",
    "            aug_text = random_deletion(aug_text, p=0.1)\n",
    "        elif technique == 'swap' and len(aug_text) > 1:\n",
    "            aug_text = random_swap(aug_text, n=1)\n",
    "        \n",
    "        augmented_texts.append(aug_text)\n",
    "    \n",
    "    return augmented_texts\n",
    "\n",
    "print(\"    ✓ Data augmentation functions ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f8ddcf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Step 3: Applying Data Augmentation for Imbalanced Classes...\n",
      "\n",
      "Original label distribution:\n",
      "  ABBR: 69 samples (1.58%)\n",
      "  DESC: 930 samples (21.32%)\n",
      "  ENTY: 1000 samples (22.93%)\n",
      "  HUM: 978 samples (22.42%)\n",
      "  LOC: 668 samples (15.31%)\n",
      "  NUM: 717 samples (16.44%)\n",
      "\n",
      "  Augmenting LOC: 668 -> 800 samples\n",
      "    Generating 132 additional samples...\n",
      "    ✓ Generated 132 augmented samples\n",
      "\n",
      "  Augmenting NUM: 717 -> 850 samples\n",
      "    Generating 133 additional samples...\n",
      "    ✓ Generated 133 augmented samples\n",
      "\n",
      "  Augmenting ENTY: 1000 -> 1300 samples\n",
      "    Generating 300 additional samples...\n",
      "    ✓ Generated 300 augmented samples\n",
      "\n",
      "  Augmenting ABBR: 69 -> 900 samples\n",
      "    Generating 831 additional samples...\n",
      "    ✓ Generated 831 augmented samples\n",
      "\n",
      "  Augmenting HUM: 978 -> 1200 samples\n",
      "    Generating 222 additional samples...\n",
      "    ✓ Generated 222 augmented samples\n",
      "\n",
      "Augmented label distribution:\n",
      "  ABBR: 900 samples (15.05%)\n",
      "  DESC: 930 samples (15.55%)\n",
      "  ENTY: 1300 samples (21.74%)\n",
      "  HUM: 1200 samples (20.07%)\n",
      "  LOC: 800 samples (13.38%)\n",
      "  NUM: 850 samples (14.21%)\n",
      "\n",
      "  Total samples: 4362 -> 5980\n",
      "  ✓ Data augmentation complete\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# Step 3: Apply Data Augmentation for Imbalanced Classes\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n>>> Step 3: Applying Data Augmentation for Imbalanced Classes...\")\n",
    "\n",
    "# Count current label distribution\n",
    "label_counts_p34 = Counter([ex.label for ex in train_data.examples])\n",
    "print(f\"\\nOriginal label distribution:\")\n",
    "for label, count in sorted(label_counts_p34.items()):\n",
    "    print(f\"  {label}: {count} samples ({count/len(train_data.examples)*100:.2f}%)\")\n",
    "\n",
    "# Augmentation targets (boost weaker topics more aggressively)\n",
    "target_counts_p34 = {\n",
    "    'ABBR': 900,   # heavy boost (~13x) to improve weakest class\n",
    "    'DESC': 930,   # keep strong class unchanged\n",
    "    'ENTY': 1300,  # moderate boost (~1.3x)\n",
    "    'HUM': 1200,   # boost (~1.23x)\n",
    "    'LOC': 800,    # modest boost (~1.2x)\n",
    "    'NUM': 850     # modest boost (~1.2x)\n",
    "}\n",
    "\n",
    "# Create augmented examples\n",
    "augmented_examples = list(train_data.examples)  # Start with all original examples\n",
    "\n",
    "for label in label_counts_p34.keys():\n",
    "    current_count = label_counts_p34[label]\n",
    "    target_count = target_counts_p34[label]\n",
    "    \n",
    "    if current_count < target_count:\n",
    "        label_examples = [ex for ex in train_data.examples if ex.label == label]\n",
    "        num_augmentations_needed = target_count - current_count\n",
    "        \n",
    "        print(f\"\\n  Augmenting {label}: {current_count} -> {target_count} samples\")\n",
    "        print(f\"    Generating {num_augmentations_needed} additional samples...\")\n",
    "        \n",
    "        augmented_count = 0\n",
    "        while augmented_count < num_augmentations_needed:\n",
    "            original_ex = random.choice(label_examples)\n",
    "            aug_texts = augment_text(original_ex.text, num_augmentations=1)\n",
    "            \n",
    "            for aug_text in aug_texts:\n",
    "                if augmented_count >= num_augmentations_needed:\n",
    "                    break\n",
    "                \n",
    "                new_ex = data.Example.fromlist([aug_text, label], \n",
    "                                               fields=[('text', TEXT), ('label', LABEL)])\n",
    "                augmented_examples.append(new_ex)\n",
    "                augmented_count += 1\n",
    "        \n",
    "        print(f\"    ✓ Generated {augmented_count} augmented samples\")\n",
    "\n",
    "# Create augmented dataset with proper field structure\n",
    "augmented_train_data = data.Dataset(augmented_examples, fields=[('text', TEXT), ('label', LABEL)])\n",
    "\n",
    "# Verify augmented distribution\n",
    "new_label_counts = Counter([ex.label for ex in augmented_examples])\n",
    "print(f\"\\nAugmented label distribution:\")\n",
    "for label, count in sorted(new_label_counts.items()):\n",
    "    print(f\"  {label}: {count} samples ({count/len(augmented_examples)*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\n  Total samples: {len(train_data.examples)} -> {len(augmented_examples)}\")\n",
    "print(f\"  ✓ Data augmentation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ec4bc0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# augmented_train_data.examples[0].label\n",
    "[(ex.text, ex.label) for ex in augmented_train_data if ex.label not in ['ABBR','DESC','ENTY','HUM','LOC','NUM']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce04616c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(None,\n",
       "            {'ENTY': 0, 'HUM': 1, 'DESC': 2, 'NUM': 3, 'LOC': 4, 'ABBR': 5})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABEL.vocab.stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9afaf5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['General', 'is', 'the', 'abbreviation', 'of', 'What', 'Motors', '?']\n",
      "ABBR\n",
      "['CNN', 'is', 'an', 'what', 'for', 'acronym', '?']\n",
      "ABBR\n",
      "['What', 'is', 'AFS', '?']\n",
      "ABBR\n",
      "['What', 'is', 'for', 'acronym', 'for', 'the', 'rating', 'system', 'the', 'air', 'conditioner', 'efficiency', '?']\n",
      "ABBR\n",
      "['What', 'is', 'Mikhail', 'Gorbachev', \"'s\", 'middle', 'initial', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'abbreviation', 'for', 'stand', 'AIDS', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'full', 'form', 'of', '.com', '?']\n",
      "ABBR\n",
      "['What', 'does', 'IQ', 'stand', 'for', 'manage', '?']\n",
      "ABBR\n",
      "['What', 'is', 'SAP', '?']\n",
      "ABBR\n",
      "['What', 'is', 'Mikhail', 'Gorbachev', \"'s\", 'middle', 'initial', '?']\n",
      "ABBR\n",
      "['What', 'does', 'stand', 'for']\n",
      "ABBR\n",
      "['What', 'does', 'R.E.M.', 'stand', 'for', ',', 'as', 'in', 'the', 'rock', 'group', 'R.E.M.', '?']\n",
      "ABBR\n",
      "['What', 'is', 'abbreviated', 'form', 'of', 'the', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'abbreviation', 'cwt', '.', '?']\n",
      "ABBR\n",
      "['What', 'do', 'the', 'letters', 'D.C.', 'stand', '?', 'in', 'Washington', ',', 'D.C.', 'for']\n",
      "ABBR\n",
      "['What', 'does', 'IQ', 'support', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NN', 'O', '`', 'secondhand', '`', 'intend', 'mean', 'when', 'used', 'as', 'a', 'prefix', 'in', 'Irish', 'surnames', '?']\n",
      "ABBR\n",
      "['What', 'missive', 'do', 'the', 'letters', 'ZIP', 'stand', 'for', 'in', 'the', 'phrase', '`', '`', 'ZIP', 'code', \"''\", '?']\n",
      "ABBR\n",
      "['What', \"'s\", 'the', 'abbreviation', 'for', 'limited', 'partnership', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'abbreviation', 'assistance', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'do', 'the', 'letters', 'ZIP', 'stand', 'for', 'in', 'the', 'phrase', '`', '`', 'ZIP', 'code', \"''\", '?']\n",
      "ABBR\n",
      "['What', 'perform', 'the', '`', '`', 'juicy', 'ribbon', \"''\", 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'abbreviation', 'the', 'does', 'AIDS', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['does', 'CNN', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'SAP', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'of', 'the', 'International', 'Olympic', 'Committee', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', '?', '`', 'c', \"''\", 'stand', 'for', 'in', 'the', 'equation', 'E', '=', 'mc2', '`']\n",
      "ABBR\n",
      "['?', 'does', 'IOC', 'stand', 'for', 'What']\n",
      "ABBR\n",
      "['What', 'does', 'buckler', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'do', 'the', 'letters', 'ZIP', 'standpoint', 'for', 'in', 'the', 'phrase', '`', '`', 'ZIP', 'code', \"''\", '?']\n",
      "ABBR\n",
      "['What', 'do', 'the', 'letters', 'ZIP', 'stand', 'for', 'in', 'the', 'phrase', 'aught', '`', '`', 'ZIP', 'code', 'inscribe', \"''\", '?']\n",
      "ABBR\n",
      "['?', 'does', 'pH', 'stand', 'for', 'What']\n",
      "ABBR\n",
      "['What', 'does', 'IOC', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'dress', 'BTU', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'is', 'SVHS', '?']\n",
      "ABBR\n",
      "['What', 'answer', 'the', 'T.S.', 'stand', 'for', 'in', 'T.S.', 'Eliot', \"'s\", 'name', '?']\n",
      "ABBR\n",
      "['What', 'does', '?', 'mean', 'LOL']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviated', 'term', 'used', 'for', 'the', 'National', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'is', 'cost', 'be', 'RAM', 'in', 'the', 'computer', '?']\n",
      "ABBR\n",
      "['What', 'does', 'CNN', 'stand', 'for', '?', 'dress']\n",
      "ABBR\n",
      "['What', 'do', 'the', 'letters', 'D.C.', 'stand', 'for', 'in', 'D.C.', ',', 'Washington', '?']\n",
      "ABBR\n",
      "['What', 'do', 'the', 'letters', 'ZIP', 'stand', 'for', 'in', 'the', '`', '`', 'ZIP', 'code', \"''\", '?']\n",
      "ABBR\n",
      "['What', 'is', 'LMDS', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'acronym', 'for', 'the', 'rating', 'system', 'for', 'air', 'conditioner', 'efficiency', '?']\n",
      "ABBR\n",
      "['What', 'does', 'IOC', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'do', 'the', 'letters', 'ZIP', 'stand', 'for', 'in', 'the', 'phrase', '`', '`', 'nil', 'code', \"''\", '?']\n",
      "ABBR\n",
      "['What', 'does', 'MSG', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', '?', 'PSI', \"'\", 'stand', 'for', '`']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'T.S.', 'stand', 'for', 'in', 'T.S.', 'Eliot', \"'s\", 'name', '?']\n",
      "ABBR\n",
      "['What', 'is', 'AFS', '?']\n",
      "ABBR\n",
      "['What', 'viewpoint', 'does', 'IBM', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['used', 'is', 'the', 'abbreviated', 'term', 'What', 'for', 'the', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'abbreviation', 'the', 'is', 'of', 'the', 'company', 'name', '`', 'General', 'Motors', \"'\", '?']\n",
      "ABBR\n",
      "['What', 'does', '`', 'PSI', \"'\", 'stand', 'for']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'abbreviation', 'AIDS', 'digest', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'initial', 'Gorbachev', \"'s\", 'middle', 'Mikhail', '?']\n",
      "ABBR\n",
      "['What', 'do', 'the', 'letters', 'ZIP', 'stand', 'for', 'in', 'the', 'phrase', '`', 'ZIP', 'code', \"''\", '?']\n",
      "ABBR\n",
      "['CNN', 'is', 'an', 'acronym', 'for', 'what', '?']\n",
      "ABBR\n",
      "['What', 'does', 'U.S.S.R.', '?', 'for', 'stand']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'acronym', 'CPR', 'mean', 'miserly', '?']\n",
      "ABBR\n",
      "['What', 'does', 'pH', 'stand', 'exercise', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviated', 'form', 'of', 'the', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviated', 'term', 'victimized', 'for', 'the', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'does', 'e.g.', 'stand', '?', 'for']\n",
      "ABBR\n",
      "['What', 'does', 'pH', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'HIV', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'BPH', '?']\n",
      "ABBR\n",
      "['constitute', 'CPR', 'is', 'the', 'abbreviation', 'for', 'what', '?']\n",
      "ABBR\n",
      "['What', 'is', 'Investigation', 'acronym', 'for', 'the', 'National', 'Bureau', 'of', 'the', '?']\n",
      "ABBR\n",
      "['What', 'is', 'HTML', '?']\n",
      "ABBR\n",
      "['When', 'reading', 'stand', 'ads', ',', 'what', 'does', 'EENTY', ':', 'other', 'classified', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'A&W', 'of', 'root', 'beer', 'for', 'stand', 'fame', '?']\n",
      "ABBR\n",
      "['What', 'make', 'JESSICA', 'average', '?']\n",
      "ABBR\n",
      "['What', '?', 'HTML', 'is']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'acronym', 'CPR', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'does', 'HIV', 'stand', 'for']\n",
      "ABBR\n",
      "['What', 'does', 'NASA', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'LMDS', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'acronym', 'cpr', 'CPR', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'does', 'mc2', 'E', 'stand', 'for', 'in', 'the', 'equation', 'E', '=', 'the', '?']\n",
      "ABBR\n",
      "['CNN', 'is', 'an', 'acronym', 'for', 'what', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviated', 'form', 'of', 'the', 'National', 'dresser', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'do', 'the', 'letters', 'D.C.', 'stand', 'for', 'suffice', 'in', 'Washington', ',', 'D.C.', '?', 'dc']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'acronym', 'for', 'the', 'rating', 'system', 'for', 'air', 'conditioner', 'efficiency', '?']\n",
      "ABBR\n",
      "['the', 'abbreviation', 'of', 'the', 'company', 'name', '`', 'General', 'Motors', \"'\", '?']\n",
      "ABBR\n",
      "['What', 'does', 'e.g.', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'constitute', 'LMDS', '?']\n",
      "ABBR\n",
      "['What', 'does', 'MSG', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'abbreviation', 'IOC', 'stand', '?']\n",
      "ABBR\n",
      "['What', 'is', 'AFS', '?']\n",
      "ABBR\n",
      "['What', 'does', 'G.M.T.', 'suffer', 'for', '?']\n",
      "ABBR\n",
      "['What', 'NASDAQ', 'does', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['letter', 'is', 'the', 'correct', 'way', 'to', 'abbreviate', 'cc', '.', 'at', 'the', 'bottom', 'of', 'a', 'business', 'What', '?']\n",
      "ABBR\n",
      "['does', 'U.S.S.R.', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'cause', 'the', 'channel', 'ESPN', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['does', 'MSG', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'do', 'the', 'letters', 'CE', 'stand', 'on', 'for', 'so', 'many', 'products', ',', 'particularly', 'electrical', ',', 'purchased', 'now', '?']\n",
      "ABBR\n",
      "['What', 'does', 'IBM', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['for', 'is', 'the', 'abbreviation', 'What', 'micro', '?']\n",
      "ABBR\n",
      "['What', 'does', '?', 'mean', 'BTU']\n",
      "ABBR\n",
      "['What', 'does', 'NAFTA', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'do', 'the', 'letters', 'D.C.', 'stand', 'for', 'Washington', 'in', ',', 'D.C.', '?']\n",
      "ABBR\n",
      "['What', 'does', 'S.O.S.', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', '`', '`', 'blueing', 'ribbon', \"''\", 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'abbreviation', 'AIDS', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', '?', 'stand', 'for', 'NASA']\n",
      "ABBR\n",
      "['What', 'LMDS', 'is', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NECROSIS', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'does', 'Ms.', ',', 'Miss', ',', 'and', 'Mrs.', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviated', 'term', 'used', 'for', 'the', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', '?', 'Mikhail', 'Gorbachev', \"'s\", 'middle', 'initial', 'is']\n",
      "ABBR\n",
      "['What', 'does', 'e.g.', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'R.E.M.', 'stand', 'aggroup', 'for', ',', 'as', 'indium', 'in', 'the', 'rock', 'group', 'R.E.M.', '?']\n",
      "ABBR\n",
      "['What', 'is', 'LMDS']\n",
      "ABBR\n",
      "['CNN', 'is', 'an', 'acronym', 'for', 'what', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'channel', 'ESPN', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'the', 'International', 'Olympic', 'Committee', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'T.S.', 'stand', 'for', 'in', 'T.S.', 'Eliot', \"'s\", 'name', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'for', 'micro', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'of', 'cosmopolitan', 'Motors', '?']\n",
      "ABBR\n",
      "['What', 'do', 'the', 'letters', 'D.C.', 'stand', 'for', 'in', 'Washington', ',', 'D.C.', '?']\n",
      "ABBR\n",
      "['What', 'the', 'abbreviation', 'of', 'General', 'Motors', '?']\n",
      "ABBR\n",
      "['What', 'is', 'p.m.', 'an', 'abbreviation', 'for', ',', 'as', 'in', '5', 'p.m.', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'channel', 'ESPN', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'correct', 'way', 'to', 'abbreviate', 'cc', '.', '?', 'the', 'bottom', 'of', 'a', 'business', 'letter', 'at']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'equation', 'stand', 'for', 'in', 'the', 'E', 'E', '=', 'mc2', '?']\n",
      "ABBR\n",
      "['When', 'reading', 'classified', 'ads', 'what', ',', 'does', 'EENTY', ':', 'other', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'HIV', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'IOC', 'endure', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'acronym', 'for', 'the', 'rating', 'system', 'for', 'air', 'conditioner', '?']\n",
      "ABBR\n",
      "['What', 'does', 'e.g.', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'exhaust', '?']\n",
      "ABBR\n",
      "['the', 'letters', 'CE', 'stand', 'for', 'on', 'so', 'many', 'products', ',', 'particularly', 'electrical', ',', 'purchased', 'now', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviated', 'term', 'used', 'for', 'the', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', \"'s\", 'the', 'abbreviation', 'for', 'trinitrotoluene', '?']\n",
      "ABBR\n",
      "['?', 'does', 'CNN', 'stand', 'for', 'What']\n",
      "ABBR\n",
      "['What', '?', 'the', 'full', 'form', 'of', '.com', 'is']\n",
      "ABBR\n",
      "['What', 'does', 'S.O.S.', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['practise', 'What', 'does', 'abide', 'e.g.', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'e.g.', 'abide', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'acronym', 'for', 'the', 'National', 'Bureau', 'of', 'Investigation']\n",
      "ABBR\n",
      "['What', '?', 'BTU', 'mean', 'does']\n",
      "ABBR\n",
      "['What', 'act', 'JESSICA', 'mean', '?']\n",
      "ABBR\n",
      "['When', 'reading', 'classified', ',', 'what', 'does', 'EENTY', ':', 'other', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NN', 'O', 'inwards', '`', '`', 'mean', 'when', 'used', 'as', 'a', 'prefix', 'in', 'Irish', 'surnames', '?']\n",
      "ABBR\n",
      "['What', 'does', 'CNN', 'standpoint', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'HDLC', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'abbreviation', 'for', 'stand', 'AIDS', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviated', 'form', 'of', 'the', 'National', 'Bureau', 'of', 'investigation', '?']\n",
      "ABBR\n",
      "['What', 'does', '`', 'PSI', \"'\", 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'dress', 'the', 'abbreviation', 'attention', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'SHIELD', 'stand', '?']\n",
      "ABBR\n",
      "['What', 'does', 'BMW', 'execute', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'do', 'the', 'letters', 'CE', 'stand', 'for', 'on', 'so', 'many', 'products', ',', 'particularly', 'electrical', ',', 'buy', 'now', '?']\n",
      "ABBR\n",
      "['is', 'HDLC']\n",
      "ABBR\n",
      "['stand', 'does', 'NASA', 'What', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'R.E.M.', 'stand', 'for', ',', 'as', 'rock', 'group', 'R.E.M.']\n",
      "ABBR\n",
      "['What', '?', 'EKG', 'stand', 'for', 'does']\n",
      "ABBR\n",
      "['What', 'is', 'p.m.', 'an', 'abbreviation', 'for', 'inward', ',', 'as', 'in', '5', 'p.m.', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'of', 'General', 'Motors', '?']\n",
      "ABBR\n",
      "['What', 'does', '?', 'stand', 'for', 'IOC']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'abbreviation', 'AIDS', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'CNN', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'e.g.', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'of', 'the', 'company', 'name', '`', 'General', 'Motors', \"'\", '?']\n",
      "ABBR\n",
      "['What', 'does', 'RCA', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'correct', 'way', 'to', 'abbreviate', 'cc', '.', 'the', 'bottom', 'of', 'a', 'business', 'letter']\n",
      "ABBR\n",
      "['What', 'is', 'SVHS', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviated', 'expression', 'for', 'the', 'National', 'Bureau', 'of', '?']\n",
      "ABBR\n",
      "['What', 'does', 'IQ', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'of', 'the', 'company', 'drive', 'name', '`', 'General', 'Motors', \"'\", '?']\n",
      "ABBR\n",
      "['What', 'is', 'AFS', '?']\n",
      "ABBR\n",
      "['is', 'What', 'SAP', '?']\n",
      "ABBR\n",
      "['What', 'does', 'G.M.T.', 'bear', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', '`', 'PSI', \"'\", 'bandstand', 'for', '?']\n",
      "ABBR\n",
      "['What', '?', 'LOL', 'mean', 'does']\n",
      "ABBR\n",
      "['What', 'HDLC', 'is', '?']\n",
      "ABBR\n",
      "['What', 'is', 'p.m.', 'an', 'abbreviation', 'for', ',', 'as', 'in', '5', 'p.m.', '?', 'equal']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'acronym', 'for', 'the', 'rating', 'system', 'for', 'air', 'conditioner', 'efficiency', '?']\n",
      "ABBR\n",
      "['abbreviated', 'is', 'the', 'What', 'term', 'used', 'for', 'the', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'is', 'acronym', 'for', 'the', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['in', 'is', 'RAM', 'What', 'the', 'computer', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviated', 'expression', 'for', 'the', 'National', 'contract', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'does', 'stand', 'PSI', \"'\", '`', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'pH', 'stand', '?', 'for']\n",
      "ABBR\n",
      "['What', 'does', 'BTU', 'mean']\n",
      "ABBR\n",
      "['What', 'does', 'the', '`', '`', 'blue', 'ribbon', \"''\", 'standstill', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'of', 'oecumenical', 'Motors', '?']\n",
      "ABBR\n",
      "['What', 'is', 'bph', '?']\n",
      "ABBR\n",
      "['of', 'is', 'the', 'acronym', 'for', 'the', 'National', 'Bureau', 'What', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'does', 'BTU', 'btu', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviate', 'form', 'of', 'the', 'interior', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'is', 'AFS', '?']\n",
      "ABBR\n",
      "['What', 'does', 'snafu', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'for', 'EKG', 'stand', 'does', '?']\n",
      "ABBR\n",
      "['What', 'does', 'for', 'stand', 'SHIELD', '?']\n",
      "ABBR\n",
      "['What', 'is', 'Bureau', 'abbreviated', 'expression', 'for', 'the', 'National', 'the', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NASA', 'stand', 'for', 'energy', '?']\n",
      "ABBR\n",
      "['What', 'is', 'p.m.', 'an', 'abbreviation', 'for', ',', 'as', 'in', '5', 'p.m.', '?']\n",
      "ABBR\n",
      "['is', 'the', 'abbreviated', 'form', 'of', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'does', 'RCA', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'BMW', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'Ms.', ',', 'Miss', ',', 'and', 'Mrs.', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['does', 'snafu', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NECROSIS', '?', 'mean']\n",
      "ABBR\n",
      "['What', 'does', 'LOL', 'perform', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', '`', '`', 'blue', 'ribbon', \"''\", 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'full', 'form', 'of', '?', '.com']\n",
      "ABBR\n",
      "['What', 'does', 'mean', 'BTU', '?']\n",
      "ABBR\n",
      "['What', 'does', 'IOC', 'suffer', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'BTU', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'abbreviation', 'cwt', '.', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NN', 'O', '`', '`', 'mean', 'when', 'used', 'in', 'a', 'prefix', 'as', 'Irish', 'surnames', '?']\n",
      "ABBR\n",
      "['What', 'does', 'abbreviation', 'the', 'IOC', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'exhaust', '?']\n",
      "ABBR\n",
      "['When', 'reading', 'classified', 'ads', ',', 'what', 'does', 'EENTY', ':', 'other', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'mean', 'LOL', '?']\n",
      "ABBR\n",
      "['What', 'is', 'DTMF', '?']\n",
      "ABBR\n",
      "['What', 'for', 'the', 'acronym', 'is', 'the', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'abbreviation', 'AIDS', 'stand', 'for', 'execute', '?']\n",
      "ABBR\n",
      "['What', 'is', 'LMDS', '?']\n",
      "ABBR\n",
      "['What', 'does', 'nasdaq', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviated', 'form', 'of', 'the', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'do', 'the', 'letters', '?', 'stand', 'for', 'in', 'Washington', ',', 'D.C.', 'D.C.']\n",
      "ABBR\n",
      "['What', 'is', 'DTMF', '?']\n",
      "ABBR\n",
      "['What', 'does', 'CNN', 'standpoint', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'A&W', '?', 'root', 'beer', 'fame', 'stand', 'for', 'of']\n",
      "ABBR\n",
      "['What', \"'s\", 'the', '?', 'for', 'trinitrotoluene', 'abbreviation']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'for', 'micro', '?']\n",
      "ABBR\n",
      "['suffice', 'What', 'does', 'RCA', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'E', 'stand', 'for', 'in', 'the', 'equality', 'E', '=', 'mc2', '?']\n",
      "ABBR\n",
      "['What', 'SVHS', '?']\n",
      "ABBR\n",
      "['What', 'rack', 'does', 'the', 'abbreviation', 'IOC', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'abbreviation', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'Bureau', 'expression', 'for', 'the', 'National', 'abbreviated', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NASA', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'p.m.', 'an', 'abbreviation', 'for', ',', 'as', 'in', '5', 'p.m.', '?']\n",
      "ABBR\n",
      "['What', \"'s\", 'the', 'abbreviation', 'for', 'limited', 'partnership', '?']\n",
      "ABBR\n",
      "['doe', 'What', 'does', 'NECROSIS', 'mean', 'energy', '?']\n",
      "ABBR\n",
      "['CNN', 'is', 'an', 'acronym', 'for', 'what', '?']\n",
      "ABBR\n",
      "['What', 'is', 'abbreviated', 'expression', 'for', 'the', 'National', 'Bureau', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'does', 'snafu', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['stand', 'does', 'G.M.T.', 'What', 'for', '?']\n",
      "ABBR\n",
      "['What', 'mean', 'LOL', 'does', '?']\n",
      "ABBR\n",
      "['What', 'does', 'IOC', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'IQ', 'standstill', 'for', '?']\n",
      "ABBR\n",
      "['CPR', 'is', 'the', 'abbreviation', 'for', 'what', 'cpr', '?']\n",
      "ABBR\n",
      "['manage', 'What', 'does', 'IOC', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'in', 'the', 'computer', '?']\n",
      "ABBR\n",
      "['What', 'is', 'an', 'for', ',', 'as', 'in', '5', 'p.m.', '?']\n",
      "ABBR\n",
      "['What', 'is', 'RAM', 'in', 'the', 'computer', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', '?', 'way', 'to', 'abbreviate', 'cc', '.', 'at', 'the', 'bottom', 'of', 'a', 'business', 'letter', 'correct']\n",
      "ABBR\n",
      "['What', 'does', 'A&W', 'root', 'fame', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'RAM', 'in', 'comprise', 'contain', 'the', 'computer', '?']\n",
      "ABBR\n",
      "['What', 'execute', 'NN', 'O', '`', '`', 'imply', 'when', 'used', 'as', 'a', 'prefix', 'in', 'Irish', 'surnames', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', '`', '`', 'blue', 'medal', \"''\", 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'IQ', 'stand', '?']\n",
      "ABBR\n",
      "['does', 'What', 'G.M.T.', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['?', 'does', 'snafu', 'stand', 'for', 'What']\n",
      "ABBR\n",
      "['?', 'is', 'the', 'abbreviation', 'of', 'General', 'Motors', 'What']\n",
      "ABBR\n",
      "['What', \"'s\", 'limited', 'abbreviation', 'for', 'the', 'partnership', '?']\n",
      "ABBR\n",
      "['What', 'is', 'DTMF', '?']\n",
      "ABBR\n",
      "['What', 'cause', 'IOC', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'sids', 'tolerate', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'of', 'the', 'company', 'name', '`', 'General', 'Motors', 'troupe', \"'\", '?']\n",
      "ABBR\n",
      "['What', 'the', 'abbreviated', 'expression', 'for', 'the', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'abbreviation', 'assist', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'HIV', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'the', 'the', 'abbreviated', 'form', 'of', 'is', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'does', 'G.M.T.', 'stand', 'for', '?', 'stall']\n",
      "ABBR\n",
      "['What', 'does', 'IBM', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'HDLC', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NN', 'indium', 'O', '`', '`', 'mean', 'when', 'used', 'as', 'a', 'prefix', 'in', 'Irish', 'surnames', '?']\n",
      "ABBR\n",
      "['?', 'does', 'the', 'abbreviation', 'AIDS', 'stand', 'for', 'What']\n",
      "ABBR\n",
      "['What', 'does', 'the', '`', '`', 'c', \"''\", 'stand', 'for', 'in', 'the', 'equation', 'E', '=', 'mc2', '?']\n",
      "ABBR\n",
      "['What', 'does', 'for', 'stand', 'SHIELD', '?']\n",
      "ABBR\n",
      "['What', '?', 'the', 'abbreviation', 'cwt', '.', 'does']\n",
      "ABBR\n",
      "['What', 'does', 'IQ', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'E', 'stand', 'for', 'in', 'the', 'equation', 'E', '=', 'mc2', '?']\n",
      "ABBR\n",
      "['What', '?', 'BPH', 'is']\n",
      "ABBR\n",
      "['What', 'does', 'stand', 'for', ',', 'as', 'the', 'rock', 'group', 'R.E.M.', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'organisation', 'acronym', 'for', 'the', 'rating', 'system', 'for', 'air', 'conditioner', 'efficiency', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'abbreviation', 'cwt']\n",
      "ABBR\n",
      "['is', 'What', 'HDLC', '?']\n",
      "ABBR\n",
      "['What', 'does', 'LOL', '?', 'mean']\n",
      "ABBR\n",
      "['What', 'does', 'NECROSIS', '?', 'mean']\n",
      "ABBR\n",
      "['DTMF', 'is', 'What', '?']\n",
      "ABBR\n",
      "['?', 'does', 'NECROSIS', 'mean', 'What']\n",
      "ABBR\n",
      "['nafta', 'What', 'does', 'NAFTA', 'stand', 'standpoint', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'acronym', 'CPR', 'mean', '?']\n",
      "ABBR\n",
      "['for', 'does', 'RCA', 'stand', 'What', '?']\n",
      "ABBR\n",
      "['What', 'does', 'e.g.', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'do', 'the', 'ZIP', 'stand', 'for', 'in', 'the', 'phrase', '`', '`', 'ZIP', 'code', \"''\", '?']\n",
      "ABBR\n",
      "['What', 'exercise', '`', 'PSI', \"'\", 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'do', 'the', 'D.C.', 'stand', 'for', 'in', 'Washington', ',', '?']\n",
      "ABBR\n",
      "['What', 'is', 'gorbachev', 'Mikhail', 'Gorbachev', 'eye', \"'s\", 'middle', 'initial', '?']\n",
      "ABBR\n",
      "['What', 'is', 'p.m.', 'an', 'abbreviation', 'for', ',', 'as', 'in', '5', 'p.m.', '?']\n",
      "ABBR\n",
      "['What', 'does', 'SHIELD', '?', 'for', 'stand']\n",
      "ABBR\n",
      "['What', '?', 'IBM', 'stand', 'for', 'does']\n",
      "ABBR\n",
      "['does', 'NASA', 'for', '?']\n",
      "ABBR\n",
      "['What', \"'s\", 'the', 'abbreviation', 'for', 'trinitrotoluene', '?', 'tnt']\n",
      "ABBR\n",
      "['term', 'is', 'the', 'abbreviated', 'What', 'used', 'for', 'the', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'is', '?', 'DTMF']\n",
      "ABBR\n",
      "['What', 'does', 'btu', 'think', '?']\n",
      "ABBR\n",
      "['What', 'the', 'abbreviated', 'expression', 'for', 'the', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', '`', '`', 'blue', 'ribbon', \"''\", 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'HIV', 'standstill', 'for', '?']\n",
      "ABBR\n",
      "['What', 'the', 'of', 'the', 'company', 'name', '`', 'General', \"'\", '?']\n",
      "ABBR\n",
      "['What', 'does', 'sphacelus', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'does', 'sids', 'digest', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviated', 'mannikin', 'of', 'the', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'truncated', 'expression', 'for', 'the', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['CPR', 'for', 'the', 'abbreviation', 'is', 'what', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'T.S.', 'stand', 'for', 'in', 'T.S.', 'suffice', 'Eliot', \"'s\", 'name', '?']\n",
      "ABBR\n",
      "['What', 'arrangement', 'is', 'the', 'acronym', 'for', 'the', 'rating', 'system', 'for', 'air', 'conditioner', 'efficiency', '?']\n",
      "ABBR\n",
      "['What', 'does', 'S.O.S.', 'suffer', 'for', '?']\n",
      "ABBR\n",
      "['does', 'What', 'pH', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'S.O.S.', 'does', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['of', 'is', 'the', 'abbreviated', 'expression', 'for', 'the', 'National', 'Bureau', 'What', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', '?', 'BPH', 'is']\n",
      "ABBR\n",
      "['What', 'does', 'MSG', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['cwt', 'does', 'the', 'abbreviation', 'What', '.', '?']\n",
      "ABBR\n",
      "['What', 'does', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'G.M.T.', 'support', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'Mikhail', 'gorbachev', \"'s\", 'middle', 'initial', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'Motors', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'abbreviation', 'quintal', '.', '?']\n",
      "ABBR\n",
      "['does', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'correct', 'way', 'to', 'abbreviate', 'cc', '.', 'at', 'the', 'bottom', 'of', 'a', 'business', 'letter', '?']\n",
      "ABBR\n",
      "['What', 'does', 'S.O.S.', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', \"'s\", 'the', 'abbreviation', 'for', 'trinitrotoluene', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', '`', '`', 'blue', 'ribbon', \"''\", 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'HTML', 'is', '?']\n",
      "ABBR\n",
      "['CPR', 'is', 'the', 'abbreviation', 'for', 'what', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'of', 'the', 'company', 'name', '`', 'general', 'Motors', \"'\", '?']\n",
      "ABBR\n",
      "['What', 'does', 'MSG', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'BTU', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'does', 'practice', 'RCA', 'stand', 'for', '?', 'viewpoint']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'full', 'form', 'of', '.com', '?']\n",
      "ABBR\n",
      "['base', 'What', 'does', 'NASDAQ', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'A&W', 'of', 'root', 'beer', 'fame', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'abbreviation', 'IOC', '?', 'for', 'stand']\n",
      "ABBR\n",
      "['What', 'does', 'BMW', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'Ms.', 'and', 'Miss', ',', ',', 'Mrs.', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'Mikhail', 'Gorbachev', \"'s\", 'centre', 'initial', '?']\n",
      "ABBR\n",
      "['What', 'is', 'SVHS', '?']\n",
      "ABBR\n",
      "['What', 'is', 'HTML', '?']\n",
      "ABBR\n",
      "['does', 'NASA', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviated', 'form', 'of', 'the', 'interior', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'does', '?', 'stand', 'for', 'NASA']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'correct', 'way', 'to', 'abbreviate', 'cc', '.', 'at', 'the', 'bottom', 'of', 'a', 'business', 'letter', '?']\n",
      "ABBR\n",
      "['What', 'does', 'IOC', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'HDLC', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NN', 'O', '`', '`', 'mean', 'when', 'used', 'as', 'prefix', 'Irish', 'surnames', '?']\n",
      "ABBR\n",
      "['What', 'middle', 'Mikhail', 'Gorbachev', \"'s\", 'is', 'initial', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'correct', 'shorten', 'way', 'to', 'abbreviate', 'cc', '.', 'at', 'the', 'bottom', 'of', 'a', 'business', 'letter', '?']\n",
      "ABBR\n",
      "['What', 'fare', 'S.O.S.', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'correct', 'way', 'the', 'abbreviate', 'cc', '.', 'at', 'to', 'bottom', 'of', 'a', 'business', 'letter', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'of', 'the', 'company', '`', 'General', 'Motors', \"'\", '?']\n",
      "ABBR\n",
      "['What', 'coif', 'RCA', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'acronym', 'for', 'the', 'atmosphere', 'rating', 'system', 'for', 'air', 'conditioner', 'efficiency', '?']\n",
      "ABBR\n",
      "['What', 'coiffe', 'BTU', 'mean', '?']\n",
      "ABBR\n",
      "['What', '?', 'the', 'T.S.', 'stand', 'for', 'in', 'T.S.', 'Eliot', \"'s\", 'name', 'does']\n",
      "ABBR\n",
      "['is', 'What', 'DTMF', '?']\n",
      "ABBR\n",
      "['What', 'does', '`', 'the', '`', 'blue', 'ribbon', \"''\", 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'channel', 'ESPN', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'correct', 'of', 'to', 'abbreviate', 'cc', '.', 'at', 'the', 'bottom', 'way', 'a', 'business', 'letter', '?']\n",
      "ABBR\n",
      "['What', 'does', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviated', 'expression', 'for', 'the', 'subject', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'does', 'nasa', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'stand', 'U.S.S.R.', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'BTU', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'does', 'LOL', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NASA', 'stand', 'nasa', 'for', '?']\n",
      "ABBR\n",
      "[\"'s\", 'the', 'abbreviation', 'for', 'limited', 'partnership', '?']\n",
      "ABBR\n",
      "['What', 'is', '?', 'SAP']\n",
      "ABBR\n",
      "['CPR', 'is', 'the', 'abbreviation', 'for', 'what', 'cpr', '?']\n",
      "ABBR\n",
      "['What', 'come', 'the', 'abbreviation', 'IOC', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'U.S.S.R.', 'stand', 'behave', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'pH', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'pH', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'G.M.T.', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'RAM', 'in', 'the', 'follow', 'computer', '?']\n",
      "ABBR\n",
      "['What', 'do', 'dc', 'the', 'letters', 'D.C.', 'stand', 'for', 'in', 'Washington', ',', 'capital', 'D.C.', '?']\n",
      "ABBR\n",
      "['What', 'does', 'IOC', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'come', 'U.S.S.R.', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'JESSICA', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'does', 'S.O.S.', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'stand', 'snafu', 'does', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'correct', 'way', 'to', 'abbreviate', 'cc', '.', 'at', 'the', 'bottom', 'of', 'a', 'business', 'letter', '?']\n",
      "ABBR\n",
      "['What', 'does', 'BTU', 'btu', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'the', 'the', 'abbreviation', 'of', 'is', 'company', 'name', '`', 'General', 'Motors', \"'\", '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'abbreviation', 'aids', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'RCA', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['?', 'does', 'IOC', 'stand', 'for', 'What']\n",
      "ABBR\n",
      "['What', '?', 'BPH', 'is']\n",
      "ABBR\n",
      "['What', 'does', 'IQ', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'the', 'abbreviation', 'for', 'limited', 'partnership', '?']\n",
      "ABBR\n",
      "['What', 'the', 'does', 'word', 'LASER', 'mean', '?']\n",
      "ABBR\n",
      "['What', \"'s\", 'partnership', 'abbreviation', 'for', 'limited', 'the', '?']\n",
      "ABBR\n",
      "['When', 'reading', 'classified', 'ads', ',', 'what', 'does', 'EENTY', ':', 'other', 'stand', 'for', '?']\n",
      "ABBR\n",
      "[\"'s\", 'the', 'for', 'trinitrotoluene', '?']\n",
      "ABBR\n",
      "['What', 'does', 'make', 'SIDS', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'HDLC', '?']\n",
      "ABBR\n",
      "['What', 'does', 'SIDS', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'SHIELD', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['Gorbachev', 'is', 'Mikhail', 'What', \"'s\", 'middle', 'initial', '?']\n",
      "ABBR\n",
      "['CNN', 'is', 'an', 'acronym', 'for', 'what', '?']\n",
      "ABBR\n",
      "['What', \"'s\", 'the', 'abbreviation', 'for', 'tnt', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviated', 'form', 'of', 'the', 'internal', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'does', 'LOL', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'is', 'SAP', '?']\n",
      "ABBR\n",
      "['What', 'does', 'sphacelus', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'is', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviated', 'term', 'used', 'for', 'the', 'National', 'dresser', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'does', 'mean', 'JESSICA', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'contract', 'form', 'of', 'the', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'caller', 'is', 'the', 'abbreviation', 'of', 'the', 'company', 'name', '`', 'General', 'Motors', \"'\", '?']\n",
      "ABBR\n",
      "['What', 'does', 'standstill', 'U.S.S.R.', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['equal', 'What', 'is', 'LMDS', '?']\n",
      "ABBR\n",
      "['CNN', 'is', 'an', 'acronym', 'for', 'what', '?']\n",
      "ABBR\n",
      "['What', 'abridge', 'is', 'the', 'abbreviated', 'term', 'used', 'for', 'the', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', \"'s\", 'the', 'abbreviation', 'for', 'limited', 'partnership', '?']\n",
      "ABBR\n",
      "['does', 'What', 'the', 'acronym', 'CPR', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'of', 'universal', 'Motors', '?']\n",
      "ABBR\n",
      "['What', 'does', 'CNN', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', '`', '`', 'blue', 'medallion', \"''\", 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'acronym', 'for', 'the', 'National', 'Bureau', 'of', 'Investigation', '?', 'live']\n",
      "ABBR\n",
      "['What', 'does', 'RCA', 'stand', 'for']\n",
      "ABBR\n",
      "['What', 'does', 'the', '`', '`', 'blue', 'ribbon', \"''\", 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'of', 'the', 'International', 'Olympic', 'Committee', '?']\n",
      "ABBR\n",
      "['What', 'is', '?', 'AFS']\n",
      "ABBR\n",
      "['does', 'What', 'NECROSIS', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'for', 'the', 'channel', 'ESPN', 'stand', 'does', '?']\n",
      "ABBR\n",
      "['What', 'is', 'BPH', 'bph', '?']\n",
      "ABBR\n",
      "['What', 'does', 'U.S.S.R.', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'abbreviation', 'IOC', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'nafta', 'suffer', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'company', 'the', 'of', 'name', '`', 'General', 'Motors', \"'\", '?']\n",
      "ABBR\n",
      "['What', 'do', 'the', 'letters', 'CE', 'stand', 'for', 'on', 'so', 'many', 'products', ',', 'particularly', 'electrical', ',', 'purchased', 'nowadays', '?']\n",
      "ABBR\n",
      "['What', '?', 'the', '`', '`', 'blue', 'ribbon', \"''\", 'stand', 'for', 'does']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'T.S.', 'stand', 'for', 'in', 'T.S.', 'name', \"'s\", 'Eliot', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'for', 'micro', '?']\n",
      "ABBR\n",
      "['What', 'does', 'MSG', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'LMDS', 'embody', '?']\n",
      "ABBR\n",
      "['What', 'does', 'EKG', '?', 'for', 'stand']\n",
      "ABBR\n",
      "['What', 'does', 'RCA', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'HDLC', '?']\n",
      "ABBR\n",
      "['What', 'does', 'EKG', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'do', 'the', 'letters', 'D.C.', 'stand', 'for', 'in', 'capital', ',', 'D.C.', '?']\n",
      "ABBR\n",
      "['What', 'does', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'SIDS', 'stand', 'resist', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'PSI', \"'\", 'stand', 'for', '?']\n",
      "ABBR\n",
      "['stand', 'does', 'NASDAQ', 'What', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'S.O.S.', 'act', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'LOL', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'for', 'the', 'abbreviation', \"'s\", 'trinitrotoluene', '?']\n",
      "ABBR\n",
      "['What', 'is', 'AFS', '?']\n",
      "ABBR\n",
      "['What', 'does', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'snafu', 'stand', '?']\n",
      "ABBR\n",
      "['What', 'does', 'stand', 'for']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'abbreviation', 'AIDS', 'stall', 'for', '?']\n",
      "ABBR\n",
      "['BPH', 'is', 'What', '?']\n",
      "ABBR\n",
      "['What', \"'s\", 'the', 'abbreviation', 'for', 'tnt', '?']\n",
      "ABBR\n",
      "['HTML', 'is', 'What', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NASA', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', '?', 'the', 'acronym', 'for', 'the', 'National', 'Bureau', 'of', 'Investigation', 'is']\n",
      "ABBR\n",
      "['What', 'does', 'IOC', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'SVHS', 'is', '?']\n",
      "ABBR\n",
      "['When', 'recitation', 'classified', 'ads', ',', 'what', 'does', 'EENTY', ':', 'other', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'the', \"'s\", 'abbreviation', 'for', 'trinitrotoluene', '?']\n",
      "ABBR\n",
      "['What', 'do', 'the', 'letters', 'ZIP', 'stand', 'for', 'in', 'the', 'phrase', '`', '`', 'ZIP', 'code', \"''\", '?']\n",
      "ABBR\n",
      "['What', 'does', 'SIDS', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'R.E.M.', 'inch', 'stand', 'for', ',', 'as', 'in', 'the', 'rock', 'group', 'R.E.M.', '?']\n",
      "ABBR\n",
      "['What', '?', 'SVHS', 'is']\n",
      "ABBR\n",
      "['CPR', 'is', 'the', 'abbreviation', 'for', 'what', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'abbreviation', 'IOC', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'doe', 'IBM', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'full', 'of', '.com', '?']\n",
      "ABBR\n",
      "['What', 'does', 'RCA', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'HDLC', '?']\n",
      "ABBR\n",
      "['What', 'does', 'IQ', 'stand', 'for']\n",
      "ABBR\n",
      "['What', 'does', 'LOL', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'is', 'AFS', '?']\n",
      "ABBR\n",
      "['What', 'does', 'IBM', 'tolerate', 'for', '?']\n",
      "ABBR\n",
      "['What', \"'s\", 'the', 'abbreviation', 'for', 'limited', 'partnership', '?']\n",
      "ABBR\n",
      "['What', 'is', 'Mikhail', 'Gorbachev', \"'s\", 'middle', 'initial', '?']\n",
      "ABBR\n",
      "['What', 'does', 'CNN', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'U.S.S.R.', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'the', 'abbreviation', 'for', 'trinitrotoluene', '?']\n",
      "ABBR\n",
      "['What', 'does', 'IBM', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['stand', 'does', 'IBM', 'What', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'T.S.', 'stand', 'for', 'in', 'T.S.', 'Eliot', \"'s\", 'name', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'micro', 'for', 'abbreviation', '?']\n",
      "ABBR\n",
      "['What', 'does', 'HIV', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'stand', 'ESPN', 'channel', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'snafu', 'bandstand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'A&W', 'root', 'of', 'beer', 'fame', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'answer', 'IQ', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'e.g.', 'stand', 'for', '?', 'resist']\n",
      "ABBR\n",
      "['What', 'does', 'LOL', 'mean']\n",
      "ABBR\n",
      "['doe', 'What', 'does', 'JESSICA', 'mean', 'miserly', '?']\n",
      "ABBR\n",
      "['What', 'practice', 'ekg', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'do', 'the', 'letters', 'D.C.', 'stand', ',', 'in', 'Washington', 'for', 'D.C.', '?']\n",
      "ABBR\n",
      "['What', 'viewpoint', 'does', '`', 'PSI', \"'\", 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', '`', '`', 'blue', 'ribbon', '?', 'stand', 'for', \"''\"]\n",
      "ABBR\n",
      "['CNN', 'is', 'an', 'acronym', 'equal', 'for', 'what', '?']\n",
      "ABBR\n",
      "['What', 'stand', 'NAFTA', 'does', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NASA', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'do', 'the', 'letters', 'CE', 'electrical', 'for', 'on', 'so', 'many', 'products', ',', 'particularly', 'stand', ',', 'purchased', 'now', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviated', 'terminus', 'employ', 'for', 'the', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'is', 'full', 'form', 'of', '.com', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'abbreviation', '.', 'cwt', '?']\n",
      "ABBR\n",
      "['What', 'does', 'CNN', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'coiffure', 'R.E.M.', 'stand', 'for', ',', 'as', 'in', 'the', 'rock', 'group', 'R.E.M.', '?']\n",
      "ABBR\n",
      "['What', 'dress', 'BTU', 'beggarly', '?']\n",
      "ABBR\n",
      "['What', 'does', 'JESSICA', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'is', 'Mikhail', 'Gorbachev', 'middle', 'initial', '?']\n",
      "ABBR\n",
      "['What', 'does', 'IQ', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'RCA', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'IQ', 'does', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['live', 'What', 'is', 'HDLC', '?']\n",
      "ABBR\n",
      "['What', 'digest', 'does', 'EKG', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['stand', 'does', 'SIDS', 'What', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'mean', 'BTU', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'way', 'correct', 'to', 'abbreviate', 'cc', '.', 'at', 'the', 'bottom', 'of', 'a', 'business', 'letter', '?']\n",
      "ABBR\n",
      "['is', 'an', 'acronym', 'for', 'what', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', '`', '`', 'c', \"''\", 'base', 'for', 'in', 'the', 'equation', 'E', '=', 'mc2', '?']\n",
      "ABBR\n",
      "['What', 'the', 'is', 'full', 'form', 'of', '.com', '?']\n",
      "ABBR\n",
      "['What', 'does', 'snafu', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'SHIELD', 'resist', 'for', '?']\n",
      "ABBR\n",
      "['What', 'the', 'correct', 'way', 'to', 'abbreviate', 'cc', '.', 'at', 'the', 'of', 'a', '?']\n",
      "ABBR\n",
      "['What', 'make', 'nasdaq', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['When', 'reading', 'classify', 'ads', ',', 'what', 'does', 'EENTY', ':', 'other', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'MSG', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'RAM', 'in', 'the', 'figurer', 'computer', 'indiana', '?']\n",
      "ABBR\n",
      "['What', 'does', 'cwt', 'abbreviation', 'the', '.', '?']\n",
      "ABBR\n",
      "['What', \"'s\", 'the', 'abbreviation', 'for', 'limit', 'partnership', '?']\n",
      "ABBR\n",
      "['is', 'What', 'the', 'acronym', 'for', 'the', 'rating', 'system', 'for', 'air', 'conditioner', 'efficiency', '?']\n",
      "ABBR\n",
      "['What', 'is', 'BPH', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NAFTA', 'base', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'acronym', 'CPR', 'mean']\n",
      "ABBR\n",
      "['What', 'do', 'the', 'letters', 'ZIP', 'stand', 'for', 'in', 'the', 'phrase', '`', '`', 'aught', 'code', \"''\", '?']\n",
      "ABBR\n",
      "['What', 'does', 'hiv', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'CNN', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['rack', 'What', 'does', 'SIDS', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'word', 'LASER', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NECROSIS', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'does', '`', 'PSI', \"'\", 'stand', 'for', '?']\n",
      "ABBR\n",
      "['does', 'What', 'A&W', 'of', 'root', 'beer', 'fame', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'of', 'the', 'Olympic', 'International', 'Committee', '?']\n",
      "ABBR\n",
      "['What', 'is', 'HDLC', '?']\n",
      "ABBR\n",
      "['does', 'the', 'T.S.', 'stand', 'for', 'T.S.', 'Eliot', \"'s\", 'name', '?']\n",
      "ABBR\n",
      "['What', 'does', 'IOC', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['endure', 'What', 'does', 'SIDS', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'SHIELD', '?', 'for', 'stand']\n",
      "ABBR\n",
      "['What', 'for', 'pH', 'stand', 'does', '?']\n",
      "ABBR\n",
      "['What', 'is', 'for', 'the', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'is', 'sap', '?']\n",
      "ABBR\n",
      "['What', 'is', 'HDLC', '?']\n",
      "ABBR\n",
      "['What', \"'s\", 'the', 'abbreviation', 'for', 'limited', 'partnership', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'acronym', 'for', 'the', 'rating', 'system', 'for', 'air', 'conditioner', 'efficiency', '?']\n",
      "ABBR\n",
      "['What', 'do', 'the', 'letters', 'CE', 'stand', 'for', 'on', 'so', 'many', 'products', ',', 'particularly', 'electrical', ',', 'purchased', 'now', '?']\n",
      "ABBR\n",
      "['What', 'does', 'EKG', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'SIDS', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'abbreviation', 'cwt', '?', '.']\n",
      "ABBR\n",
      "['What', 'LMDS', 'is', '?']\n",
      "ABBR\n",
      "['What', 'NASA', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'the', 'does', 'word', 'LASER', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'does', 'nasa', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'channel', 'ESPN', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['CPR', 'is', 'the', 'abbreviation', 'for', 'what', '?']\n",
      "ABBR\n",
      "['What', 'does', 'BTU', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'in', 'the', 'letters', 'D.C.', 'stand', 'for', 'do', 'Washington', ',', 'D.C.', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'of', 'General', 'motor', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'of', 'the', 'International', 'Olympic', 'Committee', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviated', 'form', 'of', 'the', 'National', 'Bureau', 'of', 'Investigation']\n",
      "ABBR\n",
      "['What', \"'s\", 'the', 'abbreviation', 'for', 'trinitrotoluene', '?']\n",
      "ABBR\n",
      "['What', 'is', 'an', 'p.m.', 'abbreviation', 'for', ',', 'as', 'in', '5', 'p.m.', '?']\n",
      "ABBR\n",
      "['What', \"'s\", 'the', 'partnership', 'for', 'limited', 'abbreviation', '?']\n",
      "ABBR\n",
      "['What', 'does', 'e.g.', 'stand', 'for']\n",
      "ABBR\n",
      "['What', 'does', 'G.M.T.', 'stand', '?']\n",
      "ABBR\n",
      "['What', 'does', 'SHIELD', 'standpoint', 'for', '?']\n",
      "ABBR\n",
      "['What', 'SVHS', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviated', 'term', 'used', 'for', 'of', 'National', 'Bureau', 'the', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'does', '?', 'stand', 'for', 'NASA']\n",
      "ABBR\n",
      "['What', 'does', 'MSG', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'IOC', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'BPH']\n",
      "ABBR\n",
      "['serve', 'What', 'does', '`', 'PSI', \"'\", 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'is', 'SVHS', '?']\n",
      "ABBR\n",
      "['What', 'does', 'IOC', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', '?', 'LOL', 'mean', 'does']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'of', 'General', 'Motors']\n",
      "ABBR\n",
      "['What', \"'s\", 'the', 'abbreviation', 'for', 'limited', 'partnership', '?']\n",
      "ABBR\n",
      "['What', 'do', 'the', 'letters', 'ZIP', 'stand', 'for', 'in', 'the', 'phrase', '`', '`', 'ZIP', 'code', \"''\", '?']\n",
      "ABBR\n",
      "['What', 'is', 'bph', '?']\n",
      "ABBR\n",
      "['What', 'does', 'S.O.S.', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'hiv', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'btu', 'BTU', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'is', 'SVHS', '?']\n",
      "ABBR\n",
      "['What', 'do', 'the', 'letters', 'D.C.', 'stand', 'for', 'in', 'Washington', ',', 'D.C.', '?']\n",
      "ABBR\n",
      "['What', \"'s\", 'the', 'abbreviation', 'for', 'trinitrotoluene', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'correct', 'business', 'to', 'abbreviate', 'cc', '.', 'at', 'the', 'bottom', 'of', 'a', 'way', 'letter', '?']\n",
      "ABBR\n",
      "['What', 'make', 'the', 'abbreviation', 'AIDS', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'G.M.T.', 'base', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'of', 'the', 'outside', 'olympic', 'Committee', '?']\n",
      "ABBR\n",
      "['What', 'do', 'the', 'letters', 'CE', 'stand', 'for', 'on', 'so', 'many', 'products', ',', 'especially', 'electrical', ',', 'purchased', 'now', '?']\n",
      "ABBR\n",
      "['What', 'does', 'pH', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'RCA', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NN', 'O', '`', '`', 'mean', 'when', 'secondhand', 'as', 'a', 'prefix', 'in', 'irish', 'surnames', '?']\n",
      "ABBR\n",
      "['What', 'does', 'pH', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'live', 'HTML', '?']\n",
      "ABBR\n",
      "['What', 'does', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['is', 'What', 'SVHS', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviated', 'used', 'term', 'for', 'the', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'answer', 'NASDAQ', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'IBM', 'stand', '?', 'for']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'abbreviation', 'cwt', '.', '?']\n",
      "ABBR\n",
      "['stand', 'does', 'SIDS', 'What', 'for', '?']\n",
      "ABBR\n",
      "['What', '?', 'snafu', 'stand', 'for', 'does']\n",
      "ABBR\n",
      "['What', 'does', 'pH', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['for', 'does', 'HIV', 'stand', 'What', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NASA', 'stand', 'answer', 'for', '?']\n",
      "ABBR\n",
      "['?', \"'s\", 'the', 'abbreviation', 'for', 'limited', 'partnership', 'What']\n",
      "ABBR\n",
      "['What', 'does', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NASDAQ', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['is', 'the', 'abbreviation', 'for', 'what', '?']\n",
      "ABBR\n",
      "['What', 'is', 'AFS', '?']\n",
      "ABBR\n",
      "['What', 'is', 'DTMF', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviated', 'expression', 'for', 'the', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', \"'s\", 'the', 'abbreviation', 'for', 'trinitrotoluene', '?']\n",
      "ABBR\n",
      "['What', 'does', 'HIV', 'for', 'stand', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'for', 'micro', '?']\n",
      "ABBR\n",
      "['CNN', 'is', 'an', 'acronym', 'for', 'what', '?']\n",
      "ABBR\n",
      "['CNN', 'an', 'is', 'acronym', 'for', 'what', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NN', 'O', '`', '`', 'mean', 'when', 'used', 'as', 'a', 'prefix', 'in', 'Irish', 'surnames', '?']\n",
      "ABBR\n",
      "['What', 'does', 'RCA', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'LMDS', '?']\n",
      "ABBR\n",
      "['What', 'does', 'A&W', 'of', 'root', 'beer', 'fame', 'stand', 'for', 'come', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NAFTA', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'Mikhail', 'Gorbachev', \"'s\", 'middle', 'initial', '?']\n",
      "ABBR\n",
      "['What', 'does', 'BMW', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'for', 'abbreviation', 'AIDS', 'stand', 'the', '?']\n",
      "ABBR\n",
      "['CNN', 'is', 'acronym', 'an', 'for', 'what', '?']\n",
      "ABBR\n",
      "['What', 'does', 'e.g.', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'RCA', 'for', 'stand', '?']\n",
      "ABBR\n",
      "['What', 'cause', 'IQ', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'behave', 'does', 'snafu', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['is', 'abbreviated', 'for', 'the', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviated', 'expression', 'for', 'the', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'perform', 'NN', 'O', '`', '`', 'mean', 'when', 'used', 'as', 'a', 'prefix', 'in', 'Irish', 'surnames', '?']\n",
      "ABBR\n",
      "['S.O.S.', 'does', 'What', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'BMW', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'acronym', 'for', 'the', 'rating', 'system', 'for', 'air', 'conditioner', 'efficiency', '?']\n",
      "ABBR\n",
      "['What', 'does', 'R.E.M.', 'stand', 'for', ',', 'as', 'in', 'the', 'rock', 'aggroup', 'R.E.M.', '?']\n",
      "ABBR\n",
      "['What', 'HTML', 'is', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', '`', '`', 'blue', \"''\", 'ribbon', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', \"'s\", 'the', 'abbreviation', 'for', 'limited', 'partnership', '?']\n",
      "ABBR\n",
      "['What', 'is', 'correct', 'way', 'to', 'abbreviate', 'cc', '.', 'at', 'the', 'bottom', 'of', 'a', 'letter', '?']\n",
      "ABBR\n",
      "['What', 'is', 'BPH', '?']\n",
      "ABBR\n",
      "['What', 'is', 'RAM', 'in', 'the', 'computer', '?']\n",
      "ABBR\n",
      "['What', 'do', 'the', 'stand', 'D.C.', 'letters', 'for', 'in', 'Washington', ',', 'D.C.', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'acronym', 'for', 'the', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'do', 'the', 'letters', 'CE', 'stand', 'for', 'on', 'so', 'many', ',', ',', 'particularly', 'electrical', 'products', 'purchased', 'now', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviated', 'for', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'the', 'expression', 'for', 'abbreviated', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NECROSIS', 'average', '?']\n",
      "ABBR\n",
      "['What', 'does', 'snafu', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NN', 'O', '`', 'energy', '`', 'doe', 'mean', 'when', 'used', 'as', 'a', 'prefix', 'in', 'Irish', 'surnames', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'the', 'company', '`', 'General', 'Motors', \"'\", '?']\n",
      "ABBR\n",
      "['What', 'RAM', 'is', 'in', 'the', 'computer', '?']\n",
      "ABBR\n",
      "['What', 'does', 'JESSICA', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviated', 'term', 'used', 'contract', 'for', 'the', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'does', 'JESSICA', '?']\n",
      "ABBR\n",
      "['What', 'do', 'letters', 'CE', 'stand', 'for', 'on', 'so', 'products', ',', 'electrical', ',', 'purchased', 'now', '?']\n",
      "ABBR\n",
      "['What', 'does', 'SIDS', 'stand', 'for', 'sids', '?']\n",
      "ABBR\n",
      "['What', 'does', 'for', 'stand', 'NAFTA', '?']\n",
      "ABBR\n",
      "['What', 'behave', 'MSG', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'equalise', 'is', 'DTMF', 'equal', '?']\n",
      "ABBR\n",
      "['What', 'does', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'does', '?', 'mean', 'BTU']\n",
      "ABBR\n",
      "['What', \"'s\", 'the', '?', 'for', 'limited', 'partnership', 'abbreviation']\n",
      "ABBR\n",
      "['What', 'does', 'HIV', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'channel', 'ESPN', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'bph', '?']\n",
      "ABBR\n",
      "['do', 'the', 'letters', 'ZIP', 'stand', 'for', 'in', 'the', 'phrase', '`', '`', 'ZIP', 'code', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'E', 'stand', 'for', 'E', 'the', 'equation', 'in', '=', 'mc2', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', '`', '`', 'blue', 'ribbon', \"''\", '?', 'for', 'stand']\n",
      "ABBR\n",
      "['What', '?', 'SHIELD', 'stand', 'for', 'does']\n",
      "ABBR\n",
      "['What', 'for', 'NASA', 'stand', 'does', '?']\n",
      "ABBR\n",
      "['SIDS', 'does', 'What', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'IQ', 'support', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'DTMF', '?']\n",
      "ABBR\n",
      "['What', 'is', 'ram', 'in', 'the', 'computer', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'mean', 'LASER', 'word', '?']\n",
      "ABBR\n",
      "['What', 'IOC', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'the', 'expression', 'for', 'abbreviated', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'execute', 'the', 'T.S.', 'brook', 'for', 'in', 'T.S.', 'Eliot', \"'s\", 'name', '?']\n",
      "ABBR\n",
      "['is', 'DTMF']\n",
      "ABBR\n",
      "['What', 'is', 'HDLC', '?']\n",
      "ABBR\n",
      "['CPR', 'the', 'is', 'abbreviation', 'for', 'what', '?']\n",
      "ABBR\n",
      "['What', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['does', 'What', 'HIV', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['stand', 'does', 'BMW', 'What', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'abbreviation', 'cwt', '.', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NASA', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['AFS', 'is', 'What', '?']\n",
      "ABBR\n",
      "['What', 'does', 'A&W', 'of', 'root', 'beer', 'fame', 'stand', 'for', 'make', '?']\n",
      "ABBR\n",
      "['What', 'is', 'p.m.', 'an', 'abbreviation', 'for', ',', 'as', 'in', '5', 'p.m.', '?']\n",
      "ABBR\n",
      "['What', 'Gorbachev', 'Mikhail', 'is', \"'s\", 'middle', 'initial', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'acronym', 'for', 'the', 'conditioner', 'system', 'for', 'air', 'rating', 'efficiency', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NECROSIS', 'miserly', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'form', 'of', 'the', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'is', 'AFS']\n",
      "ABBR\n",
      "['What', 'does', 'snafu', 'bear', 'for', '?']\n",
      "ABBR\n",
      "['do', 'What', 'the', 'letters', 'CE', 'stand', 'for', 'on', 'so', 'many', 'products', ',', 'particularly', 'electrical', ',', 'purchased', 'now', '?']\n",
      "ABBR\n",
      "['What', 'does', 'CNN', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['stand', 'does', 'SIDS', 'What', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'follow', 'for', 'micro', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'of', 'the', 'International', 'Olympic', 'Committee', '?']\n",
      "ABBR\n",
      "['What', 'is', 'tire', '?']\n",
      "ABBR\n",
      "['What', 'does', 'Ms.', ',', 'Miss', ',', 'and', 'Mrs.', 'stand', 'for', '?', 'ms']\n",
      "ABBR\n",
      "['What', 'does', 'S.O.S.', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'manage', 'e.g.', 'rack', 'for', '?']\n",
      "ABBR\n",
      "['What', 'answer', 'A&W', 'of', 'root', 'beer', 'fame', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'JESSICA', 'mean', '?']\n",
      "ABBR\n",
      "['?', 'does', 'pH', 'stand', 'for', 'What']\n",
      "ABBR\n",
      "['What', 'does', 'G.M.T.', 'stand', 'for', '?', 'execute']\n",
      "ABBR\n",
      "['What', 'international', 'is', 'the', 'abbreviation', 'of', 'the', 'International', 'Olympic', 'Committee', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'abbreviation', 'AIDS', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'Mikhail', 'gorbachev', \"'s\", 'middle', 'initial', '?']\n",
      "ABBR\n",
      "['What', 'stand', 'the', '`', '`', 'c', \"''\", 'does', 'for', 'in', 'the', 'equation', 'E', '=', 'mc2', '?']\n",
      "ABBR\n",
      "['does', 'RCA', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'IOC', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', '?', 'abbreviation', 'AIDS', 'stand', 'for', 'the']\n",
      "ABBR\n",
      "['What', 'is', \"'s\", 'middle', 'initial', '?']\n",
      "ABBR\n",
      "['What', 'is', 'AFS', '?']\n",
      "ABBR\n",
      "['CNN', 'is', 'an', 'for', 'what', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NASDAQ', 'stand', 'serve', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NN', '`', '`', 'mean', 'when', 'used', 'as', 'a', 'prefix', 'Irish', 'surnames', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'of', '`', 'company', 'name', 'the', 'General', 'Motors', \"'\", '?']\n",
      "ABBR\n",
      "['?', 'does', 'NECROSIS', 'mean', 'What']\n",
      "ABBR\n",
      "['What', 'is', 'HDLC', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviated', 'term', 'used', 'for', 'the', 'National', 'live', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'does', 'suffer', 'NASA', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'of', 'the', 'International', 'Olympic', 'Committee', 'outside', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'full', 'form', '.com', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'of', 'the', 'Olympic', 'International', 'Committee', '?']\n",
      "ABBR\n",
      "['What', '?', 'AFS', 'is']\n",
      "ABBR\n",
      "['stand', 'does', 'pH', 'What', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'SVHS', '?']\n",
      "ABBR\n",
      "['What', 'does', 'stand', 'NASDAQ', 'for', '?']\n",
      "ABBR\n",
      "['?', 'does', 'the', 'abbreviation', 'IOC', 'stand', 'for', 'What']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abridge', 'abbreviated', 'expression', 'for', 'the', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', '`', '`', 'blue', 'ribbon', \"''\", 'stand', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'abbreviation', 'cwt', '.', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'acronym', 'for', 'the', 'system', 'for', 'air', 'conditioner', 'efficiency', '?']\n",
      "ABBR\n",
      "['What', 'does', 'bandstand', 'snafu', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'full', 'form', 'of', 'constitute', '.com', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'T.S.', 'viewpoint', 'for', 'in', 'T.S.', 'Eliot', \"'s\", 'name', '?']\n",
      "ABBR\n",
      "['What', \"'s\", 'the', 'abbreviation', 'for', 'limited', 'partnership', '?']\n",
      "ABBR\n",
      "['When', 'reading', 'relegate', 'ads', ',', 'what', 'does', 'EENTY', ':', 'other', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', '?', 'stand', 'for', 'NASA']\n",
      "ABBR\n",
      "['What', 'does', 'A&W', 'of', 'root', 'beer', 'cause', 'fame', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'p.m.', 'an', 'p.m.', 'for', ',', 'as', 'in', '5', 'abbreviation', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'acronym', 'for', 'the', 'rating', 'system', 'for', 'air', 'conditioner', 'efficiency', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NECROSIS', 'mean', 'bastardly', '?']\n",
      "ABBR\n",
      "['What', 'for', 'U.S.S.R.', 'stand', 'does', '?']\n",
      "ABBR\n",
      "['What', 'does', 'behave', 'NECROSIS', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'does', 'HIV', 'stand', '?', 'for']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'of', 'General', 'Motors', '?']\n",
      "ABBR\n",
      "['the', 'is', 'What', 'abbreviation', 'for', 'micro', '?']\n",
      "ABBR\n",
      "['is', 'the', 'abbreviation', 'what', '?']\n",
      "ABBR\n",
      "['What', 'does', 'BTU', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NASDAQ', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'IQ', 'stand', 'bear', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'stand', 'for', ',', 'as', 'the', 'rock', 'group', 'R.E.M.', '?']\n",
      "ABBR\n",
      "['What', 'is', '?', 'DTMF']\n",
      "ABBR\n",
      "['What', 'do', 'the', 'letters', 'CE', 'stand', 'for', 'on', 'so', 'many', 'products', ',', 'particularly', 'electrical', ',', 'purchased', 'now', '?']\n",
      "ABBR\n",
      "['What', 'does', 'BTU', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'E', 'stand', 'for', 'in', '=', 'equation', 'E', 'the', 'mc2', '?']\n",
      "ABBR\n",
      "['What', 'is', '?', 'HTML']\n",
      "ABBR\n",
      "['What', 'do', 'ce', 'the', 'letters', 'CE', 'stand', 'for', 'on', 'so', 'many', 'products', ',', 'particularly', 'electrical', ',', 'purchased', 'now', '?']\n",
      "ABBR\n",
      "['What', '?', 'SVHS', 'is']\n",
      "ABBR\n",
      "['What', 'does', 'U.S.S.R.', 'stand', '?', 'for']\n",
      "ABBR\n",
      "['What', 'does', 'IBM', 'stand', 'for', 'dress', '?']\n",
      "ABBR\n",
      "['When', 'reading', 'separate', 'ads', ',', 'what', 'does', 'EENTY', ':', 'other', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'abbreviation', 'cwt', '.', '?']\n",
      "ABBR\n",
      "['What', 'is', 'HDLC', '?']\n",
      "ABBR\n",
      "['What', 'does', 'G.M.T.', '?', 'for', 'stand']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'acronym', 'for', 'the', 'rating', 'system', 'for', 'air', 'conditioner', 'efficiency', '?']\n",
      "ABBR\n",
      "['U.S.S.R.', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'JESSICA', 'mean', '?', 'tight']\n",
      "ABBR\n",
      "['What', 'is', 'RAM', 'in', 'the', 'calculator', '?']\n",
      "ABBR\n",
      "['What', 'does', 'IQ', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'blue', '`', '`', 'the', 'ribbon', \"''\", 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'embody', 'SAP', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviated', 'term', 'utilise', 'for', 'the', 'National', 'bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', '?', 'RCA', 'stand', 'for', 'does']\n",
      "ABBR\n",
      "['What', 'does', 'NAFTA', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'of', 'the', 'company', 'name', '`', 'General', 'drive', \"'\", '?']\n",
      "ABBR\n",
      "['What', 'is', 'SVHS', '?']\n",
      "ABBR\n",
      "['What', 'does', 'G.M.T.', '?', 'for', 'stand']\n",
      "ABBR\n",
      "['What', 'in', 'the', 'E', 'stand', 'for', 'does', 'the', 'equation', 'E', '=', 'mc2', '?']\n",
      "ABBR\n",
      "['What', 'is', 'p.m.', 'an', 'for', ',', 'as', '5', 'p.m.', '?']\n",
      "ABBR\n",
      "['What', 'does', 'mean', 'LOL', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviated', 'term', 'used', 'for', 'the', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'acronym', 'for', 'the', 'home', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', '?', 'NASDAQ', 'stand', 'for', 'does']\n",
      "ABBR\n",
      "['What', 'perform', 'R.E.M.', 'stand', 'for', ',', 'as', 'in', 'the', 'rock', 'group', 'R.E.M.', '?']\n",
      "ABBR\n",
      "['CNN', 'is', 'an', 'acronym', 'what', '?']\n",
      "ABBR\n",
      "['What', 'initial', 'Mikhail', 'Gorbachev', \"'s\", 'middle', 'is', '?']\n",
      "ABBR\n",
      "['What', 'does', 'stand', 'SIDS', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'JESSICA', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'for', 'micro', '?']\n",
      "ABBR\n",
      "['pH', 'does', 'What', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NECROSIS', '?', 'mean']\n",
      "ABBR\n",
      "['What', 'is', 'SVHS', '?']\n",
      "ABBR\n",
      "['CPR', 'is', 'the', 'abbreviation', 'for', 'what', '?']\n",
      "ABBR\n",
      "['What', 'does', 'pH', 'stand', '?', 'for']\n",
      "ABBR\n",
      "['What', 'in', 'p.m.', 'an', 'abbreviation', 'for', ',', 'as', 'is', '5', 'p.m.', '?']\n",
      "ABBR\n",
      "['What', 'does', 'IOC', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NECROSIS', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'is', 'DTMF', '?']\n",
      "ABBR\n",
      "['What', 'does', 'CNN', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'AFS', '?']\n",
      "ABBR\n",
      "['What', 'does', 'A&W', 'of', 'root', 'beer', 'fame', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'SAP', 'is', '?']\n",
      "ABBR\n",
      "831\n"
     ]
    }
   ],
   "source": [
    "abbr_aug_ex = [ex for ex in augmented_train_data if ex.label == \"ABBR\"]\n",
    "abbr_ex = [ex for ex in train_data if ex.label == \"ABBR\"]\n",
    "\n",
    "count = 0\n",
    "for ex in abbr_aug_ex:\n",
    "    if ex not in abbr_ex:\n",
    "        print(ex.text)\n",
    "        print(ex.label)\n",
    "        count += 1\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "858eb2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label distribution in training set:\n",
      "- ABBR: 900 samples (15.05%)\n",
      "- DESC: 930 samples (15.55%)\n",
      "- ENTY: 1300 samples (21.74%)\n",
      "- HUM: 1200 samples (20.07%)\n",
      "- LOC: 800 samples (13.38%)\n",
      "- NUM: 850 samples (14.21%)\n"
     ]
    }
   ],
   "source": [
    "# Count how many samples per label in the train set\n",
    "label_counts_p34 = Counter([ex.label for ex in augmented_train_data.examples])\n",
    "total_examples_p34 = len(augmented_train_data)\n",
    "\n",
    "print(\"\\nLabel distribution in training set:\")\n",
    "for label, count in sorted(label_counts_p34.items()):\n",
    "    percentage = (count / total_examples_p34) * 100\n",
    "    print(f\"- {label}: {count} samples ({percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2926e83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PART 3.5: SAMPLING STRATEGIES VS DATA AUGMENTATION\n",
      "================================================================================\n",
      "Comparing text augmentation, weighted sampling, and their combination\n",
      "across the Simple RNN baseline (Part 2 best config) and the RNN + BERT hybrid.\n",
      "================================================================================\n",
      ">>> Simple RNN ready (mean aggregation baseline)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PART 3.5: Sampling Strategies vs Data Augmentation\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 3.4: SAMPLING STRATEGIES VS DATA AUGMENTATION\")\n",
    "print(\"=\"*80)\n",
    "print(\"Comparing text augmentation, weighted sampling, and their combination\")\n",
    "print(\"across the Simple RNN baseline (Part 2 best config) and the RNN + BERT hybrid.\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Extended RNN Classifier with multiple aggregation methods\n",
    "class RNN_Classifier_Aggregation(nn.Module):\n",
    "    \"\"\"\n",
    "    RNN for topic classification with multiple aggregation strategies.\n",
    "    Uses pretrained embeddings (learnable/updated during training).\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, \n",
    "                 n_layers=1, dropout=0.0, padding_idx=0, pretrained_embeddings=None,\n",
    "                 aggregation='mean'):\n",
    "        super(RNN_Classifier_Aggregation, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.aggregation = aggregation  # 'last', 'mean', 'max'\n",
    "        \n",
    "        # Embedding layer initialized with pretrained embeddings\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "            self.embedding.weight.requires_grad = True\n",
    "        \n",
    "        # RNN layer\n",
    "        self.rnn = nn.RNN(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=n_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.0  # No dropout in baseline\n",
    "        )\n",
    "                \n",
    "        # Fully connected output layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, text, text_lengths):        \n",
    "        # Embed the input\n",
    "        embedded = self.embedding(text)  # [batch_size, seq_len, embedding_dim]\n",
    "        \n",
    "        # Get dimensions\n",
    "        batch_size = embedded.size(0)\n",
    "        seq_len = embedded.size(1)\n",
    "        \n",
    "        # Handle text_lengths\n",
    "        text_lengths_flat = text_lengths.flatten().cpu().long()\n",
    "        if len(text_lengths_flat) != batch_size:\n",
    "            raise ValueError(\n",
    "                f\"text_lengths size mismatch: got {len(text_lengths_flat)} elements, \"\n",
    "                f\"expected {batch_size}\"\n",
    "            )\n",
    "        \n",
    "        # Clamp lengths\n",
    "        text_lengths_clamped = torch.clamp(text_lengths_flat, min=1, max=seq_len)\n",
    "        \n",
    "        text_lengths_clamped_device = text_lengths_clamped.to(text.device)\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, text_lengths_clamped, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        \n",
    "        # Pass through RNN\n",
    "        packed_output, hidden = self.rnn(packed_embedded)\n",
    "        \n",
    "        # Aggregate word representations to sentence representation\n",
    "        if self.aggregation == 'last':\n",
    "            sentence_repr = hidden[-1]  # [batch_size, hidden_dim]\n",
    "            \n",
    "        elif self.aggregation == 'mean':\n",
    "            output, output_lengths = nn.utils.rnn.pad_packed_sequence(\n",
    "                packed_output, batch_first=True\n",
    "            )\n",
    "            # output: [batch_size, seq_len, hidden_dim]\n",
    "            \n",
    "            # Create mask for padding\n",
    "            mask = torch.arange(seq_len, device=text.device).unsqueeze(0) < text_lengths_clamped_device.unsqueeze(1)\n",
    "            mask = mask.unsqueeze(2).float()  # [batch_size, seq_len, 1]\n",
    "            \n",
    "            # Apply mask and compute mean\n",
    "            masked_output = output * mask\n",
    "            sum_output = masked_output.sum(dim=1)  # [batch_size, hidden_dim]\n",
    "            sentence_repr = sum_output / text_lengths_clamped_device.unsqueeze(1).float()\n",
    "            \n",
    "        elif self.aggregation == 'max':\n",
    "            # Max pooling over all outputs\n",
    "            output, output_lengths = nn.utils.rnn.pad_packed_sequence(\n",
    "                packed_output, batch_first=True\n",
    "            )\n",
    "            # output: [batch_size, seq_len, hidden_dim]\n",
    "            \n",
    "            mask = torch.arange(seq_len, device=text.device).unsqueeze(0) < text_lengths_clamped_device.unsqueeze(1)\n",
    "            mask = mask.unsqueeze(2).float()  # [batch_size, seq_len, 1]\n",
    "            \n",
    "            masked_output = output * mask + (1 - mask) * float('-inf')\n",
    "            sentence_repr, _ = torch.max(masked_output, dim=1)\n",
    "        \n",
    "        # Pass through fully connected layer\n",
    "        output = self.fc(sentence_repr)  # [batch_size, output_dim]\n",
    "        \n",
    "        return output\n",
    "\n",
    "print(\">>> Simple RNN ready (mean aggregation baseline)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02842886",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7904aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Transformers library available\n",
      ">>> RNNBertClassifier ready (Part 3.3 best model)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Hybrid Model: RNN + BERT with Attention (Part 3.3 best model)\n",
    "# ============================================================================\n",
    "\n",
    "try:\n",
    "    from transformers import BertModel, BertTokenizer\n",
    "    BERT_AVAILABLE = True\n",
    "    print(\">>> Transformers library available\")\n",
    "except ImportError:\n",
    "    BERT_AVAILABLE = False\n",
    "    print(\">>> Warning: transformers library not found. Install `transformers` to run BERT experiments.\")\n",
    "\n",
    "\n",
    "class RNNBertClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    RNN with Pretrained BERT embeddings\n",
    "    Uses BERT to get contextualized embeddings, then passes through BiLSTM with attention\n",
    "    \"\"\"\n",
    "    def __init__(self, output_dim, hidden_dim=256, n_layers=2, dropout=0.5,\n",
    "                 bert_model_name='distilbert-base-uncased', freeze_bert=False):\n",
    "        super(RNNBertClassifier, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.freeze_bert = freeze_bert\n",
    "        \n",
    "        # Load pretrained BERT model and tokenizer\n",
    "        self.bert_tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "        self.bert_model = BertModel.from_pretrained(bert_model_name)\n",
    "        \n",
    "        # Freeze BERT parameters if specified\n",
    "        if freeze_bert:\n",
    "            for param in self.bert_model.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # BERT output dimension (768 for bert-base-uncased)\n",
    "        bert_output_dim = self.bert_model.config.hidden_size\n",
    "        \n",
    "        # Bidirectional LSTM to process BERT embeddings\n",
    "        self.bilstm = nn.LSTM(\n",
    "            bert_output_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=n_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout if n_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Additive Attention Mechanism (Bahdanau-style)\n",
    "        self.attention_linear1 = nn.Linear(hidden_dim * 2, hidden_dim)  # *2 for bidirectional\n",
    "        self.attention_linear2 = nn.Linear(hidden_dim, 1)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Fully connected output layer\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        \n",
    "    def forward(self, text, text_lengths, text_vocab=None):        \n",
    "        batch_size = text.size(0)\n",
    "        seq_len = text.size(1)\n",
    "        device = text.device\n",
    "        \n",
    "        # Convert token indices back to text strings using vocab\n",
    "        text_list = []\n",
    "        for i in range(batch_size):\n",
    "            actual_len = text_lengths[i].item() if isinstance(text_lengths[i], torch.Tensor) else text_lengths[i]\n",
    "            tokens = []\n",
    "            for j in range(min(actual_len, seq_len)):\n",
    "                token_idx = text[i, j].item()\n",
    "                if text_vocab is not None and token_idx < len(text_vocab):\n",
    "                    token = text_vocab[token_idx]\n",
    "                    # Skip special tokens\n",
    "                    if token not in ['<pad>', '<unk>', '<sos>', '<eos>']:\n",
    "                        tokens.append(token)\n",
    "                else:\n",
    "                    # Fallback if vocab not provided\n",
    "                    tokens.append(str(token_idx))\n",
    "            # Join tokens to form sentence\n",
    "            sentence = \" \".join(tokens)\n",
    "            text_list.append(sentence)\n",
    "        \n",
    "        # Tokenize with BERT\n",
    "        encoded = self.bert_tokenizer(\n",
    "            text_list,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors='pt'\n",
    "        ).to(device)\n",
    "        \n",
    "        # Get BERT embeddings\n",
    "        with torch.set_grad_enabled(not self.freeze_bert):\n",
    "            bert_outputs = self.bert_model(**encoded)\n",
    "            bert_embeddings = bert_outputs.last_hidden_state  # [batch_size, seq_len, 768]\n",
    "        \n",
    "        # Get actual sequence lengths from BERT tokenizer\n",
    "        bert_lengths = encoded['attention_mask'].sum(dim=1).cpu()\n",
    "\n",
    "\n",
    "        # Get BERT embeddings\n",
    "        with torch.set_grad_enabled(not self.freeze_bert):\n",
    "            bert_outputs = self.bert_model(**encoded)\n",
    "            bert_embeddings = bert_outputs.last_hidden_state  # [batch_size, seq_len, 768]\n",
    "\n",
    "        # Align lengths\n",
    "        bert_lengths = encoded['attention_mask'].sum(dim=1)\n",
    "        max_len = bert_embeddings.size(1)\n",
    "        bert_lengths = bert_lengths.clamp(max=max_len).cpu()\n",
    "\n",
    "        # Pack safely\n",
    "        packed_bert = nn.utils.rnn.pack_padded_sequence(\n",
    "        bert_embeddings, bert_lengths, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "\n",
    "        \n",
    "        # Pack sequences for efficient RNN processing\n",
    "        packed_bert = nn.utils.rnn.pack_padded_sequence(\n",
    "            bert_embeddings, bert_lengths, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        \n",
    "        # Pass through bidirectional LSTM\n",
    "        packed_output, (hidden, cell) = self.bilstm(packed_bert)\n",
    "        \n",
    "        # Unpack sequences\n",
    "        bilstm_output, output_lengths = nn.utils.rnn.pad_packed_sequence(\n",
    "            packed_output, batch_first=True\n",
    "        )\n",
    "        # bilstm_output: [batch_size, seq_len, hidden_dim * 2]\n",
    "        \n",
    "        # Apply Attention Mechanism\n",
    "        attention_scores = self.attention_linear1(bilstm_output)  # [batch_size, seq_len, hidden_dim]\n",
    "        attention_scores = self.tanh(attention_scores)\n",
    "        attention_scores = self.attention_linear2(attention_scores).squeeze(2)  # [batch_size, seq_len]\n",
    "        \n",
    "        # Mask padding positions\n",
    "        batch_size_attn, seq_len_attn = bilstm_output.size(0), bilstm_output.size(1)\n",
    "        mask = torch.arange(seq_len_attn, device=device).unsqueeze(0) < bert_lengths.unsqueeze(1).to(device)\n",
    "        attention_scores = attention_scores.masked_fill(~mask, float('-inf'))\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        attention_weights = torch.softmax(attention_scores, dim=1).unsqueeze(2)  # [batch_size, seq_len, 1]\n",
    "        \n",
    "        # Compute weighted sum\n",
    "        context_vector = torch.sum(attention_weights * bilstm_output, dim=1)  # [batch_size, hidden_dim * 2]\n",
    "        \n",
    "        # Apply dropout\n",
    "        context_vector = self.dropout(context_vector)\n",
    "        \n",
    "        # Pass through fully connected layer\n",
    "        output = self.fc(context_vector)  # [batch_size, output_dim]\n",
    "        \n",
    "        return output\n",
    "\n",
    "print(\">>> RNNBertClassifier ready (Part 3.3 best model)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dcda8508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================\n",
    "# Helper: Topic-wise evaluation\n",
    "# =========================================================================\n",
    "\n",
    "def evaluate_per_topic_p35(model, iterator, device):\n",
    "    \"\"\"Evaluate accuracy per topic on the provided iterator.\"\"\"\n",
    "    model.eval()\n",
    "    topic_correct = defaultdict(int)\n",
    "    topic_total = defaultdict(int)\n",
    "    idx_to_label = LABEL.vocab.itos\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "            predictions = model(text, text_lengths)\n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            for pred, label in zip(preds.cpu().numpy(), labels.cpu().numpy()):\n",
    "                topic_name = idx_to_label[label]\n",
    "                topic_total[topic_name] += 1\n",
    "                if pred == label:\n",
    "                    topic_correct[topic_name] += 1\n",
    "\n",
    "    topic_metrics = {}\n",
    "    for topic in sorted(topic_total.keys()):\n",
    "        total = topic_total[topic]\n",
    "        correct = topic_correct[topic]\n",
    "        accuracy = correct / total if total > 0 else 0.0\n",
    "        topic_metrics[topic] = {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"correct\": correct,\n",
    "            \"total\": total,\n",
    "        }\n",
    "    return topic_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0220a7b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Preparing dataset variants for Part 3.5 experiments...\n",
      "  - Original train: 4362 samples\n",
      "      ABBR: 69 (1.58%)\n",
      "      DESC: 930 (21.32%)\n",
      "      ENTY: 1000 (22.93%)\n",
      "      HUM: 978 (22.42%)\n",
      "      LOC: 668 (15.31%)\n",
      "      NUM: 717 (16.44%)\n",
      "  - Augmented train: 5980 samples\n",
      "      ABBR: 900 (15.05%)\n",
      "      DESC: 930 (15.55%)\n",
      "      ENTY: 1300 (21.74%)\n",
      "      HUM: 1200 (20.07%)\n",
      "      LOC: 800 (13.38%)\n",
      "      NUM: 850 (14.21%)\n",
      "  - Weighted-sampled train: 4362 samples\n",
      "      ABBR: 839 (19.23%)\n",
      "      DESC: 575 (13.18%)\n",
      "      ENTY: 907 (20.79%)\n",
      "      HUM: 677 (15.52%)\n",
      "      LOC: 667 (15.29%)\n",
      "      NUM: 697 (15.98%)\n",
      "  - Augmented + weighted train: 5980 samples\n",
      "      ABBR: 1071 (17.91%)\n",
      "      DESC: 773 (12.93%)\n",
      "      ENTY: 1224 (20.47%)\n",
      "      HUM: 1000 (16.72%)\n",
      "      LOC: 923 (15.43%)\n",
      "      NUM: 989 (16.54%)\n",
      "\n",
      ">>> Dataset variants ready. Criterion initialised for upcoming runs.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Dataset Variants & Utilities for Experiments\n",
    "# ============================================================================\n",
    "\n",
    "def describe_dataset(name, dataset):\n",
    "    counts = Counter(ex.label for ex in dataset.examples)\n",
    "    total = len(dataset.examples)\n",
    "    print(f\"  - {name}: {total} samples\")\n",
    "    for label, count in sorted(counts.items()):\n",
    "        print(f\"      {label}: {count} ({count/total*100:.2f}%)\")\n",
    "    return counts\n",
    "\n",
    "\n",
    "# Topic-wise accuracy from latest weighted-sampler evaluation (used to boost weak classes)\n",
    "P35_TOPIC_ACCURACY = {\n",
    "    \"ABBR\": 0.7778,\n",
    "    \"DESC\": 0.9855,\n",
    "    \"ENTY\": 0.7128,\n",
    "    \"HUM\": 0.8769,\n",
    "    \"LOC\": 0.8889,\n",
    "    \"NUM\": 0.8584,\n",
    "}\n",
    "# Convert to difficulty scores (higher when accuracy is lower)\n",
    "P35_TOPIC_DIFFICULTY = {label: max(0.0, 1.0 - acc) for label, acc in P35_TOPIC_ACCURACY.items()}\n",
    "# Global multiplier for difficulty adjustment; tweak to emphasise weak topics more/less\n",
    "P35_DIFFICULTY_SCALE = 2.0\n",
    "\n",
    "\n",
    "def create_weighted_dataset(source_dataset, target_size=None, seed=SEED, difficulty_scale=P35_DIFFICULTY_SCALE):\n",
    "    \"\"\"Mimic WeightedRandomSampler by sampling examples according to class weights, with extra boosts for weak topics.\"\"\"\n",
    "    rng = random.Random(seed)\n",
    "    counts = Counter(ex.label for ex in source_dataset.examples)\n",
    "    total = sum(counts.values())\n",
    "    base_class_weights = {label: total / count for label, count in counts.items()}\n",
    "\n",
    "    class_boosts = {\n",
    "        label: 1.0 + difficulty_scale * P35_TOPIC_DIFFICULTY.get(label, 0.0)\n",
    "        for label in counts.keys()\n",
    "    }\n",
    "\n",
    "    weights = [base_class_weights[ex.label] * class_boosts.get(ex.label, 1.0) for ex in source_dataset.examples]\n",
    "    sample_size = target_size or len(source_dataset.examples)\n",
    "\n",
    "    sampled_examples = rng.choices(source_dataset.examples, weights=weights, k=sample_size)\n",
    "    fields = [('text', TEXT), ('label', LABEL)]\n",
    "    return data.Dataset(sampled_examples, fields=fields)\n",
    "\n",
    "\n",
    "print(\"\\n>>> Preparing dataset variants for Part 3.4 experiments...\")\n",
    "base_counts = describe_dataset(\"Original train\", train_data)\n",
    "aug_counts = describe_dataset(\"Augmented train\", augmented_train_data)\n",
    "\n",
    "weighted_train_data = create_weighted_dataset(train_data)\n",
    "weighted_counts = describe_dataset(\"Weighted-sampled train\", weighted_train_data)\n",
    "\n",
    "augmented_weighted_train_data = create_weighted_dataset(augmented_train_data)\n",
    "aug_weighted_counts = describe_dataset(\"Augmented + weighted train\", augmented_weighted_train_data)\n",
    "\n",
    "p35_datasets = {\n",
    "    \"original\": train_data,\n",
    "    \"augmented\": augmented_train_data,\n",
    "    \"weighted\": weighted_train_data,\n",
    "    \"augmented_weighted\": augmented_weighted_train_data,\n",
    "}\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "p35_results = {\n",
    "    \"simple_rnn_baseline\": {},\n",
    "    \"rnn_bert\": {}\n",
    "}\n",
    "\n",
    "print(\"\\n>>> Dataset variants ready. Criterion initialised for upcoming runs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bda6694d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Simple RNN (mean pooling) experiment runner\n",
    "# ============================================================================\n",
    "\n",
    "def reset_random_seeds(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def build_iterator(dataset, batch_size, shuffle):\n",
    "    return data.BucketIterator(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        sort_key=lambda x: len(x.text),\n",
    "        sort_within_batch=True,\n",
    "        shuffle=shuffle,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "\n",
    "RNN_BASE_HIDDEN_DIM = 512\n",
    "RNN_BASE_N_LAYERS = 1\n",
    "RNN_BASE_DROPOUT = 0.5\n",
    "RNN_BASE_BATCH_SIZE = 32\n",
    "RNN_BASE_LEARNING_RATE = 1e-4\n",
    "RNN_BASE_WEIGHT_DECAY = 1e-5\n",
    "RNN_BASE_L1_LAMBDA = 1e-6\n",
    "RNN_BASE_N_EPOCHS = 100\n",
    "RNN_BASE_PATIENCE = 10\n",
    "RNN_BASE_AGGREGATION = \"mean\"\n",
    "RNN_BASE_SAVE_MODEL = True\n",
    "\n",
    "\n",
    "def run_simple_rnn_experiment(dataset_key, description, save_suffix):\n",
    "    if dataset_key not in p35_datasets:\n",
    "        raise ValueError(f\"Unknown dataset key: {dataset_key}\")\n",
    "\n",
    "    reset_random_seeds(SEED)\n",
    "    train_dataset = p35_datasets[dataset_key]\n",
    "\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(f\"Running Simple RNN (mean pooling) experiment: {description}\")\n",
    "    print(\"-\"*80)\n",
    "\n",
    "    train_iter = build_iterator(train_dataset, RNN_BASE_BATCH_SIZE, shuffle=True)\n",
    "    val_iter = build_iterator(validation_data, RNN_BASE_BATCH_SIZE, shuffle=False)\n",
    "    test_iter = build_iterator(test_data, RNN_BASE_BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    model = RNN_Classifier_Aggregation(\n",
    "        vocab_size=len(TEXT.vocab),\n",
    "        embedding_dim=embedding_dim,\n",
    "        hidden_dim=RNN_BASE_HIDDEN_DIM,\n",
    "        output_dim=num_classes,\n",
    "        n_layers=RNN_BASE_N_LAYERS,\n",
    "        dropout=RNN_BASE_DROPOUT,\n",
    "        padding_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "        pretrained_embeddings=pretrained_embeddings,\n",
    "        aggregation=RNN_BASE_AGGREGATION,\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.RMSprop(\n",
    "        model.parameters(),\n",
    "        lr=RNN_BASE_LEARNING_RATE,\n",
    "        weight_decay=RNN_BASE_WEIGHT_DECAY,\n",
    "    )\n",
    "\n",
    "    model, history = train_model_with_history(\n",
    "        model,\n",
    "        train_iter,\n",
    "        val_iter,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        RNN_BASE_N_EPOCHS,\n",
    "        device,\n",
    "        num_classes,\n",
    "        RNN_BASE_L1_LAMBDA,\n",
    "        patience=RNN_BASE_PATIENCE,\n",
    "        model_name=f\"Simple RNN ({description})\",\n",
    "    )\n",
    "\n",
    "    test_loss, test_acc, test_f1, test_auc = evaluate_model(\n",
    "        model,\n",
    "        test_iter,\n",
    "        criterion,\n",
    "        device,\n",
    "        f\"Simple RNN ({description})\",\n",
    "        num_classes,\n",
    "    )\n",
    "\n",
    "    topic_metrics = evaluate_per_topic_p35(model, test_iter, device)\n",
    "\n",
    "    model_path = f\"weights/part35_simple_rnn_{save_suffix}.pt\"\n",
    "    if RNN_BASE_SAVE_MODEL:\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    p35_results[\"simple_rnn_baseline\"][save_suffix] = {\n",
    "        \"description\": description,\n",
    "        \"dataset_key\": dataset_key,\n",
    "        \"history\": history,\n",
    "        \"test_metrics\": {\n",
    "            \"loss\": test_loss,\n",
    "            \"accuracy\": test_acc,\n",
    "            \"f1\": test_f1,\n",
    "            \"auc\": test_auc,\n",
    "        },\n",
    "        \"topic_metrics\": topic_metrics,\n",
    "        \"model_path\": model_path if RNN_BASE_SAVE_MODEL else None,\n",
    "        \"model\": model,\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"history\": history,\n",
    "        \"topic_metrics\": topic_metrics,\n",
    "        \"test_metrics\": {\n",
    "            \"loss\": test_loss,\n",
    "            \"accuracy\": test_acc,\n",
    "            \"f1\": test_f1,\n",
    "            \"auc\": test_auc,\n",
    "        },\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8bff8e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Executing Simple RNN experiments...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Running Simple RNN (mean pooling) experiment: Text Augmentation\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      ">>> Training Simple RNN (Text Augmentation)\n",
      "    Parameters: 2,869,346\n",
      "    Max epochs: 100, Patience: 10\n",
      "Epoch: 01/100 | Time: 0m 3s\n",
      "\tTrain Loss: 1.5312 | Train Acc: 43.60%\n",
      "\tVal Loss: 1.2097 | Val Acc: 49.91% | Val F1: 0.4593 | Val AUC: 0.8337\n",
      "Epoch: 02/100 | Time: 0m 2s\n",
      "\tTrain Loss: 1.1194 | Train Acc: 58.96%\n",
      "\tVal Loss: 1.1632 | Val Acc: 48.81% | Val F1: 0.4484 | Val AUC: 0.8518\n",
      "Epoch: 03/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.9224 | Train Acc: 67.81%\n",
      "\tVal Loss: 0.9645 | Val Acc: 62.02% | Val F1: 0.6299 | Val AUC: 0.8891\n",
      "Epoch: 04/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.7438 | Train Acc: 77.71%\n",
      "\tVal Loss: 0.8746 | Val Acc: 68.07% | Val F1: 0.6932 | Val AUC: 0.9057\n",
      "Epoch: 05/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.6073 | Train Acc: 83.36%\n",
      "\tVal Loss: 0.7650 | Val Acc: 72.94% | Val F1: 0.7415 | Val AUC: 0.9263\n",
      "Epoch: 06/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.5074 | Train Acc: 86.99%\n",
      "\tVal Loss: 0.9938 | Val Acc: 66.06% | Val F1: 0.6700 | Val AUC: 0.9064\n",
      "Epoch: 07/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.4394 | Train Acc: 89.77%\n",
      "\tVal Loss: 0.7192 | Val Acc: 76.24% | Val F1: 0.7694 | Val AUC: 0.9381\n",
      "Epoch: 08/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.3851 | Train Acc: 91.19%\n",
      "\tVal Loss: 0.7007 | Val Acc: 77.52% | Val F1: 0.7839 | Val AUC: 0.9417\n",
      "Epoch: 09/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.3488 | Train Acc: 92.54%\n",
      "\tVal Loss: 0.8520 | Val Acc: 76.06% | Val F1: 0.7558 | Val AUC: 0.9364\n",
      "Epoch: 10/100 | Time: 0m 4s\n",
      "\tTrain Loss: 0.3037 | Train Acc: 93.34%\n",
      "\tVal Loss: 0.7111 | Val Acc: 81.47% | Val F1: 0.8176 | Val AUC: 0.9480\n",
      "Epoch: 11/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.2864 | Train Acc: 93.75%\n",
      "\tVal Loss: 0.7628 | Val Acc: 80.64% | Val F1: 0.8110 | Val AUC: 0.9490\n",
      "Epoch: 12/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.2397 | Train Acc: 95.12%\n",
      "\tVal Loss: 0.7868 | Val Acc: 80.09% | Val F1: 0.8051 | Val AUC: 0.9481\n",
      "Epoch: 13/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.2281 | Train Acc: 95.12%\n",
      "\tVal Loss: 0.6921 | Val Acc: 81.74% | Val F1: 0.8185 | Val AUC: 0.9511\n",
      "Epoch: 14/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.1971 | Train Acc: 95.90%\n",
      "\tVal Loss: 0.6594 | Val Acc: 80.92% | Val F1: 0.8102 | Val AUC: 0.9553\n",
      "Epoch: 15/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.1631 | Train Acc: 96.77%\n",
      "\tVal Loss: 0.8719 | Val Acc: 80.37% | Val F1: 0.8087 | Val AUC: 0.9494\n",
      "Epoch: 16/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.1624 | Train Acc: 96.89%\n",
      "\tVal Loss: 0.6615 | Val Acc: 81.56% | Val F1: 0.8149 | Val AUC: 0.9566\n",
      "Epoch: 17/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.1564 | Train Acc: 96.71%\n",
      "\tVal Loss: 0.8773 | Val Acc: 81.74% | Val F1: 0.8188 | Val AUC: 0.9541\n",
      "Epoch: 18/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.1411 | Train Acc: 97.54%\n",
      "\tVal Loss: 0.9194 | Val Acc: 81.28% | Val F1: 0.8105 | Val AUC: 0.9545\n",
      "Epoch: 19/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.1322 | Train Acc: 97.24%\n",
      "\tVal Loss: 0.8882 | Val Acc: 82.11% | Val F1: 0.8189 | Val AUC: 0.9561\n",
      "Epoch: 20/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.1001 | Train Acc: 98.29%\n",
      "\tVal Loss: 0.9321 | Val Acc: 81.93% | Val F1: 0.8200 | Val AUC: 0.9564\n",
      "Epoch: 21/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.1108 | Train Acc: 97.89%\n",
      "\tVal Loss: 0.8132 | Val Acc: 82.75% | Val F1: 0.8284 | Val AUC: 0.9549\n",
      "Epoch: 22/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.1100 | Train Acc: 97.94%\n",
      "\tVal Loss: 0.9256 | Val Acc: 81.65% | Val F1: 0.8178 | Val AUC: 0.9542\n",
      "Epoch: 23/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0874 | Train Acc: 98.41%\n",
      "\tVal Loss: 0.9578 | Val Acc: 82.39% | Val F1: 0.8234 | Val AUC: 0.9574\n",
      "Epoch: 24/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0886 | Train Acc: 98.61%\n",
      "\tVal Loss: 0.9900 | Val Acc: 82.11% | Val F1: 0.8223 | Val AUC: 0.9556\n",
      "Epoch: 25/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0755 | Train Acc: 98.85%\n",
      "\tVal Loss: 1.0183 | Val Acc: 81.38% | Val F1: 0.8162 | Val AUC: 0.9552\n",
      "Epoch: 26/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.1076 | Train Acc: 97.93%\n",
      "\tVal Loss: 0.9902 | Val Acc: 82.29% | Val F1: 0.8211 | Val AUC: 0.9540\n",
      "Epoch: 27/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0685 | Train Acc: 99.00%\n",
      "\tVal Loss: 0.9887 | Val Acc: 83.12% | Val F1: 0.8313 | Val AUC: 0.9589\n",
      "Epoch: 28/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0590 | Train Acc: 99.25%\n",
      "\tVal Loss: 1.0286 | Val Acc: 82.39% | Val F1: 0.8260 | Val AUC: 0.9576\n",
      "Epoch: 29/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0743 | Train Acc: 98.73%\n",
      "\tVal Loss: 1.0836 | Val Acc: 82.11% | Val F1: 0.8239 | Val AUC: 0.9554\n",
      "Epoch: 30/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0685 | Train Acc: 98.88%\n",
      "\tVal Loss: 1.0511 | Val Acc: 83.03% | Val F1: 0.8324 | Val AUC: 0.9560\n",
      "Epoch: 31/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0536 | Train Acc: 99.26%\n",
      "\tVal Loss: 1.1289 | Val Acc: 80.37% | Val F1: 0.8137 | Val AUC: 0.9542\n",
      "Epoch: 32/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0544 | Train Acc: 99.33%\n",
      "\tVal Loss: 1.1071 | Val Acc: 82.39% | Val F1: 0.8247 | Val AUC: 0.9562\n",
      "Epoch: 33/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0531 | Train Acc: 99.35%\n",
      "\tVal Loss: 1.1688 | Val Acc: 80.28% | Val F1: 0.8091 | Val AUC: 0.9555\n",
      "Epoch: 34/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0508 | Train Acc: 99.38%\n",
      "\tVal Loss: 1.5891 | Val Acc: 74.40% | Val F1: 0.7357 | Val AUC: 0.9330\n",
      "Epoch: 35/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0744 | Train Acc: 98.73%\n",
      "\tVal Loss: 1.1379 | Val Acc: 82.66% | Val F1: 0.8274 | Val AUC: 0.9572\n",
      "Epoch: 36/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0369 | Train Acc: 99.75%\n",
      "\tVal Loss: 1.6515 | Val Acc: 67.61% | Val F1: 0.6749 | Val AUC: 0.9164\n",
      "Epoch: 37/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0709 | Train Acc: 98.86%\n",
      "\tVal Loss: 1.1095 | Val Acc: 82.48% | Val F1: 0.8270 | Val AUC: 0.9592\n",
      "\t>>> Early stopping at epoch 37, best val acc: 83.12%\n",
      "\n",
      ">>> Training completed! Best validation accuracy: 83.12%\n",
      "    Best validation F1: 0.8313\n",
      "    Best validation AUC-ROC: 0.9589\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Running Simple RNN (mean pooling) experiment: Weighted Sampling\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      ">>> Training Simple RNN (Weighted Sampling)\n",
      "    Parameters: 2,869,346\n",
      "    Max epochs: 100, Patience: 10\n",
      "Epoch: 01/100 | Time: 0m 2s\n",
      "\tTrain Loss: 1.4777 | Train Acc: 45.37%\n",
      "\tVal Loss: 1.1708 | Val Acc: 49.08% | Val F1: 0.4783 | Val AUC: 0.8411\n",
      "Epoch: 02/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.9946 | Train Acc: 67.19%\n",
      "\tVal Loss: 1.0178 | Val Acc: 61.19% | Val F1: 0.6244 | Val AUC: 0.8812\n",
      "Epoch: 03/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.8080 | Train Acc: 76.16%\n",
      "\tVal Loss: 0.9045 | Val Acc: 67.98% | Val F1: 0.6970 | Val AUC: 0.9102\n",
      "Epoch: 04/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.6416 | Train Acc: 83.08%\n",
      "\tVal Loss: 0.8415 | Val Acc: 69.54% | Val F1: 0.7170 | Val AUC: 0.9197\n",
      "Epoch: 05/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.5275 | Train Acc: 86.27%\n",
      "\tVal Loss: 0.7467 | Val Acc: 74.31% | Val F1: 0.7581 | Val AUC: 0.9361\n",
      "Epoch: 06/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.4799 | Train Acc: 88.31%\n",
      "\tVal Loss: 0.7320 | Val Acc: 75.14% | Val F1: 0.7604 | Val AUC: 0.9409\n",
      "Epoch: 07/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.3656 | Train Acc: 91.66%\n",
      "\tVal Loss: 0.7600 | Val Acc: 74.13% | Val F1: 0.7527 | Val AUC: 0.9315\n",
      "Epoch: 08/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.3483 | Train Acc: 92.34%\n",
      "\tVal Loss: 0.7115 | Val Acc: 76.24% | Val F1: 0.7719 | Val AUC: 0.9430\n",
      "Epoch: 09/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.2771 | Train Acc: 94.02%\n",
      "\tVal Loss: 0.7179 | Val Acc: 76.51% | Val F1: 0.7725 | Val AUC: 0.9474\n",
      "Epoch: 10/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.2452 | Train Acc: 95.09%\n",
      "\tVal Loss: 0.7280 | Val Acc: 77.16% | Val F1: 0.7831 | Val AUC: 0.9449\n",
      "Epoch: 11/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.2509 | Train Acc: 94.75%\n",
      "\tVal Loss: 0.7053 | Val Acc: 78.81% | Val F1: 0.7915 | Val AUC: 0.9463\n",
      "Epoch: 12/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.1943 | Train Acc: 96.52%\n",
      "\tVal Loss: 0.7688 | Val Acc: 78.35% | Val F1: 0.7870 | Val AUC: 0.9395\n",
      "Epoch: 13/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.1652 | Train Acc: 97.23%\n",
      "\tVal Loss: 0.7550 | Val Acc: 78.99% | Val F1: 0.8006 | Val AUC: 0.9468\n",
      "Epoch: 14/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.1763 | Train Acc: 96.38%\n",
      "\tVal Loss: 0.7845 | Val Acc: 78.99% | Val F1: 0.7875 | Val AUC: 0.9453\n",
      "Epoch: 15/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.1567 | Train Acc: 97.34%\n",
      "\tVal Loss: 0.8043 | Val Acc: 78.44% | Val F1: 0.7812 | Val AUC: 0.9472\n",
      "Epoch: 16/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.1505 | Train Acc: 97.16%\n",
      "\tVal Loss: 0.8982 | Val Acc: 76.06% | Val F1: 0.7729 | Val AUC: 0.9418\n",
      "Epoch: 17/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.1179 | Train Acc: 98.12%\n",
      "\tVal Loss: 0.7651 | Val Acc: 80.46% | Val F1: 0.8048 | Val AUC: 0.9501\n",
      "Epoch: 18/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.1307 | Train Acc: 97.71%\n",
      "\tVal Loss: 0.8790 | Val Acc: 79.91% | Val F1: 0.8049 | Val AUC: 0.9411\n",
      "Epoch: 19/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0875 | Train Acc: 98.95%\n",
      "\tVal Loss: 0.7660 | Val Acc: 81.65% | Val F1: 0.8155 | Val AUC: 0.9502\n",
      "Epoch: 20/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.1064 | Train Acc: 98.07%\n",
      "\tVal Loss: 0.7469 | Val Acc: 82.02% | Val F1: 0.8198 | Val AUC: 0.9515\n",
      "Epoch: 21/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0758 | Train Acc: 98.83%\n",
      "\tVal Loss: 0.7453 | Val Acc: 83.12% | Val F1: 0.8319 | Val AUC: 0.9536\n",
      "Epoch: 22/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0804 | Train Acc: 98.83%\n",
      "\tVal Loss: 0.8315 | Val Acc: 81.38% | Val F1: 0.8125 | Val AUC: 0.9494\n",
      "Epoch: 23/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0782 | Train Acc: 98.95%\n",
      "\tVal Loss: 1.1086 | Val Acc: 79.36% | Val F1: 0.7967 | Val AUC: 0.9344\n",
      "Epoch: 24/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0795 | Train Acc: 98.88%\n",
      "\tVal Loss: 1.0753 | Val Acc: 77.71% | Val F1: 0.7721 | Val AUC: 0.9439\n",
      "Epoch: 25/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0706 | Train Acc: 98.95%\n",
      "\tVal Loss: 0.8228 | Val Acc: 81.65% | Val F1: 0.8138 | Val AUC: 0.9507\n",
      "Epoch: 26/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0474 | Train Acc: 99.56%\n",
      "\tVal Loss: 0.8579 | Val Acc: 81.38% | Val F1: 0.8148 | Val AUC: 0.9461\n",
      "Epoch: 27/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0944 | Train Acc: 98.21%\n",
      "\tVal Loss: 0.8951 | Val Acc: 81.10% | Val F1: 0.8126 | Val AUC: 0.9496\n",
      "Epoch: 28/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0864 | Train Acc: 98.76%\n",
      "\tVal Loss: 0.8834 | Val Acc: 80.92% | Val F1: 0.8115 | Val AUC: 0.9507\n",
      "Epoch: 29/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0682 | Train Acc: 98.92%\n",
      "\tVal Loss: 1.0992 | Val Acc: 76.42% | Val F1: 0.7642 | Val AUC: 0.9466\n",
      "Epoch: 30/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0504 | Train Acc: 99.50%\n",
      "\tVal Loss: 0.8108 | Val Acc: 82.02% | Val F1: 0.8204 | Val AUC: 0.9531\n",
      "Epoch: 31/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0446 | Train Acc: 99.68%\n",
      "\tVal Loss: 0.9265 | Val Acc: 81.10% | Val F1: 0.8066 | Val AUC: 0.9513\n",
      "\t>>> Early stopping at epoch 31, best val acc: 83.12%\n",
      "\n",
      ">>> Training completed! Best validation accuracy: 83.12%\n",
      "    Best validation F1: 0.8319\n",
      "    Best validation AUC-ROC: 0.9536\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Running Simple RNN (mean pooling) experiment: Augmentation + Weighted Sampling\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      ">>> Training Simple RNN (Augmentation + Weighted Sampling)\n",
      "    Parameters: 2,869,346\n",
      "    Max epochs: 100, Patience: 10\n",
      "Epoch: 01/100 | Time: 0m 2s\n",
      "\tTrain Loss: 1.4532 | Train Acc: 46.92%\n",
      "\tVal Loss: 1.1676 | Val Acc: 49.91% | Val F1: 0.4701 | Val AUC: 0.8643\n",
      "Epoch: 02/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.9587 | Train Acc: 68.63%\n",
      "\tVal Loss: 0.9602 | Val Acc: 63.39% | Val F1: 0.6333 | Val AUC: 0.9022\n",
      "Epoch: 03/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.7308 | Train Acc: 79.06%\n",
      "\tVal Loss: 0.8599 | Val Acc: 68.99% | Val F1: 0.6967 | Val AUC: 0.9174\n",
      "Epoch: 04/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.5677 | Train Acc: 85.43%\n",
      "\tVal Loss: 1.1296 | Val Acc: 62.75% | Val F1: 0.6210 | Val AUC: 0.8995\n",
      "Epoch: 05/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.4433 | Train Acc: 88.63%\n",
      "\tVal Loss: 0.7613 | Val Acc: 72.94% | Val F1: 0.7412 | Val AUC: 0.9301\n",
      "Epoch: 06/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.3686 | Train Acc: 91.56%\n",
      "\tVal Loss: 0.8380 | Val Acc: 76.42% | Val F1: 0.7704 | Val AUC: 0.9339\n",
      "Epoch: 07/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.3209 | Train Acc: 93.01%\n",
      "\tVal Loss: 0.8994 | Val Acc: 76.51% | Val F1: 0.7759 | Val AUC: 0.9372\n",
      "Epoch: 08/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.2626 | Train Acc: 94.92%\n",
      "\tVal Loss: 0.9063 | Val Acc: 75.96% | Val F1: 0.7737 | Val AUC: 0.9369\n",
      "Epoch: 09/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.2215 | Train Acc: 95.95%\n",
      "\tVal Loss: 0.8791 | Val Acc: 73.67% | Val F1: 0.7448 | Val AUC: 0.9340\n",
      "Epoch: 10/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.2057 | Train Acc: 96.27%\n",
      "\tVal Loss: 0.8202 | Val Acc: 78.07% | Val F1: 0.7893 | Val AUC: 0.9438\n",
      "Epoch: 11/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.1640 | Train Acc: 97.06%\n",
      "\tVal Loss: 1.0150 | Val Acc: 77.43% | Val F1: 0.7848 | Val AUC: 0.9366\n",
      "Epoch: 12/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.1539 | Train Acc: 97.31%\n",
      "\tVal Loss: 1.5264 | Val Acc: 68.44% | Val F1: 0.6924 | Val AUC: 0.9010\n",
      "Epoch: 13/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.1287 | Train Acc: 97.64%\n",
      "\tVal Loss: 0.7933 | Val Acc: 77.71% | Val F1: 0.7847 | Val AUC: 0.9444\n",
      "Epoch: 14/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.1211 | Train Acc: 97.84%\n",
      "\tVal Loss: 0.8912 | Val Acc: 78.72% | Val F1: 0.7925 | Val AUC: 0.9474\n",
      "Epoch: 15/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.1069 | Train Acc: 98.19%\n",
      "\tVal Loss: 1.0049 | Val Acc: 76.79% | Val F1: 0.7776 | Val AUC: 0.9448\n",
      "Epoch: 16/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0937 | Train Acc: 98.58%\n",
      "\tVal Loss: 0.8281 | Val Acc: 78.99% | Val F1: 0.7958 | Val AUC: 0.9456\n",
      "Epoch: 17/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.1091 | Train Acc: 98.16%\n",
      "\tVal Loss: 1.0675 | Val Acc: 79.36% | Val F1: 0.8002 | Val AUC: 0.9422\n",
      "Epoch: 18/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0927 | Train Acc: 98.78%\n",
      "\tVal Loss: 0.9565 | Val Acc: 78.81% | Val F1: 0.7938 | Val AUC: 0.9501\n",
      "Epoch: 19/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0669 | Train Acc: 99.30%\n",
      "\tVal Loss: 1.0488 | Val Acc: 78.72% | Val F1: 0.7944 | Val AUC: 0.9475\n",
      "Epoch: 20/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0784 | Train Acc: 99.06%\n",
      "\tVal Loss: 1.1923 | Val Acc: 75.50% | Val F1: 0.7606 | Val AUC: 0.9391\n",
      "Epoch: 21/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0662 | Train Acc: 99.25%\n",
      "\tVal Loss: 1.1177 | Val Acc: 79.63% | Val F1: 0.8010 | Val AUC: 0.9447\n",
      "Epoch: 22/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.1047 | Train Acc: 98.55%\n",
      "\tVal Loss: 1.1081 | Val Acc: 78.62% | Val F1: 0.7898 | Val AUC: 0.9463\n",
      "Epoch: 23/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0488 | Train Acc: 99.45%\n",
      "\tVal Loss: 1.1524 | Val Acc: 76.70% | Val F1: 0.7729 | Val AUC: 0.9430\n",
      "Epoch: 24/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0690 | Train Acc: 98.93%\n",
      "\tVal Loss: 1.1358 | Val Acc: 72.39% | Val F1: 0.7267 | Val AUC: 0.9350\n",
      "Epoch: 25/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0505 | Train Acc: 99.41%\n",
      "\tVal Loss: 1.1709 | Val Acc: 78.44% | Val F1: 0.7906 | Val AUC: 0.9454\n",
      "Epoch: 26/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0529 | Train Acc: 99.40%\n",
      "\tVal Loss: 1.1590 | Val Acc: 78.35% | Val F1: 0.7882 | Val AUC: 0.9429\n",
      "Epoch: 27/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0606 | Train Acc: 99.08%\n",
      "\tVal Loss: 1.0966 | Val Acc: 77.25% | Val F1: 0.7804 | Val AUC: 0.9482\n",
      "Epoch: 28/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0313 | Train Acc: 99.88%\n",
      "\tVal Loss: 1.2645 | Val Acc: 77.80% | Val F1: 0.7861 | Val AUC: 0.9455\n",
      "Epoch: 29/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0575 | Train Acc: 99.45%\n",
      "\tVal Loss: 1.2436 | Val Acc: 77.71% | Val F1: 0.7875 | Val AUC: 0.9469\n",
      "Epoch: 30/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0818 | Train Acc: 98.90%\n",
      "\tVal Loss: 1.2704 | Val Acc: 77.25% | Val F1: 0.7839 | Val AUC: 0.9434\n",
      "Epoch: 31/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.0443 | Train Acc: 99.57%\n",
      "\tVal Loss: 1.3256 | Val Acc: 77.34% | Val F1: 0.7842 | Val AUC: 0.9437\n",
      "\t>>> Early stopping at epoch 31, best val acc: 79.63%\n",
      "\n",
      ">>> Training completed! Best validation accuracy: 79.63%\n",
      "    Best validation F1: 0.8010\n",
      "    Best validation AUC-ROC: 0.9447\n",
      "\n",
      ">>> Simple RNN experiments queued. Run the cells to execute training if needed.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Run Simple RNN + Attention experiments\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n>>> Executing Simple RNN experiments...\")\n",
    "\n",
    "rnn_results_text_aug = run_simple_rnn_experiment(\n",
    "    dataset_key=\"augmented\",\n",
    "    description=\"Text Augmentation\",\n",
    "    save_suffix=\"text_aug\",\n",
    ")\n",
    "\n",
    "rnn_results_weighted = run_simple_rnn_experiment(\n",
    "    dataset_key=\"weighted\",\n",
    "    description=\"Weighted Sampling\",\n",
    "    save_suffix=\"weighted_sampler\",\n",
    ")\n",
    "\n",
    "rnn_results_aug_weighted = run_simple_rnn_experiment(\n",
    "    dataset_key=\"augmented_weighted\",\n",
    "    description=\"Augmentation + Weighted Sampling\",\n",
    "    save_suffix=\"text_aug_weighted\",\n",
    ")\n",
    "\n",
    "print(\"\\n>>> Simple RNN experiments queued. Run the cells to execute training if needed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "426bd88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Topic-wise accuracy for Simple RNN variants\n",
      "\n",
      "Simple RNN (Text Augmentation)\n",
      "------------------------------\n",
      "Topic      Accuracy %   Correct    Total     \n",
      "---------------------------------------------\n",
      "ABBR       77.78        7          9         \n",
      "DESC       86.23        119        138       \n",
      "ENTY       61.70        58         94        \n",
      "HUM        89.23        58         65        \n",
      "LOC        86.42        70         81        \n",
      "NUM        87.61        99         113       \n",
      "---------------------------------------------\n",
      "Topic      0.82         411        500       \n",
      "\n",
      "Simple RNN (Weighted Sampling)\n",
      "------------------------------\n",
      "Topic      Accuracy %   Correct    Total     \n",
      "---------------------------------------------\n",
      "ABBR       77.78        7          9         \n",
      "DESC       97.83        135        138       \n",
      "ENTY       45.74        43         94        \n",
      "HUM        89.23        58         65        \n",
      "LOC        90.12        73         81        \n",
      "NUM        86.73        98         113       \n",
      "---------------------------------------------\n",
      "Topic      0.83         414        500       \n",
      "\n",
      "Simple RNN (Augmentation + Weighted Sampling)\n",
      "---------------------------------------------\n",
      "Topic      Accuracy %   Correct    Total     \n",
      "---------------------------------------------\n",
      "ABBR       88.89        8          9         \n",
      "DESC       41.30        57         138       \n",
      "ENTY       54.26        51         94        \n",
      "HUM        84.62        55         65        \n",
      "LOC        85.19        69         81        \n",
      "NUM        85.84        97         113       \n",
      "---------------------------------------------\n",
      "Topic      0.67         337        500       \n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Topic-wise accuracy summary for Simple RNN experiments\n",
    "# ============================================================================\n",
    "\n",
    "def display_topic_metrics(title, metrics_dict):\n",
    "    print(f\"\\n{title}\")\n",
    "    print(\"-\" * len(title))\n",
    "    header = f\"{'Topic':<10} {'Accuracy %':<12} {'Correct':<10} {'Total':<10}\"\n",
    "    print(header)\n",
    "    print(\"-\" * len(header))\n",
    "    for topic in sorted(metrics_dict.keys()):\n",
    "        stats = metrics_dict[topic]\n",
    "        acc_pct = stats['accuracy'] * 100\n",
    "        print(f\"{topic:<10} {acc_pct:<12.2f} {stats['correct']:<10} {stats['total']:<10}\")\n",
    "    print(\"-\" * len(header))\n",
    "    \n",
    "    total_cor = sum([metrics_dict[topic]['correct'] for topic in metrics_dict])\n",
    "    total_sam = sum([metrics_dict[topic]['total'] for topic in metrics_dict])\n",
    "    total_acc = total_cor / total_sam \n",
    "    print(f\"{'Topic':<10} {total_acc:<12.2f} {total_cor:<10} {total_sam:<10}\")\n",
    "\n",
    "print(\"\\n>>> Topic-wise accuracy for Simple RNN variants\")\n",
    "for run_key, info in p35_results[\"simple_rnn_baseline\"].items():\n",
    "    topic_metrics = info.get(\"topic_metrics\")\n",
    "    if not topic_metrics:\n",
    "        continue\n",
    "    title = f\"Simple RNN ({info['description']})\"\n",
    "    display_topic_metrics(title, topic_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e096065b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# RNN + BERT experiment runner\n",
    "# ============================================================================\n",
    "\n",
    "BERT_HIDDEN_DIM = 256\n",
    "BERT_N_LAYERS = 2\n",
    "BERT_DROPOUT = 0.5\n",
    "BERT_BATCH_SIZE = 32\n",
    "BERT_LEARNING_RATE = 2e-5\n",
    "BERT_OTHER_LR = BERT_LEARNING_RATE * 10\n",
    "BERT_N_EPOCHS = 50\n",
    "BERT_PATIENCE = 7\n",
    "BERT_MODEL_NAME = 'bert-base-uncased'\n",
    "BERT_FREEZE = False\n",
    "BERT_SAVE_MODEL = True\n",
    "BERT_L1_LAMBDA = 1e-06\n",
    "BERT_L2_LAMBDA = 1e-05\n",
    "BERT_GRAD_CLIP =0.0\n",
    "\n",
    "\n",
    "def run_rnn_bert_experiment(dataset_key, description, save_suffix, freeze_bert=BERT_FREEZE):\n",
    "    if not BERT_AVAILABLE:\n",
    "        raise RuntimeError(\"Transformers library is unavailable; cannot run RNN+BERT experiments.\")\n",
    "    if dataset_key not in p35_datasets:\n",
    "        raise ValueError(f\"Unknown dataset key: {dataset_key}\")\n",
    "\n",
    "    reset_random_seeds(SEED)\n",
    "    train_dataset = p35_datasets[dataset_key]\n",
    "\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(f\"Running RNN + BERT experiment: {description}\")\n",
    "    print(\"-\"*80)\n",
    "\n",
    "    train_iter = build_iterator(train_dataset, BERT_BATCH_SIZE, shuffle=True)\n",
    "    val_iter = build_iterator(validation_data, BERT_BATCH_SIZE, shuffle=False)\n",
    "    test_iter = build_iterator(test_data, BERT_BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    model = RNNBertClassifier(\n",
    "        output_dim=num_classes,\n",
    "        hidden_dim=BERT_HIDDEN_DIM,\n",
    "        n_layers=BERT_N_LAYERS,\n",
    "        dropout=BERT_DROPOUT,\n",
    "        bert_model_name=BERT_MODEL_NAME,\n",
    "        freeze_bert=freeze_bert,\n",
    "    ).to(device)\n",
    "\n",
    "    bert_params = [p for p in model.bert_model.parameters() if p.requires_grad]\n",
    "    other_params = [p for n, p in model.named_parameters() if 'bert_model' not in n]\n",
    "\n",
    "    optimizer_grouped_parameters = []\n",
    "    if bert_params:\n",
    "        optimizer_grouped_parameters.append({'params': bert_params, 'lr': BERT_LEARNING_RATE})\n",
    "    if other_params:\n",
    "        optimizer_grouped_parameters.append({'params': other_params, 'lr': BERT_OTHER_LR})\n",
    "\n",
    "    optimizer = optim.RMSprop(optimizer_grouped_parameters, weight_decay=BERT_L2_LAMBDA)\n",
    "\n",
    "    model, history = train_model_with_history_bert(\n",
    "        model,\n",
    "        train_iter,\n",
    "        val_iter,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        BERT_N_EPOCHS,\n",
    "        device,\n",
    "        num_classes,\n",
    "        BERT_L1_LAMBDA,\n",
    "        patience=BERT_PATIENCE,\n",
    "        model_name=f\"RNN+BERT ({description})\",\n",
    "        text_vocab=TEXT.vocab.itos,\n",
    "    )\n",
    "\n",
    "    test_loss, test_acc, test_f1, test_auc = evaluate_model_bert(\n",
    "        model,\n",
    "        test_iter,\n",
    "        criterion,\n",
    "        device,\n",
    "        f\"RNN+BERT ({description})\",\n",
    "        num_classes,\n",
    "        text_vocab=TEXT.vocab.itos,\n",
    "    )\n",
    "\n",
    "    topic_metrics = evaluate_per_topic_p35(model, test_iter, device)\n",
    "\n",
    "    model_path = f\"weights/part35_rnn_bert_{save_suffix}.pt\"\n",
    "    if BERT_SAVE_MODEL:\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    p35_results[\"rnn_bert\"][save_suffix] = {\n",
    "        \"description\": description,\n",
    "        \"dataset_key\": dataset_key,\n",
    "        \"history\": history,\n",
    "        \"test_metrics\": {\n",
    "            \"loss\": test_loss,\n",
    "            \"accuracy\": test_acc,\n",
    "            \"f1\": test_f1,\n",
    "            \"auc\": test_auc,\n",
    "        },\n",
    "        \"topic_metrics\": topic_metrics,\n",
    "        \"model_path\": model_path if BERT_SAVE_MODEL else None,\n",
    "        \"freeze_bert\": freeze_bert,\n",
    "        \"model\": model,\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"history\": history,\n",
    "        \"topic_metrics\": topic_metrics,\n",
    "        \"test_metrics\": {\n",
    "            \"loss\": test_loss,\n",
    "            \"accuracy\": test_acc,\n",
    "            \"f1\": test_f1,\n",
    "            \"auc\": test_auc,\n",
    "        },\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c9059c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Executing RNN + BERT experiments...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Running RNN + BERT experiment: Text Augmentation\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      ">>> Training RNN+BERT (Text Augmentation)\n",
      "    Parameters: 113,295,111\n",
      "    Max epochs: 50, Patience: 7\n",
      "Epoch: 01/50 | Time: 1m 5s\n",
      "\tTrain Loss: 4.0386 | Train Acc: 85.69%\n",
      "\tVal Loss: 0.3575 | Val Acc: 90.37% | Val F1: 0.9089 | Val AUC: 0.9869\n",
      "Epoch: 02/50 | Time: 1m 4s\n",
      "\tTrain Loss: 3.6463 | Train Acc: 96.79%\n",
      "\tVal Loss: 0.4580 | Val Acc: 90.18% | Val F1: 0.9087 | Val AUC: 0.9782\n",
      "Epoch: 03/50 | Time: 1m 3s\n",
      "\tTrain Loss: 3.4942 | Train Acc: 98.71%\n",
      "\tVal Loss: 0.5510 | Val Acc: 89.63% | Val F1: 0.9042 | Val AUC: 0.9741\n",
      "Epoch: 04/50 | Time: 1m 3s\n",
      "\tTrain Loss: 3.3905 | Train Acc: 99.03%\n",
      "\tVal Loss: 0.5953 | Val Acc: 90.00% | Val F1: 0.9061 | Val AUC: 0.9840\n",
      "Epoch: 05/50 | Time: 1m 3s\n",
      "\tTrain Loss: 3.2897 | Train Acc: 99.21%\n",
      "\tVal Loss: 0.6401 | Val Acc: 90.83% | Val F1: 0.9131 | Val AUC: 0.9840\n",
      "Epoch: 06/50 | Time: 1m 3s\n",
      "\tTrain Loss: 3.1957 | Train Acc: 99.36%\n",
      "\tVal Loss: 0.7367 | Val Acc: 89.72% | Val F1: 0.9033 | Val AUC: 0.9796\n",
      "Epoch: 07/50 | Time: 1m 6s\n",
      "\tTrain Loss: 3.0957 | Train Acc: 99.63%\n",
      "\tVal Loss: 0.7036 | Val Acc: 91.28% | Val F1: 0.9181 | Val AUC: 0.9812\n",
      "Epoch: 08/50 | Time: 1m 1s\n",
      "\tTrain Loss: 3.0011 | Train Acc: 99.55%\n",
      "\tVal Loss: 0.7118 | Val Acc: 90.64% | Val F1: 0.9127 | Val AUC: 0.9781\n",
      "Epoch: 09/50 | Time: 1m 1s\n",
      "\tTrain Loss: 2.9157 | Train Acc: 99.73%\n",
      "\tVal Loss: 0.6610 | Val Acc: 91.01% | Val F1: 0.9157 | Val AUC: 0.9808\n",
      "Epoch: 10/50 | Time: 1m 1s\n",
      "\tTrain Loss: 2.8240 | Train Acc: 99.87%\n",
      "\tVal Loss: 0.8455 | Val Acc: 89.27% | Val F1: 0.9004 | Val AUC: 0.9749\n",
      "Epoch: 11/50 | Time: 1m 1s\n",
      "\tTrain Loss: 2.7530 | Train Acc: 99.62%\n",
      "\tVal Loss: 0.7489 | Val Acc: 90.37% | Val F1: 0.9094 | Val AUC: 0.9790\n",
      "Epoch: 12/50 | Time: 1m 0s\n",
      "\tTrain Loss: 2.6750 | Train Acc: 99.72%\n",
      "\tVal Loss: 0.7769 | Val Acc: 89.45% | Val F1: 0.9035 | Val AUC: 0.9746\n",
      "Epoch: 13/50 | Time: 1m 3s\n",
      "\tTrain Loss: 2.6053 | Train Acc: 99.68%\n",
      "\tVal Loss: 0.9446 | Val Acc: 87.71% | Val F1: 0.8877 | Val AUC: 0.9648\n",
      "Epoch: 14/50 | Time: 1m 2s\n",
      "\tTrain Loss: 2.5391 | Train Acc: 99.63%\n",
      "\tVal Loss: 0.7813 | Val Acc: 89.63% | Val F1: 0.9031 | Val AUC: 0.9726\n",
      "\t>>> Early stopping at epoch 14, best val acc: 91.28%\n",
      "\n",
      ">>> Training completed! Best validation accuracy: 91.28%\n",
      "    Best validation F1: 0.9181\n",
      "    Best validation AUC-ROC: 0.9812\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Running RNN + BERT experiment: Weighted Sampling\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      ">>> Training RNN+BERT (Weighted Sampling)\n",
      "    Parameters: 113,295,111\n",
      "    Max epochs: 50, Patience: 7\n",
      "Epoch: 01/50 | Time: 0m 45s\n",
      "\tTrain Loss: 4.0965 | Train Acc: 83.79%\n",
      "\tVal Loss: 0.3378 | Val Acc: 91.65% | Val F1: 0.9167 | Val AUC: 0.9854\n",
      "Epoch: 02/50 | Time: 0m 45s\n",
      "\tTrain Loss: 3.6520 | Train Acc: 97.36%\n",
      "\tVal Loss: 0.4549 | Val Acc: 90.83% | Val F1: 0.9108 | Val AUC: 0.9850\n",
      "Epoch: 03/50 | Time: 0m 46s\n",
      "\tTrain Loss: 3.5237 | Train Acc: 98.88%\n",
      "\tVal Loss: 0.4357 | Val Acc: 91.47% | Val F1: 0.9165 | Val AUC: 0.9872\n",
      "Epoch: 04/50 | Time: 0m 46s\n",
      "\tTrain Loss: 3.4311 | Train Acc: 99.36%\n",
      "\tVal Loss: 0.4450 | Val Acc: 92.57% | Val F1: 0.9261 | Val AUC: 0.9840\n",
      "Epoch: 05/50 | Time: 0m 47s\n",
      "\tTrain Loss: 3.3430 | Train Acc: 99.59%\n",
      "\tVal Loss: 0.6056 | Val Acc: 90.37% | Val F1: 0.9043 | Val AUC: 0.9795\n",
      "Epoch: 06/50 | Time: 0m 49s\n",
      "\tTrain Loss: 3.2600 | Train Acc: 99.70%\n",
      "\tVal Loss: 0.5037 | Val Acc: 92.29% | Val F1: 0.9236 | Val AUC: 0.9869\n",
      "Epoch: 07/50 | Time: 0m 47s\n",
      "\tTrain Loss: 3.1943 | Train Acc: 99.47%\n",
      "\tVal Loss: 0.5638 | Val Acc: 92.02% | Val F1: 0.9205 | Val AUC: 0.9777\n",
      "Epoch: 08/50 | Time: 0m 46s\n",
      "\tTrain Loss: 3.1038 | Train Acc: 99.59%\n",
      "\tVal Loss: 0.5549 | Val Acc: 92.48% | Val F1: 0.9257 | Val AUC: 0.9823\n",
      "Epoch: 09/50 | Time: 0m 45s\n",
      "\tTrain Loss: 3.0048 | Train Acc: 99.98%\n",
      "\tVal Loss: 0.5753 | Val Acc: 92.29% | Val F1: 0.9239 | Val AUC: 0.9803\n",
      "Epoch: 10/50 | Time: 0m 46s\n",
      "\tTrain Loss: 2.9306 | Train Acc: 99.77%\n",
      "\tVal Loss: 0.5465 | Val Acc: 91.93% | Val F1: 0.9200 | Val AUC: 0.9811\n",
      "Epoch: 11/50 | Time: 0m 45s\n",
      "\tTrain Loss: 2.8632 | Train Acc: 99.68%\n",
      "\tVal Loss: 0.5579 | Val Acc: 92.39% | Val F1: 0.9250 | Val AUC: 0.9796\n",
      "\t>>> Early stopping at epoch 11, best val acc: 92.57%\n",
      "\n",
      ">>> Training completed! Best validation accuracy: 92.57%\n",
      "    Best validation F1: 0.9261\n",
      "    Best validation AUC-ROC: 0.9840\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Running RNN + BERT experiment: Augmentation + Weighted Sampling\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      ">>> Training RNN+BERT (Augmentation + Weighted Sampling)\n",
      "    Parameters: 113,295,111\n",
      "    Max epochs: 50, Patience: 7\n",
      "Epoch: 01/50 | Time: 1m 2s\n",
      "\tTrain Loss: 3.9811 | Train Acc: 87.63%\n",
      "\tVal Loss: 0.4926 | Val Acc: 89.08% | Val F1: 0.9002 | Val AUC: 0.9748\n",
      "Epoch: 02/50 | Time: 1m 3s\n",
      "\tTrain Loss: 3.6030 | Train Acc: 97.99%\n",
      "\tVal Loss: 0.5222 | Val Acc: 89.17% | Val F1: 0.9013 | Val AUC: 0.9793\n",
      "Epoch: 03/50 | Time: 1m 2s\n",
      "\tTrain Loss: 3.4551 | Train Acc: 99.21%\n",
      "\tVal Loss: 0.5874 | Val Acc: 90.18% | Val F1: 0.9062 | Val AUC: 0.9831\n",
      "Epoch: 04/50 | Time: 1m 3s\n",
      "\tTrain Loss: 3.3487 | Train Acc: 99.45%\n",
      "\tVal Loss: 0.5996 | Val Acc: 91.01% | Val F1: 0.9161 | Val AUC: 0.9826\n",
      "Epoch: 05/50 | Time: 1m 2s\n",
      "\tTrain Loss: 3.2427 | Train Acc: 99.62%\n",
      "\tVal Loss: 0.7024 | Val Acc: 89.17% | Val F1: 0.9002 | Val AUC: 0.9790\n",
      "Epoch: 06/50 | Time: 1m 2s\n",
      "\tTrain Loss: 3.1357 | Train Acc: 99.77%\n",
      "\tVal Loss: 0.7197 | Val Acc: 89.91% | Val F1: 0.9050 | Val AUC: 0.9809\n",
      "Epoch: 07/50 | Time: 1m 3s\n",
      "\tTrain Loss: 3.0476 | Train Acc: 99.58%\n",
      "\tVal Loss: 0.6652 | Val Acc: 89.82% | Val F1: 0.9009 | Val AUC: 0.9796\n",
      "Epoch: 08/50 | Time: 1m 3s\n",
      "\tTrain Loss: 2.9507 | Train Acc: 99.73%\n",
      "\tVal Loss: 0.7101 | Val Acc: 90.55% | Val F1: 0.9126 | Val AUC: 0.9802\n",
      "Epoch: 09/50 | Time: 1m 4s\n",
      "\tTrain Loss: 2.8603 | Train Acc: 99.82%\n",
      "\tVal Loss: 0.9215 | Val Acc: 87.71% | Val F1: 0.8850 | Val AUC: 0.9665\n",
      "Epoch: 10/50 | Time: 1m 3s\n",
      "\tTrain Loss: 2.7746 | Train Acc: 99.82%\n",
      "\tVal Loss: 0.7996 | Val Acc: 89.45% | Val F1: 0.9022 | Val AUC: 0.9705\n",
      "Epoch: 11/50 | Time: 1m 2s\n",
      "\tTrain Loss: 2.6933 | Train Acc: 99.70%\n",
      "\tVal Loss: 0.8246 | Val Acc: 88.72% | Val F1: 0.8943 | Val AUC: 0.9715\n",
      "\t>>> Early stopping at epoch 11, best val acc: 91.01%\n",
      "\n",
      ">>> Training completed! Best validation accuracy: 91.01%\n",
      "    Best validation F1: 0.9161\n",
      "    Best validation AUC-ROC: 0.9826\n",
      "\n",
      ">>> RNN + BERT experiments queued. Run the cells to execute training if needed.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Run RNN + BERT experiments\n",
    "# ============================================================================\n",
    "\n",
    "if BERT_AVAILABLE:\n",
    "    print(\"\\n>>> Executing RNN + BERT experiments...\")\n",
    "\n",
    "    bert_results_text_aug = run_rnn_bert_experiment(\n",
    "        dataset_key=\"augmented\",\n",
    "        description=\"Text Augmentation\",\n",
    "        save_suffix=\"text_aug\",\n",
    "    )\n",
    "\n",
    "    bert_results_weighted = run_rnn_bert_experiment(\n",
    "        dataset_key=\"weighted\",\n",
    "        description=\"Weighted Sampling\",\n",
    "        save_suffix=\"weighted_sampler\",\n",
    "    )\n",
    "\n",
    "    bert_results_aug_weighted = run_rnn_bert_experiment(\n",
    "        dataset_key=\"augmented_weighted\",\n",
    "        description=\"Augmentation + Weighted Sampling\",\n",
    "        save_suffix=\"text_aug_weighted\",\n",
    "    )\n",
    "\n",
    "    print(\"\\n>>> RNN + BERT experiments queued. Run the cells to execute training if needed.\")\n",
    "else:\n",
    "    print(\"\\n>>> Skipping RNN + BERT experiments (transformers library unavailable).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d1ce0d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Topic-wise accuracy for RNN + BERT variants\n",
      "\n",
      "RNN + BERT (Text Augmentation)\n",
      "------------------------------\n",
      "Topic      Accuracy %   Correct    Total     \n",
      "---------------------------------------------\n",
      "ABBR       0.00         0          9         \n",
      "DESC       81.16        112        138       \n",
      "ENTY       37.23        35         94        \n",
      "HUM        0.00         0          65        \n",
      "LOC        0.00         0          81        \n",
      "NUM        1.77         2          113       \n",
      "---------------------------------------------\n",
      "Topic      0.30         149        500       \n",
      "\n",
      "RNN + BERT (Weighted Sampling)\n",
      "------------------------------\n",
      "Topic      Accuracy %   Correct    Total     \n",
      "---------------------------------------------\n",
      "ABBR       0.00         0          9         \n",
      "DESC       68.12        94         138       \n",
      "ENTY       80.85        76         94        \n",
      "HUM        0.00         0          65        \n",
      "LOC        0.00         0          81        \n",
      "NUM        2.65         3          113       \n",
      "---------------------------------------------\n",
      "Topic      0.35         173        500       \n",
      "\n",
      "RNN + BERT (Augmentation + Weighted Sampling)\n",
      "---------------------------------------------\n",
      "Topic      Accuracy %   Correct    Total     \n",
      "---------------------------------------------\n",
      "ABBR       0.00         0          9         \n",
      "DESC       0.00         0          138       \n",
      "ENTY       100.00       94         94        \n",
      "HUM        0.00         0          65        \n",
      "LOC        0.00         0          81        \n",
      "NUM        0.00         0          113       \n",
      "---------------------------------------------\n",
      "Topic      0.19         94         500       \n"
     ]
    }
   ],
   "source": [
    "# =========================================================================\n",
    "# Topic-wise accuracy summary for RNN + BERT experiments\n",
    "# =========================================================================\n",
    "\n",
    "if p35_results[\"rnn_bert\"]:\n",
    "    print(\"\\n>>> Topic-wise accuracy for RNN + BERT variants\")\n",
    "    for key, info in p35_results[\"rnn_bert\"].items():\n",
    "        topic_metrics = info.get(\"topic_metrics\")\n",
    "        if not topic_metrics:\n",
    "            continue\n",
    "        title = f\"RNN + BERT ({info['description']})\"\n",
    "        display_topic_metrics(title, topic_metrics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9b5ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PART 3.5: TEXT AUGMENTATION VS WEIGHTED SAMPLING\n",
      "================================================================================\n",
      "\n",
      ">>> Dataset Variants Used:\n",
      "  - original: 4362 samples\n",
      "  - augmented: 5980 samples\n",
      "  - weighted: 4362 samples\n",
      "  - augmented_weighted: 5980 samples\n",
      "\n",
      ">>> Simple RNN (mean pooling) Experiments:\n",
      "  Text Augmentation [text_aug]\n",
      "    - Test Accuracy: 82.20%\n",
      "    - Test F1: 0.8301\n",
      "    - Test AUC: 0.9521\n",
      "    - Test Loss: 0.7435\n",
      "    - Weakest Topic: ENTY (61.70%)\n",
      "    - Model saved to: weights/part35_simple_rnn_text_aug.pt\n",
      "  Weighted Sampling [weighted_sampler]\n",
      "    - Test Accuracy: 82.80%\n",
      "    - Test F1: 0.8181\n",
      "    - Test AUC: 0.9540\n",
      "    - Test Loss: 0.6571\n",
      "    - Weakest Topic: ENTY (45.74%)\n",
      "    - Model saved to: weights/part35_simple_rnn_weighted_sampler.pt\n",
      "  Augmentation + Weighted Sampling [text_aug_weighted]\n",
      "    - Test Accuracy: 67.40%\n",
      "    - Test F1: 0.7107\n",
      "    - Test AUC: 0.9365\n",
      "    - Test Loss: 0.9728\n",
      "    - Weakest Topic: DESC (41.30%)\n",
      "    - Model saved to: weights/part35_simple_rnn_text_aug_weighted.pt\n",
      "\n",
      ">>> RNN + BERT Experiments:\n",
      "  Text Augmentation [text_aug]\n",
      "    - Test Accuracy: 80.20%\n",
      "    - Test F1: 0.8379\n",
      "    - Test AUC: 0.9399\n",
      "    - Test Loss: 1.6796\n",
      "    - Weakest Topic: ABBR (0.00%)\n",
      "    - Model saved to: weights/part35_rnn_bert_text_aug.pt\n",
      "  Weighted Sampling [weighted_sampler]\n",
      "    - Test Accuracy: 93.40%\n",
      "    - Test F1: 0.9349\n",
      "    - Test AUC: 0.9789\n",
      "    - Test Loss: 0.4571\n",
      "    - Weakest Topic: ABBR (0.00%)\n",
      "    - Model saved to: weights/part35_rnn_bert_weighted_sampler.pt\n",
      "  Augmentation + Weighted Sampling [text_aug_weighted]\n",
      "    - Test Accuracy: 77.80%\n",
      "    - Test F1: 0.8130\n",
      "    - Test AUC: 0.9130\n",
      "    - Test Loss: 1.8654\n",
      "    - Weakest Topic: ABBR (0.00%)\n",
      "    - Model saved to: weights/part35_rnn_bert_text_aug_weighted.pt\n",
      "\n",
      ">>> Next Steps:\n",
      "  - Execute the experiment cells sequentially (they can be time-consuming).\n",
      "  - Optionally add plotting/analysis cells to compare validation curves across runs.\n",
      "  - Update the final report with observations once metrics are populated.\n",
      "\n",
      "================================================================================\n",
      "PART 3.5 SETUP COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PART 3.4 SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 3.4: TEXT AUGMENTATION VS WEIGHTED SAMPLING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n>>> Dataset Variants Used:\")\n",
    "for key in [\"original\", \"augmented\", \"weighted\", \"augmented_weighted\"]:\n",
    "    dataset = p35_datasets[key]\n",
    "    print(f\"  - {key}: {len(dataset.examples)} samples\")\n",
    "\n",
    "print(\"\\n>>> Simple RNN (mean pooling) Experiments:\")\n",
    "if p35_results[\"simple_rnn_baseline\"]:\n",
    "    for key, info in p35_results[\"simple_rnn_baseline\"].items():\n",
    "        metrics = info.get(\"test_metrics\", {})\n",
    "        print(f\"  {info['description']} [{key}]\")\n",
    "        if metrics:\n",
    "            print(f\"    - Test Accuracy: {metrics.get('accuracy', 0)*100:.2f}%\")\n",
    "            print(f\"    - Test F1: {metrics.get('f1', 0):.4f}\")\n",
    "            print(f\"    - Test AUC: {metrics.get('auc', 0):.4f}\")\n",
    "            print(f\"    - Test Loss: {metrics.get('loss', 0):.4f}\")\n",
    "        topic_metrics = info.get(\"topic_metrics\")\n",
    "        if topic_metrics:\n",
    "            weakest = min(topic_metrics.items(), key=lambda kv: kv[1]['accuracy'])\n",
    "            print(f\"    - Weakest Topic: {weakest[0]} ({weakest[1]['accuracy']*100:.2f}%)\")\n",
    "        print(f\"    - Model saved to: {info['model_path']}\")\n",
    "else:\n",
    "    print(\"  - Pending (run the experiment cells above)\")\n",
    "\n",
    "print(\"\\n>>> RNN + BERT Experiments:\")\n",
    "if p35_results[\"rnn_bert\"]:\n",
    "    for key, info in p35_results[\"rnn_bert\"].items():\n",
    "        metrics = info.get(\"test_metrics\", {})\n",
    "        print(f\"  {info['description']} [{key}]\")\n",
    "        if metrics:\n",
    "            print(f\"    - Test Accuracy: {metrics.get('accuracy', 0)*100:.2f}%\")\n",
    "            print(f\"    - Test F1: {metrics.get('f1', 0):.4f}\")\n",
    "            print(f\"    - Test AUC: {metrics.get('auc', 0):.4f}\")\n",
    "            print(f\"    - Test Loss: {metrics.get('loss', 0):.4f}\")\n",
    "        topic_metrics = info.get(\"topic_metrics\")\n",
    "        if topic_metrics:\n",
    "            weakest = min(topic_metrics.items(), key=lambda kv: kv[1]['accuracy'])\n",
    "            print(f\"    - Weakest Topic: {weakest[0]} ({weakest[1]['accuracy']*100:.2f}%)\")\n",
    "        print(f\"    - Model saved to: {info['model_path']}\")\n",
    "else:\n",
    "    print(\"  - Pending (run the experiment cells above)\")\n",
    "\n",
    "print(\"\\n>>> Next Steps:\")\n",
    "print(\"  - Execute the experiment cells sequentially (they can be time-consuming).\")\n",
    "print(\"  - Optionally add plotting/analysis cells to compare validation curves across runs.\")\n",
    "print(\"  - Update the final report with observations once metrics are populated.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 3.4 SETUP COMPLETE\")\n",
    "print(\"=\"*80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
