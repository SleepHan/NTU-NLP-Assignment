{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63bc603c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, random, torch\n",
    "\n",
    "# IMPORTANT: Fix for PyTorch/IPython compatibility issue\n",
    "# This must run BEFORE importing torch to avoid decorator conflicts\n",
    "# This fixes the \"disable() got an unexpected keyword argument 'wrapping'\" error\n",
    "\n",
    "# Method 1: Try to disable dynamo via environment variable (needs to be set before import)\n",
    "os.environ.setdefault('TORCH_COMPILE_DISABLE', '1')\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "# Method 2: Patch torch._dynamo.disable decorator after import\n",
    "try:\n",
    "    import torch._dynamo\n",
    "    # Patch the disable function to ignore the 'wrapping' parameter\n",
    "    if hasattr(torch._dynamo, 'disable'):\n",
    "        def patched_disable(fn=None, *args, **kwargs):\n",
    "            # Remove problematic 'wrapping' parameter if present\n",
    "            if 'wrapping' in kwargs:\n",
    "                kwargs.pop('wrapping')\n",
    "            if fn is None:\n",
    "                # Decorator usage: @disable\n",
    "                return lambda f: f\n",
    "            # Function usage: disable(fn) or disable(fn, **kwargs)\n",
    "            # Simply return the function unwrapped to avoid recursion\n",
    "            # The original disable was causing issues, so we bypass it entirely\n",
    "            return fn\n",
    "        torch._dynamo.disable = patched_disable\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not patch torch._dynamo: {e}\")\n",
    "    pass  # If patching fails, continue anyway\n",
    "\n",
    "from torchtext import data\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "\n",
    "os.environ['GENSIM_DATA_DIR'] = os.path.join(os.getcwd(), 'gensim-data')\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "650b5e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Prepping Data...\n",
      "[+] Test set formed!\n",
      "[+] Train and Validation sets formed!\n",
      "[+] Data prepped successfully!\n",
      "[*] Retrieving pretrained word embeddings...\n",
      "[*] Loading fasttext model...\n",
      "[+] Model loaded!\n",
      "[*] Forming embedding matrix...\n",
      "[+] Embedding matrix formed!\n",
      "[+] Embeddings retrieved successfully!\n",
      "\n",
      "Label distribution in training set:\n",
      "- ABBR: 69 samples (1.58%)\n",
      "- DESC: 930 samples (21.32%)\n",
      "- ENTY: 1000 samples (22.93%)\n",
      "- HUM: 978 samples (22.42%)\n",
      "- LOC: 668 samples (15.31%)\n",
      "- NUM: 717 samples (16.44%)\n",
      "Total samples: 4362, Sum of percentages: 100.00%\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "train_data, validation_data, test_data, LABEL, TEXT, pretrained_embed = data_prep(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea2b5c8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtext.vocab.Vocab at 0x204e00267d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dee8ca7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2ac13be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of classes: 6\n",
      "Classes: ['ENTY', 'HUM', 'DESC', 'NUM', 'LOC', 'ABBR']\n"
     ]
    }
   ],
   "source": [
    "### Part 2: Model Training & Evaluation - RNN\n",
    "\n",
    "# Build vocabulary for labels\n",
    "LABEL.build_vocab(train_data)\n",
    "num_classes = len(LABEL.vocab)\n",
    "print(f\"\\nNumber of classes: {num_classes}\")\n",
    "print(f\"Classes: {LABEL.vocab.itos}\")\n",
    "\n",
    "\n",
    "class SimpleRNNClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple RNN for topic classification (Baseline - no dropout).\n",
    "    Uses pretrained embeddings (learnable/updated during training) with OOV mitigation \n",
    "    and aggregates word representations to sentence representation using the last hidden state.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, \n",
    "                 n_layers=1, dropout=0.0, padding_idx=0, pretrained_embeddings=None):\n",
    "        super(SimpleRNNClassifier, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # Embedding layer initialized with pretrained embeddings\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "            self.embedding.weight.requires_grad = True\n",
    "        \n",
    "        # Simple RNN layer (no dropout in baseline)\n",
    "        self.rnn = nn.RNN(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=n_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.0  # No dropout in baseline\n",
    "        )\n",
    "        \n",
    "        # Fully connected output layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        \n",
    "        # Embed the input\n",
    "        embedded = self.embedding(text)  # [batch_size, seq_len, embedding_dim]\n",
    "        \n",
    "        # Get dimensions\n",
    "        batch_size = embedded.size(0)\n",
    "        seq_len = embedded.size(1)\n",
    "        \n",
    "        # Handle text_lengths: ensure it's a 1D tensor with batch_size elements\n",
    "        text_lengths_flat = text_lengths.flatten().cpu().long()\n",
    "        \n",
    "        # text_lengths should have exactly batch_size elements (one length per batch item)\n",
    "        if len(text_lengths_flat) != batch_size:\n",
    "            raise ValueError(\n",
    "                f\"text_lengths size mismatch: got {len(text_lengths_flat)} elements, \"\n",
    "                f\"expected {batch_size} (batch_size). text_lengths.shape={text_lengths.shape}, \"\n",
    "                f\"text.shape={text.shape}, embedded.shape={embedded.shape}\"\n",
    "            )\n",
    "        \n",
    "        # Clamp lengths to be at most the sequence length (safety check)\n",
    "        text_lengths_clamped = torch.clamp(text_lengths_flat, min=1, max=seq_len)\n",
    "        \n",
    "        # Pack the padded sequences for efficient processing\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, text_lengths_clamped, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        \n",
    "        # Pass through RNN\n",
    "        packed_output, hidden = self.rnn(packed_embedded)\n",
    "        # hidden: [n_layers, batch_size, hidden_dim]\n",
    "        \n",
    "        # Use the last hidden state from the last layer\n",
    "        last_hidden = hidden[-1]  # [batch_size, hidden_dim]\n",
    "        \n",
    "        # Pass through fully connected layer\n",
    "        output = self.fc(last_hidden)  # [batch_size, output_dim]\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "370ed524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8120\n"
     ]
    }
   ],
   "source": [
    "print(len(TEXT.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23562caa",
   "metadata": {},
   "source": [
    "Training order:\n",
    "1. Word aggregation\n",
    "2. Hyperparameters tuning\n",
    "3. Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9b664f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PART 2: SIMPLE RNN MODEL TRAINING\n",
      "================================================================================\n",
      "TEXT.vocab size: 8120\n",
      "FastText embedding vocab size: 8120\n",
      "\n",
      ">>> Training Baseline RNN Model\n",
      "Configuration:\n",
      "  - Hidden Dim: 256\n",
      "  - Layers: 1\n",
      "  - Dropout: 0.0 (Baseline: no regularization)\n",
      "  - Learning Rate: 0.001\n",
      "  - Batch Size: 64\n",
      "  - Epochs: 100 (no early stopping)\n",
      "  - Embedding Dim: 300 (FastText)\n",
      "  - Embeddings: LEARNABLE (updated during training)\n",
      "  - OOV Handling: FastText subword embeddings + trainable <unk> token\n",
      "\n",
      "Starting training for 100 epochs...\n",
      "Device: cuda\n",
      "Trainable parameters: 2,580,390\n",
      "Embedding layer learnable: True\n",
      "--------------------------------------------------------------------------------\n",
      "DEBUG BATCH - text shape: torch.Size([8, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 8])\n",
      "Epoch: 01/100 | Time: 0m 0s\n",
      "\tTrain Loss: 1.6053 | Train Acc: 27.79%\n",
      "\tVal Loss: 1.3862 | Val Acc: 41.56%\n",
      "\t>>> New best model saved with Val Acc: 41.56%\n",
      "DEBUG BATCH - text shape: torch.Size([14, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 14])\n",
      "Epoch: 02/100 | Time: 0m 0s\n",
      "\tTrain Loss: 1.0483 | Train Acc: 57.45%\n",
      "\tVal Loss: 1.0676 | Val Acc: 60.83%\n",
      "\t>>> New best model saved with Val Acc: 60.83%\n",
      "DEBUG BATCH - text shape: torch.Size([8, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 8])\n",
      "Epoch: 03/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.5021 | Train Acc: 83.68%\n",
      "\tVal Loss: 1.1048 | Val Acc: 63.12%\n",
      "\t>>> New best model saved with Val Acc: 63.12%\n",
      "DEBUG BATCH - text shape: torch.Size([14, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 14])\n",
      "Epoch: 04/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.1829 | Train Acc: 94.45%\n",
      "\tVal Loss: 1.5168 | Val Acc: 55.32%\n",
      "DEBUG BATCH - text shape: torch.Size([10, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 10])\n",
      "Epoch: 05/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0493 | Train Acc: 98.99%\n",
      "\tVal Loss: 1.3287 | Val Acc: 64.31%\n",
      "\t>>> New best model saved with Val Acc: 64.31%\n",
      "DEBUG BATCH - text shape: torch.Size([16, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 16])\n",
      "Epoch: 06/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0118 | Train Acc: 99.86%\n",
      "\tVal Loss: 1.3267 | Val Acc: 64.13%\n",
      "DEBUG BATCH - text shape: torch.Size([5, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 5])\n",
      "Epoch: 07/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0052 | Train Acc: 99.95%\n",
      "\tVal Loss: 1.4075 | Val Acc: 67.43%\n",
      "\t>>> New best model saved with Val Acc: 67.43%\n",
      "DEBUG BATCH - text shape: torch.Size([11, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 11])\n",
      "Epoch: 08/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0028 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.2908 | Val Acc: 67.89%\n",
      "\t>>> New best model saved with Val Acc: 67.89%\n",
      "DEBUG BATCH - text shape: torch.Size([17, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 17])\n",
      "Epoch: 09/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0018 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.3193 | Val Acc: 67.61%\n",
      "DEBUG BATCH - text shape: torch.Size([11, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 11])\n",
      "Epoch: 10/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0014 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.3552 | Val Acc: 67.80%\n",
      "DEBUG BATCH - text shape: torch.Size([6, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 6])\n",
      "Epoch: 11/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0011 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.3669 | Val Acc: 68.17%\n",
      "\t>>> New best model saved with Val Acc: 68.17%\n",
      "DEBUG BATCH - text shape: torch.Size([11, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 11])\n",
      "Epoch: 12/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0009 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.3934 | Val Acc: 67.80%\n",
      "DEBUG BATCH - text shape: torch.Size([5, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 5])\n",
      "Epoch: 13/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0008 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.4128 | Val Acc: 68.07%\n",
      "DEBUG BATCH - text shape: torch.Size([10, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 10])\n",
      "Epoch: 14/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0007 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.4227 | Val Acc: 68.35%\n",
      "\t>>> New best model saved with Val Acc: 68.35%\n",
      "DEBUG BATCH - text shape: torch.Size([5, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 5])\n",
      "Epoch: 15/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0006 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.4407 | Val Acc: 68.44%\n",
      "\t>>> New best model saved with Val Acc: 68.44%\n",
      "DEBUG BATCH - text shape: torch.Size([8, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 8])\n",
      "Epoch: 16/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0005 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.4645 | Val Acc: 68.62%\n",
      "\t>>> New best model saved with Val Acc: 68.62%\n",
      "DEBUG BATCH - text shape: torch.Size([9, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 9])\n",
      "Epoch: 17/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0005 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.4713 | Val Acc: 68.53%\n",
      "DEBUG BATCH - text shape: torch.Size([37, 10]), text_lengths shape: torch.Size([10]), labels shape: torch.Size([10])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([10, 37])\n",
      "Epoch: 18/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0004 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.4821 | Val Acc: 68.62%\n",
      "DEBUG BATCH - text shape: torch.Size([14, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 14])\n",
      "Epoch: 19/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0004 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.4964 | Val Acc: 68.72%\n",
      "\t>>> New best model saved with Val Acc: 68.72%\n",
      "DEBUG BATCH - text shape: torch.Size([16, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 16])\n",
      "Epoch: 20/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0003 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.5159 | Val Acc: 68.81%\n",
      "\t>>> New best model saved with Val Acc: 68.81%\n",
      "DEBUG BATCH - text shape: torch.Size([10, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 10])\n",
      "Epoch: 21/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0003 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.5198 | Val Acc: 68.62%\n",
      "DEBUG BATCH - text shape: torch.Size([17, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 17])\n",
      "Epoch: 22/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0003 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.5321 | Val Acc: 68.44%\n",
      "DEBUG BATCH - text shape: torch.Size([8, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 8])\n",
      "Epoch: 23/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0002 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.5455 | Val Acc: 68.81%\n",
      "DEBUG BATCH - text shape: torch.Size([13, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 13])\n",
      "Epoch: 24/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0002 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.5563 | Val Acc: 69.17%\n",
      "\t>>> New best model saved with Val Acc: 69.17%\n",
      "DEBUG BATCH - text shape: torch.Size([11, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 11])\n",
      "Epoch: 25/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0002 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.5657 | Val Acc: 69.27%\n",
      "\t>>> New best model saved with Val Acc: 69.27%\n",
      "DEBUG BATCH - text shape: torch.Size([6, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 6])\n",
      "Epoch: 26/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0002 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.5753 | Val Acc: 69.27%\n",
      "DEBUG BATCH - text shape: torch.Size([11, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 11])\n",
      "Epoch: 27/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0002 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.5789 | Val Acc: 69.45%\n",
      "\t>>> New best model saved with Val Acc: 69.45%\n",
      "DEBUG BATCH - text shape: torch.Size([14, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 14])\n",
      "Epoch: 28/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0002 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.5968 | Val Acc: 69.36%\n",
      "DEBUG BATCH - text shape: torch.Size([7, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 7])\n",
      "Epoch: 29/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0002 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.6010 | Val Acc: 69.54%\n",
      "\t>>> New best model saved with Val Acc: 69.54%\n",
      "DEBUG BATCH - text shape: torch.Size([13, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 13])\n",
      "Epoch: 30/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.6052 | Val Acc: 69.63%\n",
      "\t>>> New best model saved with Val Acc: 69.63%\n",
      "DEBUG BATCH - text shape: torch.Size([11, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 11])\n",
      "Epoch: 31/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.6171 | Val Acc: 69.63%\n",
      "DEBUG BATCH - text shape: torch.Size([10, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 10])\n",
      "Epoch: 32/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.6275 | Val Acc: 69.54%\n",
      "DEBUG BATCH - text shape: torch.Size([15, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 15])\n",
      "Epoch: 33/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.6333 | Val Acc: 69.82%\n",
      "\t>>> New best model saved with Val Acc: 69.82%\n",
      "DEBUG BATCH - text shape: torch.Size([9, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 9])\n",
      "Epoch: 34/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.6420 | Val Acc: 69.63%\n",
      "DEBUG BATCH - text shape: torch.Size([8, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 8])\n",
      "Epoch: 35/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.6565 | Val Acc: 69.72%\n",
      "DEBUG BATCH - text shape: torch.Size([8, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 8])\n",
      "Epoch: 36/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.6568 | Val Acc: 69.82%\n",
      "DEBUG BATCH - text shape: torch.Size([9, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 9])\n",
      "Epoch: 37/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.6645 | Val Acc: 69.91%\n",
      "\t>>> New best model saved with Val Acc: 69.91%\n",
      "DEBUG BATCH - text shape: torch.Size([16, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 16])\n",
      "Epoch: 38/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.6785 | Val Acc: 69.63%\n",
      "DEBUG BATCH - text shape: torch.Size([14, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 14])\n",
      "Epoch: 39/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.6841 | Val Acc: 69.91%\n",
      "DEBUG BATCH - text shape: torch.Size([5, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 5])\n",
      "Epoch: 40/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.6889 | Val Acc: 70.00%\n",
      "\t>>> New best model saved with Val Acc: 70.00%\n",
      "DEBUG BATCH - text shape: torch.Size([8, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 8])\n",
      "Epoch: 41/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.6960 | Val Acc: 70.00%\n",
      "DEBUG BATCH - text shape: torch.Size([10, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 10])\n",
      "Epoch: 42/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.7033 | Val Acc: 69.82%\n",
      "DEBUG BATCH - text shape: torch.Size([5, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 5])\n",
      "Epoch: 43/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.7059 | Val Acc: 70.00%\n",
      "DEBUG BATCH - text shape: torch.Size([12, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 12])\n",
      "Epoch: 44/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.7188 | Val Acc: 70.00%\n",
      "DEBUG BATCH - text shape: torch.Size([14, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 14])\n",
      "Epoch: 45/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.7238 | Val Acc: 70.00%\n",
      "DEBUG BATCH - text shape: torch.Size([8, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 8])\n",
      "Epoch: 46/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.7313 | Val Acc: 70.00%\n",
      "DEBUG BATCH - text shape: torch.Size([8, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 8])\n",
      "Epoch: 47/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.7375 | Val Acc: 70.00%\n",
      "DEBUG BATCH - text shape: torch.Size([9, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 9])\n",
      "Epoch: 48/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.7422 | Val Acc: 70.00%\n",
      "DEBUG BATCH - text shape: torch.Size([5, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 5])\n",
      "Epoch: 49/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.7477 | Val Acc: 70.00%\n",
      "DEBUG BATCH - text shape: torch.Size([10, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 10])\n",
      "Epoch: 50/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.7578 | Val Acc: 70.00%\n",
      "DEBUG BATCH - text shape: torch.Size([6, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 6])\n",
      "Epoch: 51/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.7610 | Val Acc: 70.00%\n",
      "DEBUG BATCH - text shape: torch.Size([12, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 12])\n",
      "Epoch: 52/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.7694 | Val Acc: 70.00%\n",
      "DEBUG BATCH - text shape: torch.Size([7, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 7])\n",
      "Epoch: 53/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.7736 | Val Acc: 70.00%\n",
      "DEBUG BATCH - text shape: torch.Size([6, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 6])\n",
      "Epoch: 54/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.7798 | Val Acc: 70.00%\n",
      "DEBUG BATCH - text shape: torch.Size([9, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 9])\n",
      "Epoch: 55/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.7858 | Val Acc: 70.00%\n",
      "DEBUG BATCH - text shape: torch.Size([8, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 8])\n",
      "Epoch: 56/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.7930 | Val Acc: 70.00%\n",
      "DEBUG BATCH - text shape: torch.Size([22, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 22])\n",
      "Epoch: 57/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.7963 | Val Acc: 69.91%\n",
      "DEBUG BATCH - text shape: torch.Size([13, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 13])\n",
      "Epoch: 58/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.8058 | Val Acc: 70.18%\n",
      "\t>>> New best model saved with Val Acc: 70.18%\n",
      "DEBUG BATCH - text shape: torch.Size([9, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 9])\n",
      "Epoch: 59/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.8082 | Val Acc: 70.18%\n",
      "DEBUG BATCH - text shape: torch.Size([11, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 11])\n",
      "Epoch: 60/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.8149 | Val Acc: 70.00%\n",
      "DEBUG BATCH - text shape: torch.Size([5, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 5])\n",
      "Epoch: 61/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.8238 | Val Acc: 70.00%\n",
      "DEBUG BATCH - text shape: torch.Size([5, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 5])\n",
      "Epoch: 62/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.8225 | Val Acc: 70.18%\n",
      "DEBUG BATCH - text shape: torch.Size([13, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 13])\n",
      "Epoch: 63/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.8345 | Val Acc: 69.91%\n",
      "DEBUG BATCH - text shape: torch.Size([14, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 14])\n",
      "Epoch: 64/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.8373 | Val Acc: 70.09%\n",
      "DEBUG BATCH - text shape: torch.Size([8, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 8])\n",
      "Epoch: 65/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.8424 | Val Acc: 70.00%\n",
      "DEBUG BATCH - text shape: torch.Size([10, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 10])\n",
      "Epoch: 66/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.8497 | Val Acc: 69.91%\n",
      "DEBUG BATCH - text shape: torch.Size([19, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 19])\n",
      "Epoch: 67/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.8563 | Val Acc: 69.72%\n",
      "DEBUG BATCH - text shape: torch.Size([7, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 7])\n",
      "Epoch: 68/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.8581 | Val Acc: 69.82%\n",
      "DEBUG BATCH - text shape: torch.Size([10, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 10])\n",
      "Epoch: 69/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.8647 | Val Acc: 69.82%\n",
      "DEBUG BATCH - text shape: torch.Size([13, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 13])\n",
      "Epoch: 70/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.8720 | Val Acc: 69.72%\n",
      "DEBUG BATCH - text shape: torch.Size([4, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 4])\n",
      "Epoch: 71/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.8762 | Val Acc: 69.82%\n",
      "DEBUG BATCH - text shape: torch.Size([10, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 10])\n",
      "Epoch: 72/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.8812 | Val Acc: 69.63%\n",
      "DEBUG BATCH - text shape: torch.Size([14, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 14])\n",
      "Epoch: 73/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.8889 | Val Acc: 69.63%\n",
      "DEBUG BATCH - text shape: torch.Size([8, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 8])\n",
      "Epoch: 74/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.8918 | Val Acc: 69.82%\n",
      "DEBUG BATCH - text shape: torch.Size([17, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 17])\n",
      "Epoch: 75/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.9003 | Val Acc: 69.72%\n",
      "DEBUG BATCH - text shape: torch.Size([12, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 12])\n",
      "Epoch: 76/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.9029 | Val Acc: 69.63%\n",
      "DEBUG BATCH - text shape: torch.Size([9, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 9])\n",
      "Epoch: 77/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.9036 | Val Acc: 69.54%\n",
      "DEBUG BATCH - text shape: torch.Size([7, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 7])\n",
      "Epoch: 78/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.9142 | Val Acc: 69.72%\n",
      "DEBUG BATCH - text shape: torch.Size([9, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 9])\n",
      "Epoch: 79/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.9190 | Val Acc: 69.63%\n",
      "DEBUG BATCH - text shape: torch.Size([12, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 12])\n",
      "Epoch: 80/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.9248 | Val Acc: 69.63%\n",
      "DEBUG BATCH - text shape: torch.Size([8, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 8])\n",
      "Epoch: 81/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.9329 | Val Acc: 69.72%\n",
      "DEBUG BATCH - text shape: torch.Size([8, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 8])\n",
      "Epoch: 82/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.9388 | Val Acc: 69.82%\n",
      "DEBUG BATCH - text shape: torch.Size([7, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 7])\n",
      "Epoch: 83/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.9412 | Val Acc: 69.91%\n",
      "DEBUG BATCH - text shape: torch.Size([12, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 12])\n",
      "Epoch: 84/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.9422 | Val Acc: 69.91%\n",
      "DEBUG BATCH - text shape: torch.Size([37, 10]), text_lengths shape: torch.Size([10]), labels shape: torch.Size([10])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([10, 37])\n",
      "Epoch: 85/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.9514 | Val Acc: 69.82%\n",
      "DEBUG BATCH - text shape: torch.Size([11, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 11])\n",
      "Epoch: 86/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.9557 | Val Acc: 69.91%\n",
      "DEBUG BATCH - text shape: torch.Size([10, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 10])\n",
      "Epoch: 87/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.9619 | Val Acc: 69.91%\n",
      "DEBUG BATCH - text shape: torch.Size([7, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 7])\n",
      "Epoch: 88/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.9644 | Val Acc: 69.91%\n",
      "DEBUG BATCH - text shape: torch.Size([6, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 6])\n",
      "Epoch: 89/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.9738 | Val Acc: 69.72%\n",
      "DEBUG BATCH - text shape: torch.Size([9, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 9])\n",
      "Epoch: 90/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.9848 | Val Acc: 69.72%\n",
      "DEBUG BATCH - text shape: torch.Size([7, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 7])\n",
      "Epoch: 91/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.9827 | Val Acc: 69.72%\n",
      "DEBUG BATCH - text shape: torch.Size([11, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 11])\n",
      "Epoch: 92/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.9885 | Val Acc: 69.82%\n",
      "DEBUG BATCH - text shape: torch.Size([11, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 11])\n",
      "Epoch: 93/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.9948 | Val Acc: 69.72%\n",
      "DEBUG BATCH - text shape: torch.Size([8, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 8])\n",
      "Epoch: 94/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.0032 | Val Acc: 69.63%\n",
      "DEBUG BATCH - text shape: torch.Size([12, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 12])\n",
      "Epoch: 95/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.0025 | Val Acc: 69.82%\n",
      "DEBUG BATCH - text shape: torch.Size([7, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 7])\n",
      "Epoch: 96/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.0086 | Val Acc: 69.72%\n",
      "DEBUG BATCH - text shape: torch.Size([9, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 9])\n",
      "Epoch: 97/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.0128 | Val Acc: 69.82%\n",
      "DEBUG BATCH - text shape: torch.Size([10, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 10])\n",
      "Epoch: 98/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.0203 | Val Acc: 69.63%\n",
      "DEBUG BATCH - text shape: torch.Size([12, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 12])\n",
      "Epoch: 99/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.0221 | Val Acc: 69.72%\n",
      "DEBUG BATCH - text shape: torch.Size([10, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 10])\n",
      "Epoch: 100/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.0355 | Val Acc: 69.63%\n",
      "--------------------------------------------------------------------------------\n",
      "Training completed! Best validation accuracy: 70.18%\n",
      "Total epochs trained: 100\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Part 2: Initial Simple RNN Model Training\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 2: SIMPLE RNN MODEL TRAINING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get pretrained embeddings from Part 1 (frozen embeddings)\n",
    "pretrained_embeddings = pretrained_embed.weight.data\n",
    "\n",
    "# Get embedding dimension and vocab size from the fasttext embedding layer\n",
    "embedding_dim = pretrained_embed.weight.shape[1]\n",
    "embedding_vocab_size = pretrained_embed.weight.shape[0]  # Vocab size from saved embedding\n",
    "\n",
    "# Verify vocab sizes match (they might differ if vocab was rebuilt)\n",
    "print(f\"TEXT.vocab size: {len(TEXT.vocab)}\")\n",
    "print(f\"FastText embedding vocab size: {embedding_vocab_size}\")\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "HIDDEN_DIM = 256\n",
    "N_LAYERS = 1\n",
    "DROPOUT = 0.0  # Baseline: no dropout\n",
    "N_EPOCHS = 100\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# Create data iterators\n",
    "train_iterator = data.BucketIterator(\n",
    "    train_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    shuffle=True,  # Shuffle for training\n",
    "    device=device\n",
    ")\n",
    "\n",
    "val_iterator = data.BucketIterator(\n",
    "    validation_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    shuffle=False,  # No shuffle for validation (deterministic)\n",
    "    device=device\n",
    ")\n",
    "\n",
    "test_iterator = data.BucketIterator(\n",
    "    test_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    shuffle=False,  # No shuffle for test (deterministic)\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Initialize simple RNN model (Baseline)\n",
    "# Use vocab size from loaded embedding to match the saved weights exactly\n",
    "model = SimpleRNNClassifier(\n",
    "    vocab_size=embedding_vocab_size,  # Must match saved embedding vocab size\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    output_dim=num_classes,\n",
    "    n_layers=N_LAYERS,\n",
    "    dropout=0.0,  # Baseline: no dropout\n",
    "    padding_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "    pretrained_embeddings=pretrained_embeddings\n",
    ").to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(f\"\\n>>> Training Baseline RNN Model\")\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  - Hidden Dim: {HIDDEN_DIM}\")\n",
    "print(f\"  - Layers: {N_LAYERS}\")\n",
    "print(f\"  - Dropout: {DROPOUT} (Baseline: no regularization)\")\n",
    "print(f\"  - Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"  - Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  - Epochs: {N_EPOCHS} (no early stopping)\")\n",
    "print(f\"  - Embedding Dim: {embedding_dim} (FastText)\")\n",
    "print(f\"  - Embeddings: LEARNABLE (updated during training)\")\n",
    "print(f\"  - OOV Handling: FastText subword embeddings + trainable <unk> token\")\n",
    "\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# ============================================================================\n",
    "# Training Loop (inline for easier debugging)\n",
    "# ============================================================================\n",
    "\n",
    "best_val_acc = 0\n",
    "\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "\n",
    "print(f\"\\nStarting training for {N_EPOCHS} epochs...\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Trainable parameters: {count_parameters(model):,}\")\n",
    "print(f\"Embedding layer learnable: {model.embedding.weight.requires_grad}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Training for one epoch (inline)\n",
    "    # ========================================================================\n",
    "    model.train()\n",
    "    train_epoch_loss = 0\n",
    "    train_all_preds = []\n",
    "    train_all_labels = []\n",
    "    \n",
    "    batch_idx = 0\n",
    "    for batch in train_iterator:\n",
    "        # Process batch (with debug only for first batch)\n",
    "        text, text_lengths, labels = process_batch(batch, debug=(batch_idx == 0))\n",
    "        batch_idx += 1\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        predictions = model(text, text_lengths)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(predictions, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        train_epoch_loss += loss.item()\n",
    "        \n",
    "        # Store predictions and labels for metrics\n",
    "        preds = torch.argmax(predictions, dim=1)\n",
    "        train_all_preds.extend(preds.cpu().numpy())\n",
    "        train_all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate training accuracy\n",
    "    train_acc = accuracy_score(train_all_labels, train_all_preds)\n",
    "    train_loss = train_epoch_loss / len(train_iterator)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Validation evaluation (inline)\n",
    "    # ========================================================================\n",
    "    model.eval()\n",
    "    val_epoch_loss = 0\n",
    "    val_all_preds = []\n",
    "    val_all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_iterator:\n",
    "            # Process batch consistently with training\n",
    "            text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "            \n",
    "            # Forward pass\n",
    "            predictions = model(text, text_lengths)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(predictions, labels)\n",
    "            val_epoch_loss += loss.item()\n",
    "            \n",
    "            # Store predictions and labels\n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            val_all_preds.extend(preds.cpu().numpy())\n",
    "            val_all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate validation accuracy\n",
    "    val_acc = accuracy_score(val_all_labels, val_all_preds)\n",
    "    val_loss = val_epoch_loss / len(val_iterator)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Logging (without early stopping)\n",
    "    # ========================================================================\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = divmod(end_time - start_time, 60)\n",
    "    \n",
    "    # Store metrics\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02}/{N_EPOCHS} | Time: {int(epoch_mins)}m {int(epoch_secs)}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\tVal Loss: {val_loss:.4f} | Val Acc: {val_acc*100:.2f}%')\n",
    "    \n",
    "    # Track best model (but don't stop early - baseline trains for all epochs)\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        # Save best model\n",
    "        torch.save(model.state_dict(), 'rnn_simple_best.pt')\n",
    "        print(f'\\t>>> New best model saved with Val Acc: {val_acc*100:.2f}%')\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(f\"Training completed! Best validation accuracy: {best_val_acc*100:.2f}%\")\n",
    "print(f\"Total epochs trained: {N_EPOCHS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ea22748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "VALIDATION SET EVALUATION (Best Model)\n",
      "================================================================================\n",
      "\n",
      ">>> Validation Set Results (Best Model):\n",
      "Validation Loss: 1.8058\n",
      "Validation Accuracy: 70.18%\n",
      "Validation F1 Score: 0.6953\n",
      "Validation AUC-ROC: 0.8912\n",
      "\n",
      "================================================================================\n",
      "TEST SET EVALUATION\n",
      "================================================================================\n",
      "\n",
      ">>> Test Set Results:\n",
      "Test Loss: 1.0124\n",
      "Test Accuracy: 83.20%\n",
      "Test F1 Score: 0.8304\n",
      "Test AUC-ROC: 0.9421\n",
      "\n",
      "================================================================================\n",
      "PART 2 INITIAL TRAINING COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Validation Set Evaluation (inline) - Evaluate best model on validation set\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VALIDATION SET EVALUATION (Best Model)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load best model and evaluate on validation set\n",
    "model.load_state_dict(torch.load('rnn_simple_best.pt'))\n",
    "\n",
    "model.eval()\n",
    "val_eval_loss = 0\n",
    "val_eval_preds = []\n",
    "val_eval_labels = []\n",
    "val_eval_probs = []  # Store probabilities for AUC-ROC\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_iterator:\n",
    "        text, text_lengths = batch.text\n",
    "        labels = batch.label\n",
    "        \n",
    "        # Process batch consistently\n",
    "        text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "        \n",
    "        # Forward pass\n",
    "        predictions = model(text, text_lengths)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(predictions, labels)\n",
    "        val_eval_loss += loss.item()\n",
    "        \n",
    "        # Store predictions, labels, and probabilities\n",
    "        probs = torch.softmax(predictions, dim=1)\n",
    "        preds = torch.argmax(predictions, dim=1)\n",
    "        val_eval_preds.extend(preds.cpu().numpy())\n",
    "        val_eval_labels.extend(labels.cpu().numpy())\n",
    "        val_eval_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "# Calculate validation metrics\n",
    "val_eval_acc = accuracy_score(val_eval_labels, val_eval_preds)\n",
    "val_eval_f1 = f1_score(val_eval_labels, val_eval_preds, average='weighted')\n",
    "val_eval_loss_final = val_eval_loss / len(val_iterator)\n",
    "\n",
    "# Calculate AUC-ROC (one-vs-rest for multiclass)\n",
    "try:\n",
    "    val_eval_probs_array = np.array(val_eval_probs)\n",
    "    val_eval_labels_bin = label_binarize(val_eval_labels, classes=range(num_classes))\n",
    "    val_eval_auc = roc_auc_score(val_eval_labels_bin, val_eval_probs_array, average='weighted', multi_class='ovr')\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not calculate AUC-ROC: {e}\")\n",
    "    val_eval_auc = 0.0\n",
    "\n",
    "print(f\"\\n>>> Validation Set Results (Best Model):\")\n",
    "print(f\"Validation Loss: {val_eval_loss_final:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_eval_acc*100:.2f}%\")\n",
    "print(f\"Validation F1 Score: {val_eval_f1:.4f}\")\n",
    "print(f\"Validation AUC-ROC: {val_eval_auc:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Test Set Evaluation (inline)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST SET EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "model.eval()\n",
    "test_epoch_loss = 0\n",
    "test_all_preds = []\n",
    "test_all_labels = []\n",
    "test_all_probs = []  # Store probabilities for AUC-ROC\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_iterator:\n",
    "        text, text_lengths = batch.text\n",
    "        labels = batch.label\n",
    "        \n",
    "        # Process batch consistently\n",
    "        text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "        \n",
    "        # Forward pass\n",
    "        predictions = model(text, text_lengths)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(predictions, labels)\n",
    "        test_epoch_loss += loss.item()\n",
    "        \n",
    "        # Store predictions, labels, and probabilities\n",
    "        probs = torch.softmax(predictions, dim=1)\n",
    "        preds = torch.argmax(predictions, dim=1)\n",
    "        test_all_preds.extend(preds.cpu().numpy())\n",
    "        test_all_labels.extend(labels.cpu().numpy())\n",
    "        test_all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "# Calculate test metrics\n",
    "test_acc = accuracy_score(test_all_labels, test_all_preds)\n",
    "test_f1 = f1_score(test_all_labels, test_all_preds, average='weighted')\n",
    "test_loss = test_epoch_loss / len(test_iterator)\n",
    "\n",
    "# Calculate AUC-ROC (one-vs-rest for multiclass)\n",
    "try:\n",
    "    test_all_probs_array = np.array(test_all_probs)\n",
    "    test_all_labels_bin = label_binarize(test_all_labels, classes=range(num_classes))\n",
    "    test_auc = roc_auc_score(test_all_labels_bin, test_all_probs_array, average='weighted', multi_class='ovr')\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not calculate AUC-ROC: {e}\")\n",
    "    test_auc = 0.0\n",
    "\n",
    "print(f\"\\n>>> Test Set Results:\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n",
    "print(f\"Test F1 Score: {test_f1:.4f}\")\n",
    "print(f\"Test AUC-ROC: {test_auc:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 2 INITIAL TRAINING COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5090ea4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PART 2.2: SEQUENTIAL HYPERPARAMETER TUNING\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "STEP 0: EPOCH + EARLY STOPPING TUNING\n",
      "================================================================================\n",
      "Testing different MAX_EPOCHS and PATIENCE configurations\n",
      "Total combinations to test: 3\n",
      "Combinations (Max_Epochs, Patience):\n",
      "  1. Max_Epochs=100, Patience=10\n",
      "  2. Max_Epochs=200, Patience=10\n",
      "  3. Max_Epochs=300, Patience=10\n",
      "\n",
      ">>> Testing: Step 0 Config 1/3\n",
      "    LR=0.001, Batch=64, Hidden=256, Opt=Adam\n",
      "    Epochs=100 (NO early stopping - training for full 100 epochs)\n",
      "    Epoch 10/100: Train Acc=100.00%, Val Acc=70.09%\n",
      "    Epoch 20/100: Train Acc=100.00%, Val Acc=70.46%\n",
      "    Epoch 30/100: Train Acc=100.00%, Val Acc=69.91%\n",
      "    Epoch 40/100: Train Acc=100.00%, Val Acc=69.91%\n",
      "    Epoch 50/100: Train Acc=100.00%, Val Acc=69.82%\n",
      "    Epoch 60/100: Train Acc=100.00%, Val Acc=69.27%\n",
      "    Epoch 70/100: Train Acc=100.00%, Val Acc=69.17%\n",
      "    Epoch 80/100: Train Acc=100.00%, Val Acc=69.17%\n",
      "    Epoch 90/100: Train Acc=100.00%, Val Acc=69.08%\n",
      "    Epoch 100/100: Train Acc=100.00%, Val Acc=68.81%\n",
      "    Final Val Acc: 68.81% | Best Val Acc: 70.64% (at epoch 18/100)\n",
      "\n",
      ">>> Testing: Step 0 Config 2/3\n",
      "    LR=0.001, Batch=64, Hidden=256, Opt=Adam\n",
      "    Epochs=200 (NO early stopping - training for full 200 epochs)\n",
      "    Epoch 10/200: Train Acc=100.00%, Val Acc=70.09%\n",
      "    Epoch 20/200: Train Acc=100.00%, Val Acc=70.46%\n",
      "    Epoch 30/200: Train Acc=100.00%, Val Acc=69.91%\n",
      "    Epoch 40/200: Train Acc=100.00%, Val Acc=69.91%\n",
      "    Epoch 50/200: Train Acc=100.00%, Val Acc=69.82%\n",
      "    Epoch 60/200: Train Acc=100.00%, Val Acc=69.27%\n",
      "    Epoch 70/200: Train Acc=100.00%, Val Acc=69.17%\n",
      "    Epoch 80/200: Train Acc=100.00%, Val Acc=69.17%\n",
      "    Epoch 90/200: Train Acc=100.00%, Val Acc=69.08%\n",
      "    Epoch 100/200: Train Acc=100.00%, Val Acc=68.81%\n",
      "    Epoch 110/200: Train Acc=100.00%, Val Acc=68.90%\n",
      "    Epoch 120/200: Train Acc=100.00%, Val Acc=68.81%\n",
      "    Epoch 130/200: Train Acc=100.00%, Val Acc=68.81%\n",
      "    Epoch 140/200: Train Acc=100.00%, Val Acc=68.26%\n",
      "    Epoch 150/200: Train Acc=100.00%, Val Acc=68.17%\n",
      "    Epoch 160/200: Train Acc=100.00%, Val Acc=68.17%\n",
      "    Epoch 170/200: Train Acc=100.00%, Val Acc=68.07%\n",
      "    Epoch 180/200: Train Acc=100.00%, Val Acc=68.07%\n",
      "    Epoch 190/200: Train Acc=100.00%, Val Acc=68.26%\n",
      "    Epoch 200/200: Train Acc=100.00%, Val Acc=68.53%\n",
      "    Final Val Acc: 68.53% | Best Val Acc: 70.64% (at epoch 18/200)\n",
      "\n",
      ">>> Testing: Step 0 Config 3/3\n",
      "    LR=0.001, Batch=64, Hidden=256, Opt=Adam\n",
      "    Epochs=300 (NO early stopping - training for full 300 epochs)\n",
      "    Epoch 10/300: Train Acc=100.00%, Val Acc=70.09%\n",
      "    Epoch 20/300: Train Acc=100.00%, Val Acc=70.46%\n",
      "    Epoch 30/300: Train Acc=100.00%, Val Acc=69.91%\n",
      "    Epoch 40/300: Train Acc=100.00%, Val Acc=69.91%\n",
      "    Epoch 50/300: Train Acc=100.00%, Val Acc=69.82%\n",
      "    Epoch 60/300: Train Acc=100.00%, Val Acc=69.27%\n",
      "    Epoch 70/300: Train Acc=100.00%, Val Acc=69.17%\n",
      "    Epoch 80/300: Train Acc=100.00%, Val Acc=69.17%\n",
      "    Epoch 90/300: Train Acc=100.00%, Val Acc=69.08%\n",
      "    Epoch 100/300: Train Acc=100.00%, Val Acc=68.81%\n",
      "    Epoch 110/300: Train Acc=100.00%, Val Acc=68.90%\n",
      "    Epoch 120/300: Train Acc=100.00%, Val Acc=68.81%\n",
      "    Epoch 130/300: Train Acc=100.00%, Val Acc=68.81%\n",
      "    Epoch 140/300: Train Acc=100.00%, Val Acc=68.26%\n",
      "    Epoch 150/300: Train Acc=100.00%, Val Acc=68.17%\n",
      "    Epoch 160/300: Train Acc=100.00%, Val Acc=68.17%\n",
      "    Epoch 170/300: Train Acc=100.00%, Val Acc=68.07%\n",
      "    Epoch 180/300: Train Acc=100.00%, Val Acc=68.07%\n",
      "    Epoch 190/300: Train Acc=100.00%, Val Acc=68.26%\n",
      "    Epoch 200/300: Train Acc=100.00%, Val Acc=68.53%\n",
      "    Epoch 210/300: Train Acc=100.00%, Val Acc=68.62%\n",
      "    Epoch 220/300: Train Acc=100.00%, Val Acc=68.35%\n",
      "    Epoch 230/300: Train Acc=100.00%, Val Acc=68.44%\n",
      "    Epoch 240/300: Train Acc=100.00%, Val Acc=68.44%\n",
      "    Epoch 250/300: Train Acc=100.00%, Val Acc=68.07%\n",
      "    Epoch 260/300: Train Acc=100.00%, Val Acc=68.35%\n",
      "    Epoch 270/300: Train Acc=100.00%, Val Acc=68.44%\n",
      "    Epoch 280/300: Train Acc=100.00%, Val Acc=68.35%\n",
      "    Epoch 290/300: Train Acc=100.00%, Val Acc=68.17%\n",
      "    Epoch 300/300: Train Acc=100.00%, Val Acc=68.35%\n",
      "    Final Val Acc: 68.35% | Best Val Acc: 70.64% (at epoch 18/300)\n",
      "\n",
      ">>> Step 0 Results:\n",
      "#    Epochs   Val Acc    Best At Epoch   Total Trained  \n",
      "------------------------------------------------------------\n",
      "1    100      70.64     % 18              100            \n",
      "2    200      70.64     % 18              200            \n",
      "3    300      70.64     % 18              300            \n",
      "\n",
      ">>> Best from Step 0: Epochs=100, Val Acc=70.64%\n",
      "    Best validation accuracy was achieved at epoch 18 out of 100\n",
      "\n",
      ">>> Using MAX_EPOCHS=100 and PATIENCE=10 for subsequent steps (with early stopping)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Part 2.2: Sequential Hyperparameter Tuning (One Variable at a Time)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 2.2: SEQUENTIAL HYPERPARAMETER TUNING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# Step 0: Epoch + Early Stopping Configuration Tuning\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 0: EPOCH + EARLY STOPPING TUNING\")\n",
    "print(\"=\"*80)\n",
    "print(\"Testing different MAX_EPOCHS and PATIENCE configurations\")\n",
    "\n",
    "# Test different epoch and patience configurations\n",
    "max_epochs_options = [100, 200, 300]\n",
    "PATIENCE = patience = 10\n",
    "\n",
    "# Use baseline config for testing epoch settings\n",
    "baseline_config = {\n",
    "    'lr': 0.001,\n",
    "    'batch_size': 64,\n",
    "    'hidden_dim': 256,\n",
    "    'optimizer': 'Adam'\n",
    "}\n",
    "\n",
    "step0_configs = []\n",
    "for max_epochs in max_epochs_options:\n",
    "        step0_configs.append({\n",
    "            'config': baseline_config.copy(),\n",
    "            'max_epochs': max_epochs,\n",
    "            'patience': patience\n",
    "        })\n",
    "\n",
    "print(f\"Total combinations to test: {len(step0_configs)}\")\n",
    "print(\"Combinations (Max_Epochs, Patience):\")\n",
    "for idx, ep_config in enumerate(step0_configs, 1):\n",
    "    print(f\"  {idx}. Max_Epochs={ep_config['max_epochs']}, Patience={ep_config['patience']}\")\n",
    "\n",
    "# Helper function to train with specific epoch/patience settings\n",
    "def train_and_evaluate_with_epochs(config, max_epochs, patience, config_name=\"config\"):\n",
    "    \"\"\"Train a model for specific number of epochs WITHOUT early stopping\"\"\"\n",
    "    print(f\"\\n>>> Testing: {config_name}\")\n",
    "    print(f\"    LR={config['lr']}, Batch={config['batch_size']}, Hidden={config['hidden_dim']}, Opt={config['optimizer']}\")\n",
    "    print(f\"    Epochs={max_epochs} (NO early stopping - training for full {max_epochs} epochs)\")\n",
    "    \n",
    "    # Create iterators with specific batch size\n",
    "    train_iter = data.BucketIterator(\n",
    "        train_data,\n",
    "        batch_size=config['batch_size'],\n",
    "        sort_key=lambda x: len(x.text),\n",
    "        sort_within_batch=True,\n",
    "        shuffle=True,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    val_iter = data.BucketIterator(\n",
    "        validation_data,\n",
    "        batch_size=config['batch_size'],\n",
    "        sort_key=lambda x: len(x.text),\n",
    "        sort_within_batch=True,\n",
    "        shuffle=False,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Reset random seeds INSIDE function to ensure fresh model for each config\n",
    "    # This is critical to ensure each max_epochs config starts from scratch\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "        torch.cuda.manual_seed_all(SEED)\n",
    "    # Ensure deterministic behavior\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    # Create model with specific hidden dimension\n",
    "    model = SimpleRNNClassifier(\n",
    "        vocab_size=embedding_vocab_size,\n",
    "        embedding_dim=embedding_dim,\n",
    "        hidden_dim=config['hidden_dim'],\n",
    "        output_dim=num_classes,\n",
    "        n_layers=N_LAYERS,\n",
    "        dropout=0.0,  # Baseline: no dropout\n",
    "        padding_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "        pretrained_embeddings=pretrained_embeddings\n",
    "    ).to(device)\n",
    "    \n",
    "    # Select optimizer\n",
    "    if config['optimizer'] == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config['lr'])\n",
    "    elif config['optimizer'] == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=config['lr'], momentum=0.9)\n",
    "    elif config['optimizer'] == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=config['lr'])\n",
    "    elif config['optimizer'] == 'Adagrad':\n",
    "        optimizer = optim.Adagrad(model.parameters(), lr=config['lr'])\n",
    "    \n",
    "    # Training loop WITHOUT early stopping - train for full num_epochs\n",
    "    best_val_acc = 0.0\n",
    "    best_val_acc_at_epoch = 0\n",
    "    final_val_acc = 0.0\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        \n",
    "        for batch in train_iter:\n",
    "            text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(text, text_lengths)\n",
    "            loss = criterion(predictions, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        train_acc = accuracy_score(train_labels, train_preds)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_iter:\n",
    "                text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "                predictions = model(text, text_lengths)\n",
    "                loss = criterion(predictions, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                preds = torch.argmax(predictions, dim=1)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_acc = accuracy_score(val_labels, val_preds)\n",
    "        \n",
    "        # Track best validation accuracy (but don't stop early)\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_val_acc_at_epoch = epoch + 1\n",
    "        \n",
    "        final_val_acc = val_acc  # Store final epoch's validation accuracy\n",
    "        \n",
    "        # Print progress every 10 epochs or at the end\n",
    "        if (epoch + 1) % 10 == 0 or (epoch + 1) == max_epochs:\n",
    "            print(f\"    Epoch {epoch+1}/{max_epochs}: Train Acc={train_acc*100:.2f}%, Val Acc={val_acc*100:.2f}%\")\n",
    "    \n",
    "    print(f\"    Final Val Acc: {final_val_acc*100:.2f}% | Best Val Acc: {best_val_acc*100:.2f}% (at epoch {best_val_acc_at_epoch}/{max_epochs})\")\n",
    "    return best_val_acc, best_val_acc_at_epoch, max_epochs\n",
    "\n",
    "step0_results = []\n",
    "for idx, ep_config in enumerate(step0_configs):\n",
    "    # Set fixed seed for reproducibility - ensures consistent batch ordering\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "    \n",
    "    val_acc, best_epoch, total_epochs = train_and_evaluate_with_epochs(\n",
    "        ep_config['config'],\n",
    "        ep_config['max_epochs'],\n",
    "        ep_config['patience'],\n",
    "        f\"Step 0 Config {idx+1}/{len(step0_configs)}\"\n",
    "    )\n",
    "    step0_results.append({\n",
    "        'num_epochs': ep_config['max_epochs'],\n",
    "        'val_acc': val_acc,\n",
    "        'best_epoch': best_epoch,\n",
    "        'total_epochs': total_epochs\n",
    "    })\n",
    "\n",
    "# Find best epoch configuration\n",
    "best_step0 = max(step0_results, key=lambda x: x['val_acc'])\n",
    "BEST_EPOCHS = best_step0['num_epochs']\n",
    "\n",
    "# Set appropriate MAX_EPOCHS and PATIENCE for subsequent steps\n",
    "# Use the best number of epochs with some buffer, and set a reasonable patience\n",
    "MAX_EPOCHS = BEST_EPOCHS\n",
    "\n",
    "print(f\"\\n>>> Step 0 Results:\")\n",
    "print(f\"{'#':<4} {'Epochs':<8} {'Val Acc':<10} {'Best At Epoch':<15} {'Total Trained':<15}\")\n",
    "print(\"-\" * 60)\n",
    "for idx, result in enumerate(step0_results, 1):\n",
    "    print(f\"{idx:<4} {result['num_epochs']:<8} {result['val_acc']*100:<10.2f}% {result['best_epoch']:<15} {result['total_epochs']:<15}\")\n",
    "print(f\"\\n>>> Best from Step 0: Epochs={BEST_EPOCHS}, Val Acc={best_step0['val_acc']*100:.2f}%\")\n",
    "print(f\"    Best validation accuracy was achieved at epoch {best_step0['best_epoch']} out of {best_step0['total_epochs']}\")\n",
    "print(f\"\\n>>> Using MAX_EPOCHS={MAX_EPOCHS} and PATIENCE={PATIENCE} for subsequent steps (with early stopping)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d874d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, iterator):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "            predictions = model(text, text_lengths)\n",
    "            loss = criterion(predictions, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    val_acc = accuracy_score(val_labels, val_preds)\n",
    "\n",
    "    return val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fc882ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to train and evaluate a model configuration\n",
    "def train_and_evaluate(config, config_name=\"config\"):\n",
    "    \"\"\"Train a model with given configuration and return validation accuracy\"\"\"\n",
    "    print(f\"\\n>>> Testing: {config_name}\")\n",
    "    print(f\"    LR={config['lr']}, Batch={config['batch_size']}, Hidden={config['hidden_dim']}, Opt={config['optimizer']}\")\n",
    "    \n",
    "    # Create iterators with specific batch size\n",
    "    train_iter = data.BucketIterator(\n",
    "        train_data,\n",
    "        batch_size=config['batch_size'],\n",
    "        sort_key=lambda x: len(x.text),\n",
    "        sort_within_batch=True,\n",
    "        shuffle=True,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    val_iter = data.BucketIterator(\n",
    "        validation_data,\n",
    "        batch_size=config['batch_size'],\n",
    "        sort_key=lambda x: len(x.text),\n",
    "        sort_within_batch=True,\n",
    "        shuffle=False,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    test_iter = data.BucketIterator(\n",
    "        test_data,\n",
    "        batch_size=config['batch_size'],\n",
    "        sort_key=lambda x: len(x.text),\n",
    "        sort_within_batch=True,\n",
    "        shuffle=False,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Create model with specific hidden dimension\n",
    "    model = SimpleRNNClassifier(\n",
    "        vocab_size=embedding_vocab_size,\n",
    "        embedding_dim=embedding_dim,\n",
    "        hidden_dim=config['hidden_dim'],\n",
    "        output_dim=num_classes,\n",
    "        n_layers=N_LAYERS,\n",
    "        dropout=0.0,  # Baseline: no dropout\n",
    "        padding_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "        pretrained_embeddings=pretrained_embeddings\n",
    "    ).to(device)\n",
    "    \n",
    "    # Select optimizer\n",
    "    if config['optimizer'] == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config['lr'])\n",
    "    elif config['optimizer'] == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=config['lr'])\n",
    "    elif config['optimizer'] == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=config['lr'])\n",
    "    elif config['optimizer'] == 'Adagrad':\n",
    "        optimizer = optim.Adagrad(model.parameters(), lr=config['lr'])\n",
    "    \n",
    "    # Training loop with early stopping\n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        \n",
    "        for batch in train_iter:\n",
    "            text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(text, text_lengths)\n",
    "            loss = criterion(predictions, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        train_acc = accuracy_score(train_labels, train_preds)\n",
    "        \n",
    "        # Validation       \n",
    "        val_acc = eval_model(model, val_iter)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(f\"    Early stopping at epoch {epoch+1}, best val acc: {best_val_acc*100:.2f}%\")\n",
    "                break\n",
    "    \n",
    "    test_acc = eval_model(model, test_iter)\n",
    "\n",
    "    print(f\"    Best Val Acc: {best_val_acc*100:.2f}% (stopped at epoch {epoch+1})\")\n",
    "    print(f\"    Test Acc: {test_acc*100:.2f}% \")\n",
    "    return best_val_acc, test_acc, epoch + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32f94eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 1: LEARNING RATE + BATCH SIZE TUNING\n",
      "================================================================================\n",
      "Testing ALL combinations of LR and Batch Size (they interact)\n",
      "Total combinations to test: 9\n",
      "Combinations:\n",
      "  1. LR=0.01, Batch=32\n",
      "  2. LR=0.01, Batch=64\n",
      "  3. LR=0.01, Batch=128\n",
      "  4. LR=0.001, Batch=32\n",
      "  5. LR=0.001, Batch=64\n",
      "  6. LR=0.001, Batch=128\n",
      "  7. LR=0.0001, Batch=32\n",
      "  8. LR=0.0001, Batch=64\n",
      "  9. LR=0.0001, Batch=128\n",
      "\n",
      ">>> Testing: Step 1 Config 1/9\n",
      "    LR=0.01, Batch=32, Hidden=256, Opt=Adam\n",
      "    Early stopping at epoch 48, best val acc: 33.49%\n",
      "    Best Val Acc: 33.49% (stopped at epoch 48)\n",
      "    Test Acc: 28.80% \n",
      "\n",
      ">>> Testing: Step 1 Config 2/9\n",
      "    LR=0.01, Batch=64, Hidden=256, Opt=Adam\n",
      "    Early stopping at epoch 34, best val acc: 35.50%\n",
      "    Best Val Acc: 35.50% (stopped at epoch 34)\n",
      "    Test Acc: 38.20% \n",
      "\n",
      ">>> Testing: Step 1 Config 3/9\n",
      "    LR=0.01, Batch=128, Hidden=256, Opt=Adam\n",
      "    Early stopping at epoch 45, best val acc: 52.94%\n",
      "    Best Val Acc: 52.94% (stopped at epoch 45)\n",
      "    Test Acc: 45.20% \n",
      "\n",
      ">>> Testing: Step 1 Config 4/9\n",
      "    LR=0.001, Batch=32, Hidden=256, Opt=Adam\n",
      "    Early stopping at epoch 28, best val acc: 69.54%\n",
      "    Best Val Acc: 69.54% (stopped at epoch 28)\n",
      "    Test Acc: 77.60% \n",
      "\n",
      ">>> Testing: Step 1 Config 5/9\n",
      "    LR=0.001, Batch=64, Hidden=256, Opt=Adam\n",
      "    Early stopping at epoch 28, best val acc: 70.64%\n",
      "    Best Val Acc: 70.64% (stopped at epoch 28)\n",
      "    Test Acc: 77.00% \n",
      "\n",
      ">>> Testing: Step 1 Config 6/9\n",
      "    LR=0.001, Batch=128, Hidden=256, Opt=Adam\n",
      "    Early stopping at epoch 15, best val acc: 72.29%\n",
      "    Best Val Acc: 72.29% (stopped at epoch 15)\n",
      "    Test Acc: 77.00% \n",
      "\n",
      ">>> Testing: Step 1 Config 7/9\n",
      "    LR=0.0001, Batch=32, Hidden=256, Opt=Adam\n",
      "    Early stopping at epoch 36, best val acc: 85.05%\n",
      "    Best Val Acc: 85.05% (stopped at epoch 36)\n",
      "    Test Acc: 86.00% \n",
      "\n",
      ">>> Testing: Step 1 Config 8/9\n",
      "    LR=0.0001, Batch=64, Hidden=256, Opt=Adam\n",
      "    Early stopping at epoch 39, best val acc: 84.68%\n",
      "    Best Val Acc: 84.68% (stopped at epoch 39)\n",
      "    Test Acc: 85.60% \n",
      "\n",
      ">>> Testing: Step 1 Config 9/9\n",
      "    LR=0.0001, Batch=128, Hidden=256, Opt=Adam\n",
      "    Early stopping at epoch 56, best val acc: 83.94%\n",
      "    Best Val Acc: 83.94% (stopped at epoch 56)\n",
      "    Test Acc: 85.80% \n",
      "\n",
      ">>> Step 1 Results:\n",
      "#    LR       Batch   Val Acc    Epochs \n",
      "----------------------------------------\n",
      "1    0.01     32      33.49     % 48     \n",
      "2    0.01     64      35.50     % 34     \n",
      "3    0.01     128     52.94     % 45     \n",
      "4    0.001    32      69.54     % 28     \n",
      "5    0.001    64      70.64     % 28     \n",
      "6    0.001    128     72.29     % 15     \n",
      "7    0.0001   32      85.05     % 36     \n",
      "8    0.0001   64      84.68     % 39     \n",
      "9    0.0001   128     83.94     % 56     \n",
      "\n",
      ">>> Best from Step 1: LR=0.0001, Batch=32, Val Acc=85.05%\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Step 1: Group 1 - Learning Rate + Batch Size (Test Together)\n",
    "# ============================================================================\n",
    "    \n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 1: LEARNING RATE + BATCH SIZE TUNING\")\n",
    "print(\"=\"*80)\n",
    "print(\"Testing ALL combinations of LR and Batch Size (they interact)\")\n",
    "\n",
    "# Test all combinations of learning rates and batch sizes\n",
    "learning_rates = [0.01, 0.001, 0.0001]\n",
    "batch_sizes = [32, 64, 128]\n",
    "\n",
    "step1_configs = []\n",
    "for lr in learning_rates:\n",
    "    for batch_size in batch_sizes:\n",
    "        step1_configs.append({\n",
    "            'lr': lr,\n",
    "            'batch_size': batch_size,\n",
    "            'hidden_dim': 256,\n",
    "            'optimizer': 'Adam'\n",
    "        })\n",
    "\n",
    "print(f\"Total combinations to test: {len(step1_configs)}\")\n",
    "print(\"Combinations:\")\n",
    "for idx, config in enumerate(step1_configs, 1):\n",
    "    print(f\"  {idx}. LR={config['lr']}, Batch={config['batch_size']}\")\n",
    "\n",
    "step1_results = []\n",
    "for idx, config in enumerate(step1_configs):\n",
    "    # Set fixed seed for reproducibility\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "    \n",
    "    val_acc, _, epoch_stopped = train_and_evaluate(config, f\"Step 1 Config {idx+1}/{len(step1_configs)}\")\n",
    "    step1_results.append({\n",
    "        'config': config,\n",
    "        'val_acc': val_acc,\n",
    "        'epoch_stopped': epoch_stopped\n",
    "    })\n",
    "\n",
    "# Find best LR + Batch Size\n",
    "best_step1 = max(step1_results, key=lambda x: x['val_acc'])\n",
    "best_lr = best_step1['config']['lr']\n",
    "best_batch_size = best_step1['config']['batch_size']\n",
    "\n",
    "print(f\"\\n>>> Step 1 Results:\")\n",
    "print(f\"{'#':<4} {'LR':<8} {'Batch':<7} {'Val Acc':<10} {'Epochs':<7}\")\n",
    "print(\"-\" * 40)\n",
    "for idx, result in enumerate(step1_results):\n",
    "    c = result['config']\n",
    "    print(f\"{idx+1:<4} {c['lr']:<8} {c['batch_size']:<7} {result['val_acc']*100:<10.2f}% {result['epoch_stopped']:<7}\")\n",
    "print(f\"\\n>>> Best from Step 1: LR={best_lr}, Batch={best_batch_size}, Val Acc={best_step1['val_acc']*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "305f8f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 2: OPTIMIZER\n",
      "================================================================================\n",
      "Using best LR=0.0001 and Batch=32 from Step 1\n",
      "\n",
      ">>> Testing: Step 2 Config 1/4\n",
      "    LR=0.0001, Batch=32, Hidden=256, Opt=Adam\n",
      "    Early stopping at epoch 36, best val acc: 85.05%\n",
      "    Best Val Acc: 85.05% (stopped at epoch 36)\n",
      "    Test Acc: 86.00% \n",
      "\n",
      ">>> Testing: Step 2 Config 2/4\n",
      "    LR=0.0001, Batch=32, Hidden=256, Opt=SGD\n",
      "    Early stopping at epoch 32, best val acc: 24.04%\n",
      "    Best Val Acc: 24.04% (stopped at epoch 32)\n",
      "    Test Acc: 18.20% \n",
      "\n",
      ">>> Testing: Step 2 Config 3/4\n",
      "    LR=0.0001, Batch=32, Hidden=256, Opt=RMSprop\n",
      "    Early stopping at epoch 30, best val acc: 84.13%\n",
      "    Best Val Acc: 84.13% (stopped at epoch 30)\n",
      "    Test Acc: 87.80% \n",
      "\n",
      ">>> Testing: Step 2 Config 4/4\n",
      "    LR=0.0001, Batch=32, Hidden=256, Opt=Adagrad\n",
      "    Early stopping at epoch 74, best val acc: 44.13%\n",
      "    Best Val Acc: 44.13% (stopped at epoch 74)\n",
      "    Test Acc: 39.80% \n",
      "\n",
      ">>> Step 2 Results:\n",
      "#    LR       Optimizer  Val Acc    Epochs \n",
      "---------------------------------------------\n",
      "1    0.0001   Adam       85.05     % 36     \n",
      "2    0.0001   SGD        24.04     % 32     \n",
      "3    0.0001   RMSprop    84.13     % 30     \n",
      "4    0.0001   Adagrad    44.13     % 74     \n",
      "\n",
      ">>> Best from Step 2: LR=0.0001, Optimizer=Adam, Val Acc=85.05%\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Step 2: Group 2 - Optimizer\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 2: OPTIMIZER\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Using best LR={best_lr} and Batch={best_batch_size} from Step 1\")\n",
    "\n",
    "step2_configs = [\n",
    "    {'lr': best_lr, 'batch_size': best_batch_size, 'hidden_dim': 256, 'optimizer': 'Adam'},\n",
    "    {'lr': best_lr, 'batch_size': best_batch_size, 'hidden_dim': 256, 'optimizer': 'SGD'},\n",
    "    {'lr': best_lr, 'batch_size': best_batch_size, 'hidden_dim': 256, 'optimizer': 'RMSprop'},\n",
    "    {'lr': best_lr, 'batch_size': best_batch_size, 'hidden_dim': 256, 'optimizer': 'Adagrad'},\n",
    "]\n",
    "\n",
    "step2_results = []\n",
    "for idx, config in enumerate(step2_configs):\n",
    "    # Set fixed seed for reproducibility\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "    \n",
    "    val_acc, _, epoch_stopped = train_and_evaluate(config, f\"Step 2 Config {idx+1}/{len(step2_configs)}\")\n",
    "    step2_results.append({\n",
    "        'config': config,\n",
    "        'val_acc': val_acc,\n",
    "        'epoch_stopped': epoch_stopped\n",
    "    })\n",
    "\n",
    "# Find best Optimizer (and potentially adjusted LR)\n",
    "best_step2 = max(step2_results, key=lambda x: x['val_acc'])\n",
    "best_optimizer = best_step2['config']['optimizer']\n",
    "final_lr = best_step2['config']['lr']  # May be different if SGD needed higher LR\n",
    "\n",
    "print(f\"\\n>>> Step 2 Results:\")\n",
    "print(f\"{'#':<4} {'LR':<8} {'Optimizer':<10} {'Val Acc':<10} {'Epochs':<7}\")\n",
    "print(\"-\" * 45)\n",
    "for idx, result in enumerate(step2_results):\n",
    "    c = result['config']\n",
    "    print(f\"{idx+1:<4} {c['lr']:<8} {c['optimizer']:<10} {result['val_acc']*100:<10.2f}% {result['epoch_stopped']:<7}\")\n",
    "print(f\"\\n>>> Best from Step 2: LR={final_lr}, Optimizer={best_optimizer}, Val Acc={best_step2['val_acc']*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c7fb36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 3: HIDDEN DIMENSION TUNING\n",
      "================================================================================\n",
      "Using best LR=0.0001, Batch=32, Optimizer=Adam from Steps 1-2\n",
      "\n",
      ">>> Testing: Step 3 Config 1/3\n",
      "    LR=0.0001, Batch=32, Hidden=128, Opt=Adam\n",
      "    Early stopping at epoch 20, best val acc: 84.31%\n",
      "    Best Val Acc: 84.31% (stopped at epoch 20)\n",
      "    Test Acc: 84.40% \n",
      "\n",
      ">>> Testing: Step 3 Config 2/3\n",
      "    LR=0.0001, Batch=32, Hidden=256, Opt=Adam\n",
      "    Early stopping at epoch 36, best val acc: 85.05%\n",
      "    Best Val Acc: 85.05% (stopped at epoch 36)\n",
      "    Test Acc: 86.00% \n",
      "\n",
      ">>> Testing: Step 3 Config 3/3\n",
      "    LR=0.0001, Batch=32, Hidden=512, Opt=Adam\n",
      "    Early stopping at epoch 23, best val acc: 86.06%\n",
      "    Best Val Acc: 86.06% (stopped at epoch 23)\n",
      "    Test Acc: 82.60% \n",
      "\n",
      ">>> Step 3 Results:\n",
      "#    Hidden Dim   Val Acc    Epochs \n",
      "-----------------------------------\n",
      "1    128          84.31     % 20     \n",
      "2    256          85.05     % 36     \n",
      "3    512          86.06     % 23     \n",
      "\n",
      ">>> Best from Step 3: Hidden Dim=512, Val Acc=86.06%, Test Acc=82.60%\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Step 3: Hidden Dimension (Test Independently)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 3: HIDDEN DIMENSION TUNING\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Using best LR={final_lr}, Batch={best_batch_size}, Optimizer={best_optimizer} from Steps 1-2\")\n",
    "\n",
    "step3_configs = [\n",
    "    {'lr': final_lr, 'batch_size': best_batch_size, 'hidden_dim': 128, 'optimizer': best_optimizer},\n",
    "    {'lr': final_lr, 'batch_size': best_batch_size, 'hidden_dim': 256, 'optimizer': best_optimizer},\n",
    "    {'lr': final_lr, 'batch_size': best_batch_size, 'hidden_dim': 512, 'optimizer': best_optimizer},\n",
    "]\n",
    "\n",
    "step3_results = []\n",
    "for idx, config in enumerate(step3_configs):\n",
    "    # Set fixed seed for reproducibility\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "    \n",
    "    val_acc, test_acc, epoch_stopped = train_and_evaluate(config, f\"Step 3 Config {idx+1}/{len(step3_configs)}\")\n",
    "    step3_results.append({\n",
    "        'config': config,\n",
    "        'val_acc': val_acc,\n",
    "        'test_acc': test_acc,\n",
    "        'epoch_stopped': epoch_stopped\n",
    "    })\n",
    "\n",
    "# Find best Hidden Dimension\n",
    "best_step3 = max(step3_results, key=lambda x: x['val_acc'])\n",
    "best_hidden_dim = best_step3['config']['hidden_dim']\n",
    "\n",
    "print(f\"\\n>>> Step 3 Results:\")\n",
    "print(f\"{'#':<4} {'Hidden Dim':<12} {'Val Acc':<10} {'Epochs':<7}\")\n",
    "print(\"-\" * 35)\n",
    "for idx, result in enumerate(step3_results):\n",
    "    c = result['config']\n",
    "    print(f\"{idx+1:<4} {c['hidden_dim']:<12} {result['val_acc']*100:<10.2f}% {result['epoch_stopped']:<7}\")\n",
    "print(f\"\\n>>> Best from Step 3: Hidden Dim={best_hidden_dim}, Val Acc={best_step3['val_acc']*100:.2f}%, Test Acc={best_step3['test_acc']*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7a23bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "HYPERPARAMETER TUNING COMPLETE - FINAL BEST CONFIGURATION\n",
      "================================================================================\n",
      "\n",
      ">>> Best Configuration Found:\n",
      "    Learning Rate: 0.0001\n",
      "    Batch Size: 32\n",
      "    Hidden Dimension: 512\n",
      "    Optimizer: Adam\n",
      "    Max Epochs: 100 (with early stopping, patience=10)\n",
      "    Best Validation Accuracy: 86.06%\n",
      "    Test Accuracy: 82.60%\n",
      "\n",
      "================================================================================\n",
      "SEQUENTIAL HYPERPARAMETER TUNING COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Final Best Configuration Summary\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"HYPERPARAMETER TUNING COMPLETE - FINAL BEST CONFIGURATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "final_best_config = {\n",
    "    'lr': final_lr,\n",
    "    'batch_size': best_batch_size,\n",
    "    'hidden_dim': best_hidden_dim,\n",
    "    'optimizer': best_optimizer,\n",
    "    'max_epochs': MAX_EPOCHS,\n",
    "    'patience': PATIENCE\n",
    "}\n",
    "\n",
    "print(f\"\\n>>> Best Configuration Found:\")\n",
    "print(f\"    Learning Rate: {final_best_config['lr']}\")\n",
    "print(f\"    Batch Size: {final_best_config['batch_size']}\")\n",
    "print(f\"    Hidden Dimension: {final_best_config['hidden_dim']}\")\n",
    "print(f\"    Optimizer: {final_best_config['optimizer']}\")\n",
    "print(f\"    Max Epochs: {final_best_config['max_epochs']} (with early stopping, patience={final_best_config['patience']})\")\n",
    "print(f\"    Best Validation Accuracy: {best_step3['val_acc']*100:.2f}%\")\n",
    "print(f\"    Test Accuracy: {best_step3['test_acc']*100:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SEQUENTIAL HYPERPARAMETER TUNING COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3086a8ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr': 0.0001,\n",
       " 'batch_size': 32,\n",
       " 'hidden_dim': 512,\n",
       " 'optimizer': 'Adam',\n",
       " 'max_epochs': 100,\n",
       " 'patience': 10}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_best_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "817c430f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "WORD AGGREGATION METHOD COMPARISON\n",
      "================================================================================\n",
      "Using best hyperparameters from tuning:\n",
      "    LR=0.0001, Batch=32, Hidden=512, Optimizer=Adam\n",
      "    Max Epochs=100, Patience=10\n",
      "\n",
      "Testing 3 aggregation methods:\n",
      "  - last\n",
      "  - mean\n",
      "  - max\n",
      "\n",
      "================================================================================\n",
      "Testing Aggregation Method: LAST\n",
      "================================================================================\n",
      "\n",
      ">>> Training model with last aggregation...\n",
      "    Epoch 10: Train Acc=93.51%, Val Acc=82.75%\n",
      "    Epoch 20: Train Acc=97.96%, Val Acc=83.67%\n",
      "    Early stopping at epoch 23, best val acc: 86.06%\n",
      "\n",
      ">>> Results for last aggregation:\n",
      "    Validation Acc: 86.06%\n",
      "    Test Acc: 87.60%\n",
      "    Test F1: 0.8746\n",
      "    Test AUC-ROC: 0.9763\n",
      "\n",
      "================================================================================\n",
      "Testing Aggregation Method: MEAN\n",
      "================================================================================\n",
      "\n",
      ">>> Training model with mean aggregation...\n",
      "    Epoch 10: Train Acc=93.76%, Val Acc=84.77%\n",
      "    Epoch 20: Train Acc=97.39%, Val Acc=85.69%\n",
      "    Early stopping at epoch 26, best val acc: 86.88%\n",
      "\n",
      ">>> Results for mean aggregation:\n",
      "    Validation Acc: 86.88%\n",
      "    Test Acc: 86.20%\n",
      "    Test F1: 0.8598\n",
      "    Test AUC-ROC: 0.9641\n",
      "\n",
      "================================================================================\n",
      "Testing Aggregation Method: MAX\n",
      "================================================================================\n",
      "\n",
      ">>> Training model with max aggregation...\n",
      "    Epoch 10: Train Acc=22.93%, Val Acc=22.94%\n",
      "    Early stopping at epoch 11, best val acc: 22.94%\n",
      "    Warning: Could not calculate AUC-ROC: Input contains NaN.\n",
      "\n",
      ">>> Results for max aggregation:\n",
      "    Validation Acc: 22.94%\n",
      "    Test Acc: 18.80%\n",
      "    Test F1: 0.0595\n",
      "    Test AUC-ROC: 0.0000\n",
      "\n",
      "================================================================================\n",
      "AGGREGATION METHOD COMPARISON - RESULTS SUMMARY\n",
      "================================================================================\n",
      "\n",
      ">>> Results Summary:\n",
      "Method       Val Acc    Test Acc   Test F1    Test AUC  \n",
      "-------------------------------------------------------\n",
      "last         86.06     % 87.60     % 0.8746     0.9763    \n",
      "mean         86.88     % 86.20     % 0.8598     0.9641    \n",
      "max          22.94     % 18.80     % 0.0595     0.0000    \n",
      "\n",
      ">>> Best Aggregation Method: MEAN\n",
      "    Validation Accuracy: 86.88%\n",
      "    Test Accuracy: 86.20%\n",
      "    Test F1 Score: 0.8598\n",
      "    Test AUC-ROC: 0.9641\n",
      "\n",
      "================================================================================\n",
      "AGGREGATION METHOD COMPARISON COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Word Aggregation Method Comparison\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"WORD AGGREGATION METHOD COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Using best hyperparameters from tuning:\")\n",
    "print(f\"    LR={final_lr}, Batch={best_batch_size}, Hidden={best_hidden_dim}, Optimizer={best_optimizer}\")\n",
    "print(f\"    Max Epochs={MAX_EPOCHS}, Patience={PATIENCE}\")\n",
    "\n",
    "# Extended RNN Classifier with multiple aggregation methods\n",
    "class RNN_Classifier_Aggregation(nn.Module):\n",
    "    \"\"\"\n",
    "    RNN for topic classification with multiple aggregation strategies.\n",
    "    Uses pretrained embeddings (learnable/updated during training).\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, \n",
    "                 n_layers=1, dropout=0.0, padding_idx=0, pretrained_embeddings=None,\n",
    "                 aggregation='last'):\n",
    "        super(RNN_Classifier_Aggregation, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.aggregation = aggregation  # 'last', 'mean', 'max'\n",
    "        \n",
    "        # Embedding layer initialized with pretrained embeddings\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "            # Make embeddings learnable (updated during training)\n",
    "            self.embedding.weight.requires_grad = True\n",
    "        \n",
    "        # RNN layer\n",
    "        self.rnn = nn.RNN(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=n_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.0  # No dropout in baseline\n",
    "        )\n",
    "                \n",
    "        # Fully connected output layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        # text: [batch_size, seq_len]\n",
    "        # text_lengths: [batch_size]\n",
    "        \n",
    "        # Embed the input\n",
    "        embedded = self.embedding(text)  # [batch_size, seq_len, embedding_dim]\n",
    "        \n",
    "        # Get dimensions\n",
    "        batch_size = embedded.size(0)\n",
    "        seq_len = embedded.size(1)\n",
    "        \n",
    "        # Handle text_lengths\n",
    "        text_lengths_flat = text_lengths.flatten().cpu().long()\n",
    "        if len(text_lengths_flat) != batch_size:\n",
    "            raise ValueError(\n",
    "                f\"text_lengths size mismatch: got {len(text_lengths_flat)} elements, \"\n",
    "                f\"expected {batch_size}\"\n",
    "            )\n",
    "        \n",
    "        # Clamp lengths\n",
    "        text_lengths_clamped = torch.clamp(text_lengths_flat, min=1, max=seq_len)\n",
    "        \n",
    "        text_lengths_clamped_device = text_lengths_clamped.to(text.device)\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, text_lengths_clamped, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        \n",
    "        # Pass through RNN\n",
    "        packed_output, hidden = self.rnn(packed_embedded)\n",
    "        # hidden: [n_layers, batch_size, hidden_dim]\n",
    "        \n",
    "        # Aggregate word representations to sentence representation\n",
    "        if self.aggregation == 'last':\n",
    "            sentence_repr = hidden[-1]  # [batch_size, hidden_dim]\n",
    "            \n",
    "        elif self.aggregation == 'mean':\n",
    "            output, output_lengths = nn.utils.rnn.pad_packed_sequence(\n",
    "                packed_output, batch_first=True\n",
    "            )\n",
    "            # output: [batch_size, seq_len, hidden_dim]\n",
    "            \n",
    "            # Create mask for padding\n",
    "            mask = torch.arange(seq_len, device=text.device).unsqueeze(0) < text_lengths_clamped_device.unsqueeze(1)\n",
    "            mask = mask.unsqueeze(2).float()  # [batch_size, seq_len, 1]\n",
    "            \n",
    "            # Apply mask and compute mean\n",
    "            masked_output = output * mask\n",
    "            sum_output = masked_output.sum(dim=1)  # [batch_size, hidden_dim]\n",
    "            sentence_repr = sum_output / text_lengths_clamped_device.unsqueeze(1).float()\n",
    "            \n",
    "        elif self.aggregation == 'max':\n",
    "            # Max pooling over all outputs\n",
    "            output, output_lengths = nn.utils.rnn.pad_packed_sequence(\n",
    "                packed_output, batch_first=True\n",
    "            )\n",
    "            # output: [batch_size, seq_len, hidden_dim]\n",
    "            \n",
    "            mask = torch.arange(seq_len, device=text.device).unsqueeze(0) < text_lengths_clamped_device.unsqueeze(1)\n",
    "            mask = mask.unsqueeze(2).float()  # [batch_size, seq_len, 1]\n",
    "            \n",
    "            masked_output = output * mask + (1 - mask) * float('-inf')\n",
    "            sentence_repr, _ = torch.max(masked_output, dim=1)\n",
    "        \n",
    "        # Pass through fully connected layer\n",
    "        output = self.fc(sentence_repr)  # [batch_size, output_dim]\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Test different aggregation methods\n",
    "aggregation_methods = ['last', 'mean', 'max']\n",
    "\n",
    "print(f\"\\nTesting {len(aggregation_methods)} aggregation methods:\")\n",
    "for method in aggregation_methods:\n",
    "    print(f\"  - {method}\")\n",
    "\n",
    "aggregation_results = []\n",
    "\n",
    "for agg_method in aggregation_methods:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Testing Aggregation Method: {agg_method.upper()}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Set fixed seed for reproducibility - ensures consistent batch ordering\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "    \n",
    "    # Create iterators with best batch size\n",
    "    train_iter = data.BucketIterator(\n",
    "        train_data,\n",
    "        batch_size=best_batch_size,\n",
    "        sort_key=lambda x: len(x.text),\n",
    "        sort_within_batch=True,\n",
    "        shuffle=True,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    val_iter = data.BucketIterator(\n",
    "        validation_data,\n",
    "        batch_size=best_batch_size,\n",
    "        sort_key=lambda x: len(x.text),\n",
    "        sort_within_batch=True,\n",
    "        shuffle=False,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    test_iter = data.BucketIterator(\n",
    "        test_data,\n",
    "        batch_size=best_batch_size,\n",
    "        sort_key=lambda x: len(x.text),\n",
    "        sort_within_batch=True,\n",
    "        shuffle=False,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Create model with best hyperparameters and specific aggregation method\n",
    "    model = RNN_Classifier_Aggregation(\n",
    "        vocab_size=embedding_vocab_size,\n",
    "        embedding_dim=embedding_dim,\n",
    "        hidden_dim=best_hidden_dim,\n",
    "        output_dim=num_classes,\n",
    "        n_layers=N_LAYERS,\n",
    "        dropout=0.0,  # Baseline: no dropout\n",
    "        padding_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "        pretrained_embeddings=pretrained_embeddings,\n",
    "        aggregation=agg_method\n",
    "    ).to(device)\n",
    "    \n",
    "    # Select optimizer with best learning rate\n",
    "    if best_optimizer == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=final_lr)\n",
    "    elif best_optimizer == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=final_lr, momentum=0.9)\n",
    "    elif best_optimizer == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=final_lr)\n",
    "    elif best_optimizer == 'Adagrad':\n",
    "        optimizer = optim.Adagrad(model.parameters(), lr=final_lr)\n",
    "    \n",
    "    # Training loop with early stopping\n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    print(f\"\\n>>> Training model with {agg_method} aggregation...\")\n",
    "    \n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        \n",
    "        for batch in train_iter:\n",
    "            text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(text, text_lengths)\n",
    "            loss = criterion(predictions, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        train_acc = accuracy_score(train_labels, train_preds)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_iter:\n",
    "                text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "                predictions = model(text, text_lengths)\n",
    "                loss = criterion(predictions, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                preds = torch.argmax(predictions, dim=1)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_acc = accuracy_score(val_labels, val_preds)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            # Save best model for this aggregation method\n",
    "            torch.save(model.state_dict(), f'rnn_agg_{agg_method}_best.pt')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(f\"    Early stopping at epoch {epoch+1}, best val acc: {best_val_acc*100:.2f}%\")\n",
    "                break\n",
    "        \n",
    "        # Print progress every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"    Epoch {epoch+1}: Train Acc={train_acc*100:.2f}%, Val Acc={val_acc*100:.2f}%\")\n",
    "    \n",
    "    # Load best model and evaluate on test set\n",
    "    model.load_state_dict(torch.load(f'rnn_agg_{agg_method}_best.pt'))\n",
    "    model.eval()\n",
    "    \n",
    "    test_loss = 0\n",
    "    test_preds = []\n",
    "    test_labels = []\n",
    "    test_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_iter:\n",
    "            text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "            predictions = model(text, text_lengths)\n",
    "            loss = criterion(predictions, labels)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            probs = torch.softmax(predictions, dim=1)\n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            test_preds.extend(preds.cpu().numpy())\n",
    "            test_labels.extend(labels.cpu().numpy())\n",
    "            test_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    # Calculate test metrics\n",
    "    test_acc = accuracy_score(test_labels, test_preds)\n",
    "    test_f1 = f1_score(test_labels, test_preds, average='weighted')\n",
    "    test_loss_avg = test_loss / len(test_iter)\n",
    "    \n",
    "    # Calculate AUC-ROC\n",
    "    try:\n",
    "        test_probs_array = np.array(test_probs)\n",
    "        test_labels_bin = label_binarize(test_labels, classes=range(num_classes))\n",
    "        test_auc = roc_auc_score(test_labels_bin, test_probs_array, average='weighted', multi_class='ovr')\n",
    "    except Exception as e:\n",
    "        print(f\"    Warning: Could not calculate AUC-ROC: {e}\")\n",
    "        test_auc = 0.0\n",
    "    \n",
    "    aggregation_results.append({\n",
    "        'method': agg_method,\n",
    "        'val_acc': best_val_acc,\n",
    "        'test_acc': test_acc,\n",
    "        'test_f1': test_f1,\n",
    "        'test_auc': test_auc,\n",
    "        'test_loss': test_loss_avg\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n>>> Results for {agg_method} aggregation:\")\n",
    "    print(f\"    Validation Acc: {best_val_acc*100:.2f}%\")\n",
    "    print(f\"    Test Acc: {test_acc*100:.2f}%\")\n",
    "    print(f\"    Test F1: {test_f1:.4f}\")\n",
    "    print(f\"    Test AUC-ROC: {test_auc:.4f}\")\n",
    "\n",
    "# Print summary table\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"AGGREGATION METHOD COMPARISON - RESULTS SUMMARY\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "print(f\"\\n>>> Results Summary:\")\n",
    "print(f\"{'Method':<12} {'Val Acc':<10} {'Test Acc':<10} {'Test F1':<10} {'Test AUC':<10}\")\n",
    "print(\"-\" * 55)\n",
    "for result in aggregation_results:\n",
    "    print(f\"{result['method']:<12} {result['val_acc']*100:<10.2f}% {result['test_acc']*100:<10.2f}% \"\n",
    "          f\"{result['test_f1']:<10.4f} {result['test_auc']:<10.4f}\")\n",
    "\n",
    "# Find best aggregation method\n",
    "best_aggregation = max(aggregation_results, key=lambda x: x['val_acc'])\n",
    "\n",
    "print(f\"\\n>>> Best Aggregation Method: {best_aggregation['method'].upper()}\")\n",
    "print(f\"    Validation Accuracy: {best_aggregation['val_acc']*100:.2f}%\")\n",
    "print(f\"    Test Accuracy: {best_aggregation['test_acc']*100:.2f}%\")\n",
    "print(f\"    Test F1 Score: {best_aggregation['test_f1']:.4f}\")\n",
    "print(f\"    Test AUC-ROC: {best_aggregation['test_auc']:.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"AGGREGATION METHOD COMPARISON COMPLETE\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "684dfeeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "REGULARIZATION STEP 0: BASELINE (NO REGULARIZATION)\n",
      "================================================================================\n",
      "Using best hyperparameters from tuning:\n",
      "    LR=0.0001, Batch=32, Hidden=512, Optimizer=Adam\n",
      "    Max Epochs=100, Patience=10\n",
      "    Best Aggregation Method: MEAN\n",
      "\n",
      "Baseline Configuration:\n",
      "    Dropout: 0.0\n",
      "    Gradient Clipping: 0.0\n",
      "    L1 Lambda: 0.0\n",
      "    L2 Lambda: 0.0\n",
      "\n",
      ">>> Training baseline model...\n",
      "    Epoch 10: Train Acc=94.15%, Val Acc=85.05%\n",
      "    Epoch 20: Train Acc=98.30%, Val Acc=85.05%\n",
      "    Early stopping at epoch 26, best val acc: 86.42%\n",
      "\n",
      ">>> Baseline Results:\n",
      "    Validation Acc: 86.42%\n",
      "    Test Acc: 86.20%\n",
      "    Test F1: 0.8590\n",
      "    Test AUC-ROC: 0.9622\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Regularization: Baseline (No Regularization)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"REGULARIZATION STEP 0: BASELINE (NO REGULARIZATION)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Using best hyperparameters from tuning:\")\n",
    "print(f\"    LR={final_lr}, Batch={best_batch_size}, Hidden={best_hidden_dim}, Optimizer={best_optimizer}\")\n",
    "print(f\"    Max Epochs={MAX_EPOCHS}, Patience={PATIENCE}\")\n",
    "print(f\"    Best Aggregation Method: {best_aggregation['method'].upper()}\")\n",
    "\n",
    "# Baseline configuration\n",
    "baseline_config = {\n",
    "    'dropout': 0.0,\n",
    "    'grad_clip': 0.0,\n",
    "    'l1_lambda': 0.0,\n",
    "    'l2_lambda': 0.0\n",
    "}\n",
    "\n",
    "print(f\"\\nBaseline Configuration:\")\n",
    "print(f\"    Dropout: {baseline_config['dropout']}\")\n",
    "print(f\"    Gradient Clipping: {baseline_config['grad_clip']}\")\n",
    "print(f\"    L1 Lambda: {baseline_config['l1_lambda']}\")\n",
    "print(f\"    L2 Lambda: {baseline_config['l2_lambda']}\")\n",
    "\n",
    "# Create iterators\n",
    "train_iter = data.BucketIterator(\n",
    "    train_data,\n",
    "    batch_size=best_batch_size,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    shuffle=True,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "val_iter = data.BucketIterator(\n",
    "    validation_data,\n",
    "    batch_size=best_batch_size,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    shuffle=False,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "test_iter = data.BucketIterator(\n",
    "    test_data,\n",
    "    batch_size=best_batch_size,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    shuffle=False,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Create model\n",
    "model = RNN_Classifier_Aggregation(\n",
    "    vocab_size=embedding_vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_dim=best_hidden_dim,\n",
    "    output_dim=num_classes,\n",
    "    n_layers=N_LAYERS,\n",
    "    dropout=baseline_config['dropout'],\n",
    "    padding_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "    pretrained_embeddings=pretrained_embeddings,\n",
    "    aggregation=best_aggregation['method']\n",
    ").to(device)\n",
    "\n",
    "# Select optimizer\n",
    "if best_optimizer == 'Adam':\n",
    "    optimizer = optim.Adam(model.parameters(), lr=final_lr, weight_decay=baseline_config['l2_lambda'])\n",
    "elif best_optimizer == 'SGD':\n",
    "    optimizer = optim.SGD(model.parameters(), lr=final_lr, momentum=0.9, weight_decay=baseline_config['l2_lambda'])\n",
    "elif best_optimizer == 'RMSprop':\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=final_lr, weight_decay=baseline_config['l2_lambda'])\n",
    "elif best_optimizer == 'Adagrad':\n",
    "    optimizer = optim.Adagrad(model.parameters(), lr=final_lr, weight_decay=baseline_config['l2_lambda'])\n",
    "\n",
    "# Training loop\n",
    "best_val_acc = 0.0\n",
    "patience_counter = 0\n",
    "\n",
    "print(f\"\\n>>> Training baseline model...\")\n",
    "\n",
    "for epoch in range(MAX_EPOCHS):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_preds = []\n",
    "    train_labels = []\n",
    "    \n",
    "    for batch in train_iter:\n",
    "        text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(text, text_lengths)\n",
    "        \n",
    "        loss = criterion(predictions, labels)\n",
    "        \n",
    "        # Add L1 regularization\n",
    "        if baseline_config['l1_lambda'] > 0:\n",
    "            loss = loss + compute_l1_loss(model, baseline_config['l1_lambda'])\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        if baseline_config['grad_clip'] > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), baseline_config['grad_clip'])\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        preds = torch.argmax(predictions, dim=1)\n",
    "        train_preds.extend(preds.cpu().numpy())\n",
    "        train_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    train_acc = accuracy_score(train_labels, train_preds)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_iter:\n",
    "            text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "            predictions = model(text, text_lengths)\n",
    "            loss = criterion(predictions, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    val_acc = accuracy_score(val_labels, val_preds)\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), 'rnn_reg_baseline_best.pt')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(f\"    Early stopping at epoch {epoch+1}, best val acc: {best_val_acc*100:.2f}%\")\n",
    "            break\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"    Epoch {epoch+1}: Train Acc={train_acc*100:.2f}%, Val Acc={val_acc*100:.2f}%\")\n",
    "\n",
    "# Evaluate on test set\n",
    "model.load_state_dict(torch.load('rnn_reg_baseline_best.pt'))\n",
    "model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "test_preds = []\n",
    "test_labels = []\n",
    "test_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_iter:\n",
    "        text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "        predictions = model(text, text_lengths)\n",
    "        loss = criterion(predictions, labels)\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        probs = torch.softmax(predictions, dim=1)\n",
    "        preds = torch.argmax(predictions, dim=1)\n",
    "        test_preds.extend(preds.cpu().numpy())\n",
    "        test_labels.extend(labels.cpu().numpy())\n",
    "        test_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "test_acc = accuracy_score(test_labels, test_preds)\n",
    "test_f1 = f1_score(test_labels, test_preds, average='weighted')\n",
    "test_loss_avg = test_loss / len(test_iter)\n",
    "\n",
    "try:\n",
    "    test_probs_array = np.array(test_probs)\n",
    "    test_labels_bin = label_binarize(test_labels, classes=range(num_classes))\n",
    "    test_auc = roc_auc_score(test_labels_bin, test_probs_array, average='weighted', multi_class='ovr')\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not calculate AUC-ROC: {e}\")\n",
    "    test_auc = 0.0\n",
    "\n",
    "baseline_results = {\n",
    "    'name': 'baseline',\n",
    "    'dropout': baseline_config['dropout'],\n",
    "    'grad_clip': baseline_config['grad_clip'],\n",
    "    'l1_lambda': baseline_config['l1_lambda'],\n",
    "    'l2_lambda': baseline_config['l2_lambda'],\n",
    "    'val_acc': best_val_acc,\n",
    "    'test_acc': test_acc,\n",
    "    'test_f1': test_f1,\n",
    "    'test_auc': test_auc\n",
    "}\n",
    "\n",
    "print(f\"\\n>>> Baseline Results:\")\n",
    "print(f\"    Validation Acc: {best_val_acc*100:.2f}%\")\n",
    "print(f\"    Test Acc: {test_acc*100:.2f}%\")\n",
    "print(f\"    Test F1: {test_f1:.4f}\")\n",
    "print(f\"    Test AUC-ROC: {test_auc:.4f}\")\n",
    "\n",
    "# Store for next steps\n",
    "best_grad_clip = baseline_config['grad_clip']\n",
    "best_dropout = baseline_config['dropout']\n",
    "best_l1_lambda = baseline_config['l1_lambda']\n",
    "best_l2_lambda = baseline_config['l2_lambda']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2a253fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "REGULARIZATION STEP 1: GRADIENT CLIPPING TUNING\n",
      "================================================================================\n",
      "Using baseline settings: dropout=0.0, L1=0.0, L2=0.0\n",
      "\n",
      "Testing gradient clipping values: [0.0, 1.0]\n",
      "\n",
      "================================================================================\n",
      "Testing: Gradient Clipping = 0.0\n",
      "================================================================================\n",
      "    Epoch 10: Train Acc=93.05%, Val Acc=82.84%\n",
      "    Epoch 20: Train Acc=97.71%, Val Acc=85.87%\n",
      "    Epoch 30: Train Acc=94.27%, Val Acc=84.59%\n",
      "    Early stopping at epoch 32, best val acc: 86.61%\n",
      "    Result: Val Acc=86.61%, Test Acc=86.60%\n",
      "\n",
      "================================================================================\n",
      "Testing: Gradient Clipping = 1.0\n",
      "================================================================================\n",
      "    Epoch 10: Train Acc=92.18%, Val Acc=81.93%\n",
      "    Epoch 20: Train Acc=97.87%, Val Acc=83.94%\n",
      "    Early stopping at epoch 25, best val acc: 84.86%\n",
      "    Result: Val Acc=84.86%, Test Acc=87.80%\n",
      "\n",
      ">>> Step 1 Results:\n",
      "Grad Clip    Val Acc    Test Acc  \n",
      "-------------------------------------\n",
      "0.0          86.61     % 86.60     %\n",
      "1.0          84.86     % 87.80     %\n",
      "\n",
      ">>> Best Gradient Clipping: 0.0, Val Acc=86.61%, Test Acc=86.60%\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Regularization Step 1: Gradient Clipping Tuning\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"REGULARIZATION STEP 1: GRADIENT CLIPPING TUNING\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Using baseline settings: dropout={best_dropout}, L1={best_l1_lambda}, L2={best_l2_lambda}\")\n",
    "\n",
    "# Test different gradient clipping values\n",
    "grad_clip_options = [0.0, 1.0]  # 0.0 = no clipping, 1.0 = clip at 1.0\n",
    "\n",
    "print(f\"\\nTesting gradient clipping values: {grad_clip_options}\")\n",
    "\n",
    "step1_results = []\n",
    "\n",
    "for grad_clip in grad_clip_options:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Testing: Gradient Clipping = {grad_clip}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Create model with baseline settings\n",
    "    model = RNN_Classifier_Aggregation(\n",
    "        vocab_size=embedding_vocab_size,\n",
    "        embedding_dim=embedding_dim,\n",
    "        hidden_dim=best_hidden_dim,\n",
    "        output_dim=num_classes,\n",
    "        n_layers=N_LAYERS,\n",
    "        dropout=best_dropout,\n",
    "        padding_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "        pretrained_embeddings=pretrained_embeddings,\n",
    "        aggregation=best_aggregation['method']\n",
    "    ).to(device)\n",
    "    \n",
    "    # Select optimizer\n",
    "    if best_optimizer == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=final_lr, weight_decay=best_l2_lambda)\n",
    "    elif best_optimizer == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=final_lr, momentum=0.9, weight_decay=best_l2_lambda)\n",
    "    elif best_optimizer == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=final_lr, weight_decay=best_l2_lambda)\n",
    "    elif best_optimizer == 'Adagrad':\n",
    "        optimizer = optim.Adagrad(model.parameters(), lr=final_lr, weight_decay=best_l2_lambda)\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        \n",
    "        for batch in train_iter:\n",
    "            text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(text, text_lengths)\n",
    "            \n",
    "            loss = criterion(predictions, labels)\n",
    "            \n",
    "            if best_l1_lambda > 0:\n",
    "                loss = loss + compute_l1_loss(model, best_l1_lambda)\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            if grad_clip > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        train_acc = accuracy_score(train_labels, train_preds)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_iter:\n",
    "                text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "                predictions = model(text, text_lengths)\n",
    "                loss = criterion(predictions, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                preds = torch.argmax(predictions, dim=1)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_acc = accuracy_score(val_labels, val_preds)\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), f'rnn_reg_step1_gradclip{grad_clip}_best.pt')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(f\"    Early stopping at epoch {epoch+1}, best val acc: {best_val_acc*100:.2f}%\")\n",
    "                break\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"    Epoch {epoch+1}: Train Acc={train_acc*100:.2f}%, Val Acc={val_acc*100:.2f}%\")\n",
    "    \n",
    "    # Evaluate on the test set using the best checkpoint for this configuration\n",
    "    model.load_state_dict(torch.load(f'rnn_reg_step1_gradclip{grad_clip}_best.pt'))\n",
    "    test_loss, test_acc, test_f1, test_auc = evaluate_model(\n",
    "        model=model,\n",
    "        iterator=test_iter,\n",
    "        criterion=criterion,\n",
    "        device=device,\n",
    "        model_name=f\"Step1_gradclip_{grad_clip}\",\n",
    "        num_classes=num_classes\n",
    "    )\n",
    "    \n",
    "    step1_results.append({\n",
    "        'grad_clip': grad_clip,\n",
    "        'val_acc': best_val_acc,\n",
    "        'test_acc': test_acc,\n",
    "        'test_f1': test_f1,\n",
    "        'test_auc': test_auc\n",
    "    })\n",
    "    \n",
    "    print(f\"    Result: Val Acc={best_val_acc*100:.2f}%, Test Acc={test_acc*100:.2f}%\")\n",
    "\n",
    "# Find best gradient clipping\n",
    "best_step1 = max(step1_results, key=lambda x: x['val_acc'])\n",
    "best_grad_clip = best_step1['grad_clip']\n",
    "\n",
    "print(f\"\\n>>> Step 1 Results:\")\n",
    "print(f\"{'Grad Clip':<12} {'Val Acc':<10} {'Test Acc':<10}\")\n",
    "print(\"-\" * 37)\n",
    "for result in step1_results:\n",
    "    print(f\"{result['grad_clip']:<12} {result['val_acc']*100:<10.2f}% {result['test_acc']*100:<10.2f}%\")\n",
    "print(f\"\\n>>> Best Gradient Clipping: {best_grad_clip}, Val Acc={best_step1['val_acc']*100:.2f}%, Test Acc={best_step1['test_acc']*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff8b7e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "REGULARIZATION STEP 2: DROPOUT TUNING\n",
      "================================================================================\n",
      "Using best from Step 1: grad_clip=0.0\n",
      "Using baseline settings: L1=0.0, L2=0.0\n",
      "\n",
      "Testing dropout values: [0.0, 0.3, 0.5, 0.7]\n",
      "\n",
      "================================================================================\n",
      "Testing: Dropout = 0.0\n",
      "================================================================================\n",
      "    Epoch 10: Train Acc=93.03%, Val Acc=85.96%\n",
      "    Early stopping at epoch 20, best val acc: 85.96%\n",
      "    Result: Val Acc=85.96%, Test Acc=86.20%\n",
      "\n",
      "================================================================================\n",
      "Testing: Dropout = 0.3\n",
      "================================================================================\n",
      "    Epoch 10: Train Acc=94.18%, Val Acc=84.31%\n",
      "    Epoch 20: Train Acc=98.62%, Val Acc=84.68%\n",
      "    Epoch 30: Train Acc=99.66%, Val Acc=84.77%\n",
      "    Epoch 40: Train Acc=99.89%, Val Acc=85.14%\n",
      "    Early stopping at epoch 44, best val acc: 86.06%\n",
      "    Result: Val Acc=86.06%, Test Acc=85.40%\n",
      "\n",
      "================================================================================\n",
      "Testing: Dropout = 0.5\n",
      "================================================================================\n",
      "    Epoch 10: Train Acc=92.80%, Val Acc=81.19%\n",
      "    Epoch 20: Train Acc=98.23%, Val Acc=85.50%\n",
      "    Early stopping at epoch 22, best val acc: 85.69%\n",
      "    Result: Val Acc=85.69%, Test Acc=86.00%\n",
      "\n",
      "================================================================================\n",
      "Testing: Dropout = 0.7\n",
      "================================================================================\n",
      "    Epoch 10: Train Acc=93.92%, Val Acc=84.13%\n",
      "    Epoch 20: Train Acc=98.74%, Val Acc=86.24%\n",
      "    Epoch 30: Train Acc=99.34%, Val Acc=85.23%\n",
      "    Early stopping at epoch 36, best val acc: 87.25%\n",
      "    Result: Val Acc=87.25%, Test Acc=86.20%\n",
      "\n",
      ">>> Step 2 Results:\n",
      "Dropout      Val Acc    Test Acc  \n",
      "-------------------------------------\n",
      "0.0          85.96     % 86.20     %\n",
      "0.3          86.06     % 85.40     %\n",
      "0.5          85.69     % 86.00     %\n",
      "0.7          87.25     % 86.20     %\n",
      "\n",
      ">>> Best Dropout: 0.7, Val Acc=87.25%, Test Acc=86.20%\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Regularization Step 2: Dropout Tuning\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"REGULARIZATION STEP 2: DROPOUT TUNING\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Using best from Step 1: grad_clip={best_grad_clip}\")\n",
    "print(f\"Using baseline settings: L1={best_l1_lambda}, L2={best_l2_lambda}\")\n",
    "\n",
    "# Test different dropout values\n",
    "dropout_options = [0.0, 0.3, 0.5, 0.7]\n",
    "\n",
    "print(f\"\\nTesting dropout values: {dropout_options}\")\n",
    "\n",
    "step2_results = []\n",
    "\n",
    "for dropout_val in dropout_options:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Testing: Dropout = {dropout_val}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Create model with best grad_clip and current dropout\n",
    "    model = RNN_Classifier_Aggregation(\n",
    "        vocab_size=embedding_vocab_size,\n",
    "        embedding_dim=embedding_dim,\n",
    "        hidden_dim=best_hidden_dim,\n",
    "        output_dim=num_classes,\n",
    "        n_layers=N_LAYERS,\n",
    "        dropout=dropout_val,\n",
    "        padding_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "        pretrained_embeddings=pretrained_embeddings,\n",
    "        aggregation=best_aggregation['method']\n",
    "    ).to(device)\n",
    "    \n",
    "    # Select optimizer\n",
    "    if best_optimizer == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=final_lr, weight_decay=best_l2_lambda)\n",
    "    elif best_optimizer == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=final_lr, momentum=0.9, weight_decay=best_l2_lambda)\n",
    "    elif best_optimizer == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=final_lr, weight_decay=best_l2_lambda)\n",
    "    elif best_optimizer == 'Adagrad':\n",
    "        optimizer = optim.Adagrad(model.parameters(), lr=final_lr, weight_decay=best_l2_lambda)\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        \n",
    "        for batch in train_iter:\n",
    "            text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(text, text_lengths)\n",
    "            \n",
    "            loss = criterion(predictions, labels)\n",
    "            \n",
    "            if best_l1_lambda > 0:\n",
    "                loss = loss + compute_l1_loss(model, best_l1_lambda)\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            if best_grad_clip > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), best_grad_clip)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        train_acc = accuracy_score(train_labels, train_preds)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_iter:\n",
    "                text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "                predictions = model(text, text_lengths)\n",
    "                loss = criterion(predictions, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                preds = torch.argmax(predictions, dim=1)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_acc = accuracy_score(val_labels, val_preds)\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), f'rnn_reg_step2_dropout{dropout_val}_best.pt')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(f\"    Early stopping at epoch {epoch+1}, best val acc: {best_val_acc*100:.2f}%\")\n",
    "                break\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"    Epoch {epoch+1}: Train Acc={train_acc*100:.2f}%, Val Acc={val_acc*100:.2f}%\")\n",
    "    \n",
    "    # Evaluate on the test set using the best checkpoint for this configuration\n",
    "    model.load_state_dict(torch.load(f'rnn_reg_step2_dropout{dropout_val}_best.pt'))\n",
    "    test_loss, test_acc, test_f1, test_auc = evaluate_model(\n",
    "        model=model,\n",
    "        iterator=test_iter,\n",
    "        criterion=criterion,\n",
    "        device=device,\n",
    "        model_name=f\"Step2_dropout_{dropout_val}\",\n",
    "        num_classes=num_classes\n",
    "    )\n",
    "    \n",
    "    step2_results.append({\n",
    "        'dropout': dropout_val,\n",
    "        'val_acc': best_val_acc,\n",
    "        'test_acc': test_acc,\n",
    "        'test_f1': test_f1,\n",
    "        'test_auc': test_auc\n",
    "    })\n",
    "    \n",
    "    print(f\"    Result: Val Acc={best_val_acc*100:.2f}%, Test Acc={test_acc*100:.2f}%\")\n",
    "\n",
    "# Find best dropout\n",
    "best_step2 = max(step2_results, key=lambda x: x['val_acc'])\n",
    "best_dropout = best_step2['dropout']\n",
    "\n",
    "print(f\"\\n>>> Step 2 Results:\")\n",
    "print(f\"{'Dropout':<12} {'Val Acc':<10} {'Test Acc':<10}\")\n",
    "print(\"-\" * 37)\n",
    "for result in step2_results:\n",
    "    print(f\"{result['dropout']:<12} {result['val_acc']*100:<10.2f}% {result['test_acc']*100:<10.2f}%\")\n",
    "print(f\"\\n>>> Best Dropout: {best_dropout}, Val Acc={best_step2['val_acc']*100:.2f}%, Test Acc={best_step2['test_acc']*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f14317b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "REGULARIZATION STEP 3: L1 REGULARIZATION TUNING\n",
      "================================================================================\n",
      "Using best from Steps 1-2: grad_clip=0.0, dropout=0.7\n",
      "Using baseline setting: L2=0.0\n",
      "\n",
      "Testing L1 lambda values: [0.0, 1e-06, 1e-05, 0.0001]\n",
      "\n",
      "================================================================================\n",
      "Testing: L1 Lambda = 0.0\n",
      "================================================================================\n",
      "    Epoch 10: Train Acc=94.20%, Val Acc=83.94%\n",
      "    Epoch 20: Train Acc=98.74%, Val Acc=85.60%\n",
      "    Early stopping at epoch 22, best val acc: 86.06%\n",
      "    Result: Val Acc=86.06%, Test Acc=86.00%\n",
      "\n",
      "================================================================================\n",
      "Testing: L1 Lambda = 1e-06\n",
      "================================================================================\n",
      "    Epoch 10: Train Acc=94.11%, Val Acc=79.45%\n",
      "    Epoch 20: Train Acc=97.27%, Val Acc=84.04%\n",
      "    Early stopping at epoch 27, best val acc: 85.96%\n",
      "    Result: Val Acc=85.96%, Test Acc=88.00%\n",
      "\n",
      "================================================================================\n",
      "Testing: L1 Lambda = 1e-05\n",
      "================================================================================\n",
      "    Epoch 10: Train Acc=90.88%, Val Acc=81.28%\n",
      "    Epoch 20: Train Acc=97.48%, Val Acc=85.05%\n",
      "    Epoch 30: Train Acc=98.56%, Val Acc=84.95%\n",
      "    Early stopping at epoch 39, best val acc: 85.14%\n",
      "    Result: Val Acc=85.14%, Test Acc=85.40%\n",
      "\n",
      "================================================================================\n",
      "Testing: L1 Lambda = 0.0001\n",
      "================================================================================\n",
      "    Epoch 10: Train Acc=70.31%, Val Acc=65.69%\n",
      "    Epoch 20: Train Acc=85.95%, Val Acc=80.83%\n",
      "    Epoch 30: Train Acc=88.58%, Val Acc=81.47%\n",
      "    Epoch 40: Train Acc=93.12%, Val Acc=83.49%\n",
      "    Early stopping at epoch 47, best val acc: 83.76%\n",
      "    Result: Val Acc=83.76%, Test Acc=85.40%\n",
      "\n",
      ">>> Step 3 Results:\n",
      "L1 Lambda    Val Acc    Test Acc  \n",
      "-------------------------------------\n",
      "0e+00        86.06     % 86.00     %\n",
      "1e-06        85.96     % 88.00     %\n",
      "1e-05        85.14     % 85.40     %\n",
      "1e-04        83.76     % 85.40     %\n",
      "\n",
      ">>> Best L1 Lambda: 0.0, Val Acc=86.06%, Test Acc=86.00%\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Regularization Step 3: L1 Regularization Tuning\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"REGULARIZATION STEP 3: L1 REGULARIZATION TUNING\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Using best from Steps 1-2: grad_clip={best_grad_clip}, dropout={best_dropout}\")\n",
    "print(f\"Using baseline setting: L2={best_l2_lambda}\")\n",
    "\n",
    "# Test different L1 lambda values\n",
    "l1_lambda_options = [0.0, 1e-6, 1e-5, 1e-4]\n",
    "\n",
    "print(f\"\\nTesting L1 lambda values: {l1_lambda_options}\")\n",
    "\n",
    "step3_results = []\n",
    "\n",
    "for l1_lambda in l1_lambda_options:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Testing: L1 Lambda = {l1_lambda}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Create model with best grad_clip, dropout, and current L1\n",
    "    model = RNN_Classifier_Aggregation(\n",
    "        vocab_size=embedding_vocab_size,\n",
    "        embedding_dim=embedding_dim,\n",
    "        hidden_dim=best_hidden_dim,\n",
    "        output_dim=num_classes,\n",
    "        n_layers=N_LAYERS,\n",
    "        dropout=best_dropout,\n",
    "        padding_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "        pretrained_embeddings=pretrained_embeddings,\n",
    "        aggregation=best_aggregation['method']\n",
    "    ).to(device)\n",
    "    \n",
    "    # Select optimizer\n",
    "    if best_optimizer == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=final_lr, weight_decay=best_l2_lambda)\n",
    "    elif best_optimizer == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=final_lr, momentum=0.9, weight_decay=best_l2_lambda)\n",
    "    elif best_optimizer == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=final_lr, weight_decay=best_l2_lambda)\n",
    "    elif best_optimizer == 'Adagrad':\n",
    "        optimizer = optim.Adagrad(model.parameters(), lr=final_lr, weight_decay=best_l2_lambda)\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        \n",
    "        for batch in train_iter:\n",
    "            text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(text, text_lengths)\n",
    "            \n",
    "            loss = criterion(predictions, labels)\n",
    "            \n",
    "            if l1_lambda > 0:\n",
    "                loss = loss + compute_l1_loss(model, l1_lambda)\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            if best_grad_clip > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), best_grad_clip)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        train_acc = accuracy_score(train_labels, train_preds)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_iter:\n",
    "                text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "                predictions = model(text, text_lengths)\n",
    "                loss = criterion(predictions, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                preds = torch.argmax(predictions, dim=1)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_acc = accuracy_score(val_labels, val_preds)\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), f'rnn_reg_step3_l1{l1_lambda}_best.pt')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(f\"    Early stopping at epoch {epoch+1}, best val acc: {best_val_acc*100:.2f}%\")\n",
    "                break\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"    Epoch {epoch+1}: Train Acc={train_acc*100:.2f}%, Val Acc={val_acc*100:.2f}%\")\n",
    "    \n",
    "    # Evaluate on the test set using the best checkpoint for this configuration\n",
    "    model.load_state_dict(torch.load(f'rnn_reg_step3_l1{l1_lambda}_best.pt'))\n",
    "    test_loss, test_acc, test_f1, test_auc = evaluate_model(\n",
    "        model=model,\n",
    "        iterator=test_iter,\n",
    "        criterion=criterion,\n",
    "        device=device,\n",
    "        model_name=f\"Step3_l1_{l1_lambda}\",\n",
    "        num_classes=num_classes\n",
    "    )\n",
    "    \n",
    "    step3_results.append({\n",
    "        'l1_lambda': l1_lambda,\n",
    "        'val_acc': best_val_acc,\n",
    "        'test_acc': test_acc,\n",
    "        'test_f1': test_f1,\n",
    "        'test_auc': test_auc\n",
    "    })\n",
    "    \n",
    "    print(f\"    Result: Val Acc={best_val_acc*100:.2f}%, Test Acc={test_acc*100:.2f}%\")\n",
    "\n",
    "# Find best L1 lambda\n",
    "best_step3 = max(step3_results, key=lambda x: x['val_acc'])\n",
    "best_l1_lambda = best_step3['l1_lambda']\n",
    "\n",
    "print(f\"\\n>>> Step 3 Results:\")\n",
    "print(f\"{'L1 Lambda':<12} {'Val Acc':<10} {'Test Acc':<10}\")\n",
    "print(\"-\" * 37)\n",
    "for result in step3_results:\n",
    "    print(f\"{result['l1_lambda']:<12.0e} {result['val_acc']*100:<10.2f}% {result['test_acc']*100:<10.2f}%\")\n",
    "print(f\"\\n>>> Best L1 Lambda: {best_l1_lambda}, Val Acc={best_step3['val_acc']*100:.2f}%, Test Acc={best_step3['test_acc']*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "89324596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "REGULARIZATION STEP 4: L2 REGULARIZATION TUNING\n",
      "================================================================================\n",
      "Using best from Steps 1-3: grad_clip=0.0, dropout=0.7, L1=0.0\n",
      "\n",
      "Testing L2 lambda values: [0.0, 1e-05, 0.0001, 0.001]\n",
      "\n",
      "================================================================================\n",
      "Testing: L2 Lambda = 0.0\n",
      "================================================================================\n",
      "    Epoch 10: Train Acc=93.40%, Val Acc=85.50%\n",
      "    Epoch 20: Train Acc=98.67%, Val Acc=86.70%\n",
      "    Epoch 30: Train Acc=98.37%, Val Acc=73.21%\n",
      "    Early stopping at epoch 34, best val acc: 86.79%\n",
      "    Result: Val Acc=86.79%, Test Acc=87.00%\n",
      "\n",
      "================================================================================\n",
      "Testing: L2 Lambda = 1e-05\n",
      "================================================================================\n",
      "    Epoch 10: Train Acc=94.27%, Val Acc=82.29%\n",
      "    Epoch 20: Train Acc=98.76%, Val Acc=85.32%\n",
      "    Early stopping at epoch 26, best val acc: 85.87%\n",
      "    Result: Val Acc=85.87%, Test Acc=86.20%\n",
      "\n",
      "================================================================================\n",
      "Testing: L2 Lambda = 0.0001\n",
      "================================================================================\n",
      "    Epoch 10: Train Acc=93.54%, Val Acc=83.76%\n",
      "    Epoch 20: Train Acc=97.73%, Val Acc=86.24%\n",
      "    Epoch 30: Train Acc=99.72%, Val Acc=85.41%\n",
      "    Early stopping at epoch 33, best val acc: 87.06%\n",
      "    Result: Val Acc=87.06%, Test Acc=86.40%\n",
      "\n",
      "================================================================================\n",
      "Testing: L2 Lambda = 0.001\n",
      "================================================================================\n",
      "    Epoch 10: Train Acc=84.57%, Val Acc=73.21%\n",
      "    Epoch 20: Train Acc=95.05%, Val Acc=83.12%\n",
      "    Early stopping at epoch 25, best val acc: 85.14%\n",
      "    Result: Val Acc=85.14%, Test Acc=84.80%\n",
      "\n",
      ">>> Step 4 Results:\n",
      "L2 Lambda    Val Acc    Test Acc  \n",
      "-------------------------------------\n",
      "0e+00        86.79     % 87.00     %\n",
      "1e-05        85.87     % 86.20     %\n",
      "1e-04        87.06     % 86.40     %\n",
      "1e-03        85.14     % 84.80     %\n",
      "\n",
      ">>> Best L2 Lambda: 0.0001, Val Acc=87.06%, Test Acc=86.40%\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Regularization Step 4: L2 Regularization Tuning\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"REGULARIZATION STEP 4: L2 REGULARIZATION TUNING\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Using best from Steps 1-3: grad_clip={best_grad_clip}, dropout={best_dropout}, L1={best_l1_lambda}\")\n",
    "\n",
    "# Test different L2 lambda values (via weight_decay)\n",
    "l2_lambda_options = [0.0, 1e-5, 1e-4, 1e-3]\n",
    "\n",
    "print(f\"\\nTesting L2 lambda values: {l2_lambda_options}\")\n",
    "\n",
    "step4_results = []\n",
    "\n",
    "for l2_lambda in l2_lambda_options:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Testing: L2 Lambda = {l2_lambda}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Create model with best grad_clip, dropout, L1, and current L2\n",
    "    model = RNN_Classifier_Aggregation(\n",
    "        vocab_size=embedding_vocab_size,\n",
    "        embedding_dim=embedding_dim,\n",
    "        hidden_dim=best_hidden_dim,\n",
    "        output_dim=num_classes,\n",
    "        n_layers=N_LAYERS,\n",
    "        dropout=best_dropout,\n",
    "        padding_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "        pretrained_embeddings=pretrained_embeddings,\n",
    "        aggregation=best_aggregation['method']\n",
    "    ).to(device)\n",
    "    \n",
    "    # Select optimizer with L2 regularization (weight_decay)\n",
    "    if best_optimizer == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=final_lr, weight_decay=l2_lambda)\n",
    "    elif best_optimizer == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=final_lr, momentum=0.9, weight_decay=l2_lambda)\n",
    "    elif best_optimizer == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=final_lr, weight_decay=l2_lambda)\n",
    "    elif best_optimizer == 'Adagrad':\n",
    "        optimizer = optim.Adagrad(model.parameters(), lr=final_lr, weight_decay=l2_lambda)\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        \n",
    "        for batch in train_iter:\n",
    "            text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(text, text_lengths)\n",
    "            \n",
    "            loss = criterion(predictions, labels)\n",
    "            \n",
    "            if best_l1_lambda > 0:\n",
    "                loss = loss + compute_l1_loss(model, best_l1_lambda)\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            if best_grad_clip > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), best_grad_clip)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        train_acc = accuracy_score(train_labels, train_preds)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_iter:\n",
    "                text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "                predictions = model(text, text_lengths)\n",
    "                loss = criterion(predictions, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                preds = torch.argmax(predictions, dim=1)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_acc = accuracy_score(val_labels, val_preds)\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), f'rnn_reg_step4_l2{l2_lambda}_best.pt')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(f\"    Early stopping at epoch {epoch+1}, best val acc: {best_val_acc*100:.2f}%\")\n",
    "                break\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"    Epoch {epoch+1}: Train Acc={train_acc*100:.2f}%, Val Acc={val_acc*100:.2f}%\")\n",
    "    \n",
    "    # Evaluate on the test set using the best checkpoint for this configuration\n",
    "    model.load_state_dict(torch.load(f'rnn_reg_step4_l2{l2_lambda}_best.pt'))\n",
    "    test_loss, test_acc, test_f1, test_auc = evaluate_model(\n",
    "        model=model,\n",
    "        iterator=test_iter,\n",
    "        criterion=criterion,\n",
    "        device=device,\n",
    "        model_name=f\"Step4_l2_{l2_lambda}\",\n",
    "        num_classes=num_classes\n",
    "    )\n",
    "    \n",
    "    step4_results.append({\n",
    "        'l2_lambda': l2_lambda,\n",
    "        'val_acc': best_val_acc,\n",
    "        'test_acc': test_acc,\n",
    "        'test_f1': test_f1,\n",
    "        'test_auc': test_auc\n",
    "    })\n",
    "    \n",
    "    print(f\"    Result: Val Acc={best_val_acc*100:.2f}%, Test Acc={test_acc*100:.2f}%\")\n",
    "\n",
    "# Find best L2 lambda\n",
    "best_step4 = max(step4_results, key=lambda x: x['val_acc'])\n",
    "best_l2_lambda = best_step4['l2_lambda']\n",
    "\n",
    "print(f\"\\n>>> Step 4 Results:\")\n",
    "print(f\"{'L2 Lambda':<12} {'Val Acc':<10} {'Test Acc':<10}\")\n",
    "print(\"-\" * 37)\n",
    "for result in step4_results:\n",
    "    print(f\"{result['l2_lambda']:<12.0e} {result['val_acc']*100:<10.2f}% {result['test_acc']*100:<10.2f}%\")\n",
    "print(f\"\\n>>> Best L2 Lambda: {best_l2_lambda}, Val Acc={best_step4['val_acc']*100:.2f}%, Test Acc={best_step4['test_acc']*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d9a5b338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "REGULARIZATION FINAL: ALL BEST SETTINGS COMBINED\n",
      "================================================================================\n",
      "Best settings from all steps:\n",
      "    Gradient Clipping: 0.0\n",
      "    Dropout: 0.7\n",
      "    L1 Lambda: 0.0\n",
      "    L2 Lambda: 0.0001\n",
      "\n",
      ">>> Training final model with all best regularization settings...\n",
      "    Epoch 10: Train Acc=92.02%, Val Acc=82.11%\n",
      "    Epoch 20: Train Acc=96.70%, Val Acc=80.37%\n",
      "    Epoch 30: Train Acc=98.21%, Val Acc=84.04%\n",
      "    Early stopping at epoch 38, best val acc: 85.50%\n",
      "\n",
      ">>> Final Combined Results:\n",
      "    Configuration:\n",
      "      - Gradient Clipping: 0.0\n",
      "      - Dropout: 0.7\n",
      "      - L1 Lambda: 0.0\n",
      "      - L2 Lambda: 0.0001\n",
      "    Validation Acc: 85.50%\n",
      "    Test Acc: 86.20%\n",
      "    Test F1: 0.8602\n",
      "    Test AUC-ROC: 0.9602\n",
      "\n",
      ">>> Comparison with Baseline:\n",
      "    Baseline Test Acc: 86.20%\n",
      "    Final Regularized Test Acc: 86.20%\n",
      "    Improvement: +0.00% (+0.00% relative)\n",
      "\n",
      ">>> Plotting training curves for best configuration and regularization...\n",
      "    Saved training curves to 'best_config_training_curves.png'\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "REGULARIZATION TUNING COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Regularization Final: All Best Combined\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"REGULARIZATION FINAL: ALL BEST SETTINGS COMBINED\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Best settings from all steps:\")\n",
    "print(f\"    Gradient Clipping: {best_grad_clip}\")\n",
    "print(f\"    Dropout: {best_dropout}\")\n",
    "print(f\"    L1 Lambda: {best_l1_lambda}\")\n",
    "print(f\"    L2 Lambda: {best_l2_lambda}\")\n",
    "\n",
    "# Create model with all best settings\n",
    "model = RNN_Classifier_Aggregation(\n",
    "    vocab_size=embedding_vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_dim=best_hidden_dim,\n",
    "    output_dim=num_classes,\n",
    "    n_layers=N_LAYERS,\n",
    "    dropout=best_dropout,\n",
    "    padding_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "    pretrained_embeddings=pretrained_embeddings,\n",
    "    aggregation=best_aggregation['method']\n",
    ").to(device)\n",
    "\n",
    "# Select optimizer with best L2 (weight_decay)\n",
    "if best_optimizer == 'Adam':\n",
    "    optimizer = optim.Adam(model.parameters(), lr=final_lr, weight_decay=best_l2_lambda)\n",
    "elif best_optimizer == 'SGD':\n",
    "    optimizer = optim.SGD(model.parameters(), lr=final_lr, momentum=0.9, weight_decay=best_l2_lambda)\n",
    "elif best_optimizer == 'RMSprop':\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=final_lr, weight_decay=best_l2_lambda)\n",
    "elif best_optimizer == 'Adagrad':\n",
    "    optimizer = optim.Adagrad(model.parameters(), lr=final_lr, weight_decay=best_l2_lambda)\n",
    "\n",
    "# Training loop\n",
    "best_val_acc = 0.0\n",
    "patience_counter = 0\n",
    "\n",
    "# Store training history for plotting\n",
    "train_losses = []\n",
    "val_accs = []\n",
    "\n",
    "print(f\"\\n>>> Training final model with all best regularization settings...\")\n",
    "\n",
    "for epoch in range(MAX_EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_preds = []\n",
    "    train_labels = []\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch in train_iter:\n",
    "        num_batches += 1\n",
    "        text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(text, text_lengths)\n",
    "        \n",
    "        loss = criterion(predictions, labels)\n",
    "        \n",
    "        if best_l1_lambda > 0:\n",
    "            loss = loss + compute_l1_loss(model, best_l1_lambda)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        if best_grad_clip > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), best_grad_clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        preds = torch.argmax(predictions, dim=1)\n",
    "        train_preds.extend(preds.cpu().numpy())\n",
    "        train_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    train_acc = accuracy_score(train_labels, train_preds)\n",
    "    num_train_batches = len(train_labels) // train_iter.batch_size + (1 if len(train_labels) % train_iter.batch_size != 0 else 0)\n",
    "    train_loss_avg = train_loss / num_train_batches if num_train_batches > 0 else train_loss\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_iter:\n",
    "            text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "            predictions = model(text, text_lengths)\n",
    "            loss = criterion(predictions, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    val_acc = accuracy_score(val_labels, val_preds)\n",
    "    \n",
    "    # Store training history for plotting\n",
    "    train_losses.append(train_loss_avg)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), 'rnn_reg_final_best.pt')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(f\"    Early stopping at epoch {epoch+1}, best val acc: {best_val_acc*100:.2f}%\")\n",
    "            break\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"    Epoch {epoch+1}: Train Acc={train_acc*100:.2f}%, Val Acc={val_acc*100:.2f}%\")\n",
    "\n",
    "# Evaluate on test set\n",
    "model.load_state_dict(torch.load('rnn_reg_final_best.pt'))\n",
    "model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "test_preds = []\n",
    "test_labels = []\n",
    "test_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_iter:\n",
    "        text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "        predictions = model(text, text_lengths)\n",
    "        loss = criterion(predictions, labels)\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        probs = torch.softmax(predictions, dim=1)\n",
    "        preds = torch.argmax(predictions, dim=1)\n",
    "        test_preds.extend(preds.cpu().numpy())\n",
    "        test_labels.extend(labels.cpu().numpy())\n",
    "        test_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "test_acc = accuracy_score(test_labels, test_preds)\n",
    "test_f1 = f1_score(test_labels, test_preds, average='weighted')\n",
    "test_loss_avg = test_loss / len(test_iter)\n",
    "\n",
    "try:\n",
    "    test_probs_array = np.array(test_probs)\n",
    "    test_labels_bin = label_binarize(test_labels, classes=range(num_classes))\n",
    "    test_auc = roc_auc_score(test_labels_bin, test_probs_array, average='weighted', multi_class='ovr')\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not calculate AUC-ROC: {e}\")\n",
    "    test_auc = 0.0\n",
    "\n",
    "final_results = {\n",
    "    'name': 'final_combined',\n",
    "    'dropout': best_dropout,\n",
    "    'grad_clip': best_grad_clip,\n",
    "    'l1_lambda': best_l1_lambda,\n",
    "    'l2_lambda': best_l2_lambda,\n",
    "    'val_acc': best_val_acc,\n",
    "    'test_acc': test_acc,\n",
    "    'test_f1': test_f1,\n",
    "    'test_auc': test_auc\n",
    "}\n",
    "\n",
    "print(f\"\\n>>> Final Combined Results:\")\n",
    "print(f\"    Configuration:\")\n",
    "print(f\"      - Gradient Clipping: {best_grad_clip}\")\n",
    "print(f\"      - Dropout: {best_dropout}\")\n",
    "print(f\"      - L1 Lambda: {best_l1_lambda}\")\n",
    "print(f\"      - L2 Lambda: {best_l2_lambda}\")\n",
    "print(f\"    Validation Acc: {best_val_acc*100:.2f}%\")\n",
    "print(f\"    Test Acc: {test_acc*100:.2f}%\")\n",
    "print(f\"    Test F1: {test_f1:.4f}\")\n",
    "print(f\"    Test AUC-ROC: {test_auc:.4f}\")\n",
    "\n",
    "# Compare with baseline\n",
    "improvement = test_acc - baseline_results['test_acc']\n",
    "improvement_pct = (improvement / baseline_results['test_acc']) * 100 if baseline_results['test_acc'] > 0 else 0\n",
    "\n",
    "print(f\"\\n>>> Comparison with Baseline:\")\n",
    "print(f\"    Baseline Test Acc: {baseline_results['test_acc']*100:.2f}%\")\n",
    "print(f\"    Final Regularized Test Acc: {test_acc*100:.2f}%\")\n",
    "print(f\"    Improvement: {improvement*100:+.2f}% ({improvement_pct:+.2f}% relative)\")\n",
    "\n",
    "# Plot training curves for best configuration and regularization\n",
    "print(f\"\\n>>> Plotting training curves for best configuration and regularization...\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Training Loss vs Epochs\n",
    "epochs = range(1, len(train_losses) + 1)\n",
    "ax1.plot(epochs, train_losses, 'b-', label='Training Loss', linewidth=2, marker='o', markersize=4)\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Training Loss', fontsize=12)\n",
    "ax1.set_title('Training Loss Curve', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xlim(left=1)\n",
    "\n",
    "# Plot 2: Validation Accuracy vs Epochs\n",
    "ax2.plot(epochs, [acc*100 for acc in val_accs], 'r-', label='Validation Accuracy', linewidth=2, marker='s', markersize=4)\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Validation Accuracy (%)', fontsize=12)\n",
    "ax2.set_title('Validation Accuracy Curve', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xlim(left=1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('best_config_training_curves.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"    Saved training curves to 'best_config_training_curves.png'\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"REGULARIZATION TUNING COMPLETE\")\n",
    "print(f\"{'='*80}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c348d0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "REGULARIZATION SUMMARY: BASELINE VS REGULARIZATION STEPS\n",
      "================================================================================\n",
      "Method                                Val Acc   Test Acc   Test F1  Test AUC\n",
      "----------------------------------------------------------------------------\n",
      "Baseline (No Reg)                      86.42%     86.20%   0.8590   0.9622\n",
      "Step 1 - Grad Clip 0.0                 86.61%     86.60%   0.8643   0.9595\n",
      "Step 2 - Dropout 0.7                   87.25%     86.20%   0.8591   0.9604\n",
      "Step 3 - L1 0.0                        86.06%     86.00%   0.8571   0.9608\n",
      "Step 4 - L2 0.0001                     87.06%     86.40%   0.8636   0.9621\n",
      "Final Combined                         85.50%     86.20%   0.8602   0.9602\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Regularization Summary Comparison\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"REGULARIZATION SUMMARY: BASELINE VS REGULARIZATION STEPS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary_rows = [\n",
    "    {\n",
    "        \"method\": \"Baseline (No Reg)\",\n",
    "        \"val_acc\": baseline_results['val_acc'],\n",
    "        \"test_acc\": baseline_results['test_acc'],\n",
    "        \"test_f1\": baseline_results['test_f1'],\n",
    "        \"test_auc\": baseline_results['test_auc'],\n",
    "    },\n",
    "    {\n",
    "        \"method\": f\"Step 1 - Grad Clip {best_step1['grad_clip']}\",\n",
    "        \"val_acc\": best_step1['val_acc'],\n",
    "        \"test_acc\": best_step1['test_acc'],\n",
    "        \"test_f1\": best_step1['test_f1'],\n",
    "        \"test_auc\": best_step1['test_auc'],\n",
    "    },\n",
    "    {\n",
    "        \"method\": f\"Step 2 - Dropout {best_step2['dropout']}\",\n",
    "        \"val_acc\": best_step2['val_acc'],\n",
    "        \"test_acc\": best_step2['test_acc'],\n",
    "        \"test_f1\": best_step2['test_f1'],\n",
    "        \"test_auc\": best_step2['test_auc'],\n",
    "    },\n",
    "    {\n",
    "        \"method\": f\"Step 3 - L1 {best_step3['l1_lambda']}\",\n",
    "        \"val_acc\": best_step3['val_acc'],\n",
    "        \"test_acc\": best_step3['test_acc'],\n",
    "        \"test_f1\": best_step3['test_f1'],\n",
    "        \"test_auc\": best_step3['test_auc'],\n",
    "    },\n",
    "    {\n",
    "        \"method\": f\"Step 4 - L2 {best_step4['l2_lambda']}\",\n",
    "        \"val_acc\": best_step4['val_acc'],\n",
    "        \"test_acc\": best_step4['test_acc'],\n",
    "        \"test_f1\": best_step4['test_f1'],\n",
    "        \"test_auc\": best_step4['test_auc'],\n",
    "    },\n",
    "    {\n",
    "        \"method\": \"Final Combined\",\n",
    "        \"val_acc\": final_results['val_acc'],\n",
    "        \"test_acc\": final_results['test_acc'],\n",
    "        \"test_f1\": final_results['test_f1'],\n",
    "        \"test_auc\": final_results['test_auc'],\n",
    "    },\n",
    "]\n",
    "\n",
    "# Keep a copy for later reporting if needed\n",
    "summary_results = summary_rows\n",
    "\n",
    "header = f\"{'Method':<35} {'Val Acc':>9} {'Test Acc':>10} {'Test F1':>9} {'Test AUC':>9}\"\n",
    "print(header)\n",
    "print(\"-\" * len(header))\n",
    "for row in summary_rows:\n",
    "    print(\n",
    "        f\"{row['method']:<35} \"\n",
    "        f\"{row['val_acc']*100:>8.2f}% \"\n",
    "        f\"{row['test_acc']*100:>9.2f}% \"\n",
    "        f\"{row['test_f1']:>8.4f} \"\n",
    "        f\"{row['test_auc']:>8.4f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "70f4086d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PART 2(e): TOPIC-WISE ACCURACY EVALUATION\n",
      "================================================================================\n",
      "\n",
      ">>> Using model from regularization tuning...\n",
      "     Using existing model from previous cell\n",
      "    Model vocab size: 8120\n",
      "\n",
      ">>> Evaluating on test set...\n",
      "\n",
      "================================================================================\n",
      "TOPIC-WISE ACCURACY ON TEST SET\n",
      "================================================================================\n",
      "Topic      Accuracy     Correct    Total      % of Test Set  \n",
      "--------------------------------------------------------------------------------\n",
      "ABBR       77.78        7          9          1.80           \n",
      "DESC       98.55        136        138        27.60          \n",
      "ENTY       72.34        68         94         18.80          \n",
      "HUM        87.69        57         65         13.00          \n",
      "LOC        82.72        67         81         16.20          \n",
      "NUM        84.96        96         113        22.60          \n",
      "--------------------------------------------------------------------------------\n",
      "OVERALL    86.20        431        500        100.00         \n",
      "\n",
      "================================================================================\n",
      "DISCUSSION: FACTORS AFFECTING TOPIC-WISE ACCURACY\n",
      "================================================================================\n",
      "\n",
      "CLASS IMBALANCE IN TRAINING DATA:\n",
      "Topic      Train Count     Train %      Test Count   Test %      \n",
      "----------------------------------------------------------------------\n",
      "ABBR       69              1.58         9            1.80        \n",
      "DESC       930             21.32        138          27.60       \n",
      "ENTY       1000            22.93        94           18.80       \n",
      "HUM        978             22.42        65           13.00       \n",
      "LOC        668             15.31        81           16.20       \n",
      "NUM        717             16.44        113          22.60       \n",
      "\n",
      "================================================================================\n",
      "TOPIC-WISE EVALUATION COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Part 2(e): Topic-wise Accuracy Evaluation on Test Set\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 2(e): TOPIC-WISE ACCURACY EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load the best model from regularization tuning\n",
    "# Use the model from the previous cell (regularization final) if available\n",
    "# Otherwise, load it from the saved checkpoint\n",
    "\n",
    "print(\"\\n>>> Using model from regularization tuning...\")\n",
    "\n",
    "# Check if 'model' variable exists from the previous cell (cell 26)\n",
    "try:\n",
    "    # Try to use the model from the previous cell\n",
    "    if 'model' in locals() or 'model' in globals():\n",
    "        # Verify it's a valid model instance\n",
    "        if hasattr(model, 'embedding') and hasattr(model, 'eval'):\n",
    "            final_model = model\n",
    "            final_model.eval()\n",
    "            saved_vocab_size = final_model.embedding.weight.shape[0]\n",
    "            print(f\"     Using existing model from previous cell\")\n",
    "            print(f\"    Model vocab size: {saved_vocab_size}\")\n",
    "        else:\n",
    "            raise AttributeError(\"Model exists but is not valid\")\n",
    "    else:\n",
    "        raise NameError(\"Model variable not found\")\n",
    "except (NameError, AttributeError):\n",
    "    # Model doesn't exist or is invalid, load from checkpoint\n",
    "    print(\"    Model not found in previous cell, loading from checkpoint...\")\n",
    "    try:\n",
    "        checkpoint = torch.load('weights/rnn_reg_final_best.pt', map_location=device)\n",
    "    except FileNotFoundError:\n",
    "        checkpoint = torch.load('rnn_reg_final_best.pt', map_location=device)\n",
    "    \n",
    "    # Infer configuration from saved state dict\n",
    "    saved_vocab_size = checkpoint['embedding.weight'].shape[0]\n",
    "    saved_hidden_dim = checkpoint['rnn.weight_ih_l0'].shape[0]\n",
    "    has_attention = 'attention.weight' in checkpoint\n",
    "    saved_aggregation = 'attention' if has_attention else 'last'\n",
    "    \n",
    "    # Recreate model\n",
    "    final_model = RNN_Classifier_Aggregation(\n",
    "        vocab_size=saved_vocab_size,\n",
    "        embedding_dim=embedding_dim,\n",
    "        hidden_dim=saved_hidden_dim,\n",
    "        output_dim=num_classes,\n",
    "        n_layers=N_LAYERS,\n",
    "        dropout=best_dropout,\n",
    "        padding_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "        pretrained_embeddings=None,\n",
    "        aggregation=saved_aggregation\n",
    "    ).to(device)\n",
    "    \n",
    "    final_model.load_state_dict(checkpoint, strict=True)\n",
    "    final_model.eval()\n",
    "    print(f\"     Model loaded from checkpoint (vocab_size={saved_vocab_size})\")\n",
    "\n",
    "# Function to evaluate per topic\n",
    "def evaluate_per_topic(model, iterator, device, max_vocab_size=None):\n",
    "    \"\"\"\n",
    "    Evaluate model performance per topic category on the test set.\n",
    "    Returns a dictionary with accuracy for each topic.\n",
    "    \n",
    "    Args:\n",
    "        model: The trained model\n",
    "        iterator: Data iterator for test set\n",
    "        device: Device to run on\n",
    "        max_vocab_size: Maximum valid vocabulary size (to clip token indices)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Dictionary to store correct and total counts per topic\n",
    "    topic_correct = defaultdict(int)\n",
    "    topic_total = defaultdict(int)\n",
    "    \n",
    "    # Get label vocabulary for mapping\n",
    "    label_to_idx = LABEL.vocab.stoi\n",
    "    idx_to_label = LABEL.vocab.itos\n",
    "    \n",
    "    # Get <unk> token index for mapping out-of-range tokens\n",
    "    unk_idx = TEXT.vocab.stoi.get(TEXT.unk_token, 0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            # Process batch (should be on CPU from the iterator)\n",
    "            text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "            \n",
    "            # Clip token indices to valid range if max_vocab_size is specified\n",
    "            # This handles cases where the current vocab is larger than the saved model's vocab\n",
    "            if max_vocab_size is not None:\n",
    "                # Map any indices >= max_vocab_size to <unk> token\n",
    "                text = torch.where(text >= max_vocab_size, \n",
    "                                 torch.tensor(unk_idx, device=text.device, dtype=text.dtype), \n",
    "                                 text)\n",
    "                # Also ensure no negative indices\n",
    "                text = torch.clamp(text, min=0)\n",
    "            \n",
    "            # Move tensors to the actual device after clipping\n",
    "            text = text.to(device)\n",
    "            text_lengths = text_lengths.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            predictions = model(text, text_lengths)\n",
    "            \n",
    "            # Get predicted labels\n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            \n",
    "            # Convert to numpy for easier processing\n",
    "            preds_np = preds.cpu().numpy()\n",
    "            labels_np = labels.cpu().numpy()\n",
    "            \n",
    "            # Count correct and total for each topic\n",
    "            for pred_idx, true_idx in zip(preds_np, labels_np):\n",
    "                true_label = idx_to_label[true_idx]\n",
    "                topic_total[true_label] += 1\n",
    "                \n",
    "                if pred_idx == true_idx:\n",
    "                    topic_correct[true_label] += 1\n",
    "    \n",
    "    # Calculate accuracy per topic\n",
    "    topic_accuracies = {}\n",
    "    for label in sorted(topic_total.keys()):\n",
    "        if topic_total[label] > 0:\n",
    "            acc = topic_correct[label] / topic_total[label]\n",
    "            topic_accuracies[label] = {\n",
    "                'accuracy': acc,\n",
    "                'correct': topic_correct[label],\n",
    "                'total': topic_total[label]\n",
    "            }\n",
    "    \n",
    "    return topic_accuracies\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"\\n>>> Evaluating on test set...\")\n",
    "\n",
    "# Create a CPU iterator to avoid CUDA errors during numericalization with invalid token indices\n",
    "# We'll process on CPU, clip indices, then move to device\n",
    "from torchtext import data\n",
    "cpu_device = torch.device('cpu')\n",
    "test_iter_cpu = data.BucketIterator(\n",
    "    test_data,\n",
    "    batch_size=test_iter.batch_size,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    device=cpu_device,\n",
    "    sort=False,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Pass max_vocab_size to clip token indices to the saved model's vocabulary size\n",
    "topic_accuracies = evaluate_per_topic(final_model, test_iter_cpu, device, max_vocab_size=saved_vocab_size)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TOPIC-WISE ACCURACY ON TEST SET\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Topic':<10} {'Accuracy':<12} {'Correct':<10} {'Total':<10} {'% of Test Set':<15}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Calculate total test samples for percentage calculation\n",
    "total_test_samples = sum(acc['total'] for acc in topic_accuracies.values())\n",
    "\n",
    "for topic in sorted(topic_accuracies.keys()):\n",
    "    acc_info = topic_accuracies[topic]\n",
    "    acc_pct = acc_info['accuracy'] * 100\n",
    "    correct = acc_info['correct']\n",
    "    total = acc_info['total']\n",
    "    pct_of_test = (total / total_test_samples) * 100 if total_test_samples > 0 else 0\n",
    "    \n",
    "    print(f\"{topic:<10} {acc_pct:<12.2f} {correct:<10} {total:<10} {pct_of_test:<15.2f}\")\n",
    "\n",
    "# Calculate overall accuracy\n",
    "overall_correct = sum(acc_info['correct'] for acc_info in topic_accuracies.values())\n",
    "overall_total = sum(acc_info['total'] for acc_info in topic_accuracies.values())\n",
    "overall_acc = overall_correct / overall_total if overall_total > 0 else 0\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'OVERALL':<10} {overall_acc*100:<12.2f} {overall_correct:<10} {overall_total:<10} {'100.00':<15}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Discussion: What may cause differences in accuracies across topics\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DISCUSSION: FACTORS AFFECTING TOPIC-WISE ACCURACY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get training distribution for comparison\n",
    "train_label_counts = Counter([ex.label for ex in train_data.examples])\n",
    "total_train = len(train_data.examples)\n",
    "\n",
    "print(\"\\nCLASS IMBALANCE IN TRAINING DATA:\")\n",
    "print(f\"{'Topic':<10} {'Train Count':<15} {'Train %':<12} {'Test Count':<12} {'Test %':<12}\")\n",
    "print(\"-\" * 70)\n",
    "for topic in sorted(topic_accuracies.keys()):\n",
    "    train_count = train_label_counts.get(topic, 0)\n",
    "    train_pct = (train_count / total_train) * 100 if total_train > 0 else 0\n",
    "    test_count = topic_accuracies[topic]['total']\n",
    "    test_pct = (test_count / total_test_samples) * 100 if total_test_samples > 0 else 0\n",
    "    print(f\"{topic:<10} {train_count:<15} {train_pct:<12.2f} {test_count:<12} {test_pct:<12.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TOPIC-WISE EVALUATION COMPLETE\")\n",
    "print(\"=\"*80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
