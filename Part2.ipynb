{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bc603c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, random, torch\n",
    "\n",
    "# IMPORTANT: Fix for PyTorch/IPython compatibility issue\n",
    "# This must run BEFORE importing torch to avoid decorator conflicts\n",
    "# This fixes the \"disable() got an unexpected keyword argument 'wrapping'\" error\n",
    "\n",
    "# Method 1: Try to disable dynamo via environment variable (needs to be set before import)\n",
    "os.environ.setdefault('TORCH_COMPILE_DISABLE', '1')\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "# Method 2: Patch torch._dynamo.disable decorator after import\n",
    "try:\n",
    "    import torch._dynamo\n",
    "    # Patch the disable function to ignore the 'wrapping' parameter\n",
    "    if hasattr(torch._dynamo, 'disable'):\n",
    "        def patched_disable(fn=None, *args, **kwargs):\n",
    "            # Remove problematic 'wrapping' parameter if present\n",
    "            if 'wrapping' in kwargs:\n",
    "                kwargs.pop('wrapping')\n",
    "            if fn is None:\n",
    "                # Decorator usage: @disable\n",
    "                return lambda f: f\n",
    "            # Function usage: disable(fn) or disable(fn, **kwargs)\n",
    "            # Simply return the function unwrapped to avoid recursion\n",
    "            # The original disable was causing issues, so we bypass it entirely\n",
    "            return fn\n",
    "        torch._dynamo.disable = patched_disable\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not patch torch._dynamo: {e}\")\n",
    "    pass  # If patching fails, continue anyway\n",
    "\n",
    "from torchtext import data\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "\n",
    "os.environ['GENSIM_DATA_DIR'] = os.path.join(os.getcwd(), 'gensim-data')\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from utils import data_prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "650b5e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Prepping Data...\n",
      "[+] Test set formed!\n",
      "[+] Train and Validation sets formed!\n",
      "[+] Data prepped successfully!\n",
      "[*] Retrieving pretrained word embeddings...\n",
      "[*] Loading fasttext model...\n",
      "[+] Model loaded!\n",
      "[*] Forming embedding matrix...\n",
      "[+] Embedding matrix formed!\n",
      "[+] Embeddings retrieved successfully!\n",
      "\n",
      "Label distribution in training set:\n",
      "- ABBR: 69 samples (1.58%)\n",
      "- DESC: 930 samples (21.32%)\n",
      "- ENTY: 1000 samples (22.93%)\n",
      "- HUM: 978 samples (22.42%)\n",
      "- LOC: 668 samples (15.31%)\n",
      "- NUM: 717 samples (16.44%)\n",
      "Total samples: 4362, Sum of percentages: 100.00%\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "train_data, validation_data, test_data, LABEL, TEXT, pretrained_embed = data_prep(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea2b5c8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtext.vocab.Vocab at 0x1dd862d3c20>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dee8ca7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2ac13be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of classes: 6\n",
      "Classes: ['ENTY', 'HUM', 'DESC', 'NUM', 'LOC', 'ABBR']\n"
     ]
    }
   ],
   "source": [
    "### Part 2: Model Training & Evaluation - RNN\n",
    "\n",
    "# Build vocabulary for labels\n",
    "LABEL.build_vocab(train_data)\n",
    "num_classes = len(LABEL.vocab)\n",
    "print(f\"\\nNumber of classes: {num_classes}\")\n",
    "print(f\"Classes: {LABEL.vocab.itos}\")\n",
    "\n",
    "# Create iterators for batching (inline for easier debugging)\n",
    "# train_iterator = data.BucketIterator(...)\n",
    "# val_iterator = data.BucketIterator(...)\n",
    "# test_iterator = data.BucketIterator(...)\n",
    "# (Used directly in Part 2 execution below)\n",
    "\n",
    "\n",
    "class SimpleRNNClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple RNN for topic classification (Baseline - no dropout).\n",
    "    Uses pretrained embeddings (learnable/updated during training) with OOV mitigation \n",
    "    and aggregates word representations to sentence representation using the last hidden state.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, \n",
    "                 n_layers=1, dropout=0.0, padding_idx=0, pretrained_embeddings=None):\n",
    "        super(SimpleRNNClassifier, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # Embedding layer initialized with pretrained embeddings\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "            # IMPORTANT: Make embeddings learnable (updated during training)\n",
    "            # This allows fine-tuning of embeddings including OOV words handled by FastText\n",
    "            self.embedding.weight.requires_grad = True\n",
    "        \n",
    "        # Simple RNN layer (no dropout in baseline)\n",
    "        self.rnn = nn.RNN(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=n_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.0  # No dropout in baseline\n",
    "        )\n",
    "        \n",
    "        # Removed: Dropout layer (baseline has no regularization)\n",
    "        # self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Fully connected output layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        # text: [batch_size, seq_len]\n",
    "        # text_lengths: [batch_size]\n",
    "        \n",
    "        # Embed the input\n",
    "        embedded = self.embedding(text)  # [batch_size, seq_len, embedding_dim]\n",
    "        \n",
    "        # Get dimensions\n",
    "        batch_size = embedded.size(0)\n",
    "        seq_len = embedded.size(1)\n",
    "        \n",
    "        # Handle text_lengths: ensure it's a 1D tensor with batch_size elements\n",
    "        text_lengths_flat = text_lengths.flatten().cpu().long()\n",
    "        \n",
    "        # text_lengths should have exactly batch_size elements (one length per batch item)\n",
    "        if len(text_lengths_flat) != batch_size:\n",
    "            raise ValueError(\n",
    "                f\"text_lengths size mismatch: got {len(text_lengths_flat)} elements, \"\n",
    "                f\"expected {batch_size} (batch_size). text_lengths.shape={text_lengths.shape}, \"\n",
    "                f\"text.shape={text.shape}, embedded.shape={embedded.shape}\"\n",
    "            )\n",
    "        \n",
    "        # Clamp lengths to be at most the sequence length (safety check)\n",
    "        text_lengths_clamped = torch.clamp(text_lengths_flat, min=1, max=seq_len)\n",
    "        \n",
    "        # Pack the padded sequences for efficient processing\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, text_lengths_clamped, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        \n",
    "        # Pass through RNN\n",
    "        packed_output, hidden = self.rnn(packed_embedded)\n",
    "        # hidden: [n_layers, batch_size, hidden_dim]\n",
    "        \n",
    "        # Use the last hidden state from the last layer\n",
    "        last_hidden = hidden[-1]  # [batch_size, hidden_dim]\n",
    "        \n",
    "        # Removed: Apply dropout (baseline has no regularization)\n",
    "        # last_hidden = self.dropout(last_hidden)\n",
    "        \n",
    "        # Pass through fully connected layer\n",
    "        output = self.fc(last_hidden)  # [batch_size, output_dim]\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "# Utility function for counting parameters\n",
    "def count_parameters(model):\n",
    "    \"\"\"Count trainable parameters\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Training and evaluation functions removed - code is now inline below for easier debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "370ed524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8118\n"
     ]
    }
   ],
   "source": [
    "print(len(TEXT.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23562caa",
   "metadata": {},
   "source": [
    "Training order:\n",
    "1. Word aggregation\n",
    "2. Hyperparameters tuning\n",
    "3. Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9b664f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PART 2: SIMPLE RNN MODEL TRAINING\n",
      "================================================================================\n",
      "TEXT.vocab size: 8118\n",
      "FastText embedding vocab size: 8118\n",
      "\n",
      ">>> Training Baseline RNN Model\n",
      "Configuration:\n",
      "  - Hidden Dim: 256\n",
      "  - Layers: 1\n",
      "  - Dropout: 0.0 (Baseline: no regularization)\n",
      "  - Learning Rate: 0.001\n",
      "  - Batch Size: 64\n",
      "  - Epochs: 100 (no early stopping)\n",
      "  - Embedding Dim: 300 (FastText)\n",
      "  - Embeddings: LEARNABLE (updated during training)\n",
      "  - OOV Handling: FastText subword embeddings + trainable <unk> token\n",
      "\n",
      "Starting training for 100 epochs...\n",
      "Device: cuda\n",
      "Trainable parameters: 2,579,790\n",
      "Embedding layer learnable: True\n",
      "--------------------------------------------------------------------------------\n",
      "DEBUG BATCH - text shape: torch.Size([8, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 8])\n",
      "Epoch: 01/100 | Time: 0m 1s\n",
      "\tTrain Loss: 1.5937 | Train Acc: 29.41%\n",
      "\tVal Loss: 1.2681 | Val Acc: 49.91%\n",
      "\t>>> New best model saved with Val Acc: 49.91%\n",
      "DEBUG BATCH - text shape: torch.Size([13, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 13])\n",
      "Epoch: 02/100 | Time: 0m 0s\n",
      "\tTrain Loss: 1.0674 | Train Acc: 55.32%\n",
      "\tVal Loss: 0.9707 | Val Acc: 62.39%\n",
      "\t>>> New best model saved with Val Acc: 62.39%\n",
      "DEBUG BATCH - text shape: torch.Size([8, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 8])\n",
      "Epoch: 03/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.5606 | Train Acc: 81.84%\n",
      "\tVal Loss: 1.3527 | Val Acc: 50.37%\n",
      "DEBUG BATCH - text shape: torch.Size([14, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 14])\n",
      "Epoch: 04/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.2630 | Train Acc: 91.91%\n",
      "\tVal Loss: 1.2619 | Val Acc: 61.19%\n",
      "DEBUG BATCH - text shape: torch.Size([10, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 10])\n",
      "Epoch: 05/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0624 | Train Acc: 98.42%\n",
      "\tVal Loss: 1.3074 | Val Acc: 59.27%\n",
      "DEBUG BATCH - text shape: torch.Size([15, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 15])\n",
      "Epoch: 06/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0214 | Train Acc: 99.72%\n",
      "\tVal Loss: 1.1835 | Val Acc: 68.26%\n",
      "\t>>> New best model saved with Val Acc: 68.26%\n",
      "DEBUG BATCH - text shape: torch.Size([5, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 5])\n",
      "Epoch: 07/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0146 | Train Acc: 99.77%\n",
      "\tVal Loss: 1.8766 | Val Acc: 55.69%\n",
      "DEBUG BATCH - text shape: torch.Size([11, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 11])\n",
      "Epoch: 08/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0549 | Train Acc: 98.56%\n",
      "\tVal Loss: 2.0537 | Val Acc: 56.42%\n",
      "DEBUG BATCH - text shape: torch.Size([17, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 17])\n",
      "Epoch: 09/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0239 | Train Acc: 99.38%\n",
      "\tVal Loss: 1.4335 | Val Acc: 64.95%\n",
      "DEBUG BATCH - text shape: torch.Size([11, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 11])\n",
      "Epoch: 10/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0081 | Train Acc: 99.91%\n",
      "\tVal Loss: 1.7102 | Val Acc: 60.64%\n",
      "DEBUG BATCH - text shape: torch.Size([6, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 6])\n",
      "Epoch: 11/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0046 | Train Acc: 99.95%\n",
      "\tVal Loss: 1.5497 | Val Acc: 65.32%\n",
      "DEBUG BATCH - text shape: torch.Size([11, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 11])\n",
      "Epoch: 12/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0014 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.5853 | Val Acc: 64.77%\n",
      "DEBUG BATCH - text shape: torch.Size([5, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 5])\n",
      "Epoch: 13/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0010 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.6041 | Val Acc: 64.77%\n",
      "DEBUG BATCH - text shape: torch.Size([10, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 10])\n",
      "Epoch: 14/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0008 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.6198 | Val Acc: 64.68%\n",
      "DEBUG BATCH - text shape: torch.Size([5, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 5])\n",
      "Epoch: 15/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0007 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.6344 | Val Acc: 64.77%\n",
      "DEBUG BATCH - text shape: torch.Size([8, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 8])\n",
      "Epoch: 16/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0006 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.6489 | Val Acc: 64.86%\n",
      "DEBUG BATCH - text shape: torch.Size([9, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 9])\n",
      "Epoch: 17/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0005 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.6593 | Val Acc: 64.86%\n",
      "DEBUG BATCH - text shape: torch.Size([37, 10]), text_lengths shape: torch.Size([10]), labels shape: torch.Size([10])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([10, 37])\n",
      "Epoch: 18/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0005 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.6706 | Val Acc: 65.05%\n",
      "DEBUG BATCH - text shape: torch.Size([14, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 14])\n",
      "Epoch: 19/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0004 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.6822 | Val Acc: 64.95%\n",
      "DEBUG BATCH - text shape: torch.Size([16, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 16])\n",
      "Epoch: 20/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0004 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.6925 | Val Acc: 64.95%\n",
      "DEBUG BATCH - text shape: torch.Size([10, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 10])\n",
      "Epoch: 21/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0003 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.7030 | Val Acc: 65.23%\n",
      "DEBUG BATCH - text shape: torch.Size([17, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 17])\n",
      "Epoch: 22/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0003 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.7123 | Val Acc: 64.95%\n",
      "DEBUG BATCH - text shape: torch.Size([8, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 8])\n",
      "Epoch: 23/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0003 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.7214 | Val Acc: 64.86%\n",
      "DEBUG BATCH - text shape: torch.Size([13, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 13])\n",
      "Epoch: 24/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0003 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.7311 | Val Acc: 64.86%\n",
      "DEBUG BATCH - text shape: torch.Size([11, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 11])\n",
      "Epoch: 25/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0002 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.7395 | Val Acc: 64.95%\n",
      "DEBUG BATCH - text shape: torch.Size([6, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 6])\n",
      "Epoch: 26/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0002 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.7484 | Val Acc: 65.05%\n",
      "DEBUG BATCH - text shape: torch.Size([11, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 11])\n",
      "Epoch: 27/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0002 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.7554 | Val Acc: 65.05%\n",
      "DEBUG BATCH - text shape: torch.Size([14, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 14])\n",
      "Epoch: 28/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0002 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.7644 | Val Acc: 65.05%\n",
      "DEBUG BATCH - text shape: torch.Size([7, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 7])\n",
      "Epoch: 29/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0002 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.7718 | Val Acc: 65.05%\n",
      "DEBUG BATCH - text shape: torch.Size([13, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 13])\n",
      "Epoch: 30/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0002 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.7784 | Val Acc: 64.95%\n",
      "DEBUG BATCH - text shape: torch.Size([11, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 11])\n",
      "Epoch: 31/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0002 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.7870 | Val Acc: 65.23%\n",
      "DEBUG BATCH - text shape: torch.Size([10, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 10])\n",
      "Epoch: 32/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.7951 | Val Acc: 65.32%\n",
      "DEBUG BATCH - text shape: torch.Size([15, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 15])\n",
      "Epoch: 33/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.8014 | Val Acc: 65.32%\n",
      "DEBUG BATCH - text shape: torch.Size([9, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 9])\n",
      "Epoch: 34/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.8084 | Val Acc: 65.23%\n",
      "DEBUG BATCH - text shape: torch.Size([8, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 8])\n",
      "Epoch: 35/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.8146 | Val Acc: 65.41%\n",
      "DEBUG BATCH - text shape: torch.Size([8, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 8])\n",
      "Epoch: 36/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.8219 | Val Acc: 65.23%\n",
      "DEBUG BATCH - text shape: torch.Size([9, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 9])\n",
      "Epoch: 37/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.8268 | Val Acc: 65.32%\n",
      "DEBUG BATCH - text shape: torch.Size([15, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 15])\n",
      "Epoch: 38/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.8360 | Val Acc: 65.41%\n",
      "DEBUG BATCH - text shape: torch.Size([14, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 14])\n",
      "Epoch: 39/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.8405 | Val Acc: 65.23%\n",
      "DEBUG BATCH - text shape: torch.Size([5, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 5])\n",
      "Epoch: 40/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.8471 | Val Acc: 65.14%\n",
      "DEBUG BATCH - text shape: torch.Size([8, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 8])\n",
      "Epoch: 41/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.8541 | Val Acc: 65.50%\n",
      "DEBUG BATCH - text shape: torch.Size([10, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 10])\n",
      "Epoch: 42/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.8601 | Val Acc: 65.60%\n",
      "DEBUG BATCH - text shape: torch.Size([5, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 5])\n",
      "Epoch: 43/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.8651 | Val Acc: 65.50%\n",
      "DEBUG BATCH - text shape: torch.Size([12, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 12])\n",
      "Epoch: 44/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.8711 | Val Acc: 65.32%\n",
      "DEBUG BATCH - text shape: torch.Size([14, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 14])\n",
      "Epoch: 45/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.8784 | Val Acc: 65.32%\n",
      "DEBUG BATCH - text shape: torch.Size([8, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 8])\n",
      "Epoch: 46/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.8835 | Val Acc: 65.23%\n",
      "DEBUG BATCH - text shape: torch.Size([8, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 8])\n",
      "Epoch: 47/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.8893 | Val Acc: 65.14%\n",
      "DEBUG BATCH - text shape: torch.Size([9, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 9])\n",
      "Epoch: 48/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.8962 | Val Acc: 65.23%\n",
      "DEBUG BATCH - text shape: torch.Size([5, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 5])\n",
      "Epoch: 49/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.9014 | Val Acc: 65.14%\n",
      "DEBUG BATCH - text shape: torch.Size([10, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 10])\n",
      "Epoch: 50/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.9059 | Val Acc: 65.14%\n",
      "DEBUG BATCH - text shape: torch.Size([6, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 6])\n",
      "Epoch: 51/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.9125 | Val Acc: 65.23%\n",
      "DEBUG BATCH - text shape: torch.Size([12, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 12])\n",
      "Epoch: 52/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.9180 | Val Acc: 65.41%\n",
      "DEBUG BATCH - text shape: torch.Size([7, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 7])\n",
      "Epoch: 53/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.9240 | Val Acc: 65.14%\n",
      "DEBUG BATCH - text shape: torch.Size([6, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 6])\n",
      "Epoch: 54/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.9277 | Val Acc: 65.05%\n",
      "DEBUG BATCH - text shape: torch.Size([9, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 9])\n",
      "Epoch: 55/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.9343 | Val Acc: 65.05%\n",
      "DEBUG BATCH - text shape: torch.Size([8, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 8])\n",
      "Epoch: 56/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.9398 | Val Acc: 65.14%\n",
      "DEBUG BATCH - text shape: torch.Size([22, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 22])\n",
      "Epoch: 57/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.9454 | Val Acc: 65.05%\n",
      "DEBUG BATCH - text shape: torch.Size([13, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 13])\n",
      "Epoch: 58/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.9511 | Val Acc: 65.14%\n",
      "DEBUG BATCH - text shape: torch.Size([9, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 9])\n",
      "Epoch: 59/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.9557 | Val Acc: 65.14%\n",
      "DEBUG BATCH - text shape: torch.Size([11, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 11])\n",
      "Epoch: 60/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.9620 | Val Acc: 65.05%\n",
      "DEBUG BATCH - text shape: torch.Size([5, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 5])\n",
      "Epoch: 61/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.9667 | Val Acc: 65.14%\n",
      "DEBUG BATCH - text shape: torch.Size([5, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 5])\n",
      "Epoch: 62/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.9722 | Val Acc: 64.95%\n",
      "DEBUG BATCH - text shape: torch.Size([13, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 13])\n",
      "Epoch: 63/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.9775 | Val Acc: 65.14%\n",
      "DEBUG BATCH - text shape: torch.Size([13, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 13])\n",
      "Epoch: 64/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.9824 | Val Acc: 65.23%\n",
      "DEBUG BATCH - text shape: torch.Size([8, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 8])\n",
      "Epoch: 65/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.9881 | Val Acc: 65.32%\n",
      "DEBUG BATCH - text shape: torch.Size([10, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 10])\n",
      "Epoch: 66/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.9933 | Val Acc: 65.23%\n",
      "DEBUG BATCH - text shape: torch.Size([18, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 18])\n",
      "Epoch: 67/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.9984 | Val Acc: 65.05%\n",
      "DEBUG BATCH - text shape: torch.Size([7, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 7])\n",
      "Epoch: 68/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.0025 | Val Acc: 65.23%\n",
      "DEBUG BATCH - text shape: torch.Size([10, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 10])\n",
      "Epoch: 69/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.0075 | Val Acc: 65.32%\n",
      "DEBUG BATCH - text shape: torch.Size([12, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 12])\n",
      "Epoch: 70/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.0132 | Val Acc: 65.32%\n",
      "DEBUG BATCH - text shape: torch.Size([4, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 4])\n",
      "Epoch: 71/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.0179 | Val Acc: 65.32%\n",
      "DEBUG BATCH - text shape: torch.Size([10, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 10])\n",
      "Epoch: 72/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.0231 | Val Acc: 65.41%\n",
      "DEBUG BATCH - text shape: torch.Size([14, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 14])\n",
      "Epoch: 73/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.0295 | Val Acc: 65.60%\n",
      "DEBUG BATCH - text shape: torch.Size([8, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 8])\n",
      "Epoch: 74/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.0328 | Val Acc: 65.50%\n",
      "DEBUG BATCH - text shape: torch.Size([17, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 17])\n",
      "Epoch: 75/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.0386 | Val Acc: 65.60%\n",
      "DEBUG BATCH - text shape: torch.Size([12, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 12])\n",
      "Epoch: 76/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.0430 | Val Acc: 65.60%\n",
      "DEBUG BATCH - text shape: torch.Size([9, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 9])\n",
      "Epoch: 77/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.0482 | Val Acc: 65.60%\n",
      "DEBUG BATCH - text shape: torch.Size([7, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 7])\n",
      "Epoch: 78/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.0536 | Val Acc: 65.50%\n",
      "DEBUG BATCH - text shape: torch.Size([9, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 9])\n",
      "Epoch: 79/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.0586 | Val Acc: 65.41%\n",
      "DEBUG BATCH - text shape: torch.Size([12, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 12])\n",
      "Epoch: 80/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.0636 | Val Acc: 65.41%\n",
      "DEBUG BATCH - text shape: torch.Size([8, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 8])\n",
      "Epoch: 81/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.0681 | Val Acc: 65.41%\n",
      "DEBUG BATCH - text shape: torch.Size([8, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 8])\n",
      "Epoch: 82/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.0733 | Val Acc: 65.41%\n",
      "DEBUG BATCH - text shape: torch.Size([7, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 7])\n",
      "Epoch: 83/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.0774 | Val Acc: 65.50%\n",
      "DEBUG BATCH - text shape: torch.Size([12, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 12])\n",
      "Epoch: 84/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.0829 | Val Acc: 65.60%\n",
      "DEBUG BATCH - text shape: torch.Size([37, 10]), text_lengths shape: torch.Size([10]), labels shape: torch.Size([10])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([10, 37])\n",
      "Epoch: 85/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.0883 | Val Acc: 65.50%\n",
      "DEBUG BATCH - text shape: torch.Size([11, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 11])\n",
      "Epoch: 86/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.0913 | Val Acc: 65.60%\n",
      "DEBUG BATCH - text shape: torch.Size([10, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 10])\n",
      "Epoch: 87/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.0986 | Val Acc: 65.78%\n",
      "DEBUG BATCH - text shape: torch.Size([7, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 7])\n",
      "Epoch: 88/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.1025 | Val Acc: 65.78%\n",
      "DEBUG BATCH - text shape: torch.Size([6, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 6])\n",
      "Epoch: 89/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.1073 | Val Acc: 65.78%\n",
      "DEBUG BATCH - text shape: torch.Size([9, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 9])\n",
      "Epoch: 90/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.1119 | Val Acc: 65.78%\n",
      "DEBUG BATCH - text shape: torch.Size([7, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 7])\n",
      "Epoch: 91/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.1155 | Val Acc: 65.69%\n",
      "DEBUG BATCH - text shape: torch.Size([11, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 11])\n",
      "Epoch: 92/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.1210 | Val Acc: 65.60%\n",
      "DEBUG BATCH - text shape: torch.Size([11, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 11])\n",
      "Epoch: 93/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.1270 | Val Acc: 65.87%\n",
      "DEBUG BATCH - text shape: torch.Size([8, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 8])\n",
      "Epoch: 94/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.1313 | Val Acc: 65.87%\n",
      "DEBUG BATCH - text shape: torch.Size([12, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 12])\n",
      "Epoch: 95/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.1356 | Val Acc: 65.96%\n",
      "DEBUG BATCH - text shape: torch.Size([7, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 7])\n",
      "Epoch: 96/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.1420 | Val Acc: 65.87%\n",
      "DEBUG BATCH - text shape: torch.Size([9, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 9])\n",
      "Epoch: 97/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.1453 | Val Acc: 65.87%\n",
      "DEBUG BATCH - text shape: torch.Size([10, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 10])\n",
      "Epoch: 98/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.1514 | Val Acc: 65.96%\n",
      "DEBUG BATCH - text shape: torch.Size([12, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 12])\n",
      "Epoch: 99/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.1552 | Val Acc: 65.96%\n",
      "DEBUG BATCH - text shape: torch.Size([10, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 10])\n",
      "Epoch: 100/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.1598 | Val Acc: 66.06%\n",
      "--------------------------------------------------------------------------------\n",
      "Training completed! Best validation accuracy: 68.26%\n",
      "Total epochs trained: 100\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Part 2: Initial Simple RNN Model Training\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 2: SIMPLE RNN MODEL TRAINING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get pretrained embeddings from Part 1 (frozen embeddings)\n",
    "# TODO: Check if this step is redundant\n",
    "pretrained_embeddings = pretrained_embed.weight.data\n",
    "\n",
    "# Get embedding dimension and vocab size from the fasttext embedding layer\n",
    "embedding_dim = pretrained_embed.weight.shape[1]\n",
    "embedding_vocab_size = pretrained_embed.weight.shape[0]  # Vocab size from saved embedding\n",
    "\n",
    "# Verify vocab sizes match (they might differ if vocab was rebuilt)\n",
    "print(f\"TEXT.vocab size: {len(TEXT.vocab)}\")\n",
    "print(f\"FastText embedding vocab size: {embedding_vocab_size}\")\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "HIDDEN_DIM = 256\n",
    "N_LAYERS = 1\n",
    "DROPOUT = 0.0  # Baseline: no dropout\n",
    "N_EPOCHS = 100\n",
    "LEARNING_RATE = 0.001\n",
    "# Removed: PATIENCE = 10  # Baseline: no early stopping\n",
    "\n",
    "# Create data iterators (inline for easier debugging)\n",
    "# Note: Different sequence lengths per batch are normal - BucketIterator groups similar-length sequences\n",
    "train_iterator = data.BucketIterator(\n",
    "    train_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    shuffle=True,  # Shuffle for training\n",
    "    device=device\n",
    ")\n",
    "\n",
    "val_iterator = data.BucketIterator(\n",
    "    validation_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    shuffle=False,  # No shuffle for validation (deterministic)\n",
    "    device=device\n",
    ")\n",
    "\n",
    "test_iterator = data.BucketIterator(\n",
    "    test_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    shuffle=False,  # No shuffle for test (deterministic)\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Initialize simple RNN model (Baseline)\n",
    "# Use vocab size from loaded embedding to match the saved weights exactly\n",
    "model = SimpleRNNClassifier(\n",
    "    vocab_size=embedding_vocab_size,  # Must match saved embedding vocab size\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    output_dim=num_classes,\n",
    "    n_layers=N_LAYERS,\n",
    "    dropout=0.0,  # Baseline: no dropout\n",
    "    padding_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "    pretrained_embeddings=pretrained_embeddings\n",
    ").to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(f\"\\n>>> Training Baseline RNN Model\")\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  - Hidden Dim: {HIDDEN_DIM}\")\n",
    "print(f\"  - Layers: {N_LAYERS}\")\n",
    "print(f\"  - Dropout: {DROPOUT} (Baseline: no regularization)\")\n",
    "print(f\"  - Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"  - Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  - Epochs: {N_EPOCHS} (no early stopping)\")\n",
    "print(f\"  - Embedding Dim: {embedding_dim} (FastText)\")\n",
    "print(f\"  - Embeddings: LEARNABLE (updated during training)\")\n",
    "print(f\"  - OOV Handling: FastText subword embeddings + trainable <unk> token\")\n",
    "\n",
    "# ============================================================================\n",
    "# Helper function to process batches consistently\n",
    "# ============================================================================\n",
    "\n",
    "def process_batch(batch, debug=False):\n",
    "    \"\"\"\n",
    "    Process a batch from BucketIterator, handling text transpose correctly.\n",
    "    Returns: text, text_lengths, labels (all properly formatted)\n",
    "    \"\"\"\n",
    "    text, text_lengths = batch.text\n",
    "    labels = batch.label\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"DEBUG BATCH - text shape: {text.shape}, text_lengths shape: {text_lengths.shape}, labels shape: {labels.shape}\")\n",
    "    \n",
    "    # torchtext BucketIterator returns text as [seq_len, batch_size] by default\n",
    "    # We need [batch_size, seq_len] for batch_first=True in the model\n",
    "    expected_batch_size = labels.shape[0]\n",
    "    \n",
    "    if text.dim() == 2:\n",
    "        if text.shape[1] == expected_batch_size and len(text_lengths) == expected_batch_size:\n",
    "            # text is [seq_len, batch_size], transpose to [batch_size, seq_len]\n",
    "            text = text.transpose(0, 1)\n",
    "            if debug:\n",
    "                print(f\"DEBUG BATCH - Transposed text to [batch_size, seq_len]: {text.shape}\")\n",
    "        elif text.shape[0] == expected_batch_size and len(text_lengths) == expected_batch_size:\n",
    "            # text is already [batch_size, seq_len]\n",
    "            if debug:\n",
    "                print(f\"DEBUG BATCH - text already in correct format: {text.shape}\")\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Cannot determine text format: text.shape={text.shape}, \"\n",
    "                f\"text_lengths.shape={text_lengths.shape}, labels.shape={labels.shape}\"\n",
    "            )\n",
    "    \n",
    "    # Verify dimensions match\n",
    "    assert text.shape[0] == len(text_lengths) == labels.shape[0], \\\n",
    "        f\"Batch size mismatch: text.shape[0]={text.shape[0]}, len(text_lengths)={len(text_lengths)}, labels.shape[0]={labels.shape[0]}\"\n",
    "    \n",
    "    return text, text_lengths, labels\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "# Ensure deterministic behavior\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Note: PyTorch/IPython compatibility fix is already applied in cell 0\n",
    "# The torch._dynamo.disable decorator has been patched to handle the 'wrapping' parameter\n",
    "\n",
    "# ============================================================================\n",
    "# Training Loop (inline for easier debugging)\n",
    "# ============================================================================\n",
    "\n",
    "best_val_acc = 0\n",
    "# Removed: patience_counter = 0  # Baseline: no early stopping\n",
    "\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "\n",
    "print(f\"\\nStarting training for {N_EPOCHS} epochs...\")  # Removed \"up to\" - no early stopping\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Trainable parameters: {count_parameters(model):,}\")\n",
    "print(f\"Embedding layer learnable: {model.embedding.weight.requires_grad}\")\n",
    "# Removed: print(f\"Early stopping patience: {PATIENCE} epochs\")  # Baseline: no early stopping\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Training for one epoch (inline)\n",
    "    # ========================================================================\n",
    "    model.train()\n",
    "    train_epoch_loss = 0\n",
    "    train_all_preds = []\n",
    "    train_all_labels = []\n",
    "    \n",
    "    batch_idx = 0\n",
    "    for batch in train_iterator:\n",
    "        # Process batch (with debug only for first batch)\n",
    "        text, text_lengths, labels = process_batch(batch, debug=(batch_idx == 0))\n",
    "        batch_idx += 1\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        predictions = model(text, text_lengths)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(predictions, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Removed: Gradient clipping (baseline has no gradient clipping)\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_epoch_loss += loss.item()\n",
    "        \n",
    "        # Store predictions and labels for metrics\n",
    "        preds = torch.argmax(predictions, dim=1)\n",
    "        train_all_preds.extend(preds.cpu().numpy())\n",
    "        train_all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate training accuracy\n",
    "    train_acc = accuracy_score(train_all_labels, train_all_preds)\n",
    "    train_loss = train_epoch_loss / len(train_iterator)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Validation evaluation (inline)\n",
    "    # ========================================================================\n",
    "    model.eval()\n",
    "    val_epoch_loss = 0\n",
    "    val_all_preds = []\n",
    "    val_all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_iterator:\n",
    "            # Process batch consistently with training\n",
    "            text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "            \n",
    "            # Forward pass\n",
    "            predictions = model(text, text_lengths)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(predictions, labels)\n",
    "            val_epoch_loss += loss.item()\n",
    "            \n",
    "            # Store predictions and labels\n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            val_all_preds.extend(preds.cpu().numpy())\n",
    "            val_all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate validation accuracy\n",
    "    val_acc = accuracy_score(val_all_labels, val_all_preds)\n",
    "    val_loss = val_epoch_loss / len(val_iterator)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Logging (without early stopping)\n",
    "    # ========================================================================\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = divmod(end_time - start_time, 60)\n",
    "    \n",
    "    # Store metrics\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02}/{N_EPOCHS} | Time: {int(epoch_mins)}m {int(epoch_secs)}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\tVal Loss: {val_loss:.4f} | Val Acc: {val_acc*100:.2f}%')\n",
    "    \n",
    "    # Track best model (but don't stop early - baseline trains for all epochs)\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        # Save best model\n",
    "        torch.save(model.state_dict(), 'rnn_simple_best.pt')\n",
    "        print(f'\\t>>> New best model saved with Val Acc: {val_acc*100:.2f}%')\n",
    "    # Removed: Early stopping break logic (baseline trains for all epochs)\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(f\"Training completed! Best validation accuracy: {best_val_acc*100:.2f}%\")\n",
    "print(f\"Total epochs trained: {N_EPOCHS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08f521a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ea22748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "VALIDATION SET EVALUATION (Best Model)\n",
      "================================================================================\n",
      "\n",
      ">>> Validation Set Results (Best Model):\n",
      "Validation Loss: 1.1835\n",
      "Validation Accuracy: 68.26%\n",
      "Validation F1 Score: 0.6779\n",
      "Validation AUC-ROC: 0.8943\n",
      "\n",
      "================================================================================\n",
      "TEST SET EVALUATION\n",
      "================================================================================\n",
      "\n",
      ">>> Test Set Results:\n",
      "Test Loss: 0.7079\n",
      "Test Accuracy: 79.20%\n",
      "Test F1 Score: 0.7921\n",
      "Test AUC-ROC: 0.9388\n",
      "\n",
      "================================================================================\n",
      "PART 2 INITIAL TRAINING COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Validation Set Evaluation (inline) - Evaluate best model on validation set\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VALIDATION SET EVALUATION (Best Model)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load best model and evaluate on validation set\n",
    "model.load_state_dict(torch.load('rnn_simple_best.pt'))\n",
    "\n",
    "model.eval()\n",
    "val_eval_loss = 0\n",
    "val_eval_preds = []\n",
    "val_eval_labels = []\n",
    "val_eval_probs = []  # Store probabilities for AUC-ROC\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_iterator:\n",
    "        text, text_lengths = batch.text\n",
    "        labels = batch.label\n",
    "        \n",
    "        # Process batch consistently\n",
    "        text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "        \n",
    "        # Forward pass\n",
    "        predictions = model(text, text_lengths)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(predictions, labels)\n",
    "        val_eval_loss += loss.item()\n",
    "        \n",
    "        # Store predictions, labels, and probabilities\n",
    "        probs = torch.softmax(predictions, dim=1)\n",
    "        preds = torch.argmax(predictions, dim=1)\n",
    "        val_eval_preds.extend(preds.cpu().numpy())\n",
    "        val_eval_labels.extend(labels.cpu().numpy())\n",
    "        val_eval_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "# Calculate validation metrics\n",
    "val_eval_acc = accuracy_score(val_eval_labels, val_eval_preds)\n",
    "val_eval_f1 = f1_score(val_eval_labels, val_eval_preds, average='weighted')\n",
    "val_eval_loss_final = val_eval_loss / len(val_iterator)\n",
    "\n",
    "# Calculate AUC-ROC (one-vs-rest for multiclass)\n",
    "try:\n",
    "    val_eval_probs_array = np.array(val_eval_probs)\n",
    "    val_eval_labels_bin = label_binarize(val_eval_labels, classes=range(num_classes))\n",
    "    val_eval_auc = roc_auc_score(val_eval_labels_bin, val_eval_probs_array, average='weighted', multi_class='ovr')\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not calculate AUC-ROC: {e}\")\n",
    "    val_eval_auc = 0.0\n",
    "\n",
    "print(f\"\\n>>> Validation Set Results (Best Model):\")\n",
    "print(f\"Validation Loss: {val_eval_loss_final:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_eval_acc*100:.2f}%\")\n",
    "print(f\"Validation F1 Score: {val_eval_f1:.4f}\")\n",
    "print(f\"Validation AUC-ROC: {val_eval_auc:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Test Set Evaluation (inline)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST SET EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "model.eval()\n",
    "test_epoch_loss = 0\n",
    "test_all_preds = []\n",
    "test_all_labels = []\n",
    "test_all_probs = []  # Store probabilities for AUC-ROC\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_iterator:\n",
    "        text, text_lengths = batch.text\n",
    "        labels = batch.label\n",
    "        \n",
    "        # Process batch consistently\n",
    "        text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "        \n",
    "        # Forward pass\n",
    "        predictions = model(text, text_lengths)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(predictions, labels)\n",
    "        test_epoch_loss += loss.item()\n",
    "        \n",
    "        # Store predictions, labels, and probabilities\n",
    "        probs = torch.softmax(predictions, dim=1)\n",
    "        preds = torch.argmax(predictions, dim=1)\n",
    "        test_all_preds.extend(preds.cpu().numpy())\n",
    "        test_all_labels.extend(labels.cpu().numpy())\n",
    "        test_all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "# Calculate test metrics\n",
    "test_acc = accuracy_score(test_all_labels, test_all_preds)\n",
    "test_f1 = f1_score(test_all_labels, test_all_preds, average='weighted')\n",
    "test_loss = test_epoch_loss / len(test_iterator)\n",
    "\n",
    "# Calculate AUC-ROC (one-vs-rest for multiclass)\n",
    "try:\n",
    "    test_all_probs_array = np.array(test_all_probs)\n",
    "    test_all_labels_bin = label_binarize(test_all_labels, classes=range(num_classes))\n",
    "    test_auc = roc_auc_score(test_all_labels_bin, test_all_probs_array, average='weighted', multi_class='ovr')\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not calculate AUC-ROC: {e}\")\n",
    "    test_auc = 0.0\n",
    "\n",
    "print(f\"\\n>>> Test Set Results:\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n",
    "print(f\"Test F1 Score: {test_f1:.4f}\")\n",
    "print(f\"Test AUC-ROC: {test_auc:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 2 INITIAL TRAINING COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c50c08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5090ea4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PART 2.2: SEQUENTIAL HYPERPARAMETER TUNING\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "STEP 0: EPOCH + EARLY STOPPING TUNING\n",
      "================================================================================\n",
      "Testing different MAX_EPOCHS and PATIENCE configurations\n",
      "Total combinations to test: 3\n",
      "Combinations (Max_Epochs, Patience):\n",
      "  1. Max_Epochs=100, Patience=10\n",
      "  2. Max_Epochs=200, Patience=10\n",
      "  3. Max_Epochs=300, Patience=10\n",
      "\n",
      ">>> Testing: Step 0 Config 1/3\n",
      "    LR=0.001, Batch=64, Hidden=256, Opt=Adam\n",
      "    Epochs=100 (NO early stopping - training for full 100 epochs)\n",
      "    Epoch 10/100: Train Acc=100.00%, Val Acc=64.77%\n",
      "    Epoch 20/100: Train Acc=100.00%, Val Acc=64.59%\n",
      "    Epoch 30/100: Train Acc=100.00%, Val Acc=64.22%\n",
      "    Epoch 40/100: Train Acc=100.00%, Val Acc=64.68%\n",
      "    Epoch 50/100: Train Acc=100.00%, Val Acc=65.05%\n",
      "    Epoch 60/100: Train Acc=100.00%, Val Acc=65.23%\n",
      "    Epoch 70/100: Train Acc=100.00%, Val Acc=64.95%\n",
      "    Epoch 80/100: Train Acc=100.00%, Val Acc=64.59%\n",
      "    Epoch 90/100: Train Acc=100.00%, Val Acc=64.68%\n",
      "    Epoch 100/100: Train Acc=100.00%, Val Acc=64.77%\n",
      "    Final Val Acc: 64.77% | Best Val Acc: 65.32% (at epoch 54/100)\n",
      "\n",
      ">>> Testing: Step 0 Config 2/3\n",
      "    LR=0.001, Batch=64, Hidden=256, Opt=Adam\n",
      "    Epochs=200 (NO early stopping - training for full 200 epochs)\n",
      "    Epoch 10/200: Train Acc=100.00%, Val Acc=64.77%\n",
      "    Epoch 20/200: Train Acc=100.00%, Val Acc=64.59%\n",
      "    Epoch 30/200: Train Acc=100.00%, Val Acc=64.22%\n",
      "    Epoch 40/200: Train Acc=100.00%, Val Acc=64.68%\n",
      "    Epoch 50/200: Train Acc=100.00%, Val Acc=65.05%\n",
      "    Epoch 60/200: Train Acc=100.00%, Val Acc=65.23%\n",
      "    Epoch 70/200: Train Acc=100.00%, Val Acc=64.95%\n",
      "    Epoch 80/200: Train Acc=100.00%, Val Acc=64.59%\n",
      "    Epoch 90/200: Train Acc=100.00%, Val Acc=64.68%\n",
      "    Epoch 100/200: Train Acc=100.00%, Val Acc=64.77%\n",
      "    Epoch 110/200: Train Acc=100.00%, Val Acc=64.77%\n",
      "    Epoch 120/200: Train Acc=100.00%, Val Acc=64.95%\n",
      "    Epoch 130/200: Train Acc=100.00%, Val Acc=65.05%\n",
      "    Epoch 140/200: Train Acc=100.00%, Val Acc=65.05%\n",
      "    Epoch 150/200: Train Acc=100.00%, Val Acc=65.05%\n",
      "    Epoch 160/200: Train Acc=100.00%, Val Acc=64.86%\n",
      "    Epoch 170/200: Train Acc=100.00%, Val Acc=65.14%\n",
      "    Epoch 180/200: Train Acc=100.00%, Val Acc=64.95%\n",
      "    Epoch 190/200: Train Acc=100.00%, Val Acc=65.14%\n",
      "    Epoch 200/200: Train Acc=100.00%, Val Acc=65.05%\n",
      "    Final Val Acc: 65.05% | Best Val Acc: 65.41% (at epoch 138/200)\n",
      "\n",
      ">>> Testing: Step 0 Config 3/3\n",
      "    LR=0.001, Batch=64, Hidden=256, Opt=Adam\n",
      "    Epochs=300 (NO early stopping - training for full 300 epochs)\n",
      "    Epoch 10/300: Train Acc=100.00%, Val Acc=64.77%\n",
      "    Epoch 20/300: Train Acc=100.00%, Val Acc=64.59%\n",
      "    Epoch 30/300: Train Acc=100.00%, Val Acc=64.22%\n",
      "    Epoch 40/300: Train Acc=100.00%, Val Acc=64.68%\n",
      "    Epoch 50/300: Train Acc=100.00%, Val Acc=65.05%\n",
      "    Epoch 60/300: Train Acc=100.00%, Val Acc=65.23%\n",
      "    Epoch 70/300: Train Acc=100.00%, Val Acc=64.95%\n",
      "    Epoch 80/300: Train Acc=100.00%, Val Acc=64.59%\n",
      "    Epoch 90/300: Train Acc=100.00%, Val Acc=64.68%\n",
      "    Epoch 100/300: Train Acc=100.00%, Val Acc=64.77%\n",
      "    Epoch 110/300: Train Acc=100.00%, Val Acc=64.77%\n",
      "    Epoch 120/300: Train Acc=100.00%, Val Acc=64.95%\n",
      "    Epoch 130/300: Train Acc=100.00%, Val Acc=65.05%\n",
      "    Epoch 140/300: Train Acc=100.00%, Val Acc=65.05%\n",
      "    Epoch 150/300: Train Acc=100.00%, Val Acc=65.05%\n",
      "    Epoch 160/300: Train Acc=100.00%, Val Acc=64.86%\n",
      "    Epoch 170/300: Train Acc=100.00%, Val Acc=65.14%\n",
      "    Epoch 180/300: Train Acc=100.00%, Val Acc=64.95%\n",
      "    Epoch 190/300: Train Acc=100.00%, Val Acc=65.14%\n",
      "    Epoch 200/300: Train Acc=100.00%, Val Acc=65.05%\n",
      "    Epoch 210/300: Train Acc=100.00%, Val Acc=64.59%\n",
      "    Epoch 220/300: Train Acc=100.00%, Val Acc=64.86%\n",
      "    Epoch 230/300: Train Acc=100.00%, Val Acc=64.86%\n",
      "    Epoch 240/300: Train Acc=100.00%, Val Acc=64.86%\n",
      "    Epoch 250/300: Train Acc=100.00%, Val Acc=64.86%\n",
      "    Epoch 260/300: Train Acc=100.00%, Val Acc=64.86%\n",
      "    Epoch 270/300: Train Acc=100.00%, Val Acc=64.86%\n",
      "    Epoch 280/300: Train Acc=100.00%, Val Acc=64.77%\n",
      "    Epoch 290/300: Train Acc=100.00%, Val Acc=64.77%\n",
      "    Epoch 300/300: Train Acc=100.00%, Val Acc=64.77%\n",
      "    Final Val Acc: 64.77% | Best Val Acc: 65.41% (at epoch 138/300)\n",
      "\n",
      ">>> Step 0 Results:\n",
      "#    Epochs   Val Acc    Best At Epoch   Total Trained  \n",
      "------------------------------------------------------------\n",
      "1    100      65.32     % 54              100            \n",
      "2    200      65.41     % 138             200            \n",
      "3    300      65.41     % 138             300            \n",
      "\n",
      ">>> Best from Step 0: Epochs=200, Val Acc=65.41%\n",
      "    Best validation accuracy was achieved at epoch 138 out of 200\n",
      "\n",
      ">>> Using MAX_EPOCHS=200 and PATIENCE=7 for subsequent steps (with early stopping)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Part 2.2: Sequential Hyperparameter Tuning (One Variable at a Time)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 2.2: SEQUENTIAL HYPERPARAMETER TUNING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# ============================================================================\n",
    "# Step 0: Epoch + Early Stopping Configuration Tuning\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 0: EPOCH + EARLY STOPPING TUNING\")\n",
    "print(\"=\"*80)\n",
    "print(\"Testing different MAX_EPOCHS and PATIENCE configurations\")\n",
    "\n",
    "# Test different epoch and patience configurations\n",
    "max_epochs_options = [100, 200, 300]\n",
    "patience = 10\n",
    "\n",
    "# Use baseline config for testing epoch settings\n",
    "baseline_config = {\n",
    "    'lr': 0.001,\n",
    "    'batch_size': 64,\n",
    "    'hidden_dim': 256,\n",
    "    'optimizer': 'Adam'\n",
    "}\n",
    "\n",
    "step0_configs = []\n",
    "for max_epochs in max_epochs_options:\n",
    "        step0_configs.append({\n",
    "            'config': baseline_config.copy(),\n",
    "            'max_epochs': max_epochs,\n",
    "            'patience': patience\n",
    "        })\n",
    "\n",
    "print(f\"Total combinations to test: {len(step0_configs)}\")\n",
    "print(\"Combinations (Max_Epochs, Patience):\")\n",
    "for idx, ep_config in enumerate(step0_configs, 1):\n",
    "    print(f\"  {idx}. Max_Epochs={ep_config['max_epochs']}, Patience={ep_config['patience']}\")\n",
    "\n",
    "# Helper function to train with specific epoch/patience settings\n",
    "def train_and_evaluate_with_epochs(config, max_epochs, patience, config_name=\"config\"):\n",
    "    \"\"\"Train a model for specific number of epochs WITHOUT early stopping\"\"\"\n",
    "    print(f\"\\n>>> Testing: {config_name}\")\n",
    "    print(f\"    LR={config['lr']}, Batch={config['batch_size']}, Hidden={config['hidden_dim']}, Opt={config['optimizer']}\")\n",
    "    print(f\"    Epochs={max_epochs} (NO early stopping - training for full {max_epochs} epochs)\")\n",
    "    \n",
    "    # Create iterators with specific batch size\n",
    "    train_iter = data.BucketIterator(\n",
    "        train_data,\n",
    "        batch_size=config['batch_size'],\n",
    "        sort_key=lambda x: len(x.text),\n",
    "        sort_within_batch=True,\n",
    "        shuffle=True,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    val_iter = data.BucketIterator(\n",
    "        validation_data,\n",
    "        batch_size=config['batch_size'],\n",
    "        sort_key=lambda x: len(x.text),\n",
    "        sort_within_batch=True,\n",
    "        shuffle=False,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Reset random seeds INSIDE function to ensure fresh model for each config\n",
    "    # This is critical to ensure each max_epochs config starts from scratch\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "        torch.cuda.manual_seed_all(SEED)\n",
    "    # Ensure deterministic behavior\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    # Create model with specific hidden dimension\n",
    "    model = SimpleRNNClassifier(\n",
    "        vocab_size=embedding_vocab_size,\n",
    "        embedding_dim=embedding_dim,\n",
    "        hidden_dim=config['hidden_dim'],\n",
    "        output_dim=num_classes,\n",
    "        n_layers=N_LAYERS,\n",
    "        dropout=0.0,  # Baseline: no dropout\n",
    "        padding_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "        pretrained_embeddings=pretrained_embeddings\n",
    "    ).to(device)\n",
    "    \n",
    "    # Select optimizer\n",
    "    if config['optimizer'] == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config['lr'])\n",
    "    elif config['optimizer'] == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=config['lr'], momentum=0.9)\n",
    "    elif config['optimizer'] == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=config['lr'])\n",
    "    elif config['optimizer'] == 'Adagrad':\n",
    "        optimizer = optim.Adagrad(model.parameters(), lr=config['lr'])\n",
    "    \n",
    "    # Training loop WITHOUT early stopping - train for full num_epochs\n",
    "    best_val_acc = 0.0\n",
    "    best_val_acc_at_epoch = 0\n",
    "    final_val_acc = 0.0\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        \n",
    "        for batch in train_iter:\n",
    "            text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(text, text_lengths)\n",
    "            loss = criterion(predictions, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        train_acc = accuracy_score(train_labels, train_preds)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_iter:\n",
    "                text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "                predictions = model(text, text_lengths)\n",
    "                loss = criterion(predictions, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                preds = torch.argmax(predictions, dim=1)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_acc = accuracy_score(val_labels, val_preds)\n",
    "        \n",
    "        # Track best validation accuracy (but don't stop early)\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_val_acc_at_epoch = epoch + 1\n",
    "        \n",
    "        final_val_acc = val_acc  # Store final epoch's validation accuracy\n",
    "        \n",
    "        # Optional: print progress every 10 epochs or at the end\n",
    "        if (epoch + 1) % 10 == 0 or (epoch + 1) == max_epochs:\n",
    "            print(f\"    Epoch {epoch+1}/{max_epochs}: Train Acc={train_acc*100:.2f}%, Val Acc={val_acc*100:.2f}%\")\n",
    "    \n",
    "    print(f\"    Final Val Acc: {final_val_acc*100:.2f}% | Best Val Acc: {best_val_acc*100:.2f}% (at epoch {best_val_acc_at_epoch}/{max_epochs})\")\n",
    "    return best_val_acc, best_val_acc_at_epoch, max_epochs\n",
    "\n",
    "step0_results = []\n",
    "for idx, ep_config in enumerate(step0_configs):\n",
    "    # Set fixed seed for reproducibility - ensures consistent batch ordering\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "    \n",
    "    val_acc, best_epoch, total_epochs = train_and_evaluate_with_epochs(\n",
    "        ep_config['config'],\n",
    "        ep_config['max_epochs'],\n",
    "        ep_config['patience'],\n",
    "        f\"Step 0 Config {idx+1}/{len(step0_configs)}\"\n",
    "    )\n",
    "    step0_results.append({\n",
    "        'num_epochs': ep_config['max_epochs'],\n",
    "        'val_acc': val_acc,\n",
    "        'best_epoch': best_epoch,\n",
    "        'total_epochs': total_epochs\n",
    "    })\n",
    "\n",
    "# Find best epoch configuration\n",
    "best_step0 = max(step0_results, key=lambda x: x['val_acc'])\n",
    "BEST_EPOCHS = best_step0['num_epochs']\n",
    "\n",
    "# Set appropriate MAX_EPOCHS and PATIENCE for subsequent steps\n",
    "# Use the best number of epochs with some buffer, and set a reasonable patience\n",
    "MAX_EPOCHS = BEST_EPOCHS\n",
    "PATIENCE = 7  # Default patience for early stopping in subsequent steps\n",
    "\n",
    "print(f\"\\n>>> Step 0 Results:\")\n",
    "print(f\"{'#':<4} {'Epochs':<8} {'Val Acc':<10} {'Best At Epoch':<15} {'Total Trained':<15}\")\n",
    "print(\"-\" * 60)\n",
    "for idx, result in enumerate(step0_results, 1):\n",
    "    print(f\"{idx:<4} {result['num_epochs']:<8} {result['val_acc']*100:<10.2f}% {result['best_epoch']:<15} {result['total_epochs']:<15}\")\n",
    "print(f\"\\n>>> Best from Step 0: Epochs={BEST_EPOCHS}, Val Acc={best_step0['val_acc']*100:.2f}%\")\n",
    "print(f\"    Best validation accuracy was achieved at epoch {best_step0['best_epoch']} out of {best_step0['total_epochs']}\")\n",
    "print(f\"\\n>>> Using MAX_EPOCHS={MAX_EPOCHS} and PATIENCE={PATIENCE} for subsequent steps (with early stopping)\")\n",
    "\n",
    "# Helper function to train and evaluate a model configuration\n",
    "def train_and_evaluate(config, config_name=\"config\"):\n",
    "    \"\"\"Train a model with given configuration and return validation accuracy\"\"\"\n",
    "    print(f\"\\n>>> Testing: {config_name}\")\n",
    "    print(f\"    LR={config['lr']}, Batch={config['batch_size']}, Hidden={config['hidden_dim']}, Opt={config['optimizer']}\")\n",
    "    \n",
    "    # Create iterators with specific batch size\n",
    "    train_iter = data.BucketIterator(\n",
    "        train_data,\n",
    "        batch_size=config['batch_size'],\n",
    "        sort_key=lambda x: len(x.text),\n",
    "        sort_within_batch=True,\n",
    "        shuffle=True,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    val_iter = data.BucketIterator(\n",
    "        validation_data,\n",
    "        batch_size=config['batch_size'],\n",
    "        sort_key=lambda x: len(x.text),\n",
    "        sort_within_batch=True,\n",
    "        shuffle=False,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Create model with specific hidden dimension\n",
    "    model = SimpleRNNClassifier(\n",
    "        vocab_size=embedding_vocab_size,\n",
    "        embedding_dim=embedding_dim,\n",
    "        hidden_dim=config['hidden_dim'],\n",
    "        output_dim=num_classes,\n",
    "        n_layers=N_LAYERS,\n",
    "        dropout=0.0,  # Baseline: no dropout\n",
    "        padding_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "        pretrained_embeddings=pretrained_embeddings\n",
    "    ).to(device)\n",
    "    \n",
    "    # Select optimizer\n",
    "    if config['optimizer'] == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config['lr'])\n",
    "    elif config['optimizer'] == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=config['lr'])\n",
    "    elif config['optimizer'] == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=config['lr'])\n",
    "    elif config['optimizer'] == 'Adagrad':\n",
    "        optimizer = optim.Adagrad(model.parameters(), lr=config['lr'])\n",
    "    \n",
    "    # Training loop with early stopping\n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        \n",
    "        for batch in train_iter:\n",
    "            text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(text, text_lengths)\n",
    "            loss = criterion(predictions, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        train_acc = accuracy_score(train_labels, train_preds)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_iter:\n",
    "                text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "                predictions = model(text, text_lengths)\n",
    "                loss = criterion(predictions, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                preds = torch.argmax(predictions, dim=1)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_acc = accuracy_score(val_labels, val_preds)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(f\"    Early stopping at epoch {epoch+1}, best val acc: {best_val_acc*100:.2f}%\")\n",
    "                break\n",
    "    \n",
    "    print(f\"    Best Val Acc: {best_val_acc*100:.2f}% (stopped at epoch {epoch+1})\")\n",
    "    return best_val_acc, epoch + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f207d368",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32f94eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 1: LEARNING RATE + BATCH SIZE TUNING\n",
      "================================================================================\n",
      "Testing ALL combinations of LR and Batch Size (they interact)\n",
      "Total combinations to test: 9\n",
      "Combinations:\n",
      "  1. LR=0.01, Batch=32\n",
      "  2. LR=0.01, Batch=64\n",
      "  3. LR=0.01, Batch=128\n",
      "  4. LR=0.001, Batch=32\n",
      "  5. LR=0.001, Batch=64\n",
      "  6. LR=0.001, Batch=128\n",
      "  7. LR=0.0001, Batch=32\n",
      "  8. LR=0.0001, Batch=64\n",
      "  9. LR=0.0001, Batch=128\n",
      "\n",
      ">>> Testing: Step 1 Config 1/9\n",
      "    LR=0.01, Batch=32, Hidden=256, Opt=Adam\n",
      "    Early stopping at epoch 13, best val acc: 47.71%\n",
      "    Best Val Acc: 47.71% (stopped at epoch 13)\n",
      "\n",
      ">>> Testing: Step 1 Config 2/9\n",
      "    LR=0.01, Batch=64, Hidden=256, Opt=Adam\n",
      "    Early stopping at epoch 8, best val acc: 22.84%\n",
      "    Best Val Acc: 22.84% (stopped at epoch 8)\n",
      "\n",
      ">>> Testing: Step 1 Config 3/9\n",
      "    LR=0.01, Batch=128, Hidden=256, Opt=Adam\n",
      "    Early stopping at epoch 34, best val acc: 54.68%\n",
      "    Best Val Acc: 54.68% (stopped at epoch 34)\n",
      "\n",
      ">>> Testing: Step 1 Config 4/9\n",
      "    LR=0.001, Batch=32, Hidden=256, Opt=Adam\n",
      "    Early stopping at epoch 18, best val acc: 68.44%\n",
      "    Best Val Acc: 68.44% (stopped at epoch 18)\n",
      "\n",
      ">>> Testing: Step 1 Config 5/9\n",
      "    LR=0.001, Batch=64, Hidden=256, Opt=Adam\n",
      "    Early stopping at epoch 16, best val acc: 65.05%\n",
      "    Best Val Acc: 65.05% (stopped at epoch 16)\n",
      "\n",
      ">>> Testing: Step 1 Config 6/9\n",
      "    LR=0.001, Batch=128, Hidden=256, Opt=Adam\n",
      "    Early stopping at epoch 57, best val acc: 63.39%\n",
      "    Best Val Acc: 63.39% (stopped at epoch 57)\n",
      "\n",
      ">>> Testing: Step 1 Config 7/9\n",
      "    LR=0.0001, Batch=32, Hidden=256, Opt=Adam\n",
      "    Early stopping at epoch 26, best val acc: 81.38%\n",
      "    Best Val Acc: 81.38% (stopped at epoch 26)\n",
      "\n",
      ">>> Testing: Step 1 Config 8/9\n",
      "    LR=0.0001, Batch=64, Hidden=256, Opt=Adam\n",
      "    Early stopping at epoch 24, best val acc: 81.10%\n",
      "    Best Val Acc: 81.10% (stopped at epoch 24)\n",
      "\n",
      ">>> Testing: Step 1 Config 9/9\n",
      "    LR=0.0001, Batch=128, Hidden=256, Opt=Adam\n",
      "    Early stopping at epoch 26, best val acc: 80.83%\n",
      "    Best Val Acc: 80.83% (stopped at epoch 26)\n",
      "\n",
      ">>> Step 1 Results:\n",
      "#    LR       Batch   Val Acc    Epochs \n",
      "----------------------------------------\n",
      "1    0.01     32      47.71     % 13     \n",
      "2    0.01     64      22.84     % 8      \n",
      "3    0.01     128     54.68     % 34     \n",
      "4    0.001    32      68.44     % 18     \n",
      "5    0.001    64      65.05     % 16     \n",
      "6    0.001    128     63.39     % 57     \n",
      "7    0.0001   32      81.38     % 26     \n",
      "8    0.0001   64      81.10     % 24     \n",
      "9    0.0001   128     80.83     % 26     \n",
      "\n",
      ">>> Best from Step 1: LR=0.0001, Batch=32, Val Acc=81.38%\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Step 1: Group 1 - Learning Rate + Batch Size (Test Together)\n",
    "# ============================================================================\n",
    "\n",
    "# Helper function to train and evaluate a model configuration (uses best MAX_EPOCHS and PATIENCE from Step 0)\n",
    "def train_and_evaluate(config, config_name=\"config\"):\n",
    "    \"\"\"Train a model with given configuration and return validation accuracy\"\"\"\n",
    "    print(f\"\\n>>> Testing: {config_name}\")\n",
    "    print(f\"    LR={config['lr']}, Batch={config['batch_size']}, Hidden={config['hidden_dim']}, Opt={config['optimizer']}\")\n",
    "    \n",
    "    # Create iterators with specific batch size\n",
    "    train_iter = data.BucketIterator(\n",
    "        train_data,\n",
    "        batch_size=config['batch_size'],\n",
    "        sort_key=lambda x: len(x.text),\n",
    "        sort_within_batch=True,\n",
    "        shuffle=True,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    val_iter = data.BucketIterator(\n",
    "        validation_data,\n",
    "        batch_size=config['batch_size'],\n",
    "        sort_key=lambda x: len(x.text),\n",
    "        sort_within_batch=True,\n",
    "        shuffle=False,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Create model with specific hidden dimension\n",
    "    model = SimpleRNNClassifier(\n",
    "        vocab_size=embedding_vocab_size,\n",
    "        embedding_dim=embedding_dim,\n",
    "        hidden_dim=config['hidden_dim'],\n",
    "        output_dim=num_classes,\n",
    "        n_layers=N_LAYERS,\n",
    "        dropout=0.0,  # Baseline: no dropout\n",
    "        padding_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "        pretrained_embeddings=pretrained_embeddings\n",
    "    ).to(device)\n",
    "    \n",
    "    # Select optimizer\n",
    "    if config['optimizer'] == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config['lr'])\n",
    "    elif config['optimizer'] == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=config['lr'], momentum=0.9)\n",
    "    elif config['optimizer'] == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=config['lr'])\n",
    "    elif config['optimizer'] == 'Adagrad':\n",
    "        optimizer = optim.Adagrad(model.parameters(), lr=config['lr'])\n",
    "    \n",
    "    # Training loop with early stopping (using best MAX_EPOCHS and PATIENCE from Step 0)\n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        \n",
    "        for batch in train_iter:\n",
    "            text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(text, text_lengths)\n",
    "            loss = criterion(predictions, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        train_acc = accuracy_score(train_labels, train_preds)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_iter:\n",
    "                text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "                predictions = model(text, text_lengths)\n",
    "                loss = criterion(predictions, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                preds = torch.argmax(predictions, dim=1)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_acc = accuracy_score(val_labels, val_preds)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(f\"    Early stopping at epoch {epoch+1}, best val acc: {best_val_acc*100:.2f}%\")\n",
    "                break\n",
    "    \n",
    "    print(f\"    Best Val Acc: {best_val_acc*100:.2f}% (stopped at epoch {epoch+1})\")\n",
    "    return best_val_acc, epoch + 1\n",
    "    \n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 1: LEARNING RATE + BATCH SIZE TUNING\")\n",
    "print(\"=\"*80)\n",
    "print(\"Testing ALL combinations of LR and Batch Size (they interact)\")\n",
    "\n",
    "# Test all combinations of learning rates and batch sizes\n",
    "learning_rates = [0.01, 0.001, 0.0001]\n",
    "batch_sizes = [32, 64, 128]\n",
    "\n",
    "step1_configs = []\n",
    "for lr in learning_rates:\n",
    "    for batch_size in batch_sizes:\n",
    "        step1_configs.append({\n",
    "            'lr': lr,\n",
    "            'batch_size': batch_size,\n",
    "            'hidden_dim': 256,\n",
    "            'optimizer': 'Adam'\n",
    "        })\n",
    "\n",
    "print(f\"Total combinations to test: {len(step1_configs)}\")\n",
    "print(\"Combinations:\")\n",
    "for idx, config in enumerate(step1_configs, 1):\n",
    "    print(f\"  {idx}. LR={config['lr']}, Batch={config['batch_size']}\")\n",
    "\n",
    "step1_results = []\n",
    "for idx, config in enumerate(step1_configs):\n",
    "    # Set fixed seed for reproducibility\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "    \n",
    "    val_acc, epoch_stopped = train_and_evaluate(config, f\"Step 1 Config {idx+1}/{len(step1_configs)}\")\n",
    "    step1_results.append({\n",
    "        'config': config,\n",
    "        'val_acc': val_acc,\n",
    "        'epoch_stopped': epoch_stopped\n",
    "    })\n",
    "\n",
    "# Find best LR + Batch Size\n",
    "best_step1 = max(step1_results, key=lambda x: x['val_acc'])\n",
    "best_lr = best_step1['config']['lr']\n",
    "best_batch_size = best_step1['config']['batch_size']\n",
    "\n",
    "print(f\"\\n>>> Step 1 Results:\")\n",
    "print(f\"{'#':<4} {'LR':<8} {'Batch':<7} {'Val Acc':<10} {'Epochs':<7}\")\n",
    "print(\"-\" * 40)\n",
    "for idx, result in enumerate(step1_results):\n",
    "    c = result['config']\n",
    "    print(f\"{idx+1:<4} {c['lr']:<8} {c['batch_size']:<7} {result['val_acc']*100:<10.2f}% {result['epoch_stopped']:<7}\")\n",
    "print(f\"\\n>>> Best from Step 1: LR={best_lr}, Batch={best_batch_size}, Val Acc={best_step1['val_acc']*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "305f8f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 2: OPTIMIZER\n",
      "================================================================================\n",
      "Using best LR=0.0001 and Batch=32 from Step 1\n",
      "\n",
      ">>> Testing: Step 2 Config 1/4\n",
      "    LR=0.0001, Batch=32, Hidden=256, Opt=Adam\n",
      "    Early stopping at epoch 26, best val acc: 81.38%\n",
      "    Best Val Acc: 81.38% (stopped at epoch 26)\n",
      "\n",
      ">>> Testing: Step 2 Config 2/4\n",
      "    LR=0.0001, Batch=32, Hidden=256, Opt=SGD\n",
      "    Early stopping at epoch 11, best val acc: 23.03%\n",
      "    Best Val Acc: 23.03% (stopped at epoch 11)\n",
      "\n",
      ">>> Testing: Step 2 Config 3/4\n",
      "    LR=0.0001, Batch=32, Hidden=256, Opt=RMSprop\n",
      "    Early stopping at epoch 24, best val acc: 82.66%\n",
      "    Best Val Acc: 82.66% (stopped at epoch 24)\n",
      "\n",
      ">>> Testing: Step 2 Config 4/4\n",
      "    LR=0.0001, Batch=32, Hidden=256, Opt=Adagrad\n",
      "    Early stopping at epoch 39, best val acc: 32.66%\n",
      "    Best Val Acc: 32.66% (stopped at epoch 39)\n",
      "\n",
      ">>> Step 2 Results:\n",
      "#    LR       Optimizer  Val Acc    Epochs \n",
      "---------------------------------------------\n",
      "1    0.0001   Adam       81.38     % 26     \n",
      "2    0.0001   SGD        23.03     % 11     \n",
      "3    0.0001   RMSprop    82.66     % 24     \n",
      "4    0.0001   Adagrad    32.66     % 39     \n",
      "\n",
      ">>> Best from Step 2: LR=0.0001, Optimizer=RMSprop, Val Acc=82.66%\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Step 2: Group 2 - Optimizer\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 2: OPTIMIZER\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Using best LR={best_lr} and Batch={best_batch_size} from Step 1\")\n",
    "\n",
    "step2_configs = [\n",
    "    {'lr': best_lr, 'batch_size': best_batch_size, 'hidden_dim': 256, 'optimizer': 'Adam'},\n",
    "    {'lr': best_lr, 'batch_size': best_batch_size, 'hidden_dim': 256, 'optimizer': 'SGD'},\n",
    "    {'lr': best_lr, 'batch_size': best_batch_size, 'hidden_dim': 256, 'optimizer': 'RMSprop'},\n",
    "    {'lr': best_lr, 'batch_size': best_batch_size, 'hidden_dim': 256, 'optimizer': 'Adagrad'},\n",
    "]\n",
    "\n",
    "step2_results = []\n",
    "for idx, config in enumerate(step2_configs):\n",
    "    # Set fixed seed for reproducibility\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "    \n",
    "    val_acc, epoch_stopped = train_and_evaluate(config, f\"Step 2 Config {idx+1}/{len(step2_configs)}\")\n",
    "    step2_results.append({\n",
    "        'config': config,\n",
    "        'val_acc': val_acc,\n",
    "        'epoch_stopped': epoch_stopped\n",
    "    })\n",
    "\n",
    "# Find best Optimizer (and potentially adjusted LR)\n",
    "best_step2 = max(step2_results, key=lambda x: x['val_acc'])\n",
    "best_optimizer = best_step2['config']['optimizer']\n",
    "final_lr = best_step2['config']['lr']  # May be different if SGD needed higher LR\n",
    "\n",
    "print(f\"\\n>>> Step 2 Results:\")\n",
    "print(f\"{'#':<4} {'LR':<8} {'Optimizer':<10} {'Val Acc':<10} {'Epochs':<7}\")\n",
    "print(\"-\" * 45)\n",
    "for idx, result in enumerate(step2_results):\n",
    "    c = result['config']\n",
    "    print(f\"{idx+1:<4} {c['lr']:<8} {c['optimizer']:<10} {result['val_acc']*100:<10.2f}% {result['epoch_stopped']:<7}\")\n",
    "print(f\"\\n>>> Best from Step 2: LR={final_lr}, Optimizer={best_optimizer}, Val Acc={best_step2['val_acc']*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c7fb36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 3: HIDDEN DIMENSION TUNING\n",
      "================================================================================\n",
      "Using best LR=0.0001, Batch=32, Optimizer=RMSprop from Steps 1-2\n",
      "\n",
      ">>> Testing: Step 3 Config 1/3\n",
      "    LR=0.0001, Batch=32, Hidden=128, Opt=RMSprop\n",
      "    Early stopping at epoch 30, best val acc: 82.66%\n",
      "    Best Val Acc: 82.66% (stopped at epoch 30)\n",
      "\n",
      ">>> Testing: Step 3 Config 2/3\n",
      "    LR=0.0001, Batch=32, Hidden=256, Opt=RMSprop\n",
      "    Early stopping at epoch 24, best val acc: 82.66%\n",
      "    Best Val Acc: 82.66% (stopped at epoch 24)\n",
      "\n",
      ">>> Testing: Step 3 Config 3/3\n",
      "    LR=0.0001, Batch=32, Hidden=512, Opt=RMSprop\n",
      "    Early stopping at epoch 17, best val acc: 83.03%\n",
      "    Best Val Acc: 83.03% (stopped at epoch 17)\n",
      "\n",
      ">>> Step 3 Results:\n",
      "#    Hidden Dim   Val Acc    Epochs \n",
      "-----------------------------------\n",
      "1    128          82.66     % 30     \n",
      "2    256          82.66     % 24     \n",
      "3    512          83.03     % 17     \n",
      "\n",
      ">>> Best from Step 3: Hidden Dim=512, Val Acc=83.03%\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Step 3: Hidden Dimension (Test Independently)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 3: HIDDEN DIMENSION TUNING\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Using best LR={final_lr}, Batch={best_batch_size}, Optimizer={best_optimizer} from Steps 1-2\")\n",
    "\n",
    "step3_configs = [\n",
    "    {'lr': final_lr, 'batch_size': best_batch_size, 'hidden_dim': 128, 'optimizer': best_optimizer},\n",
    "    {'lr': final_lr, 'batch_size': best_batch_size, 'hidden_dim': 256, 'optimizer': best_optimizer},\n",
    "    {'lr': final_lr, 'batch_size': best_batch_size, 'hidden_dim': 512, 'optimizer': best_optimizer},\n",
    "]\n",
    "\n",
    "step3_results = []\n",
    "for idx, config in enumerate(step3_configs):\n",
    "    # Set fixed seed for reproducibility\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "    \n",
    "    val_acc, epoch_stopped = train_and_evaluate(config, f\"Step 3 Config {idx+1}/{len(step3_configs)}\")\n",
    "    step3_results.append({\n",
    "        'config': config,\n",
    "        'val_acc': val_acc,\n",
    "        'epoch_stopped': epoch_stopped\n",
    "    })\n",
    "\n",
    "# Find best Hidden Dimension\n",
    "best_step3 = max(step3_results, key=lambda x: x['val_acc'])\n",
    "best_hidden_dim = best_step3['config']['hidden_dim']\n",
    "\n",
    "print(f\"\\n>>> Step 3 Results:\")\n",
    "print(f\"{'#':<4} {'Hidden Dim':<12} {'Val Acc':<10} {'Epochs':<7}\")\n",
    "print(\"-\" * 35)\n",
    "for idx, result in enumerate(step3_results):\n",
    "    c = result['config']\n",
    "    print(f\"{idx+1:<4} {c['hidden_dim']:<12} {result['val_acc']*100:<10.2f}% {result['epoch_stopped']:<7}\")\n",
    "print(f\"\\n>>> Best from Step 3: Hidden Dim={best_hidden_dim}, Val Acc={best_step3['val_acc']*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7a23bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "HYPERPARAMETER TUNING COMPLETE - FINAL BEST CONFIGURATION\n",
      "================================================================================\n",
      "\n",
      ">>> Best Configuration Found:\n",
      "    Learning Rate: 0.0001\n",
      "    Batch Size: 32\n",
      "    Hidden Dimension: 512\n",
      "    Optimizer: RMSprop\n",
      "    Max Epochs: 200 (with early stopping, patience=7)\n",
      "    Best Validation Accuracy: 83.03%\n",
      "\n",
      "================================================================================\n",
      "SEQUENTIAL HYPERPARAMETER TUNING COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Final Best Configuration Summary\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"HYPERPARAMETER TUNING COMPLETE - FINAL BEST CONFIGURATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "final_best_config = {\n",
    "    'lr': final_lr,\n",
    "    'batch_size': best_batch_size,\n",
    "    'hidden_dim': best_hidden_dim,\n",
    "    'optimizer': best_optimizer,\n",
    "    'max_epochs': MAX_EPOCHS,\n",
    "    'patience': PATIENCE\n",
    "}\n",
    "\n",
    "print(f\"\\n>>> Best Configuration Found:\")\n",
    "print(f\"    Learning Rate: {final_best_config['lr']}\")\n",
    "print(f\"    Batch Size: {final_best_config['batch_size']}\")\n",
    "print(f\"    Hidden Dimension: {final_best_config['hidden_dim']}\")\n",
    "print(f\"    Optimizer: {final_best_config['optimizer']}\")\n",
    "print(f\"    Max Epochs: {final_best_config['max_epochs']} (with early stopping, patience={final_best_config['patience']})\")\n",
    "print(f\"    Best Validation Accuracy: {best_step3['val_acc']*100:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SEQUENTIAL HYPERPARAMETER TUNING COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "817c430f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "WORD AGGREGATION METHOD COMPARISON\n",
      "================================================================================\n",
      "Using best hyperparameters from tuning:\n",
      "    LR=0.0001, Batch=32, Hidden=512, Optimizer=RMSprop\n",
      "    Max Epochs=200, Patience=7\n",
      "\n",
      "Testing 4 aggregation methods:\n",
      "  - last\n",
      "  - mean\n",
      "  - max\n",
      "  - attention\n",
      "\n",
      "================================================================================\n",
      "Testing Aggregation Method: LAST\n",
      "================================================================================\n",
      "\n",
      ">>> Training model with last aggregation...\n",
      "    Epoch 10: Train Acc=92.43%, Val Acc=83.03%\n",
      "    Early stopping at epoch 17, best val acc: 83.03%\n",
      "\n",
      ">>> Results for last aggregation:\n",
      "    Validation Acc: 83.03%\n",
      "    Test Acc: 85.00%\n",
      "    Test F1: 0.8399\n",
      "    Test AUC-ROC: 0.9733\n",
      "\n",
      "================================================================================\n",
      "Testing Aggregation Method: MEAN\n",
      "================================================================================\n",
      "\n",
      ">>> Training model with mean aggregation...\n",
      "    Epoch 10: Train Acc=91.98%, Val Acc=81.83%\n",
      "    Epoch 20: Train Acc=96.74%, Val Acc=83.21%\n",
      "    Early stopping at epoch 21, best val acc: 83.76%\n",
      "\n",
      ">>> Results for mean aggregation:\n",
      "    Validation Acc: 83.76%\n",
      "    Test Acc: 87.60%\n",
      "    Test F1: 0.8738\n",
      "    Test AUC-ROC: 0.9701\n",
      "\n",
      "================================================================================\n",
      "Testing Aggregation Method: MAX\n",
      "================================================================================\n",
      "\n",
      ">>> Training model with max aggregation...\n",
      "    Early stopping at epoch 8, best val acc: 22.94%\n",
      "    Warning: Could not calculate AUC-ROC: Input contains NaN.\n",
      "\n",
      ">>> Results for max aggregation:\n",
      "    Validation Acc: 22.94%\n",
      "    Test Acc: 18.80%\n",
      "    Test F1: 0.0595\n",
      "    Test AUC-ROC: 0.0000\n",
      "\n",
      "================================================================================\n",
      "Testing Aggregation Method: ATTENTION\n",
      "================================================================================\n",
      "\n",
      ">>> Training model with attention aggregation...\n",
      "    Epoch 10: Train Acc=95.74%, Val Acc=83.49%\n",
      "    Early stopping at epoch 19, best val acc: 84.68%\n",
      "\n",
      ">>> Results for attention aggregation:\n",
      "    Validation Acc: 84.68%\n",
      "    Test Acc: 87.80%\n",
      "    Test F1: 0.8770\n",
      "    Test AUC-ROC: 0.9633\n",
      "\n",
      "================================================================================\n",
      "AGGREGATION METHOD COMPARISON - RESULTS SUMMARY\n",
      "================================================================================\n",
      "\n",
      ">>> Results Summary:\n",
      "Method       Val Acc    Test Acc   Test F1    Test AUC  \n",
      "-------------------------------------------------------\n",
      "last         83.03     % 85.00     % 0.8399     0.9733    \n",
      "mean         83.76     % 87.60     % 0.8738     0.9701    \n",
      "max          22.94     % 18.80     % 0.0595     0.0000    \n",
      "attention    84.68     % 87.80     % 0.8770     0.9633    \n",
      "\n",
      ">>> Best Aggregation Method: ATTENTION\n",
      "    Validation Accuracy: 84.68%\n",
      "    Test Accuracy: 87.80%\n",
      "    Test F1 Score: 0.8770\n",
      "    Test AUC-ROC: 0.9633\n",
      "\n",
      "================================================================================\n",
      "AGGREGATION METHOD COMPARISON COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Word Aggregation Method Comparison\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"WORD AGGREGATION METHOD COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Using best hyperparameters from tuning:\")\n",
    "print(f\"    LR={final_lr}, Batch={best_batch_size}, Hidden={best_hidden_dim}, Optimizer={best_optimizer}\")\n",
    "print(f\"    Max Epochs={MAX_EPOCHS}, Patience={PATIENCE}\")\n",
    "\n",
    "# Extended RNN Classifier with multiple aggregation methods\n",
    "class RNN_Classifier_Aggregation(nn.Module):\n",
    "    \"\"\"\n",
    "    RNN for topic classification with multiple aggregation strategies.\n",
    "    Uses pretrained embeddings (learnable/updated during training).\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, \n",
    "                 n_layers=1, dropout=0.0, padding_idx=0, pretrained_embeddings=None,\n",
    "                 aggregation='last'):\n",
    "        super(RNN_Classifier_Aggregation, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.aggregation = aggregation  # 'last', 'mean', 'max', 'attention'\n",
    "        \n",
    "        # Embedding layer initialized with pretrained embeddings\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "            # Make embeddings learnable (updated during training)\n",
    "            self.embedding.weight.requires_grad = True\n",
    "        \n",
    "        # RNN layer\n",
    "        self.rnn = nn.RNN(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=n_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.0  # No dropout in baseline\n",
    "        )\n",
    "        \n",
    "        # Attention mechanism for aggregation (only created if needed)\n",
    "        if aggregation == 'attention':\n",
    "            self.attention = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        # Fully connected output layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        # text: [batch_size, seq_len]\n",
    "        # text_lengths: [batch_size]\n",
    "        \n",
    "        # Embed the input\n",
    "        embedded = self.embedding(text)  # [batch_size, seq_len, embedding_dim]\n",
    "        \n",
    "        # Get dimensions\n",
    "        batch_size = embedded.size(0)\n",
    "        seq_len = embedded.size(1)\n",
    "        \n",
    "        # Handle text_lengths\n",
    "        text_lengths_flat = text_lengths.flatten().cpu().long()\n",
    "        if len(text_lengths_flat) != batch_size:\n",
    "            raise ValueError(\n",
    "                f\"text_lengths size mismatch: got {len(text_lengths_flat)} elements, \"\n",
    "                f\"expected {batch_size}\"\n",
    "            )\n",
    "        \n",
    "        # Clamp lengths\n",
    "        text_lengths_clamped = torch.clamp(text_lengths_flat, min=1, max=seq_len)\n",
    "        \n",
    "        # Move to same device as text for mask operations later\n",
    "        # Keep CPU version for pack_padded_sequence (requires CPU)\n",
    "        # Create device version for mask operations later\n",
    "        text_lengths_clamped_device = text_lengths_clamped.to(text.device)\n",
    "        # Pack the padded sequences\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, text_lengths_clamped, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        \n",
    "        # Pass through RNN\n",
    "        packed_output, hidden = self.rnn(packed_embedded)\n",
    "        # hidden: [n_layers, batch_size, hidden_dim]\n",
    "        \n",
    "        # Aggregate word representations to sentence representation\n",
    "        if self.aggregation == 'last':\n",
    "            # Use the last hidden state from the last layer\n",
    "            sentence_repr = hidden[-1]  # [batch_size, hidden_dim]\n",
    "            \n",
    "        elif self.aggregation == 'mean':\n",
    "            # Mean pooling over all outputs (ignoring padding)\n",
    "            # Unpack the sequences first\n",
    "            output, output_lengths = nn.utils.rnn.pad_packed_sequence(\n",
    "                packed_output, batch_first=True\n",
    "            )\n",
    "            # output: [batch_size, seq_len, hidden_dim]\n",
    "            \n",
    "            # Create mask for padding\n",
    "            mask = torch.arange(seq_len, device=text.device).unsqueeze(0) < text_lengths_clamped_device.unsqueeze(1)\n",
    "            mask = mask.unsqueeze(2).float()  # [batch_size, seq_len, 1]\n",
    "            \n",
    "            # Apply mask and compute mean\n",
    "            masked_output = output * mask\n",
    "            sum_output = masked_output.sum(dim=1)  # [batch_size, hidden_dim]\n",
    "            sentence_repr = sum_output / text_lengths_clamped_device.unsqueeze(1).float()\n",
    "            \n",
    "        elif self.aggregation == 'max':\n",
    "            # Max pooling over all outputs\n",
    "            output, output_lengths = nn.utils.rnn.pad_packed_sequence(\n",
    "                packed_output, batch_first=True\n",
    "            )\n",
    "            # output: [batch_size, seq_len, hidden_dim]\n",
    "            \n",
    "            # Create mask for padding (set padding to -inf before max)\n",
    "            mask = torch.arange(seq_len, device=text.device).unsqueeze(0) < text_lengths_clamped_device.unsqueeze(1)\n",
    "            mask = mask.unsqueeze(2).float()  # [batch_size, seq_len, 1]\n",
    "            \n",
    "            masked_output = output * mask + (1 - mask) * float('-inf')\n",
    "            sentence_repr, _ = torch.max(masked_output, dim=1)\n",
    "            \n",
    "        elif self.aggregation == 'attention':\n",
    "            # Attention mechanism\n",
    "            output, output_lengths = nn.utils.rnn.pad_packed_sequence(\n",
    "                packed_output, batch_first=True\n",
    "            )\n",
    "            # output: [batch_size, seq_len, hidden_dim]\n",
    "            \n",
    "            # Compute attention scores\n",
    "            attn_scores = self.attention(output).squeeze(2)  # [batch_size, seq_len]\n",
    "            \n",
    "            # Mask padding positions\n",
    "            mask = torch.arange(seq_len, device=text.device).unsqueeze(0) < text_lengths_clamped_device.unsqueeze(1)\n",
    "            attn_scores = attn_scores.masked_fill(~mask, float('-inf'))\n",
    "            \n",
    "            # Apply softmax\n",
    "            attn_weights = torch.softmax(attn_scores, dim=1).unsqueeze(1)  # [batch_size, 1, seq_len]\n",
    "            \n",
    "            # Weighted sum\n",
    "            sentence_repr = torch.bmm(attn_weights, output).squeeze(1)  # [batch_size, hidden_dim]\n",
    "        \n",
    "        # Pass through fully connected layer\n",
    "        output = self.fc(sentence_repr)  # [batch_size, output_dim]\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Test different aggregation methods\n",
    "aggregation_methods = ['last', 'mean', 'max', 'attention']\n",
    "\n",
    "print(f\"\\nTesting {len(aggregation_methods)} aggregation methods:\")\n",
    "for method in aggregation_methods:\n",
    "    print(f\"  - {method}\")\n",
    "\n",
    "aggregation_results = []\n",
    "\n",
    "for agg_method in aggregation_methods:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Testing Aggregation Method: {agg_method.upper()}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Set fixed seed for reproducibility - ensures consistent batch ordering\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "    \n",
    "    # Create iterators with best batch size\n",
    "    train_iter = data.BucketIterator(\n",
    "        train_data,\n",
    "        batch_size=best_batch_size,\n",
    "        sort_key=lambda x: len(x.text),\n",
    "        sort_within_batch=True,\n",
    "        shuffle=True,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    val_iter = data.BucketIterator(\n",
    "        validation_data,\n",
    "        batch_size=best_batch_size,\n",
    "        sort_key=lambda x: len(x.text),\n",
    "        sort_within_batch=True,\n",
    "        shuffle=False,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    test_iter = data.BucketIterator(\n",
    "        test_data,\n",
    "        batch_size=best_batch_size,\n",
    "        sort_key=lambda x: len(x.text),\n",
    "        sort_within_batch=True,\n",
    "        shuffle=False,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Create model with best hyperparameters and specific aggregation method\n",
    "    model = RNN_Classifier_Aggregation(\n",
    "        vocab_size=embedding_vocab_size,\n",
    "        embedding_dim=embedding_dim,\n",
    "        hidden_dim=best_hidden_dim,\n",
    "        output_dim=num_classes,\n",
    "        n_layers=N_LAYERS,\n",
    "        dropout=0.0,  # Baseline: no dropout\n",
    "        padding_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "        pretrained_embeddings=pretrained_embeddings,\n",
    "        aggregation=agg_method\n",
    "    ).to(device)\n",
    "    \n",
    "    # Select optimizer with best learning rate\n",
    "    if best_optimizer == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=final_lr)\n",
    "    elif best_optimizer == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=final_lr, momentum=0.9)\n",
    "    elif best_optimizer == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=final_lr)\n",
    "    elif best_optimizer == 'Adagrad':\n",
    "        optimizer = optim.Adagrad(model.parameters(), lr=final_lr)\n",
    "    \n",
    "    # Training loop with early stopping\n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    print(f\"\\n>>> Training model with {agg_method} aggregation...\")\n",
    "    \n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        \n",
    "        for batch in train_iter:\n",
    "            text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(text, text_lengths)\n",
    "            loss = criterion(predictions, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        train_acc = accuracy_score(train_labels, train_preds)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_iter:\n",
    "                text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "                predictions = model(text, text_lengths)\n",
    "                loss = criterion(predictions, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                preds = torch.argmax(predictions, dim=1)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_acc = accuracy_score(val_labels, val_preds)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            # Save best model for this aggregation method\n",
    "            torch.save(model.state_dict(), f'rnn_agg_{agg_method}_best.pt')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(f\"    Early stopping at epoch {epoch+1}, best val acc: {best_val_acc*100:.2f}%\")\n",
    "                break\n",
    "        \n",
    "        # Print progress every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"    Epoch {epoch+1}: Train Acc={train_acc*100:.2f}%, Val Acc={val_acc*100:.2f}%\")\n",
    "    \n",
    "    # Load best model and evaluate on test set\n",
    "    model.load_state_dict(torch.load(f'rnn_agg_{agg_method}_best.pt'))\n",
    "    model.eval()\n",
    "    \n",
    "    test_loss = 0\n",
    "    test_preds = []\n",
    "    test_labels = []\n",
    "    test_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_iter:\n",
    "            text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "            predictions = model(text, text_lengths)\n",
    "            loss = criterion(predictions, labels)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            probs = torch.softmax(predictions, dim=1)\n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            test_preds.extend(preds.cpu().numpy())\n",
    "            test_labels.extend(labels.cpu().numpy())\n",
    "            test_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    # Calculate test metrics\n",
    "    test_acc = accuracy_score(test_labels, test_preds)\n",
    "    test_f1 = f1_score(test_labels, test_preds, average='weighted')\n",
    "    test_loss_avg = test_loss / len(test_iter)\n",
    "    \n",
    "    # Calculate AUC-ROC\n",
    "    try:\n",
    "        test_probs_array = np.array(test_probs)\n",
    "        test_labels_bin = label_binarize(test_labels, classes=range(num_classes))\n",
    "        test_auc = roc_auc_score(test_labels_bin, test_probs_array, average='weighted', multi_class='ovr')\n",
    "    except Exception as e:\n",
    "        print(f\"    Warning: Could not calculate AUC-ROC: {e}\")\n",
    "        test_auc = 0.0\n",
    "    \n",
    "    aggregation_results.append({\n",
    "        'method': agg_method,\n",
    "        'val_acc': best_val_acc,\n",
    "        'test_acc': test_acc,\n",
    "        'test_f1': test_f1,\n",
    "        'test_auc': test_auc,\n",
    "        'test_loss': test_loss_avg\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n>>> Results for {agg_method} aggregation:\")\n",
    "    print(f\"    Validation Acc: {best_val_acc*100:.2f}%\")\n",
    "    print(f\"    Test Acc: {test_acc*100:.2f}%\")\n",
    "    print(f\"    Test F1: {test_f1:.4f}\")\n",
    "    print(f\"    Test AUC-ROC: {test_auc:.4f}\")\n",
    "\n",
    "# Print summary table\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"AGGREGATION METHOD COMPARISON - RESULTS SUMMARY\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "print(f\"\\n>>> Results Summary:\")\n",
    "print(f\"{'Method':<12} {'Val Acc':<10} {'Test Acc':<10} {'Test F1':<10} {'Test AUC':<10}\")\n",
    "print(\"-\" * 55)\n",
    "for result in aggregation_results:\n",
    "    print(f\"{result['method']:<12} {result['val_acc']*100:<10.2f}% {result['test_acc']*100:<10.2f}% \"\n",
    "          f\"{result['test_f1']:<10.4f} {result['test_auc']:<10.4f}\")\n",
    "\n",
    "# Find best aggregation method\n",
    "best_aggregation = max(aggregation_results, key=lambda x: x['val_acc'])\n",
    "\n",
    "print(f\"\\n>>> Best Aggregation Method: {best_aggregation['method'].upper()}\")\n",
    "print(f\"    Validation Accuracy: {best_aggregation['val_acc']*100:.2f}%\")\n",
    "print(f\"    Test Accuracy: {best_aggregation['test_acc']*100:.2f}%\")\n",
    "print(f\"    Test F1 Score: {best_aggregation['test_f1']:.4f}\")\n",
    "print(f\"    Test AUC-ROC: {best_aggregation['test_auc']:.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"AGGREGATION METHOD COMPARISON COMPLETE\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "684dfeeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "REGULARIZATION STEP 0: BASELINE (NO REGULARIZATION)\n",
      "================================================================================\n",
      "Using best hyperparameters from tuning:\n",
      "    LR=0.0001, Batch=32, Hidden=512, Optimizer=RMSprop\n",
      "    Max Epochs=200, Patience=7\n",
      "    Best Aggregation Method: ATTENTION\n",
      "\n",
      "Baseline Configuration:\n",
      "    Dropout: 0.0\n",
      "    Gradient Clipping: 0.0\n",
      "    L1 Lambda: 0.0\n",
      "    L2 Lambda: 0.0\n",
      "\n",
      ">>> Training baseline model...\n",
      "    Epoch 10: Train Acc=95.09%, Val Acc=84.68%\n",
      "    Epoch 20: Train Acc=99.31%, Val Acc=83.21%\n",
      "    Early stopping at epoch 22, best val acc: 85.87%\n",
      "\n",
      ">>> Baseline Results:\n",
      "    Validation Acc: 85.87%\n",
      "    Test Acc: 89.60%\n",
      "    Test F1: 0.8968\n",
      "    Test AUC-ROC: 0.9713\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Regularization: Baseline (No Regularization)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"REGULARIZATION STEP 0: BASELINE (NO REGULARIZATION)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Using best hyperparameters from tuning:\")\n",
    "print(f\"    LR={final_lr}, Batch={best_batch_size}, Hidden={best_hidden_dim}, Optimizer={best_optimizer}\")\n",
    "print(f\"    Max Epochs={MAX_EPOCHS}, Patience={PATIENCE}\")\n",
    "print(f\"    Best Aggregation Method: {best_aggregation['method'].upper()}\")\n",
    "\n",
    "# Baseline configuration\n",
    "baseline_config = {\n",
    "    'dropout': 0.0,\n",
    "    'grad_clip': 0.0,\n",
    "    'l1_lambda': 0.0,\n",
    "    'l2_lambda': 0.0\n",
    "}\n",
    "\n",
    "print(f\"\\nBaseline Configuration:\")\n",
    "print(f\"    Dropout: {baseline_config['dropout']}\")\n",
    "print(f\"    Gradient Clipping: {baseline_config['grad_clip']}\")\n",
    "print(f\"    L1 Lambda: {baseline_config['l1_lambda']}\")\n",
    "print(f\"    L2 Lambda: {baseline_config['l2_lambda']}\")\n",
    "\n",
    "# Create iterators\n",
    "train_iter = data.BucketIterator(\n",
    "    train_data,\n",
    "    batch_size=best_batch_size,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    shuffle=True,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "val_iter = data.BucketIterator(\n",
    "    validation_data,\n",
    "    batch_size=best_batch_size,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    shuffle=False,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "test_iter = data.BucketIterator(\n",
    "    test_data,\n",
    "    batch_size=best_batch_size,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    shuffle=False,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Create model\n",
    "model = RNN_Classifier_Aggregation(\n",
    "    vocab_size=embedding_vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_dim=best_hidden_dim,\n",
    "    output_dim=num_classes,\n",
    "    n_layers=N_LAYERS,\n",
    "    dropout=baseline_config['dropout'],\n",
    "    padding_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "    pretrained_embeddings=pretrained_embeddings,\n",
    "    aggregation=best_aggregation['method']\n",
    ").to(device)\n",
    "\n",
    "# Select optimizer\n",
    "if best_optimizer == 'Adam':\n",
    "    optimizer = optim.Adam(model.parameters(), lr=final_lr, weight_decay=baseline_config['l2_lambda'])\n",
    "elif best_optimizer == 'SGD':\n",
    "    optimizer = optim.SGD(model.parameters(), lr=final_lr, momentum=0.9, weight_decay=baseline_config['l2_lambda'])\n",
    "elif best_optimizer == 'RMSprop':\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=final_lr, weight_decay=baseline_config['l2_lambda'])\n",
    "elif best_optimizer == 'Adagrad':\n",
    "    optimizer = optim.Adagrad(model.parameters(), lr=final_lr, weight_decay=baseline_config['l2_lambda'])\n",
    "\n",
    "# Helper function for L1 regularization\n",
    "def compute_l1_loss(model, l1_lambda):\n",
    "    \"\"\"Compute L1 regularization loss\"\"\"\n",
    "    if l1_lambda > 0:\n",
    "        return l1_lambda * sum(p.abs().sum() for p in model.parameters() if p.requires_grad)\n",
    "    return 0.0\n",
    "\n",
    "# Training loop\n",
    "best_val_acc = 0.0\n",
    "patience_counter = 0\n",
    "\n",
    "print(f\"\\n>>> Training baseline model...\")\n",
    "\n",
    "for epoch in range(MAX_EPOCHS):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_preds = []\n",
    "    train_labels = []\n",
    "    \n",
    "    for batch in train_iter:\n",
    "        text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(text, text_lengths)\n",
    "        \n",
    "        loss = criterion(predictions, labels)\n",
    "        \n",
    "        # Add L1 regularization\n",
    "        if baseline_config['l1_lambda'] > 0:\n",
    "            loss = loss + compute_l1_loss(model, baseline_config['l1_lambda'])\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        if baseline_config['grad_clip'] > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), baseline_config['grad_clip'])\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        preds = torch.argmax(predictions, dim=1)\n",
    "        train_preds.extend(preds.cpu().numpy())\n",
    "        train_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    train_acc = accuracy_score(train_labels, train_preds)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_iter:\n",
    "            text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "            predictions = model(text, text_lengths)\n",
    "            loss = criterion(predictions, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    val_acc = accuracy_score(val_labels, val_preds)\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), 'rnn_reg_baseline_best.pt')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(f\"    Early stopping at epoch {epoch+1}, best val acc: {best_val_acc*100:.2f}%\")\n",
    "            break\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"    Epoch {epoch+1}: Train Acc={train_acc*100:.2f}%, Val Acc={val_acc*100:.2f}%\")\n",
    "\n",
    "# Evaluate on test set\n",
    "model.load_state_dict(torch.load('rnn_reg_baseline_best.pt'))\n",
    "model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "test_preds = []\n",
    "test_labels = []\n",
    "test_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_iter:\n",
    "        text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "        predictions = model(text, text_lengths)\n",
    "        loss = criterion(predictions, labels)\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        probs = torch.softmax(predictions, dim=1)\n",
    "        preds = torch.argmax(predictions, dim=1)\n",
    "        test_preds.extend(preds.cpu().numpy())\n",
    "        test_labels.extend(labels.cpu().numpy())\n",
    "        test_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "test_acc = accuracy_score(test_labels, test_preds)\n",
    "test_f1 = f1_score(test_labels, test_preds, average='weighted')\n",
    "test_loss_avg = test_loss / len(test_iter)\n",
    "\n",
    "try:\n",
    "    test_probs_array = np.array(test_probs)\n",
    "    test_labels_bin = label_binarize(test_labels, classes=range(num_classes))\n",
    "    test_auc = roc_auc_score(test_labels_bin, test_probs_array, average='weighted', multi_class='ovr')\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not calculate AUC-ROC: {e}\")\n",
    "    test_auc = 0.0\n",
    "\n",
    "baseline_results = {\n",
    "    'name': 'baseline',\n",
    "    'dropout': baseline_config['dropout'],\n",
    "    'grad_clip': baseline_config['grad_clip'],\n",
    "    'l1_lambda': baseline_config['l1_lambda'],\n",
    "    'l2_lambda': baseline_config['l2_lambda'],\n",
    "    'val_acc': best_val_acc,\n",
    "    'test_acc': test_acc,\n",
    "    'test_f1': test_f1,\n",
    "    'test_auc': test_auc\n",
    "}\n",
    "\n",
    "print(f\"\\n>>> Baseline Results:\")\n",
    "print(f\"    Validation Acc: {best_val_acc*100:.2f}%\")\n",
    "print(f\"    Test Acc: {test_acc*100:.2f}%\")\n",
    "print(f\"    Test F1: {test_f1:.4f}\")\n",
    "print(f\"    Test AUC-ROC: {test_auc:.4f}\")\n",
    "\n",
    "# Store for next steps\n",
    "best_grad_clip = baseline_config['grad_clip']  # Will be updated in Step 1\n",
    "best_dropout = baseline_config['dropout']  # Will be updated in Step 2\n",
    "best_l1_lambda = baseline_config['l1_lambda']  # Will be updated in Step 3\n",
    "best_l2_lambda = baseline_config['l2_lambda']  # Will be updated in Step 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2a253fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "REGULARIZATION STEP 1: GRADIENT CLIPPING TUNING\n",
      "================================================================================\n",
      "Using baseline settings: dropout=0.0, L1=0.0, L2=0.0\n",
      "\n",
      "Testing gradient clipping values: [0.0, 1.0]\n",
      "\n",
      "================================================================================\n",
      "Testing: Gradient Clipping = 0.0\n",
      "================================================================================\n",
      "    Epoch 10: Train Acc=94.66%, Val Acc=82.39%\n",
      "    Early stopping at epoch 19, best val acc: 84.40%\n",
      "    Result: Val Acc=84.40%\n",
      "\n",
      "================================================================================\n",
      "Testing: Gradient Clipping = 1.0\n",
      "================================================================================\n",
      "    Epoch 10: Train Acc=93.12%, Val Acc=81.65%\n",
      "    Early stopping at epoch 19, best val acc: 84.04%\n",
      "    Result: Val Acc=84.04%\n",
      "\n",
      ">>> Step 1 Results:\n",
      "Grad Clip    Val Acc   \n",
      "-------------------------\n",
      "0.0          84.40     %\n",
      "1.0          84.04     %\n",
      "\n",
      ">>> Best Gradient Clipping: 0.0, Val Acc=84.40%\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Regularization Step 1: Gradient Clipping Tuning\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"REGULARIZATION STEP 1: GRADIENT CLIPPING TUNING\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Using baseline settings: dropout={best_dropout}, L1={best_l1_lambda}, L2={best_l2_lambda}\")\n",
    "\n",
    "# Test different gradient clipping values\n",
    "grad_clip_options = [0.0, 1.0]  # 0.0 = no clipping, 1.0 = clip at 1.0\n",
    "\n",
    "print(f\"\\nTesting gradient clipping values: {grad_clip_options}\")\n",
    "\n",
    "step1_results = []\n",
    "\n",
    "for grad_clip in grad_clip_options:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Testing: Gradient Clipping = {grad_clip}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Create model with baseline settings\n",
    "    model = RNN_Classifier_Aggregation(\n",
    "        vocab_size=embedding_vocab_size,\n",
    "        embedding_dim=embedding_dim,\n",
    "        hidden_dim=best_hidden_dim,\n",
    "        output_dim=num_classes,\n",
    "        n_layers=N_LAYERS,\n",
    "        dropout=best_dropout,\n",
    "        padding_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "        pretrained_embeddings=pretrained_embeddings,\n",
    "        aggregation=best_aggregation['method']\n",
    "    ).to(device)\n",
    "    \n",
    "    # Select optimizer\n",
    "    if best_optimizer == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=final_lr, weight_decay=best_l2_lambda)\n",
    "    elif best_optimizer == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=final_lr, momentum=0.9, weight_decay=best_l2_lambda)\n",
    "    elif best_optimizer == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=final_lr, weight_decay=best_l2_lambda)\n",
    "    elif best_optimizer == 'Adagrad':\n",
    "        optimizer = optim.Adagrad(model.parameters(), lr=final_lr, weight_decay=best_l2_lambda)\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        \n",
    "        for batch in train_iter:\n",
    "            text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(text, text_lengths)\n",
    "            \n",
    "            loss = criterion(predictions, labels)\n",
    "            \n",
    "            if best_l1_lambda > 0:\n",
    "                loss = loss + compute_l1_loss(model, best_l1_lambda)\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            if grad_clip > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        train_acc = accuracy_score(train_labels, train_preds)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_iter:\n",
    "                text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "                predictions = model(text, text_lengths)\n",
    "                loss = criterion(predictions, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                preds = torch.argmax(predictions, dim=1)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_acc = accuracy_score(val_labels, val_preds)\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), f'rnn_reg_step1_gradclip{grad_clip}_best.pt')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(f\"    Early stopping at epoch {epoch+1}, best val acc: {best_val_acc*100:.2f}%\")\n",
    "                break\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"    Epoch {epoch+1}: Train Acc={train_acc*100:.2f}%, Val Acc={val_acc*100:.2f}%\")\n",
    "    \n",
    "    step1_results.append({\n",
    "        'grad_clip': grad_clip,\n",
    "        'val_acc': best_val_acc\n",
    "    })\n",
    "    \n",
    "    print(f\"    Result: Val Acc={best_val_acc*100:.2f}%\")\n",
    "\n",
    "# Find best gradient clipping\n",
    "best_step1 = max(step1_results, key=lambda x: x['val_acc'])\n",
    "best_grad_clip = best_step1['grad_clip']\n",
    "\n",
    "print(f\"\\n>>> Step 1 Results:\")\n",
    "print(f\"{'Grad Clip':<12} {'Val Acc':<10}\")\n",
    "print(\"-\" * 25)\n",
    "for result in step1_results:\n",
    "    print(f\"{result['grad_clip']:<12} {result['val_acc']*100:<10.2f}%\")\n",
    "print(f\"\\n>>> Best Gradient Clipping: {best_grad_clip}, Val Acc={best_step1['val_acc']*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff8b7e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "REGULARIZATION STEP 2: DROPOUT TUNING\n",
      "================================================================================\n",
      "Using best from Step 1: grad_clip=0.0\n",
      "Using baseline settings: L1=0.0, L2=0.0\n",
      "\n",
      "Testing dropout values: [0.0, 0.3, 0.5, 0.7]\n",
      "\n",
      "================================================================================\n",
      "Testing: Dropout = 0.0\n",
      "================================================================================\n",
      "    Epoch 10: Train Acc=94.70%, Val Acc=85.05%\n",
      "    Epoch 20: Train Acc=99.13%, Val Acc=83.03%\n",
      "    Early stopping at epoch 22, best val acc: 85.41%\n",
      "    Result: Val Acc=85.41%\n",
      "\n",
      "================================================================================\n",
      "Testing: Dropout = 0.3\n",
      "================================================================================\n",
      "    Epoch 10: Train Acc=94.84%, Val Acc=81.93%\n",
      "    Epoch 20: Train Acc=98.85%, Val Acc=82.94%\n",
      "    Early stopping at epoch 21, best val acc: 84.95%\n",
      "    Result: Val Acc=84.95%\n",
      "\n",
      "================================================================================\n",
      "Testing: Dropout = 0.5\n",
      "================================================================================\n",
      "    Epoch 10: Train Acc=93.86%, Val Acc=79.45%\n",
      "    Epoch 20: Train Acc=98.17%, Val Acc=83.03%\n",
      "    Early stopping at epoch 26, best val acc: 84.13%\n",
      "    Result: Val Acc=84.13%\n",
      "\n",
      "================================================================================\n",
      "Testing: Dropout = 0.7\n",
      "================================================================================\n",
      "    Epoch 10: Train Acc=95.03%, Val Acc=81.83%\n",
      "    Epoch 20: Train Acc=98.97%, Val Acc=83.03%\n",
      "    Early stopping at epoch 23, best val acc: 84.77%\n",
      "    Result: Val Acc=84.77%\n",
      "\n",
      ">>> Step 2 Results:\n",
      "Dropout      Val Acc   \n",
      "-------------------------\n",
      "0.0          85.41     %\n",
      "0.3          84.95     %\n",
      "0.5          84.13     %\n",
      "0.7          84.77     %\n",
      "\n",
      ">>> Best Dropout: 0.0, Val Acc=85.41%\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Regularization Step 2: Dropout Tuning\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"REGULARIZATION STEP 2: DROPOUT TUNING\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Using best from Step 1: grad_clip={best_grad_clip}\")\n",
    "print(f\"Using baseline settings: L1={best_l1_lambda}, L2={best_l2_lambda}\")\n",
    "\n",
    "# Test different dropout values\n",
    "dropout_options = [0.0, 0.3, 0.5, 0.7]\n",
    "\n",
    "print(f\"\\nTesting dropout values: {dropout_options}\")\n",
    "\n",
    "step2_results = []\n",
    "\n",
    "for dropout_val in dropout_options:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Testing: Dropout = {dropout_val}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Create model with best grad_clip and current dropout\n",
    "    model = RNN_Classifier_Aggregation(\n",
    "        vocab_size=embedding_vocab_size,\n",
    "        embedding_dim=embedding_dim,\n",
    "        hidden_dim=best_hidden_dim,\n",
    "        output_dim=num_classes,\n",
    "        n_layers=N_LAYERS,\n",
    "        dropout=dropout_val,\n",
    "        padding_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "        pretrained_embeddings=pretrained_embeddings,\n",
    "        aggregation=best_aggregation['method']\n",
    "    ).to(device)\n",
    "    \n",
    "    # Select optimizer\n",
    "    if best_optimizer == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=final_lr, weight_decay=best_l2_lambda)\n",
    "    elif best_optimizer == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=final_lr, momentum=0.9, weight_decay=best_l2_lambda)\n",
    "    elif best_optimizer == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=final_lr, weight_decay=best_l2_lambda)\n",
    "    elif best_optimizer == 'Adagrad':\n",
    "        optimizer = optim.Adagrad(model.parameters(), lr=final_lr, weight_decay=best_l2_lambda)\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        \n",
    "        for batch in train_iter:\n",
    "            text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(text, text_lengths)\n",
    "            \n",
    "            loss = criterion(predictions, labels)\n",
    "            \n",
    "            if best_l1_lambda > 0:\n",
    "                loss = loss + compute_l1_loss(model, best_l1_lambda)\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            if best_grad_clip > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), best_grad_clip)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        train_acc = accuracy_score(train_labels, train_preds)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_iter:\n",
    "                text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "                predictions = model(text, text_lengths)\n",
    "                loss = criterion(predictions, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                preds = torch.argmax(predictions, dim=1)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_acc = accuracy_score(val_labels, val_preds)\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), f'rnn_reg_step2_dropout{dropout_val}_best.pt')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(f\"    Early stopping at epoch {epoch+1}, best val acc: {best_val_acc*100:.2f}%\")\n",
    "                break\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"    Epoch {epoch+1}: Train Acc={train_acc*100:.2f}%, Val Acc={val_acc*100:.2f}%\")\n",
    "    \n",
    "    step2_results.append({\n",
    "        'dropout': dropout_val,\n",
    "        'val_acc': best_val_acc\n",
    "    })\n",
    "    \n",
    "    print(f\"    Result: Val Acc={best_val_acc*100:.2f}%\")\n",
    "\n",
    "# Find best dropout\n",
    "best_step2 = max(step2_results, key=lambda x: x['val_acc'])\n",
    "best_dropout = best_step2['dropout']\n",
    "\n",
    "print(f\"\\n>>> Step 2 Results:\")\n",
    "print(f\"{'Dropout':<12} {'Val Acc':<10}\")\n",
    "print(\"-\" * 25)\n",
    "for result in step2_results:\n",
    "    print(f\"{result['dropout']:<12} {result['val_acc']*100:<10.2f}%\")\n",
    "print(f\"\\n>>> Best Dropout: {best_dropout}, Val Acc={best_step2['val_acc']*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f14317b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "REGULARIZATION STEP 3: L1 REGULARIZATION TUNING\n",
      "================================================================================\n",
      "Using best from Steps 1-2: grad_clip=0.0, dropout=0.0\n",
      "Using baseline setting: L2=0.0\n",
      "\n",
      "Testing L1 lambda values: [0.0, 1e-06, 1e-05, 0.0001]\n",
      "\n",
      "================================================================================\n",
      "Testing: L1 Lambda = 0.0\n",
      "================================================================================\n",
      "    Epoch 10: Train Acc=94.29%, Val Acc=82.20%\n",
      "    Epoch 20: Train Acc=98.95%, Val Acc=74.50%\n",
      "    Early stopping at epoch 29, best val acc: 83.85%\n",
      "    Result: Val Acc=83.85%\n",
      "\n",
      "================================================================================\n",
      "Testing: L1 Lambda = 1e-06\n",
      "================================================================================\n",
      "    Epoch 10: Train Acc=93.63%, Val Acc=82.48%\n",
      "    Epoch 20: Train Acc=98.83%, Val Acc=83.30%\n",
      "    Early stopping at epoch 26, best val acc: 84.13%\n",
      "    Result: Val Acc=84.13%\n",
      "\n",
      "================================================================================\n",
      "Testing: L1 Lambda = 1e-05\n",
      "================================================================================\n",
      "    Epoch 10: Train Acc=90.51%, Val Acc=79.72%\n",
      "    Epoch 20: Train Acc=96.45%, Val Acc=83.21%\n",
      "    Epoch 30: Train Acc=98.42%, Val Acc=81.47%\n",
      "    Early stopping at epoch 31, best val acc: 83.30%\n",
      "    Result: Val Acc=83.30%\n",
      "\n",
      "================================================================================\n",
      "Testing: L1 Lambda = 0.0001\n",
      "================================================================================\n",
      "    Epoch 10: Train Acc=75.70%, Val Acc=71.56%\n",
      "    Epoch 20: Train Acc=85.05%, Val Acc=76.15%\n",
      "    Epoch 30: Train Acc=89.43%, Val Acc=78.81%\n",
      "    Early stopping at epoch 33, best val acc: 79.36%\n",
      "    Result: Val Acc=79.36%\n",
      "\n",
      ">>> Step 3 Results:\n",
      "L1 Lambda    Val Acc   \n",
      "-------------------------\n",
      "0e+00        83.85     %\n",
      "1e-06        84.13     %\n",
      "1e-05        83.30     %\n",
      "1e-04        79.36     %\n",
      "\n",
      ">>> Best L1 Lambda: 1e-06, Val Acc=84.13%\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Regularization Step 3: L1 Regularization Tuning\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"REGULARIZATION STEP 3: L1 REGULARIZATION TUNING\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Using best from Steps 1-2: grad_clip={best_grad_clip}, dropout={best_dropout}\")\n",
    "print(f\"Using baseline setting: L2={best_l2_lambda}\")\n",
    "\n",
    "# Test different L1 lambda values\n",
    "l1_lambda_options = [0.0, 1e-6, 1e-5, 1e-4]\n",
    "\n",
    "print(f\"\\nTesting L1 lambda values: {l1_lambda_options}\")\n",
    "\n",
    "step3_results = []\n",
    "\n",
    "for l1_lambda in l1_lambda_options:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Testing: L1 Lambda = {l1_lambda}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Create model with best grad_clip, dropout, and current L1\n",
    "    model = RNN_Classifier_Aggregation(\n",
    "        vocab_size=embedding_vocab_size,\n",
    "        embedding_dim=embedding_dim,\n",
    "        hidden_dim=best_hidden_dim,\n",
    "        output_dim=num_classes,\n",
    "        n_layers=N_LAYERS,\n",
    "        dropout=best_dropout,\n",
    "        padding_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "        pretrained_embeddings=pretrained_embeddings,\n",
    "        aggregation=best_aggregation['method']\n",
    "    ).to(device)\n",
    "    \n",
    "    # Select optimizer\n",
    "    if best_optimizer == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=final_lr, weight_decay=best_l2_lambda)\n",
    "    elif best_optimizer == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=final_lr, momentum=0.9, weight_decay=best_l2_lambda)\n",
    "    elif best_optimizer == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=final_lr, weight_decay=best_l2_lambda)\n",
    "    elif best_optimizer == 'Adagrad':\n",
    "        optimizer = optim.Adagrad(model.parameters(), lr=final_lr, weight_decay=best_l2_lambda)\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        \n",
    "        for batch in train_iter:\n",
    "            text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(text, text_lengths)\n",
    "            \n",
    "            loss = criterion(predictions, labels)\n",
    "            \n",
    "            if l1_lambda > 0:\n",
    "                loss = loss + compute_l1_loss(model, l1_lambda)\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            if best_grad_clip > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), best_grad_clip)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        train_acc = accuracy_score(train_labels, train_preds)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_iter:\n",
    "                text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "                predictions = model(text, text_lengths)\n",
    "                loss = criterion(predictions, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                preds = torch.argmax(predictions, dim=1)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_acc = accuracy_score(val_labels, val_preds)\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), f'rnn_reg_step3_l1{l1_lambda}_best.pt')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(f\"    Early stopping at epoch {epoch+1}, best val acc: {best_val_acc*100:.2f}%\")\n",
    "                break\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"    Epoch {epoch+1}: Train Acc={train_acc*100:.2f}%, Val Acc={val_acc*100:.2f}%\")\n",
    "    \n",
    "    step3_results.append({\n",
    "        'l1_lambda': l1_lambda,\n",
    "        'val_acc': best_val_acc\n",
    "    })\n",
    "    \n",
    "    print(f\"    Result: Val Acc={best_val_acc*100:.2f}%\")\n",
    "\n",
    "# Find best L1 lambda\n",
    "best_step3 = max(step3_results, key=lambda x: x['val_acc'])\n",
    "best_l1_lambda = best_step3['l1_lambda']\n",
    "\n",
    "print(f\"\\n>>> Step 3 Results:\")\n",
    "print(f\"{'L1 Lambda':<12} {'Val Acc':<10}\")\n",
    "print(\"-\" * 25)\n",
    "for result in step3_results:\n",
    "    print(f\"{result['l1_lambda']:<12.0e} {result['val_acc']*100:<10.2f}%\")\n",
    "print(f\"\\n>>> Best L1 Lambda: {best_l1_lambda}, Val Acc={best_step3['val_acc']*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89324596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "REGULARIZATION STEP 4: L2 REGULARIZATION TUNING\n",
      "================================================================================\n",
      "Using best from Steps 1-3: grad_clip=0.0, dropout=0.0, L1=1e-06\n",
      "\n",
      "Testing L2 lambda values: [0.0, 1e-05, 0.0001, 0.001]\n",
      "\n",
      "================================================================================\n",
      "Testing: L2 Lambda = 0.0\n",
      "================================================================================\n",
      "    Epoch 10: Train Acc=92.73%, Val Acc=81.83%\n",
      "    Epoch 20: Train Acc=97.78%, Val Acc=84.40%\n",
      "    Early stopping at epoch 23, best val acc: 84.40%\n",
      "    Result: Val Acc=84.40%\n",
      "\n",
      "================================================================================\n",
      "Testing: L2 Lambda = 1e-05\n",
      "================================================================================\n",
      "    Epoch 10: Train Acc=93.83%, Val Acc=80.55%\n",
      "    Early stopping at epoch 19, best val acc: 83.94%\n",
      "    Result: Val Acc=83.94%\n",
      "\n",
      "================================================================================\n",
      "Testing: L2 Lambda = 0.0001\n",
      "================================================================================\n",
      "    Epoch 10: Train Acc=94.38%, Val Acc=83.21%\n",
      "    Early stopping at epoch 19, best val acc: 83.39%\n",
      "    Result: Val Acc=83.39%\n",
      "\n",
      "================================================================================\n",
      "Testing: L2 Lambda = 0.001\n",
      "================================================================================\n",
      "    Epoch 10: Train Acc=91.79%, Val Acc=79.82%\n",
      "    Epoch 20: Train Acc=97.25%, Val Acc=81.83%\n",
      "    Early stopping at epoch 24, best val acc: 83.58%\n",
      "    Result: Val Acc=83.58%\n",
      "\n",
      ">>> Step 4 Results:\n",
      "L2 Lambda    Val Acc   \n",
      "-------------------------\n",
      "0e+00        84.40     %\n",
      "1e-05        83.94     %\n",
      "1e-04        83.39     %\n",
      "1e-03        83.58     %\n",
      "\n",
      ">>> Best L2 Lambda: 0.0, Val Acc=84.40%\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Regularization Step 4: L2 Regularization Tuning\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"REGULARIZATION STEP 4: L2 REGULARIZATION TUNING\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Using best from Steps 1-3: grad_clip={best_grad_clip}, dropout={best_dropout}, L1={best_l1_lambda}\")\n",
    "\n",
    "# Test different L2 lambda values (via weight_decay)\n",
    "l2_lambda_options = [0.0, 1e-5, 1e-4, 1e-3]\n",
    "\n",
    "print(f\"\\nTesting L2 lambda values: {l2_lambda_options}\")\n",
    "\n",
    "step4_results = []\n",
    "\n",
    "for l2_lambda in l2_lambda_options:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Testing: L2 Lambda = {l2_lambda}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Create model with best grad_clip, dropout, L1, and current L2\n",
    "    model = RNN_Classifier_Aggregation(\n",
    "        vocab_size=embedding_vocab_size,\n",
    "        embedding_dim=embedding_dim,\n",
    "        hidden_dim=best_hidden_dim,\n",
    "        output_dim=num_classes,\n",
    "        n_layers=N_LAYERS,\n",
    "        dropout=best_dropout,\n",
    "        padding_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "        pretrained_embeddings=pretrained_embeddings,\n",
    "        aggregation=best_aggregation['method']\n",
    "    ).to(device)\n",
    "    \n",
    "    # Select optimizer with L2 regularization (weight_decay)\n",
    "    if best_optimizer == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=final_lr, weight_decay=l2_lambda)\n",
    "    elif best_optimizer == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=final_lr, momentum=0.9, weight_decay=l2_lambda)\n",
    "    elif best_optimizer == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=final_lr, weight_decay=l2_lambda)\n",
    "    elif best_optimizer == 'Adagrad':\n",
    "        optimizer = optim.Adagrad(model.parameters(), lr=final_lr, weight_decay=l2_lambda)\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        \n",
    "        for batch in train_iter:\n",
    "            text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(text, text_lengths)\n",
    "            \n",
    "            loss = criterion(predictions, labels)\n",
    "            \n",
    "            if best_l1_lambda > 0:\n",
    "                loss = loss + compute_l1_loss(model, best_l1_lambda)\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            if best_grad_clip > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), best_grad_clip)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        train_acc = accuracy_score(train_labels, train_preds)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_iter:\n",
    "                text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "                predictions = model(text, text_lengths)\n",
    "                loss = criterion(predictions, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                preds = torch.argmax(predictions, dim=1)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_acc = accuracy_score(val_labels, val_preds)\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), f'rnn_reg_step4_l2{l2_lambda}_best.pt')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(f\"    Early stopping at epoch {epoch+1}, best val acc: {best_val_acc*100:.2f}%\")\n",
    "                break\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"    Epoch {epoch+1}: Train Acc={train_acc*100:.2f}%, Val Acc={val_acc*100:.2f}%\")\n",
    "    \n",
    "    step4_results.append({\n",
    "        'l2_lambda': l2_lambda,\n",
    "        'val_acc': best_val_acc\n",
    "    })\n",
    "    \n",
    "    print(f\"    Result: Val Acc={best_val_acc*100:.2f}%\")\n",
    "\n",
    "# Find best L2 lambda\n",
    "best_step4 = max(step4_results, key=lambda x: x['val_acc'])\n",
    "best_l2_lambda = best_step4['l2_lambda']\n",
    "\n",
    "print(f\"\\n>>> Step 4 Results:\")\n",
    "print(f\"{'L2 Lambda':<12} {'Val Acc':<10}\")\n",
    "print(\"-\" * 25)\n",
    "for result in step4_results:\n",
    "    print(f\"{result['l2_lambda']:<12.0e} {result['val_acc']*100:<10.2f}%\")\n",
    "print(f\"\\n>>> Best L2 Lambda: {best_l2_lambda}, Val Acc={best_step4['val_acc']*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9a5b338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "REGULARIZATION FINAL: ALL BEST SETTINGS COMBINED\n",
      "================================================================================\n",
      "Best settings from all steps:\n",
      "    Gradient Clipping: 0.0\n",
      "    Dropout: 0.0\n",
      "    L1 Lambda: 1e-06\n",
      "    L2 Lambda: 0.0\n",
      "\n",
      ">>> Training final model with all best regularization settings...\n",
      "    Epoch 10: Train Acc=93.33%, Val Acc=79.63%\n",
      "    Epoch 20: Train Acc=98.26%, Val Acc=80.92%\n",
      "    Early stopping at epoch 22, best val acc: 83.58%\n",
      "\n",
      ">>> Final Combined Results:\n",
      "    Configuration:\n",
      "      - Gradient Clipping: 0.0\n",
      "      - Dropout: 0.0\n",
      "      - L1 Lambda: 1e-06\n",
      "      - L2 Lambda: 0.0\n",
      "    Validation Acc: 83.58%\n",
      "    Test Acc: 87.20%\n",
      "    Test F1: 0.8702\n",
      "    Test AUC-ROC: 0.9619\n",
      "\n",
      ">>> Comparison with Baseline:\n",
      "    Baseline Test Acc: 89.60%\n",
      "    Final Regularized Test Acc: 87.20%\n",
      "    Improvement: -2.40% (-2.68% relative)\n",
      "\n",
      ">>> Plotting training curves for best configuration and regularization...\n",
      "    Saved training curves to 'best_config_training_curves.png'\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "REGULARIZATION TUNING COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Regularization Final: All Best Combined\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"REGULARIZATION FINAL: ALL BEST SETTINGS COMBINED\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Best settings from all steps:\")\n",
    "print(f\"    Gradient Clipping: {best_grad_clip}\")\n",
    "print(f\"    Dropout: {best_dropout}\")\n",
    "print(f\"    L1 Lambda: {best_l1_lambda}\")\n",
    "print(f\"    L2 Lambda: {best_l2_lambda}\")\n",
    "\n",
    "# Create model with all best settings\n",
    "model = RNN_Classifier_Aggregation(\n",
    "    vocab_size=embedding_vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_dim=best_hidden_dim,\n",
    "    output_dim=num_classes,\n",
    "    n_layers=N_LAYERS,\n",
    "    dropout=best_dropout,\n",
    "    padding_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "    pretrained_embeddings=pretrained_embeddings,\n",
    "    aggregation=best_aggregation['method']\n",
    ").to(device)\n",
    "\n",
    "# Select optimizer with best L2 (weight_decay)\n",
    "if best_optimizer == 'Adam':\n",
    "    optimizer = optim.Adam(model.parameters(), lr=final_lr, weight_decay=best_l2_lambda)\n",
    "elif best_optimizer == 'SGD':\n",
    "    optimizer = optim.SGD(model.parameters(), lr=final_lr, momentum=0.9, weight_decay=best_l2_lambda)\n",
    "elif best_optimizer == 'RMSprop':\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=final_lr, weight_decay=best_l2_lambda)\n",
    "elif best_optimizer == 'Adagrad':\n",
    "    optimizer = optim.Adagrad(model.parameters(), lr=final_lr, weight_decay=best_l2_lambda)\n",
    "\n",
    "# Training loop\n",
    "best_val_acc = 0.0\n",
    "patience_counter = 0\n",
    "\n",
    "# Store training history for plotting\n",
    "train_losses = []\n",
    "val_accs = []\n",
    "\n",
    "print(f\"\\n>>> Training final model with all best regularization settings...\")\n",
    "\n",
    "for epoch in range(MAX_EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_preds = []\n",
    "    train_labels = []\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch in train_iter:\n",
    "        num_batches += 1\n",
    "        text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(text, text_lengths)\n",
    "        \n",
    "        loss = criterion(predictions, labels)\n",
    "        \n",
    "        if best_l1_lambda > 0:\n",
    "            loss = loss + compute_l1_loss(model, best_l1_lambda)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        if best_grad_clip > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), best_grad_clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        preds = torch.argmax(predictions, dim=1)\n",
    "        train_preds.extend(preds.cpu().numpy())\n",
    "        train_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    train_acc = accuracy_score(train_labels, train_preds)\n",
    "    # Calculate average training loss (train_loss is sum over all batches)\n",
    "    num_train_batches = len(train_labels) // train_iter.batch_size + (1 if len(train_labels) % train_iter.batch_size != 0 else 0)\n",
    "    train_loss_avg = train_loss / num_train_batches if num_train_batches > 0 else train_loss\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_iter:\n",
    "            text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "            predictions = model(text, text_lengths)\n",
    "            loss = criterion(predictions, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    val_acc = accuracy_score(val_labels, val_preds)\n",
    "    \n",
    "    # Store training history for plotting\n",
    "    train_losses.append(train_loss_avg)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), 'rnn_reg_final_best.pt')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(f\"    Early stopping at epoch {epoch+1}, best val acc: {best_val_acc*100:.2f}%\")\n",
    "            break\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"    Epoch {epoch+1}: Train Acc={train_acc*100:.2f}%, Val Acc={val_acc*100:.2f}%\")\n",
    "\n",
    "# Evaluate on test set\n",
    "model.load_state_dict(torch.load('rnn_reg_final_best.pt'))\n",
    "model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "test_preds = []\n",
    "test_labels = []\n",
    "test_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_iter:\n",
    "        text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "        predictions = model(text, text_lengths)\n",
    "        loss = criterion(predictions, labels)\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        probs = torch.softmax(predictions, dim=1)\n",
    "        preds = torch.argmax(predictions, dim=1)\n",
    "        test_preds.extend(preds.cpu().numpy())\n",
    "        test_labels.extend(labels.cpu().numpy())\n",
    "        test_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "test_acc = accuracy_score(test_labels, test_preds)\n",
    "test_f1 = f1_score(test_labels, test_preds, average='weighted')\n",
    "test_loss_avg = test_loss / len(test_iter)\n",
    "\n",
    "try:\n",
    "    test_probs_array = np.array(test_probs)\n",
    "    test_labels_bin = label_binarize(test_labels, classes=range(num_classes))\n",
    "    test_auc = roc_auc_score(test_labels_bin, test_probs_array, average='weighted', multi_class='ovr')\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not calculate AUC-ROC: {e}\")\n",
    "    test_auc = 0.0\n",
    "\n",
    "final_results = {\n",
    "    'name': 'final_combined',\n",
    "    'dropout': best_dropout,\n",
    "    'grad_clip': best_grad_clip,\n",
    "    'l1_lambda': best_l1_lambda,\n",
    "    'l2_lambda': best_l2_lambda,\n",
    "    'val_acc': best_val_acc,\n",
    "    'test_acc': test_acc,\n",
    "    'test_f1': test_f1,\n",
    "    'test_auc': test_auc\n",
    "}\n",
    "\n",
    "print(f\"\\n>>> Final Combined Results:\")\n",
    "print(f\"    Configuration:\")\n",
    "print(f\"      - Gradient Clipping: {best_grad_clip}\")\n",
    "print(f\"      - Dropout: {best_dropout}\")\n",
    "print(f\"      - L1 Lambda: {best_l1_lambda}\")\n",
    "print(f\"      - L2 Lambda: {best_l2_lambda}\")\n",
    "print(f\"    Validation Acc: {best_val_acc*100:.2f}%\")\n",
    "print(f\"    Test Acc: {test_acc*100:.2f}%\")\n",
    "print(f\"    Test F1: {test_f1:.4f}\")\n",
    "print(f\"    Test AUC-ROC: {test_auc:.4f}\")\n",
    "\n",
    "# Compare with baseline\n",
    "improvement = test_acc - baseline_results['test_acc']\n",
    "improvement_pct = (improvement / baseline_results['test_acc']) * 100 if baseline_results['test_acc'] > 0 else 0\n",
    "\n",
    "print(f\"\\n>>> Comparison with Baseline:\")\n",
    "print(f\"    Baseline Test Acc: {baseline_results['test_acc']*100:.2f}%\")\n",
    "print(f\"    Final Regularized Test Acc: {test_acc*100:.2f}%\")\n",
    "print(f\"    Improvement: {improvement*100:+.2f}% ({improvement_pct:+.2f}% relative)\")\n",
    "\n",
    "# Plot training curves for best configuration and regularization\n",
    "print(f\"\\n>>> Plotting training curves for best configuration and regularization...\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Training Loss vs Epochs\n",
    "epochs = range(1, len(train_losses) + 1)\n",
    "ax1.plot(epochs, train_losses, 'b-', label='Training Loss', linewidth=2, marker='o', markersize=4)\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Training Loss', fontsize=12)\n",
    "ax1.set_title('Training Loss Curve', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xlim(left=1)\n",
    "\n",
    "# Plot 2: Validation Accuracy vs Epochs\n",
    "ax2.plot(epochs, [acc*100 for acc in val_accs], 'r-', label='Validation Accuracy', linewidth=2, marker='s', markersize=4)\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Validation Accuracy (%)', fontsize=12)\n",
    "ax2.set_title('Validation Accuracy Curve', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xlim(left=1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('best_config_training_curves.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"    Saved training curves to 'best_config_training_curves.png'\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"REGULARIZATION TUNING COMPLETE\")\n",
    "print(f\"{'='*80}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "70f4086d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PART 2(e): TOPIC-WISE ACCURACY EVALUATION\n",
      "================================================================================\n",
      "\n",
      ">>> Using model from regularization tuning...\n",
      "     Using existing model from previous cell\n",
      "    Model vocab size: 8118\n",
      "\n",
      ">>> Evaluating on test set...\n",
      "\n",
      "================================================================================\n",
      "TOPIC-WISE ACCURACY ON TEST SET\n",
      "================================================================================\n",
      "Topic      Accuracy     Correct    Total      % of Test Set  \n",
      "--------------------------------------------------------------------------------\n",
      "ABBR       66.67        6          9          1.80           \n",
      "DESC       99.28        137        138        27.60          \n",
      "ENTY       69.15        65         94         18.80          \n",
      "HUM        90.77        59         65         13.00          \n",
      "LOC        90.12        73         81         16.20          \n",
      "NUM        84.96        96         113        22.60          \n",
      "--------------------------------------------------------------------------------\n",
      "OVERALL    87.20        436        500        100.00         \n",
      "\n",
      "================================================================================\n",
      "DISCUSSION: FACTORS AFFECTING TOPIC-WISE ACCURACY\n",
      "================================================================================\n",
      "\n",
      "1. CLASS IMBALANCE IN TRAINING DATA:\n",
      "Topic      Train Count     Train %      Test Count   Test %      \n",
      "----------------------------------------------------------------------\n",
      "ABBR       69              1.58         9            1.80        \n",
      "DESC       930             21.32        138          27.60       \n",
      "ENTY       1000            22.93        94           18.80       \n",
      "HUM        978             22.42        65           13.00       \n",
      "LOC        668             15.31        81           16.20       \n",
      "NUM        717             16.44        113          22.60       \n",
      "\n",
      "2. KEY OBSERVATIONS AND POTENTIAL CAUSES:\n",
      "\n",
      "   a) Class Imbalance Effect:\n",
      "      - Topics with fewer training examples (e.g., ABBR with only 1.58% of data)\n",
      "        may have lower accuracy due to insufficient learning signal\n",
      "      - The model may be biased toward more frequent classes during training\n",
      "\n",
      "   b) Semantic Complexity:\n",
      "      - Some topics may have more ambiguous or overlapping characteristics\n",
      "      - For example, ABBR (abbreviations) might be confused with other categories\n",
      "        if the context is not clear enough\n",
      "\n",
      "   c) Vocabulary and OOV Rates:\n",
      "      - Topics with higher OOV rates (as seen in Part 1) may have lower accuracy\n",
      "      - ABBR had 9.70% OOV rate (highest), which could contribute to lower performance\n",
      "\n",
      "   d) Question Type Characteristics:\n",
      "      - ENTY (entities) and HUM (humans) are more distinct and may be easier to classify\n",
      "      - DESC (descriptions) might overlap with other categories semantically\n",
      "      - NUM (numeric) questions may have distinctive patterns that aid classification\n",
      "\n",
      "   e) Model Capacity and Representation:\n",
      "      - The RNN may capture certain patterns better than others\n",
      "      - Aggregation method (attention/mean/max/last) may favor certain topic structures\n",
      "\n",
      "3. RECOMMENDATIONS FOR IMPROVEMENT:\n",
      "   - Use class weights in loss function to handle imbalance\n",
      "   - Apply data augmentation or oversampling for minority classes\n",
      "   - Consider focal loss to focus on hard examples\n",
      "   - Fine-tune embeddings specifically for underrepresented topics\n",
      "   - Use ensemble methods combining multiple models\n",
      "\n",
      "================================================================================\n",
      "TOPIC-WISE EVALUATION COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Part 2(e): Topic-wise Accuracy Evaluation on Test Set\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 2(e): TOPIC-WISE ACCURACY EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load the best model from regularization tuning\n",
    "# Use the model from the previous cell (regularization final) if available\n",
    "# Otherwise, load it from the saved checkpoint\n",
    "\n",
    "print(\"\\n>>> Using model from regularization tuning...\")\n",
    "\n",
    "# Check if 'model' variable exists from the previous cell (cell 26)\n",
    "try:\n",
    "    # Try to use the model from the previous cell\n",
    "    if 'model' in locals() or 'model' in globals():\n",
    "        # Verify it's a valid model instance\n",
    "        if hasattr(model, 'embedding') and hasattr(model, 'eval'):\n",
    "            final_model = model\n",
    "            final_model.eval()\n",
    "            saved_vocab_size = final_model.embedding.weight.shape[0]\n",
    "            print(f\"     Using existing model from previous cell\")\n",
    "            print(f\"    Model vocab size: {saved_vocab_size}\")\n",
    "        else:\n",
    "            raise AttributeError(\"Model exists but is not valid\")\n",
    "    else:\n",
    "        raise NameError(\"Model variable not found\")\n",
    "except (NameError, AttributeError):\n",
    "    # Model doesn't exist or is invalid, load from checkpoint\n",
    "    print(\"    Model not found in previous cell, loading from checkpoint...\")\n",
    "    try:\n",
    "        checkpoint = torch.load('weights/rnn_reg_final_best.pt', map_location=device)\n",
    "    except FileNotFoundError:\n",
    "        checkpoint = torch.load('rnn_reg_final_best.pt', map_location=device)\n",
    "    \n",
    "    # Infer configuration from saved state dict\n",
    "    saved_vocab_size = checkpoint['embedding.weight'].shape[0]\n",
    "    saved_hidden_dim = checkpoint['rnn.weight_ih_l0'].shape[0]\n",
    "    has_attention = 'attention.weight' in checkpoint\n",
    "    saved_aggregation = 'attention' if has_attention else 'last'\n",
    "    \n",
    "    # Recreate model\n",
    "    final_model = RNN_Classifier_Aggregation(\n",
    "        vocab_size=saved_vocab_size,\n",
    "        embedding_dim=embedding_dim,\n",
    "        hidden_dim=saved_hidden_dim,\n",
    "        output_dim=num_classes,\n",
    "        n_layers=N_LAYERS,\n",
    "        dropout=best_dropout,\n",
    "        padding_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "        pretrained_embeddings=None,\n",
    "        aggregation=saved_aggregation\n",
    "    ).to(device)\n",
    "    \n",
    "    final_model.load_state_dict(checkpoint, strict=True)\n",
    "    final_model.eval()\n",
    "    print(f\"     Model loaded from checkpoint (vocab_size={saved_vocab_size})\")\n",
    "\n",
    "# Function to evaluate per topic\n",
    "def evaluate_per_topic(model, iterator, device, max_vocab_size=None):\n",
    "    \"\"\"\n",
    "    Evaluate model performance per topic category on the test set.\n",
    "    Returns a dictionary with accuracy for each topic.\n",
    "    \n",
    "    Args:\n",
    "        model: The trained model\n",
    "        iterator: Data iterator for test set\n",
    "        device: Device to run on\n",
    "        max_vocab_size: Maximum valid vocabulary size (to clip token indices)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Dictionary to store correct and total counts per topic\n",
    "    topic_correct = defaultdict(int)\n",
    "    topic_total = defaultdict(int)\n",
    "    \n",
    "    # Get label vocabulary for mapping\n",
    "    label_to_idx = LABEL.vocab.stoi\n",
    "    idx_to_label = LABEL.vocab.itos\n",
    "    \n",
    "    # Get <unk> token index for mapping out-of-range tokens\n",
    "    unk_idx = TEXT.vocab.stoi.get(TEXT.unk_token, 0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            # Process batch (should be on CPU from the iterator)\n",
    "            text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "            \n",
    "            # Clip token indices to valid range if max_vocab_size is specified\n",
    "            # This handles cases where the current vocab is larger than the saved model's vocab\n",
    "            if max_vocab_size is not None:\n",
    "                # Map any indices >= max_vocab_size to <unk> token\n",
    "                text = torch.where(text >= max_vocab_size, \n",
    "                                 torch.tensor(unk_idx, device=text.device, dtype=text.dtype), \n",
    "                                 text)\n",
    "                # Also ensure no negative indices\n",
    "                text = torch.clamp(text, min=0)\n",
    "            \n",
    "            # Move tensors to the actual device after clipping\n",
    "            text = text.to(device)\n",
    "            text_lengths = text_lengths.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            predictions = model(text, text_lengths)\n",
    "            \n",
    "            # Get predicted labels\n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            \n",
    "            # Convert to numpy for easier processing\n",
    "            preds_np = preds.cpu().numpy()\n",
    "            labels_np = labels.cpu().numpy()\n",
    "            \n",
    "            # Count correct and total for each topic\n",
    "            for pred_idx, true_idx in zip(preds_np, labels_np):\n",
    "                true_label = idx_to_label[true_idx]\n",
    "                topic_total[true_label] += 1\n",
    "                \n",
    "                if pred_idx == true_idx:\n",
    "                    topic_correct[true_label] += 1\n",
    "    \n",
    "    # Calculate accuracy per topic\n",
    "    topic_accuracies = {}\n",
    "    for label in sorted(topic_total.keys()):\n",
    "        if topic_total[label] > 0:\n",
    "            acc = topic_correct[label] / topic_total[label]\n",
    "            topic_accuracies[label] = {\n",
    "                'accuracy': acc,\n",
    "                'correct': topic_correct[label],\n",
    "                'total': topic_total[label]\n",
    "            }\n",
    "    \n",
    "    return topic_accuracies\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"\\n>>> Evaluating on test set...\")\n",
    "\n",
    "# Create a CPU iterator to avoid CUDA errors during numericalization with invalid token indices\n",
    "# We'll process on CPU, clip indices, then move to device\n",
    "from torchtext import data\n",
    "cpu_device = torch.device('cpu')\n",
    "test_iter_cpu = data.BucketIterator(\n",
    "    test_data,\n",
    "    batch_size=test_iter.batch_size,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    device=cpu_device,\n",
    "    sort=False,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Pass max_vocab_size to clip token indices to the saved model's vocabulary size\n",
    "topic_accuracies = evaluate_per_topic(final_model, test_iter_cpu, device, max_vocab_size=saved_vocab_size)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TOPIC-WISE ACCURACY ON TEST SET\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Topic':<10} {'Accuracy':<12} {'Correct':<10} {'Total':<10} {'% of Test Set':<15}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Calculate total test samples for percentage calculation\n",
    "total_test_samples = sum(acc['total'] for acc in topic_accuracies.values())\n",
    "\n",
    "for topic in sorted(topic_accuracies.keys()):\n",
    "    acc_info = topic_accuracies[topic]\n",
    "    acc_pct = acc_info['accuracy'] * 100\n",
    "    correct = acc_info['correct']\n",
    "    total = acc_info['total']\n",
    "    pct_of_test = (total / total_test_samples) * 100 if total_test_samples > 0 else 0\n",
    "    \n",
    "    print(f\"{topic:<10} {acc_pct:<12.2f} {correct:<10} {total:<10} {pct_of_test:<15.2f}\")\n",
    "\n",
    "# Calculate overall accuracy\n",
    "overall_correct = sum(acc_info['correct'] for acc_info in topic_accuracies.values())\n",
    "overall_total = sum(acc_info['total'] for acc_info in topic_accuracies.values())\n",
    "overall_acc = overall_correct / overall_total if overall_total > 0 else 0\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'OVERALL':<10} {overall_acc*100:<12.2f} {overall_correct:<10} {overall_total:<10} {'100.00':<15}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Discussion: What may cause differences in accuracies across topics\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DISCUSSION: FACTORS AFFECTING TOPIC-WISE ACCURACY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get training distribution for comparison\n",
    "train_label_counts = Counter([ex.label for ex in train_data.examples])\n",
    "total_train = len(train_data.examples)\n",
    "\n",
    "print(\"\\n1. CLASS IMBALANCE IN TRAINING DATA:\")\n",
    "print(f\"{'Topic':<10} {'Train Count':<15} {'Train %':<12} {'Test Count':<12} {'Test %':<12}\")\n",
    "print(\"-\" * 70)\n",
    "for topic in sorted(topic_accuracies.keys()):\n",
    "    train_count = train_label_counts.get(topic, 0)\n",
    "    train_pct = (train_count / total_train) * 100 if total_train > 0 else 0\n",
    "    test_count = topic_accuracies[topic]['total']\n",
    "    test_pct = (test_count / total_test_samples) * 100 if total_test_samples > 0 else 0\n",
    "    print(f\"{topic:<10} {train_count:<15} {train_pct:<12.2f} {test_count:<12} {test_pct:<12.2f}\")\n",
    "\n",
    "print(\"\\n2. KEY OBSERVATIONS AND POTENTIAL CAUSES:\")\n",
    "print(\"\\n   a) Class Imbalance Effect:\")\n",
    "print(\"      - Topics with fewer training examples (e.g., ABBR with only 1.58% of data)\")\n",
    "print(\"        may have lower accuracy due to insufficient learning signal\")\n",
    "print(\"      - The model may be biased toward more frequent classes during training\")\n",
    "\n",
    "print(\"\\n   b) Semantic Complexity:\")\n",
    "print(\"      - Some topics may have more ambiguous or overlapping characteristics\")\n",
    "print(\"      - For example, ABBR (abbreviations) might be confused with other categories\")\n",
    "print(\"        if the context is not clear enough\")\n",
    "\n",
    "print(\"\\n   c) Vocabulary and OOV Rates:\")\n",
    "print(\"      - Topics with higher OOV rates (as seen in Part 1) may have lower accuracy\")\n",
    "print(\"      - ABBR had 9.70% OOV rate (highest), which could contribute to lower performance\")\n",
    "\n",
    "print(\"\\n   d) Question Type Characteristics:\")\n",
    "print(\"      - ENTY (entities) and HUM (humans) are more distinct and may be easier to classify\")\n",
    "print(\"      - DESC (descriptions) might overlap with other categories semantically\")\n",
    "print(\"      - NUM (numeric) questions may have distinctive patterns that aid classification\")\n",
    "\n",
    "print(\"\\n   e) Model Capacity and Representation:\")\n",
    "print(\"      - The RNN may capture certain patterns better than others\")\n",
    "print(\"      - Aggregation method (attention/mean/max/last) may favor certain topic structures\")\n",
    "\n",
    "print(\"\\n3. RECOMMENDATIONS FOR IMPROVEMENT:\")\n",
    "print(\"   - Use class weights in loss function to handle imbalance\")\n",
    "print(\"   - Apply data augmentation or oversampling for minority classes\")\n",
    "print(\"   - Consider focal loss to focus on hard examples\")\n",
    "print(\"   - Fine-tune embeddings specifically for underrepresented topics\")\n",
    "print(\"   - Use ensemble methods combining multiple models\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TOPIC-WISE EVALUATION COMPLETE\")\n",
    "print(\"=\"*80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
