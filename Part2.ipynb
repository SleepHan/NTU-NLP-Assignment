{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63bc603c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, random, torch\n",
    "\n",
    "# IMPORTANT: Fix for PyTorch/IPython compatibility issue\n",
    "# This must run BEFORE importing torch to avoid decorator conflicts\n",
    "# This fixes the \"disable() got an unexpected keyword argument 'wrapping'\" error\n",
    "\n",
    "# Method 1: Try to disable dynamo via environment variable (needs to be set before import)\n",
    "os.environ.setdefault('TORCH_COMPILE_DISABLE', '1')\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "# Method 2: Patch torch._dynamo.disable decorator after import\n",
    "try:\n",
    "    import torch._dynamo\n",
    "    # Patch the disable function to ignore the 'wrapping' parameter\n",
    "    if hasattr(torch._dynamo, 'disable'):\n",
    "        def patched_disable(fn=None, *args, **kwargs):\n",
    "            # Remove problematic 'wrapping' parameter if present\n",
    "            if 'wrapping' in kwargs:\n",
    "                kwargs.pop('wrapping')\n",
    "            if fn is None:\n",
    "                # Decorator usage: @disable\n",
    "                return lambda f: f\n",
    "            # Function usage: disable(fn) or disable(fn, **kwargs)\n",
    "            # Simply return the function unwrapped to avoid recursion\n",
    "            # The original disable was causing issues, so we bypass it entirely\n",
    "            return fn\n",
    "        torch._dynamo.disable = patched_disable\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not patch torch._dynamo: {e}\")\n",
    "    pass  # If patching fails, continue anyway\n",
    "\n",
    "from torchtext import data\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "\n",
    "os.environ['GENSIM_DATA_DIR'] = os.path.join(os.getcwd(), 'gensim-data')\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from utils import data_prep, count_parameters, process_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "650b5e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Prepping Data...\n",
      "[+] Test set formed!\n",
      "[+] Train and Validation sets formed!\n",
      "[+] Data prepped successfully!\n",
      "[*] Retrieving pretrained word embeddings...\n",
      "[*] Loading fasttext model...\n",
      "[+] Model loaded!\n",
      "[*] Forming embedding matrix...\n",
      "[+] Embedding matrix formed!\n",
      "[+] Embeddings retrieved successfully!\n",
      "\n",
      "Label distribution in training set:\n",
      "- ABBR: 69 samples (1.58%)\n",
      "- DESC: 930 samples (21.32%)\n",
      "- ENTY: 1000 samples (22.93%)\n",
      "- HUM: 978 samples (22.42%)\n",
      "- LOC: 668 samples (15.31%)\n",
      "- NUM: 717 samples (16.44%)\n",
      "Total samples: 4362, Sum of percentages: 100.00%\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "train_data, validation_data, test_data, LABEL, TEXT, pretrained_embed = data_prep(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea2b5c8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtext.vocab.Vocab at 0x1e918fc3470>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dee8ca7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2ac13be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of classes: 6\n",
      "Classes: ['ENTY', 'HUM', 'DESC', 'NUM', 'LOC', 'ABBR']\n"
     ]
    }
   ],
   "source": [
    "### Part 2: Model Training & Evaluation - RNN\n",
    "\n",
    "# Build vocabulary for labels\n",
    "LABEL.build_vocab(train_data)\n",
    "num_classes = len(LABEL.vocab)\n",
    "print(f\"\\nNumber of classes: {num_classes}\")\n",
    "print(f\"Classes: {LABEL.vocab.itos}\")\n",
    "\n",
    "\n",
    "class SimpleRNNClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple RNN for topic classification (Baseline - no dropout).\n",
    "    Uses pretrained embeddings (learnable/updated during training) with OOV mitigation \n",
    "    and aggregates word representations to sentence representation using the last hidden state.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, \n",
    "                 n_layers=1, dropout=0.0, padding_idx=0, pretrained_embeddings=None):\n",
    "        super(SimpleRNNClassifier, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # Embedding layer initialized with pretrained embeddings\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "            self.embedding.weight.requires_grad = True\n",
    "        \n",
    "        # Simple RNN layer (no dropout in baseline)\n",
    "        self.rnn = nn.RNN(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=n_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.0  # No dropout in baseline\n",
    "        )\n",
    "        \n",
    "        # Fully connected output layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        \n",
    "        # Embed the input\n",
    "        embedded = self.embedding(text)  # [batch_size, seq_len, embedding_dim]\n",
    "        \n",
    "        # Get dimensions\n",
    "        batch_size = embedded.size(0)\n",
    "        seq_len = embedded.size(1)\n",
    "        \n",
    "        # Handle text_lengths: ensure it's a 1D tensor with batch_size elements\n",
    "        text_lengths_flat = text_lengths.flatten().cpu().long()\n",
    "        \n",
    "        # text_lengths should have exactly batch_size elements (one length per batch item)\n",
    "        if len(text_lengths_flat) != batch_size:\n",
    "            raise ValueError(\n",
    "                f\"text_lengths size mismatch: got {len(text_lengths_flat)} elements, \"\n",
    "                f\"expected {batch_size} (batch_size). text_lengths.shape={text_lengths.shape}, \"\n",
    "                f\"text.shape={text.shape}, embedded.shape={embedded.shape}\"\n",
    "            )\n",
    "        \n",
    "        # Clamp lengths to be at most the sequence length (safety check)\n",
    "        text_lengths_clamped = torch.clamp(text_lengths_flat, min=1, max=seq_len)\n",
    "        \n",
    "        # Pack the padded sequences for efficient processing\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, text_lengths_clamped, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        \n",
    "        # Pass through RNN\n",
    "        packed_output, hidden = self.rnn(packed_embedded)\n",
    "        # hidden: [n_layers, batch_size, hidden_dim]\n",
    "        \n",
    "        # Use the last hidden state from the last layer\n",
    "        last_hidden = hidden[-1]  # [batch_size, hidden_dim]\n",
    "        \n",
    "        # Pass through fully connected layer\n",
    "        output = self.fc(last_hidden)  # [batch_size, output_dim]\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "370ed524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8172\n"
     ]
    }
   ],
   "source": [
    "print(len(TEXT.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23562caa",
   "metadata": {},
   "source": [
    "Training order:\n",
    "1. Word aggregation\n",
    "2. Hyperparameters tuning\n",
    "3. Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9b664f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PART 2: SIMPLE RNN MODEL TRAINING\n",
      "================================================================================\n",
      "TEXT.vocab size: 8172\n",
      "FastText embedding vocab size: 8172\n",
      "\n",
      ">>> Training Baseline RNN Model\n",
      "Configuration:\n",
      "  - Hidden Dim: 256\n",
      "  - Layers: 1\n",
      "  - Dropout: 0.0 (Baseline: no regularization)\n",
      "  - Learning Rate: 0.001\n",
      "  - Batch Size: 64\n",
      "  - Epochs: 100 (no early stopping)\n",
      "  - Embedding Dim: 300 (FastText)\n",
      "  - Embeddings: LEARNABLE (updated during training)\n",
      "  - OOV Handling: FastText subword embeddings + trainable <unk> token\n",
      "\n",
      "Starting training for 100 epochs...\n",
      "Device: cuda\n",
      "Trainable parameters: 2,595,990\n",
      "Embedding layer learnable: True\n",
      "--------------------------------------------------------------------------------\n",
      "DEBUG BATCH - text shape: torch.Size([8, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 8])\n",
      "Epoch: 01/100 | Time: 0m 1s\n",
      "\tTrain Loss: 1.5974 | Train Acc: 27.83%\n",
      "\tVal Loss: 1.4356 | Val Acc: 32.94%\n",
      "\t>>> New best model saved with Val Acc: 32.94%\n",
      "DEBUG BATCH - text shape: torch.Size([14, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 14])\n",
      "Epoch: 02/100 | Time: 0m 0s\n",
      "\tTrain Loss: 1.1063 | Train Acc: 56.46%\n",
      "\tVal Loss: 0.9911 | Val Acc: 59.27%\n",
      "\t>>> New best model saved with Val Acc: 59.27%\n",
      "DEBUG BATCH - text shape: torch.Size([8, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 8])\n",
      "Epoch: 03/100 | Time: 0m 1s\n",
      "\tTrain Loss: 0.6136 | Train Acc: 79.69%\n",
      "\tVal Loss: 0.9685 | Val Acc: 60.92%\n",
      "\t>>> New best model saved with Val Acc: 60.92%\n",
      "DEBUG BATCH - text shape: torch.Size([14, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 14])\n",
      "Epoch: 04/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.3044 | Train Acc: 90.23%\n",
      "\tVal Loss: 1.1209 | Val Acc: 61.65%\n",
      "\t>>> New best model saved with Val Acc: 61.65%\n",
      "DEBUG BATCH - text shape: torch.Size([10, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 10])\n",
      "Epoch: 05/100 | Time: 0m 1s\n",
      "\tTrain Loss: 0.1144 | Train Acc: 96.74%\n",
      "\tVal Loss: 1.3670 | Val Acc: 57.34%\n",
      "DEBUG BATCH - text shape: torch.Size([16, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 16])\n",
      "Epoch: 06/100 | Time: 0m 1s\n",
      "\tTrain Loss: 0.0379 | Train Acc: 99.11%\n",
      "\tVal Loss: 1.2922 | Val Acc: 59.72%\n",
      "DEBUG BATCH - text shape: torch.Size([5, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 5])\n",
      "Epoch: 07/100 | Time: 0m 1s\n",
      "\tTrain Loss: 0.0115 | Train Acc: 99.84%\n",
      "\tVal Loss: 1.4781 | Val Acc: 58.17%\n",
      "DEBUG BATCH - text shape: torch.Size([11, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 11])\n",
      "Epoch: 08/100 | Time: 0m 1s\n",
      "\tTrain Loss: 0.0066 | Train Acc: 99.93%\n",
      "\tVal Loss: 1.3898 | Val Acc: 61.38%\n",
      "DEBUG BATCH - text shape: torch.Size([17, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 17])\n",
      "Epoch: 09/100 | Time: 0m 1s\n",
      "\tTrain Loss: 0.0025 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.3971 | Val Acc: 62.66%\n",
      "\t>>> New best model saved with Val Acc: 62.66%\n",
      "DEBUG BATCH - text shape: torch.Size([11, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 11])\n",
      "Epoch: 10/100 | Time: 0m 1s\n",
      "\tTrain Loss: 0.0017 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.4318 | Val Acc: 62.48%\n",
      "DEBUG BATCH - text shape: torch.Size([6, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 6])\n",
      "Epoch: 11/100 | Time: 0m 1s\n",
      "\tTrain Loss: 0.0014 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.4593 | Val Acc: 62.29%\n",
      "DEBUG BATCH - text shape: torch.Size([11, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 11])\n",
      "Epoch: 12/100 | Time: 0m 1s\n",
      "\tTrain Loss: 0.0011 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.4771 | Val Acc: 62.20%\n",
      "DEBUG BATCH - text shape: torch.Size([5, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 5])\n",
      "Epoch: 13/100 | Time: 0m 1s\n",
      "\tTrain Loss: 0.0009 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.5031 | Val Acc: 62.11%\n",
      "DEBUG BATCH - text shape: torch.Size([10, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 10])\n",
      "Epoch: 14/100 | Time: 0m 1s\n",
      "\tTrain Loss: 0.0008 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.5207 | Val Acc: 62.11%\n",
      "DEBUG BATCH - text shape: torch.Size([6, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 6])\n",
      "Epoch: 15/100 | Time: 0m 1s\n",
      "\tTrain Loss: 0.0007 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.5351 | Val Acc: 62.39%\n",
      "DEBUG BATCH - text shape: torch.Size([8, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 8])\n",
      "Epoch: 16/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0006 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.5515 | Val Acc: 62.02%\n",
      "DEBUG BATCH - text shape: torch.Size([9, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 9])\n",
      "Epoch: 17/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0005 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.5683 | Val Acc: 61.56%\n",
      "DEBUG BATCH - text shape: torch.Size([37, 10]), text_lengths shape: torch.Size([10]), labels shape: torch.Size([10])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([10, 37])\n",
      "Epoch: 18/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0005 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.5821 | Val Acc: 61.74%\n",
      "DEBUG BATCH - text shape: torch.Size([14, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 14])\n",
      "Epoch: 19/100 | Time: 0m 1s\n",
      "\tTrain Loss: 0.0004 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.5942 | Val Acc: 61.74%\n",
      "DEBUG BATCH - text shape: torch.Size([16, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 16])\n",
      "Epoch: 20/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0004 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.6075 | Val Acc: 61.83%\n",
      "DEBUG BATCH - text shape: torch.Size([10, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 10])\n",
      "Epoch: 21/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0004 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.6213 | Val Acc: 61.74%\n",
      "DEBUG BATCH - text shape: torch.Size([17, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 17])\n",
      "Epoch: 22/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0003 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.6343 | Val Acc: 61.56%\n",
      "DEBUG BATCH - text shape: torch.Size([8, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 8])\n",
      "Epoch: 23/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0003 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.6444 | Val Acc: 61.65%\n",
      "DEBUG BATCH - text shape: torch.Size([13, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 13])\n",
      "Epoch: 24/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0003 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.6561 | Val Acc: 61.56%\n",
      "DEBUG BATCH - text shape: torch.Size([11, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 11])\n",
      "Epoch: 25/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0002 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.6667 | Val Acc: 61.38%\n",
      "DEBUG BATCH - text shape: torch.Size([6, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 6])\n",
      "Epoch: 26/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0002 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.6774 | Val Acc: 61.38%\n",
      "DEBUG BATCH - text shape: torch.Size([11, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 11])\n",
      "Epoch: 27/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0002 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.6871 | Val Acc: 61.38%\n",
      "DEBUG BATCH - text shape: torch.Size([14, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 14])\n",
      "Epoch: 28/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0002 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.6977 | Val Acc: 61.56%\n",
      "DEBUG BATCH - text shape: torch.Size([7, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 7])\n",
      "Epoch: 29/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0002 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.7079 | Val Acc: 61.28%\n",
      "DEBUG BATCH - text shape: torch.Size([13, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 13])\n",
      "Epoch: 30/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0002 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.7183 | Val Acc: 61.28%\n",
      "DEBUG BATCH - text shape: torch.Size([11, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 11])\n",
      "Epoch: 31/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0002 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.7264 | Val Acc: 61.38%\n",
      "DEBUG BATCH - text shape: torch.Size([10, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 10])\n",
      "Epoch: 32/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.7371 | Val Acc: 61.19%\n",
      "DEBUG BATCH - text shape: torch.Size([15, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 15])\n",
      "Epoch: 33/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.7466 | Val Acc: 61.19%\n",
      "DEBUG BATCH - text shape: torch.Size([9, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 9])\n",
      "Epoch: 34/100 | Time: 0m 1s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.7522 | Val Acc: 61.10%\n",
      "DEBUG BATCH - text shape: torch.Size([8, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 8])\n",
      "Epoch: 35/100 | Time: 0m 1s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.7614 | Val Acc: 61.10%\n",
      "DEBUG BATCH - text shape: torch.Size([8, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 8])\n",
      "Epoch: 36/100 | Time: 0m 1s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.7706 | Val Acc: 61.01%\n",
      "DEBUG BATCH - text shape: torch.Size([9, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 9])\n",
      "Epoch: 37/100 | Time: 0m 1s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.7825 | Val Acc: 61.19%\n",
      "DEBUG BATCH - text shape: torch.Size([16, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 16])\n",
      "Epoch: 38/100 | Time: 0m 1s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.7906 | Val Acc: 61.19%\n",
      "DEBUG BATCH - text shape: torch.Size([14, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 14])\n",
      "Epoch: 39/100 | Time: 0m 1s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.7958 | Val Acc: 61.01%\n",
      "DEBUG BATCH - text shape: torch.Size([5, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 5])\n",
      "Epoch: 40/100 | Time: 0m 1s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.8041 | Val Acc: 61.10%\n",
      "DEBUG BATCH - text shape: torch.Size([8, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 8])\n",
      "Epoch: 41/100 | Time: 0m 1s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.8139 | Val Acc: 61.10%\n",
      "DEBUG BATCH - text shape: torch.Size([10, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 10])\n",
      "Epoch: 42/100 | Time: 0m 1s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.8204 | Val Acc: 61.01%\n",
      "DEBUG BATCH - text shape: torch.Size([5, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 5])\n",
      "Epoch: 43/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.8295 | Val Acc: 61.01%\n",
      "DEBUG BATCH - text shape: torch.Size([12, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 12])\n",
      "Epoch: 44/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.8349 | Val Acc: 61.01%\n",
      "DEBUG BATCH - text shape: torch.Size([14, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 14])\n",
      "Epoch: 45/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.8423 | Val Acc: 61.01%\n",
      "DEBUG BATCH - text shape: torch.Size([8, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 8])\n",
      "Epoch: 46/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.8512 | Val Acc: 60.92%\n",
      "DEBUG BATCH - text shape: torch.Size([8, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 8])\n",
      "Epoch: 47/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.8577 | Val Acc: 60.92%\n",
      "DEBUG BATCH - text shape: torch.Size([9, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 9])\n",
      "Epoch: 48/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.8649 | Val Acc: 61.01%\n",
      "DEBUG BATCH - text shape: torch.Size([5, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 5])\n",
      "Epoch: 49/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.8733 | Val Acc: 60.92%\n",
      "DEBUG BATCH - text shape: torch.Size([10, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 10])\n",
      "Epoch: 50/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.8794 | Val Acc: 60.92%\n",
      "DEBUG BATCH - text shape: torch.Size([6, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 6])\n",
      "Epoch: 51/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.8847 | Val Acc: 60.92%\n",
      "DEBUG BATCH - text shape: torch.Size([12, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 12])\n",
      "Epoch: 52/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.8931 | Val Acc: 61.01%\n",
      "DEBUG BATCH - text shape: torch.Size([7, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 7])\n",
      "Epoch: 53/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.9000 | Val Acc: 60.92%\n",
      "DEBUG BATCH - text shape: torch.Size([6, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 6])\n",
      "Epoch: 54/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.9059 | Val Acc: 61.01%\n",
      "DEBUG BATCH - text shape: torch.Size([9, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 9])\n",
      "Epoch: 55/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.9159 | Val Acc: 60.73%\n",
      "DEBUG BATCH - text shape: torch.Size([8, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 8])\n",
      "Epoch: 56/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.9205 | Val Acc: 61.01%\n",
      "DEBUG BATCH - text shape: torch.Size([22, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 22])\n",
      "Epoch: 57/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.9263 | Val Acc: 60.92%\n",
      "DEBUG BATCH - text shape: torch.Size([13, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 13])\n",
      "Epoch: 58/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.9331 | Val Acc: 60.92%\n",
      "DEBUG BATCH - text shape: torch.Size([9, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 9])\n",
      "Epoch: 59/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.9412 | Val Acc: 60.83%\n",
      "DEBUG BATCH - text shape: torch.Size([11, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 11])\n",
      "Epoch: 60/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.9473 | Val Acc: 60.83%\n",
      "DEBUG BATCH - text shape: torch.Size([5, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 5])\n",
      "Epoch: 61/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.9530 | Val Acc: 60.92%\n",
      "DEBUG BATCH - text shape: torch.Size([5, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 5])\n",
      "Epoch: 62/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.9604 | Val Acc: 60.83%\n",
      "DEBUG BATCH - text shape: torch.Size([13, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 13])\n",
      "Epoch: 63/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.9648 | Val Acc: 60.92%\n",
      "DEBUG BATCH - text shape: torch.Size([14, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 14])\n",
      "Epoch: 64/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.9719 | Val Acc: 60.83%\n",
      "DEBUG BATCH - text shape: torch.Size([8, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 8])\n",
      "Epoch: 65/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.9791 | Val Acc: 60.92%\n",
      "DEBUG BATCH - text shape: torch.Size([10, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 10])\n",
      "Epoch: 66/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.9844 | Val Acc: 60.92%\n",
      "DEBUG BATCH - text shape: torch.Size([18, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 18])\n",
      "Epoch: 67/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.9928 | Val Acc: 60.92%\n",
      "DEBUG BATCH - text shape: torch.Size([7, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 7])\n",
      "Epoch: 68/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.9969 | Val Acc: 60.92%\n",
      "DEBUG BATCH - text shape: torch.Size([10, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 10])\n",
      "Epoch: 69/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.0031 | Val Acc: 60.83%\n",
      "DEBUG BATCH - text shape: torch.Size([13, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 13])\n",
      "Epoch: 70/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.0093 | Val Acc: 60.73%\n",
      "DEBUG BATCH - text shape: torch.Size([4, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 4])\n",
      "Epoch: 71/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.0157 | Val Acc: 60.73%\n",
      "DEBUG BATCH - text shape: torch.Size([10, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 10])\n",
      "Epoch: 72/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.0216 | Val Acc: 60.83%\n",
      "DEBUG BATCH - text shape: torch.Size([14, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 14])\n",
      "Epoch: 73/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.0285 | Val Acc: 60.73%\n",
      "DEBUG BATCH - text shape: torch.Size([8, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 8])\n",
      "Epoch: 74/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.0335 | Val Acc: 60.73%\n",
      "DEBUG BATCH - text shape: torch.Size([17, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 17])\n",
      "Epoch: 75/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.0394 | Val Acc: 60.73%\n",
      "DEBUG BATCH - text shape: torch.Size([12, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 12])\n",
      "Epoch: 76/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.0447 | Val Acc: 60.73%\n",
      "DEBUG BATCH - text shape: torch.Size([9, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 9])\n",
      "Epoch: 77/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.0513 | Val Acc: 60.83%\n",
      "DEBUG BATCH - text shape: torch.Size([7, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 7])\n",
      "Epoch: 78/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.0581 | Val Acc: 60.73%\n",
      "DEBUG BATCH - text shape: torch.Size([9, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 9])\n",
      "Epoch: 79/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.0630 | Val Acc: 60.73%\n",
      "DEBUG BATCH - text shape: torch.Size([12, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 12])\n",
      "Epoch: 80/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.0695 | Val Acc: 60.64%\n",
      "DEBUG BATCH - text shape: torch.Size([8, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 8])\n",
      "Epoch: 81/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.0760 | Val Acc: 60.55%\n",
      "DEBUG BATCH - text shape: torch.Size([8, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 8])\n",
      "Epoch: 82/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.0811 | Val Acc: 60.55%\n",
      "DEBUG BATCH - text shape: torch.Size([7, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 7])\n",
      "Epoch: 83/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.0856 | Val Acc: 60.46%\n",
      "DEBUG BATCH - text shape: torch.Size([12, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 12])\n",
      "Epoch: 84/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.0931 | Val Acc: 60.37%\n",
      "DEBUG BATCH - text shape: torch.Size([37, 10]), text_lengths shape: torch.Size([10]), labels shape: torch.Size([10])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([10, 37])\n",
      "Epoch: 85/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.0988 | Val Acc: 60.28%\n",
      "DEBUG BATCH - text shape: torch.Size([11, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 11])\n",
      "Epoch: 86/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.1049 | Val Acc: 60.18%\n",
      "DEBUG BATCH - text shape: torch.Size([10, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 10])\n",
      "Epoch: 87/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.1118 | Val Acc: 60.18%\n",
      "DEBUG BATCH - text shape: torch.Size([7, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 7])\n",
      "Epoch: 88/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.1175 | Val Acc: 60.09%\n",
      "DEBUG BATCH - text shape: torch.Size([6, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 6])\n",
      "Epoch: 89/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.1216 | Val Acc: 60.18%\n",
      "DEBUG BATCH - text shape: torch.Size([9, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 9])\n",
      "Epoch: 90/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.1268 | Val Acc: 60.18%\n",
      "DEBUG BATCH - text shape: torch.Size([7, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 7])\n",
      "Epoch: 91/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.1336 | Val Acc: 60.18%\n",
      "DEBUG BATCH - text shape: torch.Size([11, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 11])\n",
      "Epoch: 92/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.1387 | Val Acc: 60.09%\n",
      "DEBUG BATCH - text shape: torch.Size([11, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 11])\n",
      "Epoch: 93/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.1437 | Val Acc: 60.18%\n",
      "DEBUG BATCH - text shape: torch.Size([8, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 8])\n",
      "Epoch: 94/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.1511 | Val Acc: 60.00%\n",
      "DEBUG BATCH - text shape: torch.Size([12, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 12])\n",
      "Epoch: 95/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.1550 | Val Acc: 60.09%\n",
      "DEBUG BATCH - text shape: torch.Size([7, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 7])\n",
      "Epoch: 96/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.1625 | Val Acc: 60.18%\n",
      "DEBUG BATCH - text shape: torch.Size([9, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 9])\n",
      "Epoch: 97/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.1663 | Val Acc: 60.18%\n",
      "DEBUG BATCH - text shape: torch.Size([10, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 10])\n",
      "Epoch: 98/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.1738 | Val Acc: 60.18%\n",
      "DEBUG BATCH - text shape: torch.Size([12, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 12])\n",
      "Epoch: 99/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.1780 | Val Acc: 60.09%\n",
      "DEBUG BATCH - text shape: torch.Size([10, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 10])\n",
      "Epoch: 100/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.1832 | Val Acc: 60.09%\n",
      "--------------------------------------------------------------------------------\n",
      "Training completed! Best validation accuracy: 62.66%\n",
      "Total epochs trained: 100\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Part 2: Initial Simple RNN Model Training\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 2: SIMPLE RNN MODEL TRAINING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get pretrained embeddings from Part 1 (frozen embeddings)\n",
    "pretrained_embeddings = pretrained_embed.weight.data\n",
    "\n",
    "# Get embedding dimension and vocab size from the fasttext embedding layer\n",
    "embedding_dim = pretrained_embed.weight.shape[1]\n",
    "embedding_vocab_size = pretrained_embed.weight.shape[0]  # Vocab size from saved embedding\n",
    "\n",
    "# Verify vocab sizes match (they might differ if vocab was rebuilt)\n",
    "print(f\"TEXT.vocab size: {len(TEXT.vocab)}\")\n",
    "print(f\"FastText embedding vocab size: {embedding_vocab_size}\")\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "HIDDEN_DIM = 256\n",
    "N_LAYERS = 1\n",
    "DROPOUT = 0.0  # Baseline: no dropout\n",
    "N_EPOCHS = 100\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# Create data iterators\n",
    "train_iterator = data.BucketIterator(\n",
    "    train_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    shuffle=True,  # Shuffle for training\n",
    "    device=device\n",
    ")\n",
    "\n",
    "val_iterator = data.BucketIterator(\n",
    "    validation_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    shuffle=False,  # No shuffle for validation (deterministic)\n",
    "    device=device\n",
    ")\n",
    "\n",
    "test_iterator = data.BucketIterator(\n",
    "    test_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    shuffle=False,  # No shuffle for test (deterministic)\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Initialize simple RNN model (Baseline)\n",
    "# Use vocab size from loaded embedding to match the saved weights exactly\n",
    "model = SimpleRNNClassifier(\n",
    "    vocab_size=embedding_vocab_size,  # Must match saved embedding vocab size\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    output_dim=num_classes,\n",
    "    n_layers=N_LAYERS,\n",
    "    dropout=0.0,  # Baseline: no dropout\n",
    "    padding_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "    pretrained_embeddings=pretrained_embeddings\n",
    ").to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(f\"\\n>>> Training Baseline RNN Model\")\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  - Hidden Dim: {HIDDEN_DIM}\")\n",
    "print(f\"  - Layers: {N_LAYERS}\")\n",
    "print(f\"  - Dropout: {DROPOUT} (Baseline: no regularization)\")\n",
    "print(f\"  - Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"  - Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  - Epochs: {N_EPOCHS} (no early stopping)\")\n",
    "print(f\"  - Embedding Dim: {embedding_dim} (FastText)\")\n",
    "print(f\"  - Embeddings: LEARNABLE (updated during training)\")\n",
    "print(f\"  - OOV Handling: FastText subword embeddings + trainable <unk> token\")\n",
    "\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# ============================================================================\n",
    "# Training Loop (inline for easier debugging)\n",
    "# ============================================================================\n",
    "\n",
    "best_val_acc = 0\n",
    "\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "\n",
    "print(f\"\\nStarting training for {N_EPOCHS} epochs...\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Trainable parameters: {count_parameters(model):,}\")\n",
    "print(f\"Embedding layer learnable: {model.embedding.weight.requires_grad}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Training for one epoch (inline)\n",
    "    # ========================================================================\n",
    "    model.train()\n",
    "    train_epoch_loss = 0\n",
    "    train_all_preds = []\n",
    "    train_all_labels = []\n",
    "    \n",
    "    batch_idx = 0\n",
    "    for batch in train_iterator:\n",
    "        # Process batch (with debug only for first batch)\n",
    "        text, text_lengths, labels = process_batch(batch, debug=(batch_idx == 0))\n",
    "        batch_idx += 1\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        predictions = model(text, text_lengths)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(predictions, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        train_epoch_loss += loss.item()\n",
    "        \n",
    "        # Store predictions and labels for metrics\n",
    "        preds = torch.argmax(predictions, dim=1)\n",
    "        train_all_preds.extend(preds.cpu().numpy())\n",
    "        train_all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate training accuracy\n",
    "    train_acc = accuracy_score(train_all_labels, train_all_preds)\n",
    "    train_loss = train_epoch_loss / len(train_iterator)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Validation evaluation (inline)\n",
    "    # ========================================================================\n",
    "    model.eval()\n",
    "    val_epoch_loss = 0\n",
    "    val_all_preds = []\n",
    "    val_all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_iterator:\n",
    "            # Process batch consistently with training\n",
    "            text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "            \n",
    "            # Forward pass\n",
    "            predictions = model(text, text_lengths)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(predictions, labels)\n",
    "            val_epoch_loss += loss.item()\n",
    "            \n",
    "            # Store predictions and labels\n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            val_all_preds.extend(preds.cpu().numpy())\n",
    "            val_all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate validation accuracy\n",
    "    val_acc = accuracy_score(val_all_labels, val_all_preds)\n",
    "    val_loss = val_epoch_loss / len(val_iterator)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Logging (without early stopping)\n",
    "    # ========================================================================\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = divmod(end_time - start_time, 60)\n",
    "    \n",
    "    # Store metrics\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02}/{N_EPOCHS} | Time: {int(epoch_mins)}m {int(epoch_secs)}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\tVal Loss: {val_loss:.4f} | Val Acc: {val_acc*100:.2f}%')\n",
    "    \n",
    "    # Track best model (but don't stop early - baseline trains for all epochs)\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        # Save best model\n",
    "        torch.save(model.state_dict(), 'rnn_simple_best.pt')\n",
    "        print(f'\\t>>> New best model saved with Val Acc: {val_acc*100:.2f}%')\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(f\"Training completed! Best validation accuracy: {best_val_acc*100:.2f}%\")\n",
    "print(f\"Total epochs trained: {N_EPOCHS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ea22748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "VALIDATION SET EVALUATION (Best Model)\n",
      "================================================================================\n",
      "\n",
      ">>> Validation Set Results (Best Model):\n",
      "Validation Loss: 1.3971\n",
      "Validation Accuracy: 62.66%\n",
      "Validation F1 Score: 0.6281\n",
      "Validation AUC-ROC: 0.8676\n",
      "\n",
      "================================================================================\n",
      "TEST SET EVALUATION\n",
      "================================================================================\n",
      "\n",
      ">>> Test Set Results:\n",
      "Test Loss: 1.1831\n",
      "Test Accuracy: 67.80%\n",
      "Test F1 Score: 0.6731\n",
      "Test AUC-ROC: 0.8901\n",
      "\n",
      "================================================================================\n",
      "PART 2 INITIAL TRAINING COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Validation Set Evaluation (inline) - Evaluate best model on validation set\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VALIDATION SET EVALUATION (Best Model)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load best model and evaluate on validation set\n",
    "model.load_state_dict(torch.load('rnn_simple_best.pt'))\n",
    "\n",
    "model.eval()\n",
    "val_eval_loss = 0\n",
    "val_eval_preds = []\n",
    "val_eval_labels = []\n",
    "val_eval_probs = []  # Store probabilities for AUC-ROC\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_iterator:\n",
    "        text, text_lengths = batch.text\n",
    "        labels = batch.label\n",
    "        \n",
    "        # Process batch consistently\n",
    "        text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "        \n",
    "        # Forward pass\n",
    "        predictions = model(text, text_lengths)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(predictions, labels)\n",
    "        val_eval_loss += loss.item()\n",
    "        \n",
    "        # Store predictions, labels, and probabilities\n",
    "        probs = torch.softmax(predictions, dim=1)\n",
    "        preds = torch.argmax(predictions, dim=1)\n",
    "        val_eval_preds.extend(preds.cpu().numpy())\n",
    "        val_eval_labels.extend(labels.cpu().numpy())\n",
    "        val_eval_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "# Calculate validation metrics\n",
    "val_eval_acc = accuracy_score(val_eval_labels, val_eval_preds)\n",
    "val_eval_f1 = f1_score(val_eval_labels, val_eval_preds, average='weighted')\n",
    "val_eval_loss_final = val_eval_loss / len(val_iterator)\n",
    "\n",
    "# Calculate AUC-ROC (one-vs-rest for multiclass)\n",
    "try:\n",
    "    val_eval_probs_array = np.array(val_eval_probs)\n",
    "    val_eval_labels_bin = label_binarize(val_eval_labels, classes=range(num_classes))\n",
    "    val_eval_auc = roc_auc_score(val_eval_labels_bin, val_eval_probs_array, average='weighted', multi_class='ovr')\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not calculate AUC-ROC: {e}\")\n",
    "    val_eval_auc = 0.0\n",
    "\n",
    "print(f\"\\n>>> Validation Set Results (Best Model):\")\n",
    "print(f\"Validation Loss: {val_eval_loss_final:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_eval_acc*100:.2f}%\")\n",
    "print(f\"Validation F1 Score: {val_eval_f1:.4f}\")\n",
    "print(f\"Validation AUC-ROC: {val_eval_auc:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Test Set Evaluation (inline)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST SET EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "model.eval()\n",
    "test_epoch_loss = 0\n",
    "test_all_preds = []\n",
    "test_all_labels = []\n",
    "test_all_probs = []  # Store probabilities for AUC-ROC\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_iterator:\n",
    "        text, text_lengths = batch.text\n",
    "        labels = batch.label\n",
    "        \n",
    "        # Process batch consistently\n",
    "        text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "        \n",
    "        # Forward pass\n",
    "        predictions = model(text, text_lengths)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(predictions, labels)\n",
    "        test_epoch_loss += loss.item()\n",
    "        \n",
    "        # Store predictions, labels, and probabilities\n",
    "        probs = torch.softmax(predictions, dim=1)\n",
    "        preds = torch.argmax(predictions, dim=1)\n",
    "        test_all_preds.extend(preds.cpu().numpy())\n",
    "        test_all_labels.extend(labels.cpu().numpy())\n",
    "        test_all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "# Calculate test metrics\n",
    "test_acc = accuracy_score(test_all_labels, test_all_preds)\n",
    "test_f1 = f1_score(test_all_labels, test_all_preds, average='weighted')\n",
    "test_loss = test_epoch_loss / len(test_iterator)\n",
    "\n",
    "# Calculate AUC-ROC (one-vs-rest for multiclass)\n",
    "try:\n",
    "    test_all_probs_array = np.array(test_all_probs)\n",
    "    test_all_labels_bin = label_binarize(test_all_labels, classes=range(num_classes))\n",
    "    test_auc = roc_auc_score(test_all_labels_bin, test_all_probs_array, average='weighted', multi_class='ovr')\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not calculate AUC-ROC: {e}\")\n",
    "    test_auc = 0.0\n",
    "\n",
    "print(f\"\\n>>> Test Set Results:\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n",
    "print(f\"Test F1 Score: {test_f1:.4f}\")\n",
    "print(f\"Test AUC-ROC: {test_auc:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 2 INITIAL TRAINING COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5090ea4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PART 2.2: SEQUENTIAL HYPERPARAMETER TUNING\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "STEP 0: EPOCH + EARLY STOPPING TUNING\n",
      "================================================================================\n",
      "Testing different MAX_EPOCHS and PATIENCE configurations\n",
      "Total combinations to test: 3\n",
      "Combinations (Max_Epochs, Patience):\n",
      "  1. Max_Epochs=100, Patience=10\n",
      "  2. Max_Epochs=200, Patience=10\n",
      "  3. Max_Epochs=300, Patience=10\n",
      "\n",
      ">>> Testing: Step 0 Config 1/3\n",
      "    LR=0.001, Batch=64, Hidden=256, Opt=Adam\n",
      "    Epochs=100 (NO early stopping - training for full 100 epochs)\n",
      "    Epoch 10/100: Train Acc=100.00%, Val Acc=63.49%\n",
      "    Epoch 20/100: Train Acc=100.00%, Val Acc=64.31%\n",
      "    Epoch 30/100: Train Acc=100.00%, Val Acc=64.50%\n",
      "    Epoch 40/100: Train Acc=100.00%, Val Acc=64.77%\n",
      "    Epoch 50/100: Train Acc=100.00%, Val Acc=64.77%\n",
      "    Epoch 60/100: Train Acc=100.00%, Val Acc=65.23%\n",
      "    Epoch 70/100: Train Acc=100.00%, Val Acc=65.32%\n",
      "    Epoch 80/100: Train Acc=100.00%, Val Acc=65.41%\n",
      "    Epoch 90/100: Train Acc=100.00%, Val Acc=65.41%\n",
      "    Epoch 100/100: Train Acc=100.00%, Val Acc=65.23%\n",
      "    Final Val Acc: 65.23% | Best Val Acc: 65.60% (at epoch 84/100)\n",
      "\n",
      ">>> Testing: Step 0 Config 2/3\n",
      "    LR=0.001, Batch=64, Hidden=256, Opt=Adam\n",
      "    Epochs=200 (NO early stopping - training for full 200 epochs)\n",
      "    Epoch 10/200: Train Acc=100.00%, Val Acc=63.49%\n",
      "    Epoch 20/200: Train Acc=100.00%, Val Acc=64.31%\n",
      "    Epoch 30/200: Train Acc=100.00%, Val Acc=64.50%\n",
      "    Epoch 40/200: Train Acc=100.00%, Val Acc=64.77%\n",
      "    Epoch 50/200: Train Acc=100.00%, Val Acc=64.77%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 171\u001b[39m\n\u001b[32m    168\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available():\n\u001b[32m    169\u001b[39m         torch.cuda.manual_seed(SEED)\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m     val_acc, best_epoch, total_epochs = \u001b[43mtrain_and_evaluate_with_epochs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m        \u001b[49m\u001b[43mep_config\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mconfig\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m        \u001b[49m\u001b[43mep_config\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmax_epochs\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m        \u001b[49m\u001b[43mep_config\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpatience\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mStep 0 Config \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43midx\u001b[49m\u001b[43m+\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstep0_configs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    176\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m     step0_results.append({\n\u001b[32m    178\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mnum_epochs\u001b[39m\u001b[33m'\u001b[39m: ep_config[\u001b[33m'\u001b[39m\u001b[33mmax_epochs\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m    179\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mval_acc\u001b[39m\u001b[33m'\u001b[39m: val_acc,\n\u001b[32m    180\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mbest_epoch\u001b[39m\u001b[33m'\u001b[39m: best_epoch,\n\u001b[32m    181\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mtotal_epochs\u001b[39m\u001b[33m'\u001b[39m: total_epochs\n\u001b[32m    182\u001b[39m     })\n\u001b[32m    184\u001b[39m \u001b[38;5;66;03m# Find best epoch configuration\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 117\u001b[39m, in \u001b[36mtrain_and_evaluate_with_epochs\u001b[39m\u001b[34m(config, max_epochs, patience, config_name)\u001b[39m\n\u001b[32m    115\u001b[39m text, text_lengths, labels = process_batch(batch, debug=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    116\u001b[39m optimizer.zero_grad()\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m predictions = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_lengths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m loss = criterion(predictions, labels)\n\u001b[32m    119\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hiast\\Desktop\\School\\NTU-NLP-Assignment\\nlpEnv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hiast\\Desktop\\School\\NTU-NLP-Assignment\\nlpEnv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36mSimpleRNNClassifier.forward\u001b[39m\u001b[34m(self, text, text_lengths)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, text, text_lengths):\n\u001b[32m     42\u001b[39m \n\u001b[32m     43\u001b[39m     \u001b[38;5;66;03m# Embed the input\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     embedded = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch_size, seq_len, embedding_dim]\u001b[39;00m\n\u001b[32m     46\u001b[39m     \u001b[38;5;66;03m# Get dimensions\u001b[39;00m\n\u001b[32m     47\u001b[39m     batch_size = embedded.size(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hiast\\Desktop\\School\\NTU-NLP-Assignment\\nlpEnv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hiast\\Desktop\\School\\NTU-NLP-Assignment\\nlpEnv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hiast\\Desktop\\School\\NTU-NLP-Assignment\\nlpEnv\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:190\u001b[39m, in \u001b[36mEmbedding.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hiast\\Desktop\\School\\NTU-NLP-Assignment\\nlpEnv\\Lib\\site-packages\\torch\\nn\\functional.py:2551\u001b[39m, in \u001b[36membedding\u001b[39m\u001b[34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[39m\n\u001b[32m   2545\u001b[39m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[32m   2546\u001b[39m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[32m   2547\u001b[39m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[32m   2548\u001b[39m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[32m   2549\u001b[39m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[32m   2550\u001b[39m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[32m-> \u001b[39m\u001b[32m2551\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Part 2.2: Sequential Hyperparameter Tuning (One Variable at a Time)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 2.2: SEQUENTIAL HYPERPARAMETER TUNING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# Step 0: Epoch + Early Stopping Configuration Tuning\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 0: EPOCH + EARLY STOPPING TUNING\")\n",
    "print(\"=\"*80)\n",
    "print(\"Testing different MAX_EPOCHS and PATIENCE configurations\")\n",
    "\n",
    "# Test different epoch and patience configurations\n",
    "max_epochs_options = [100, 200, 300]\n",
    "patience = 10\n",
    "\n",
    "# Use baseline config for testing epoch settings\n",
    "baseline_config = {\n",
    "    'lr': 0.001,\n",
    "    'batch_size': 64,\n",
    "    'hidden_dim': 256,\n",
    "    'optimizer': 'Adam'\n",
    "}\n",
    "\n",
    "step0_configs = []\n",
    "for max_epochs in max_epochs_options:\n",
    "        step0_configs.append({\n",
    "            'config': baseline_config.copy(),\n",
    "            'max_epochs': max_epochs,\n",
    "            'patience': patience\n",
    "        })\n",
    "\n",
    "print(f\"Total combinations to test: {len(step0_configs)}\")\n",
    "print(\"Combinations (Max_Epochs, Patience):\")\n",
    "for idx, ep_config in enumerate(step0_configs, 1):\n",
    "    print(f\"  {idx}. Max_Epochs={ep_config['max_epochs']}, Patience={ep_config['patience']}\")\n",
    "\n",
    "# Helper function to train with specific epoch/patience settings\n",
    "def train_and_evaluate_with_epochs(config, max_epochs, patience, config_name=\"config\"):\n",
    "    \"\"\"Train a model for specific number of epochs WITHOUT early stopping\"\"\"\n",
    "    print(f\"\\n>>> Testing: {config_name}\")\n",
    "    print(f\"    LR={config['lr']}, Batch={config['batch_size']}, Hidden={config['hidden_dim']}, Opt={config['optimizer']}\")\n",
    "    print(f\"    Epochs={max_epochs} (NO early stopping - training for full {max_epochs} epochs)\")\n",
    "    \n",
    "    # Create iterators with specific batch size\n",
    "    train_iter = data.BucketIterator(\n",
    "        train_data,\n",
    "        batch_size=config['batch_size'],\n",
    "        sort_key=lambda x: len(x.text),\n",
    "        sort_within_batch=True,\n",
    "        shuffle=True,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    val_iter = data.BucketIterator(\n",
    "        validation_data,\n",
    "        batch_size=config['batch_size'],\n",
    "        sort_key=lambda x: len(x.text),\n",
    "        sort_within_batch=True,\n",
    "        shuffle=False,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Reset random seeds INSIDE function to ensure fresh model for each config\n",
    "    # This is critical to ensure each max_epochs config starts from scratch\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "        torch.cuda.manual_seed_all(SEED)\n",
    "    # Ensure deterministic behavior\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    # Create model with specific hidden dimension\n",
    "    model = SimpleRNNClassifier(\n",
    "        vocab_size=embedding_vocab_size,\n",
    "        embedding_dim=embedding_dim,\n",
    "        hidden_dim=config['hidden_dim'],\n",
    "        output_dim=num_classes,\n",
    "        n_layers=N_LAYERS,\n",
    "        dropout=0.0,  # Baseline: no dropout\n",
    "        padding_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "        pretrained_embeddings=pretrained_embeddings\n",
    "    ).to(device)\n",
    "    \n",
    "    # Select optimizer\n",
    "    if config['optimizer'] == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config['lr'])\n",
    "    elif config['optimizer'] == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=config['lr'], momentum=0.9)\n",
    "    elif config['optimizer'] == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=config['lr'])\n",
    "    elif config['optimizer'] == 'Adagrad':\n",
    "        optimizer = optim.Adagrad(model.parameters(), lr=config['lr'])\n",
    "    \n",
    "    # Training loop WITHOUT early stopping - train for full num_epochs\n",
    "    best_val_acc = 0.0\n",
    "    best_val_acc_at_epoch = 0\n",
    "    final_val_acc = 0.0\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        \n",
    "        for batch in train_iter:\n",
    "            text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(text, text_lengths)\n",
    "            loss = criterion(predictions, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        train_acc = accuracy_score(train_labels, train_preds)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_iter:\n",
    "                text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "                predictions = model(text, text_lengths)\n",
    "                loss = criterion(predictions, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                preds = torch.argmax(predictions, dim=1)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_acc = accuracy_score(val_labels, val_preds)\n",
    "        \n",
    "        # Track best validation accuracy (but don't stop early)\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_val_acc_at_epoch = epoch + 1\n",
    "        \n",
    "        final_val_acc = val_acc  # Store final epoch's validation accuracy\n",
    "        \n",
    "        # Print progress every 10 epochs or at the end\n",
    "        if (epoch + 1) % 10 == 0 or (epoch + 1) == max_epochs:\n",
    "            print(f\"    Epoch {epoch+1}/{max_epochs}: Train Acc={train_acc*100:.2f}%, Val Acc={val_acc*100:.2f}%\")\n",
    "    \n",
    "    print(f\"    Final Val Acc: {final_val_acc*100:.2f}% | Best Val Acc: {best_val_acc*100:.2f}% (at epoch {best_val_acc_at_epoch}/{max_epochs})\")\n",
    "    return best_val_acc, best_val_acc_at_epoch, max_epochs\n",
    "\n",
    "step0_results = []\n",
    "for idx, ep_config in enumerate(step0_configs):\n",
    "    # Set fixed seed for reproducibility - ensures consistent batch ordering\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "    \n",
    "    val_acc, best_epoch, total_epochs = train_and_evaluate_with_epochs(\n",
    "        ep_config['config'],\n",
    "        ep_config['max_epochs'],\n",
    "        ep_config['patience'],\n",
    "        f\"Step 0 Config {idx+1}/{len(step0_configs)}\"\n",
    "    )\n",
    "    step0_results.append({\n",
    "        'num_epochs': ep_config['max_epochs'],\n",
    "        'val_acc': val_acc,\n",
    "        'best_epoch': best_epoch,\n",
    "        'total_epochs': total_epochs\n",
    "    })\n",
    "\n",
    "# Find best epoch configuration\n",
    "best_step0 = max(step0_results, key=lambda x: x['val_acc'])\n",
    "BEST_EPOCHS = best_step0['num_epochs']\n",
    "\n",
    "# Set appropriate MAX_EPOCHS and PATIENCE for subsequent steps\n",
    "# Use the best number of epochs with some buffer, and set a reasonable patience\n",
    "MAX_EPOCHS = BEST_EPOCHS\n",
    "PATIENCE = 7  # Default patience for early stopping in subsequent steps\n",
    "\n",
    "print(f\"\\n>>> Step 0 Results:\")\n",
    "print(f\"{'#':<4} {'Epochs':<8} {'Val Acc':<10} {'Best At Epoch':<15} {'Total Trained':<15}\")\n",
    "print(\"-\" * 60)\n",
    "for idx, result in enumerate(step0_results, 1):\n",
    "    print(f\"{idx:<4} {result['num_epochs']:<8} {result['val_acc']*100:<10.2f}% {result['best_epoch']:<15} {result['total_epochs']:<15}\")\n",
    "print(f\"\\n>>> Best from Step 0: Epochs={BEST_EPOCHS}, Val Acc={best_step0['val_acc']*100:.2f}%\")\n",
    "print(f\"    Best validation accuracy was achieved at epoch {best_step0['best_epoch']} out of {best_step0['total_epochs']}\")\n",
    "print(f\"\\n>>> Using MAX_EPOCHS={MAX_EPOCHS} and PATIENCE={PATIENCE} for subsequent steps (with early stopping)\")\n",
    "\n",
    "# Helper function to train and evaluate a model configuration\n",
    "def train_and_evaluate(config, config_name=\"config\"):\n",
    "    \"\"\"Train a model with given configuration and return validation accuracy\"\"\"\n",
    "    print(f\"\\n>>> Testing: {config_name}\")\n",
    "    print(f\"    LR={config['lr']}, Batch={config['batch_size']}, Hidden={config['hidden_dim']}, Opt={config['optimizer']}\")\n",
    "    \n",
    "    # Create iterators with specific batch size\n",
    "    train_iter = data.BucketIterator(\n",
    "        train_data,\n",
    "        batch_size=config['batch_size'],\n",
    "        sort_key=lambda x: len(x.text),\n",
    "        sort_within_batch=True,\n",
    "        shuffle=True,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    val_iter = data.BucketIterator(\n",
    "        validation_data,\n",
    "        batch_size=config['batch_size'],\n",
    "        sort_key=lambda x: len(x.text),\n",
    "        sort_within_batch=True,\n",
    "        shuffle=False,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Create model with specific hidden dimension\n",
    "    model = SimpleRNNClassifier(\n",
    "        vocab_size=embedding_vocab_size,\n",
    "        embedding_dim=embedding_dim,\n",
    "        hidden_dim=config['hidden_dim'],\n",
    "        output_dim=num_classes,\n",
    "        n_layers=N_LAYERS,\n",
    "        dropout=0.0,  # Baseline: no dropout\n",
    "        padding_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "        pretrained_embeddings=pretrained_embeddings\n",
    "    ).to(device)\n",
    "    \n",
    "    # Select optimizer\n",
    "    if config['optimizer'] == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config['lr'])\n",
    "    elif config['optimizer'] == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=config['lr'])\n",
    "    elif config['optimizer'] == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=config['lr'])\n",
    "    elif config['optimizer'] == 'Adagrad':\n",
    "        optimizer = optim.Adagrad(model.parameters(), lr=config['lr'])\n",
    "    \n",
    "    # Training loop with early stopping\n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        \n",
    "        for batch in train_iter:\n",
    "            text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(text, text_lengths)\n",
    "            loss = criterion(predictions, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        train_acc = accuracy_score(train_labels, train_preds)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_iter:\n",
    "                text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "                predictions = model(text, text_lengths)\n",
    "                loss = criterion(predictions, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                preds = torch.argmax(predictions, dim=1)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_acc = accuracy_score(val_labels, val_preds)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(f\"    Early stopping at epoch {epoch+1}, best val acc: {best_val_acc*100:.2f}%\")\n",
    "                break\n",
    "    \n",
    "    print(f\"    Best Val Acc: {best_val_acc*100:.2f}% (stopped at epoch {epoch+1})\")\n",
    "    return best_val_acc, epoch + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f94eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 1: LEARNING RATE + BATCH SIZE TUNING\n",
      "================================================================================\n",
      "Testing ALL combinations of LR and Batch Size (they interact)\n",
      "Total combinations to test: 9\n",
      "Combinations:\n",
      "  1. LR=0.01, Batch=32\n",
      "  2. LR=0.01, Batch=64\n",
      "  3. LR=0.01, Batch=128\n",
      "  4. LR=0.001, Batch=32\n",
      "  5. LR=0.001, Batch=64\n",
      "  6. LR=0.001, Batch=128\n",
      "  7. LR=0.0001, Batch=32\n",
      "  8. LR=0.0001, Batch=64\n",
      "  9. LR=0.0001, Batch=128\n",
      "\n",
      ">>> Testing: Step 1 Config 1/9\n",
      "    LR=0.01, Batch=32, Hidden=256, Opt=Adam\n",
      "    Early stopping at epoch 23, best val acc: 51.93%\n",
      "    Best Val Acc: 51.93% (stopped at epoch 23)\n",
      "\n",
      ">>> Testing: Step 1 Config 2/9\n",
      "    LR=0.01, Batch=64, Hidden=256, Opt=Adam\n",
      "    Early stopping at epoch 31, best val acc: 46.15%\n",
      "    Best Val Acc: 46.15% (stopped at epoch 31)\n",
      "\n",
      ">>> Testing: Step 1 Config 3/9\n",
      "    LR=0.01, Batch=128, Hidden=256, Opt=Adam\n",
      "    Early stopping at epoch 36, best val acc: 44.77%\n",
      "    Best Val Acc: 44.77% (stopped at epoch 36)\n",
      "\n",
      ">>> Testing: Step 1 Config 4/9\n",
      "    LR=0.001, Batch=32, Hidden=256, Opt=Adam\n",
      "    Early stopping at epoch 10, best val acc: 73.12%\n",
      "    Best Val Acc: 73.12% (stopped at epoch 10)\n",
      "\n",
      ">>> Testing: Step 1 Config 5/9\n",
      "    LR=0.001, Batch=64, Hidden=256, Opt=Adam\n",
      "    Early stopping at epoch 18, best val acc: 73.58%\n",
      "    Best Val Acc: 73.58% (stopped at epoch 18)\n",
      "\n",
      ">>> Testing: Step 1 Config 6/9\n",
      "    LR=0.001, Batch=128, Hidden=256, Opt=Adam\n",
      "    Early stopping at epoch 12, best val acc: 75.14%\n",
      "    Best Val Acc: 75.14% (stopped at epoch 12)\n",
      "\n",
      ">>> Testing: Step 1 Config 7/9\n",
      "    LR=0.0001, Batch=32, Hidden=256, Opt=Adam\n",
      "    Early stopping at epoch 40, best val acc: 86.24%\n",
      "    Best Val Acc: 86.24% (stopped at epoch 40)\n",
      "\n",
      ">>> Testing: Step 1 Config 8/9\n",
      "    LR=0.0001, Batch=64, Hidden=256, Opt=Adam\n",
      "    Early stopping at epoch 37, best val acc: 85.60%\n",
      "    Best Val Acc: 85.60% (stopped at epoch 37)\n",
      "\n",
      ">>> Testing: Step 1 Config 9/9\n",
      "    LR=0.0001, Batch=128, Hidden=256, Opt=Adam\n",
      "    Early stopping at epoch 34, best val acc: 81.93%\n",
      "    Best Val Acc: 81.93% (stopped at epoch 34)\n",
      "\n",
      ">>> Step 1 Results:\n",
      "#    LR       Batch   Val Acc    Epochs \n",
      "----------------------------------------\n",
      "1    0.01     32      51.93     % 23     \n",
      "2    0.01     64      46.15     % 31     \n",
      "3    0.01     128     44.77     % 36     \n",
      "4    0.001    32      73.12     % 10     \n",
      "5    0.001    64      73.58     % 18     \n",
      "6    0.001    128     75.14     % 12     \n",
      "7    0.0001   32      86.24     % 40     \n",
      "8    0.0001   64      85.60     % 37     \n",
      "9    0.0001   128     81.93     % 34     \n",
      "\n",
      ">>> Best from Step 1: LR=0.0001, Batch=32, Val Acc=86.24%\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Step 1: Group 1 - Learning Rate + Batch Size (Test Together)\n",
    "# ============================================================================\n",
    "\n",
    "# Helper function to train and evaluate a model configuration (uses best MAX_EPOCHS and PATIENCE from Step 0)\n",
    "def train_and_evaluate(config, config_name=\"config\"):\n",
    "    \"\"\"Train a model with given configuration and return validation accuracy\"\"\"\n",
    "    print(f\"\\n>>> Testing: {config_name}\")\n",
    "    print(f\"    LR={config['lr']}, Batch={config['batch_size']}, Hidden={config['hidden_dim']}, Opt={config['optimizer']}\")\n",
    "    \n",
    "    # Create iterators with specific batch size\n",
    "    train_iter = data.BucketIterator(\n",
    "        train_data,\n",
    "        batch_size=config['batch_size'],\n",
    "        sort_key=lambda x: len(x.text),\n",
    "        sort_within_batch=True,\n",
    "        shuffle=True,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    val_iter = data.BucketIterator(\n",
    "        validation_data,\n",
    "        batch_size=config['batch_size'],\n",
    "        sort_key=lambda x: len(x.text),\n",
    "        sort_within_batch=True,\n",
    "        shuffle=False,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Create model with specific hidden dimension\n",
    "    model = SimpleRNNClassifier(\n",
    "        vocab_size=embedding_vocab_size,\n",
    "        embedding_dim=embedding_dim,\n",
    "        hidden_dim=config['hidden_dim'],\n",
    "        output_dim=num_classes,\n",
    "        n_layers=N_LAYERS,\n",
    "        dropout=0.0,  # Baseline: no dropout\n",
    "        padding_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "        pretrained_embeddings=pretrained_embeddings\n",
    "    ).to(device)\n",
    "    \n",
    "    # Select optimizer\n",
    "    if config['optimizer'] == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config['lr'])\n",
    "    elif config['optimizer'] == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=config['lr'], momentum=0.9)\n",
    "    elif config['optimizer'] == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=config['lr'])\n",
    "    elif config['optimizer'] == 'Adagrad':\n",
    "        optimizer = optim.Adagrad(model.parameters(), lr=config['lr'])\n",
    "    \n",
    "    # Training loop with early stopping (using best MAX_EPOCHS and PATIENCE from Step 0)\n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        \n",
    "        for batch in train_iter:\n",
    "            text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(text, text_lengths)\n",
    "            loss = criterion(predictions, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        train_acc = accuracy_score(train_labels, train_preds)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_iter:\n",
    "                text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "                predictions = model(text, text_lengths)\n",
    "                loss = criterion(predictions, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                preds = torch.argmax(predictions, dim=1)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_acc = accuracy_score(val_labels, val_preds)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(f\"    Early stopping at epoch {epoch+1}, best val acc: {best_val_acc*100:.2f}%\")\n",
    "                break\n",
    "    \n",
    "    print(f\"    Best Val Acc: {best_val_acc*100:.2f}% (stopped at epoch {epoch+1})\")\n",
    "    return best_val_acc, epoch + 1\n",
    "    \n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 1: LEARNING RATE + BATCH SIZE TUNING\")\n",
    "print(\"=\"*80)\n",
    "print(\"Testing ALL combinations of LR and Batch Size (they interact)\")\n",
    "\n",
    "# Test all combinations of learning rates and batch sizes\n",
    "learning_rates = [0.01, 0.001, 0.0001]\n",
    "batch_sizes = [32, 64, 128]\n",
    "\n",
    "step1_configs = []\n",
    "for lr in learning_rates:\n",
    "    for batch_size in batch_sizes:\n",
    "        step1_configs.append({\n",
    "            'lr': lr,\n",
    "            'batch_size': batch_size,\n",
    "            'hidden_dim': 256,\n",
    "            'optimizer': 'Adam'\n",
    "        })\n",
    "\n",
    "print(f\"Total combinations to test: {len(step1_configs)}\")\n",
    "print(\"Combinations:\")\n",
    "for idx, config in enumerate(step1_configs, 1):\n",
    "    print(f\"  {idx}. LR={config['lr']}, Batch={config['batch_size']}\")\n",
    "\n",
    "step1_results = []\n",
    "for idx, config in enumerate(step1_configs):\n",
    "    # Set fixed seed for reproducibility\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "    \n",
    "    val_acc, epoch_stopped = train_and_evaluate(config, f\"Step 1 Config {idx+1}/{len(step1_configs)}\")\n",
    "    step1_results.append({\n",
    "        'config': config,\n",
    "        'val_acc': val_acc,\n",
    "        'epoch_stopped': epoch_stopped\n",
    "    })\n",
    "\n",
    "# Find best LR + Batch Size\n",
    "best_step1 = max(step1_results, key=lambda x: x['val_acc'])\n",
    "best_lr = best_step1['config']['lr']\n",
    "best_batch_size = best_step1['config']['batch_size']\n",
    "\n",
    "print(f\"\\n>>> Step 1 Results:\")\n",
    "print(f\"{'#':<4} {'LR':<8} {'Batch':<7} {'Val Acc':<10} {'Epochs':<7}\")\n",
    "print(\"-\" * 40)\n",
    "for idx, result in enumerate(step1_results):\n",
    "    c = result['config']\n",
    "    print(f\"{idx+1:<4} {c['lr']:<8} {c['batch_size']:<7} {result['val_acc']*100:<10.2f}% {result['epoch_stopped']:<7}\")\n",
    "print(f\"\\n>>> Best from Step 1: LR={best_lr}, Batch={best_batch_size}, Val Acc={best_step1['val_acc']*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305f8f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 2: OPTIMIZER\n",
      "================================================================================\n",
      "Using best LR=0.0001 and Batch=32 from Step 1\n",
      "\n",
      ">>> Testing: Step 2 Config 1/4\n",
      "    LR=0.0001, Batch=32, Hidden=256, Opt=Adam\n",
      "    Early stopping at epoch 40, best val acc: 86.24%\n",
      "    Best Val Acc: 86.24% (stopped at epoch 40)\n",
      "\n",
      ">>> Testing: Step 2 Config 2/4\n",
      "    LR=0.0001, Batch=32, Hidden=256, Opt=SGD\n",
      "    Early stopping at epoch 12, best val acc: 23.12%\n",
      "    Best Val Acc: 23.12% (stopped at epoch 12)\n",
      "\n",
      ">>> Testing: Step 2 Config 3/4\n",
      "    LR=0.0001, Batch=32, Hidden=256, Opt=RMSprop\n",
      "    Early stopping at epoch 25, best val acc: 85.69%\n",
      "    Best Val Acc: 85.69% (stopped at epoch 25)\n",
      "\n",
      ">>> Testing: Step 2 Config 4/4\n",
      "    LR=0.0001, Batch=32, Hidden=256, Opt=Adagrad\n",
      "    Early stopping at epoch 27, best val acc: 33.58%\n",
      "    Best Val Acc: 33.58% (stopped at epoch 27)\n",
      "\n",
      ">>> Step 2 Results:\n",
      "#    LR       Optimizer  Val Acc    Epochs \n",
      "---------------------------------------------\n",
      "1    0.0001   Adam       86.24     % 40     \n",
      "2    0.0001   SGD        23.12     % 12     \n",
      "3    0.0001   RMSprop    85.69     % 25     \n",
      "4    0.0001   Adagrad    33.58     % 27     \n",
      "\n",
      ">>> Best from Step 2: LR=0.0001, Optimizer=Adam, Val Acc=86.24%\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Step 2: Group 2 - Optimizer\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 2: OPTIMIZER\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Using best LR={best_lr} and Batch={best_batch_size} from Step 1\")\n",
    "\n",
    "step2_configs = [\n",
    "    {'lr': best_lr, 'batch_size': best_batch_size, 'hidden_dim': 256, 'optimizer': 'Adam'},\n",
    "    {'lr': best_lr, 'batch_size': best_batch_size, 'hidden_dim': 256, 'optimizer': 'SGD'},\n",
    "    {'lr': best_lr, 'batch_size': best_batch_size, 'hidden_dim': 256, 'optimizer': 'RMSprop'},\n",
    "    {'lr': best_lr, 'batch_size': best_batch_size, 'hidden_dim': 256, 'optimizer': 'Adagrad'},\n",
    "]\n",
    "\n",
    "step2_results = []\n",
    "for idx, config in enumerate(step2_configs):\n",
    "    # Set fixed seed for reproducibility\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "    \n",
    "    val_acc, epoch_stopped = train_and_evaluate(config, f\"Step 2 Config {idx+1}/{len(step2_configs)}\")\n",
    "    step2_results.append({\n",
    "        'config': config,\n",
    "        'val_acc': val_acc,\n",
    "        'epoch_stopped': epoch_stopped\n",
    "    })\n",
    "\n",
    "# Find best Optimizer (and potentially adjusted LR)\n",
    "best_step2 = max(step2_results, key=lambda x: x['val_acc'])\n",
    "best_optimizer = best_step2['config']['optimizer']\n",
    "final_lr = best_step2['config']['lr']  # May be different if SGD needed higher LR\n",
    "\n",
    "print(f\"\\n>>> Step 2 Results:\")\n",
    "print(f\"{'#':<4} {'LR':<8} {'Optimizer':<10} {'Val Acc':<10} {'Epochs':<7}\")\n",
    "print(\"-\" * 45)\n",
    "for idx, result in enumerate(step2_results):\n",
    "    c = result['config']\n",
    "    print(f\"{idx+1:<4} {c['lr']:<8} {c['optimizer']:<10} {result['val_acc']*100:<10.2f}% {result['epoch_stopped']:<7}\")\n",
    "print(f\"\\n>>> Best from Step 2: LR={final_lr}, Optimizer={best_optimizer}, Val Acc={best_step2['val_acc']*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7fb36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 3: HIDDEN DIMENSION TUNING\n",
      "================================================================================\n",
      "Using best LR=0.0001, Batch=32, Optimizer=Adam from Steps 1-2\n",
      "\n",
      ">>> Testing: Step 3 Config 1/3\n",
      "    LR=0.0001, Batch=32, Hidden=128, Opt=Adam\n",
      "    Early stopping at epoch 26, best val acc: 86.51%\n",
      "    Best Val Acc: 86.51% (stopped at epoch 26)\n",
      "\n",
      ">>> Testing: Step 3 Config 2/3\n",
      "    LR=0.0001, Batch=32, Hidden=256, Opt=Adam\n",
      "    Early stopping at epoch 40, best val acc: 86.24%\n",
      "    Best Val Acc: 86.24% (stopped at epoch 40)\n",
      "\n",
      ">>> Testing: Step 3 Config 3/3\n",
      "    LR=0.0001, Batch=32, Hidden=512, Opt=Adam\n",
      "    Early stopping at epoch 20, best val acc: 86.33%\n",
      "    Best Val Acc: 86.33% (stopped at epoch 20)\n",
      "\n",
      ">>> Step 3 Results:\n",
      "#    Hidden Dim   Val Acc    Epochs \n",
      "-----------------------------------\n",
      "1    128          86.51     % 26     \n",
      "2    256          86.24     % 40     \n",
      "3    512          86.33     % 20     \n",
      "\n",
      ">>> Best from Step 3: Hidden Dim=128, Val Acc=86.51%\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Step 3: Hidden Dimension (Test Independently)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 3: HIDDEN DIMENSION TUNING\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Using best LR={final_lr}, Batch={best_batch_size}, Optimizer={best_optimizer} from Steps 1-2\")\n",
    "\n",
    "step3_configs = [\n",
    "    {'lr': final_lr, 'batch_size': best_batch_size, 'hidden_dim': 128, 'optimizer': best_optimizer},\n",
    "    {'lr': final_lr, 'batch_size': best_batch_size, 'hidden_dim': 256, 'optimizer': best_optimizer},\n",
    "    {'lr': final_lr, 'batch_size': best_batch_size, 'hidden_dim': 512, 'optimizer': best_optimizer},\n",
    "]\n",
    "\n",
    "step3_results = []\n",
    "for idx, config in enumerate(step3_configs):\n",
    "    # Set fixed seed for reproducibility\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "    \n",
    "    val_acc, epoch_stopped = train_and_evaluate(config, f\"Step 3 Config {idx+1}/{len(step3_configs)}\")\n",
    "    step3_results.append({\n",
    "        'config': config,\n",
    "        'val_acc': val_acc,\n",
    "        'epoch_stopped': epoch_stopped\n",
    "    })\n",
    "\n",
    "# Find best Hidden Dimension\n",
    "best_step3 = max(step3_results, key=lambda x: x['val_acc'])\n",
    "best_hidden_dim = best_step3['config']['hidden_dim']\n",
    "\n",
    "print(f\"\\n>>> Step 3 Results:\")\n",
    "print(f\"{'#':<4} {'Hidden Dim':<12} {'Val Acc':<10} {'Epochs':<7}\")\n",
    "print(\"-\" * 35)\n",
    "for idx, result in enumerate(step3_results):\n",
    "    c = result['config']\n",
    "    print(f\"{idx+1:<4} {c['hidden_dim']:<12} {result['val_acc']*100:<10.2f}% {result['epoch_stopped']:<7}\")\n",
    "print(f\"\\n>>> Best from Step 3: Hidden Dim={best_hidden_dim}, Val Acc={best_step3['val_acc']*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a23bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "HYPERPARAMETER TUNING COMPLETE - FINAL BEST CONFIGURATION\n",
      "================================================================================\n",
      "\n",
      ">>> Best Configuration Found:\n",
      "    Learning Rate: 0.0001\n",
      "    Batch Size: 32\n",
      "    Hidden Dimension: 128\n",
      "    Optimizer: Adam\n",
      "    Max Epochs: 100 (with early stopping, patience=7)\n",
      "    Best Validation Accuracy: 86.51%\n",
      "\n",
      "================================================================================\n",
      "SEQUENTIAL HYPERPARAMETER TUNING COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Final Best Configuration Summary\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"HYPERPARAMETER TUNING COMPLETE - FINAL BEST CONFIGURATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "final_best_config = {\n",
    "    'lr': final_lr,\n",
    "    'batch_size': best_batch_size,\n",
    "    'hidden_dim': best_hidden_dim,\n",
    "    'optimizer': best_optimizer,\n",
    "    'max_epochs': MAX_EPOCHS,\n",
    "    'patience': PATIENCE\n",
    "}\n",
    "\n",
    "print(f\"\\n>>> Best Configuration Found:\")\n",
    "print(f\"    Learning Rate: {final_best_config['lr']}\")\n",
    "print(f\"    Batch Size: {final_best_config['batch_size']}\")\n",
    "print(f\"    Hidden Dimension: {final_best_config['hidden_dim']}\")\n",
    "print(f\"    Optimizer: {final_best_config['optimizer']}\")\n",
    "print(f\"    Max Epochs: {final_best_config['max_epochs']} (with early stopping, patience={final_best_config['patience']})\")\n",
    "print(f\"    Best Validation Accuracy: {best_step3['val_acc']*100:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SEQUENTIAL HYPERPARAMETER TUNING COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817c430f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "WORD AGGREGATION METHOD COMPARISON\n",
      "================================================================================\n",
      "Using best hyperparameters from tuning:\n",
      "    LR=0.0001, Batch=32, Hidden=128, Optimizer=Adam\n",
      "    Max Epochs=100, Patience=7\n",
      "\n",
      "Testing 4 aggregation methods:\n",
      "  - last\n",
      "  - mean\n",
      "  - max\n",
      "  - attention\n",
      "\n",
      "================================================================================\n",
      "Testing Aggregation Method: LAST\n",
      "================================================================================\n",
      "\n",
      ">>> Training model with last aggregation...\n",
      "    Epoch 10: Train Acc=93.21%, Val Acc=82.84%\n",
      "    Epoch 20: Train Acc=98.97%, Val Acc=85.96%\n",
      "    Early stopping at epoch 26, best val acc: 86.51%\n",
      "\n",
      ">>> Results for last aggregation:\n",
      "    Validation Acc: 86.51%\n",
      "    Test Acc: 88.00%\n",
      "    Test F1: 0.8781\n",
      "    Test AUC-ROC: 0.9646\n",
      "\n",
      "================================================================================\n",
      "Testing Aggregation Method: MEAN\n",
      "================================================================================\n",
      "\n",
      ">>> Training model with mean aggregation...\n",
      "    Epoch 10: Train Acc=93.90%, Val Acc=83.76%\n",
      "    Epoch 20: Train Acc=98.40%, Val Acc=85.05%\n",
      "    Early stopping at epoch 26, best val acc: 86.06%\n",
      "\n",
      ">>> Results for mean aggregation:\n",
      "    Validation Acc: 86.06%\n",
      "    Test Acc: 88.00%\n",
      "    Test F1: 0.8786\n",
      "    Test AUC-ROC: 0.9673\n",
      "\n",
      "================================================================================\n",
      "Testing Aggregation Method: MAX\n",
      "================================================================================\n",
      "\n",
      ">>> Training model with max aggregation...\n",
      "    Early stopping at epoch 8, best val acc: 22.94%\n",
      "    Warning: Could not calculate AUC-ROC: Input contains NaN.\n",
      "\n",
      ">>> Results for max aggregation:\n",
      "    Validation Acc: 22.94%\n",
      "    Test Acc: 18.80%\n",
      "    Test F1: 0.0595\n",
      "    Test AUC-ROC: 0.0000\n",
      "\n",
      "================================================================================\n",
      "Testing Aggregation Method: ATTENTION\n",
      "================================================================================\n",
      "\n",
      ">>> Training model with attention aggregation...\n",
      "    Epoch 10: Train Acc=96.03%, Val Acc=85.32%\n",
      "    Epoch 20: Train Acc=98.88%, Val Acc=86.97%\n",
      "    Early stopping at epoch 27, best val acc: 86.97%\n",
      "\n",
      ">>> Results for attention aggregation:\n",
      "    Validation Acc: 86.97%\n",
      "    Test Acc: 89.40%\n",
      "    Test F1: 0.8943\n",
      "    Test AUC-ROC: 0.9685\n",
      "\n",
      "================================================================================\n",
      "AGGREGATION METHOD COMPARISON - RESULTS SUMMARY\n",
      "================================================================================\n",
      "\n",
      ">>> Results Summary:\n",
      "Method       Val Acc    Test Acc   Test F1    Test AUC  \n",
      "-------------------------------------------------------\n",
      "last         86.51     % 88.00     % 0.8781     0.9646    \n",
      "mean         86.06     % 88.00     % 0.8786     0.9673    \n",
      "max          22.94     % 18.80     % 0.0595     0.0000    \n",
      "attention    86.97     % 89.40     % 0.8943     0.9685    \n",
      "\n",
      ">>> Best Aggregation Method: ATTENTION\n",
      "    Validation Accuracy: 86.97%\n",
      "    Test Accuracy: 89.40%\n",
      "    Test F1 Score: 0.8943\n",
      "    Test AUC-ROC: 0.9685\n",
      "\n",
      "================================================================================\n",
      "AGGREGATION METHOD COMPARISON COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Word Aggregation Method Comparison\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"WORD AGGREGATION METHOD COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Using best hyperparameters from tuning:\")\n",
    "print(f\"    LR={final_lr}, Batch={best_batch_size}, Hidden={best_hidden_dim}, Optimizer={best_optimizer}\")\n",
    "print(f\"    Max Epochs={MAX_EPOCHS}, Patience={PATIENCE}\")\n",
    "\n",
    "# Extended RNN Classifier with multiple aggregation methods\n",
    "class RNN_Classifier_Aggregation(nn.Module):\n",
    "    \"\"\"\n",
    "    RNN for topic classification with multiple aggregation strategies.\n",
    "    Uses pretrained embeddings (learnable/updated during training).\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, \n",
    "                 n_layers=1, dropout=0.0, padding_idx=0, pretrained_embeddings=None,\n",
    "                 aggregation='last'):\n",
    "        super(RNN_Classifier_Aggregation, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.aggregation = aggregation  # 'last', 'mean', 'max', 'attention'\n",
    "        \n",
    "        # Embedding layer initialized with pretrained embeddings\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "            # Make embeddings learnable (updated during training)\n",
    "            self.embedding.weight.requires_grad = True\n",
    "        \n",
    "        # RNN layer\n",
    "        self.rnn = nn.RNN(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=n_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.0  # No dropout in baseline\n",
    "        )\n",
    "        \n",
    "        # Attention mechanism for aggregation (only created if needed)\n",
    "        if aggregation == 'attention':\n",
    "            self.attention = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        # Fully connected output layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        # text: [batch_size, seq_len]\n",
    "        # text_lengths: [batch_size]\n",
    "        \n",
    "        # Embed the input\n",
    "        embedded = self.embedding(text)  # [batch_size, seq_len, embedding_dim]\n",
    "        \n",
    "        # Get dimensions\n",
    "        batch_size = embedded.size(0)\n",
    "        seq_len = embedded.size(1)\n",
    "        \n",
    "        # Handle text_lengths\n",
    "        text_lengths_flat = text_lengths.flatten().cpu().long()\n",
    "        if len(text_lengths_flat) != batch_size:\n",
    "            raise ValueError(\n",
    "                f\"text_lengths size mismatch: got {len(text_lengths_flat)} elements, \"\n",
    "                f\"expected {batch_size}\"\n",
    "            )\n",
    "        \n",
    "        # Clamp lengths\n",
    "        text_lengths_clamped = torch.clamp(text_lengths_flat, min=1, max=seq_len)\n",
    "        \n",
    "        text_lengths_clamped_device = text_lengths_clamped.to(text.device)\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, text_lengths_clamped, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        \n",
    "        # Pass through RNN\n",
    "        packed_output, hidden = self.rnn(packed_embedded)\n",
    "        # hidden: [n_layers, batch_size, hidden_dim]\n",
    "        \n",
    "        # Aggregate word representations to sentence representation\n",
    "        if self.aggregation == 'last':\n",
    "            sentence_repr = hidden[-1]  # [batch_size, hidden_dim]\n",
    "            \n",
    "        elif self.aggregation == 'mean':\n",
    "            output, output_lengths = nn.utils.rnn.pad_packed_sequence(\n",
    "                packed_output, batch_first=True\n",
    "            )\n",
    "            # output: [batch_size, seq_len, hidden_dim]\n",
    "            \n",
    "            # Create mask for padding\n",
    "            mask = torch.arange(seq_len, device=text.device).unsqueeze(0) < text_lengths_clamped_device.unsqueeze(1)\n",
    "            mask = mask.unsqueeze(2).float()  # [batch_size, seq_len, 1]\n",
    "            \n",
    "            # Apply mask and compute mean\n",
    "            masked_output = output * mask\n",
    "            sum_output = masked_output.sum(dim=1)  # [batch_size, hidden_dim]\n",
    "            sentence_repr = sum_output / text_lengths_clamped_device.unsqueeze(1).float()\n",
    "            \n",
    "        elif self.aggregation == 'max':\n",
    "            # Max pooling over all outputs\n",
    "            output, output_lengths = nn.utils.rnn.pad_packed_sequence(\n",
    "                packed_output, batch_first=True\n",
    "            )\n",
    "            # output: [batch_size, seq_len, hidden_dim]\n",
    "            \n",
    "            mask = torch.arange(seq_len, device=text.device).unsqueeze(0) < text_lengths_clamped_device.unsqueeze(1)\n",
    "            mask = mask.unsqueeze(2).float()  # [batch_size, seq_len, 1]\n",
    "            \n",
    "            masked_output = output * mask + (1 - mask) * float('-inf')\n",
    "            sentence_repr, _ = torch.max(masked_output, dim=1)\n",
    "            \n",
    "        elif self.aggregation == 'attention':\n",
    "            # Attention mechanism\n",
    "            output, output_lengths = nn.utils.rnn.pad_packed_sequence(\n",
    "                packed_output, batch_first=True\n",
    "            )\n",
    "            # output: [batch_size, seq_len, hidden_dim]\n",
    "            \n",
    "            # Compute attention scores\n",
    "            attn_scores = self.attention(output).squeeze(2)  # [batch_size, seq_len]\n",
    "            \n",
    "            mask = torch.arange(seq_len, device=text.device).unsqueeze(0) < text_lengths_clamped_device.unsqueeze(1)\n",
    "            attn_scores = attn_scores.masked_fill(~mask, float('-inf'))\n",
    "            \n",
    "            # Apply softmax\n",
    "            attn_weights = torch.softmax(attn_scores, dim=1).unsqueeze(1)  # [batch_size, 1, seq_len]\n",
    "            \n",
    "            # Weighted sum\n",
    "            sentence_repr = torch.bmm(attn_weights, output).squeeze(1)  # [batch_size, hidden_dim]\n",
    "        \n",
    "        # Pass through fully connected layer\n",
    "        output = self.fc(sentence_repr)  # [batch_size, output_dim]\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Test different aggregation methods\n",
    "aggregation_methods = ['last', 'mean', 'max', 'attention']\n",
    "\n",
    "print(f\"\\nTesting {len(aggregation_methods)} aggregation methods:\")\n",
    "for method in aggregation_methods:\n",
    "    print(f\"  - {method}\")\n",
    "\n",
    "aggregation_results = []\n",
    "\n",
    "for agg_method in aggregation_methods:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Testing Aggregation Method: {agg_method.upper()}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Set fixed seed for reproducibility - ensures consistent batch ordering\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "    \n",
    "    # Create iterators with best batch size\n",
    "    train_iter = data.BucketIterator(\n",
    "        train_data,\n",
    "        batch_size=best_batch_size,\n",
    "        sort_key=lambda x: len(x.text),\n",
    "        sort_within_batch=True,\n",
    "        shuffle=True,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    val_iter = data.BucketIterator(\n",
    "        validation_data,\n",
    "        batch_size=best_batch_size,\n",
    "        sort_key=lambda x: len(x.text),\n",
    "        sort_within_batch=True,\n",
    "        shuffle=False,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    test_iter = data.BucketIterator(\n",
    "        test_data,\n",
    "        batch_size=best_batch_size,\n",
    "        sort_key=lambda x: len(x.text),\n",
    "        sort_within_batch=True,\n",
    "        shuffle=False,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Create model with best hyperparameters and specific aggregation method\n",
    "    model = RNN_Classifier_Aggregation(\n",
    "        vocab_size=embedding_vocab_size,\n",
    "        embedding_dim=embedding_dim,\n",
    "        hidden_dim=best_hidden_dim,\n",
    "        output_dim=num_classes,\n",
    "        n_layers=N_LAYERS,\n",
    "        dropout=0.0,  # Baseline: no dropout\n",
    "        padding_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "        pretrained_embeddings=pretrained_embeddings,\n",
    "        aggregation=agg_method\n",
    "    ).to(device)\n",
    "    \n",
    "    # Select optimizer with best learning rate\n",
    "    if best_optimizer == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=final_lr)\n",
    "    elif best_optimizer == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=final_lr, momentum=0.9)\n",
    "    elif best_optimizer == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=final_lr)\n",
    "    elif best_optimizer == 'Adagrad':\n",
    "        optimizer = optim.Adagrad(model.parameters(), lr=final_lr)\n",
    "    \n",
    "    # Training loop with early stopping\n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    print(f\"\\n>>> Training model with {agg_method} aggregation...\")\n",
    "    \n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        \n",
    "        for batch in train_iter:\n",
    "            text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(text, text_lengths)\n",
    "            loss = criterion(predictions, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        train_acc = accuracy_score(train_labels, train_preds)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_iter:\n",
    "                text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "                predictions = model(text, text_lengths)\n",
    "                loss = criterion(predictions, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                preds = torch.argmax(predictions, dim=1)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_acc = accuracy_score(val_labels, val_preds)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            # Save best model for this aggregation method\n",
    "            torch.save(model.state_dict(), f'rnn_agg_{agg_method}_best.pt')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(f\"    Early stopping at epoch {epoch+1}, best val acc: {best_val_acc*100:.2f}%\")\n",
    "                break\n",
    "        \n",
    "        # Print progress every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"    Epoch {epoch+1}: Train Acc={train_acc*100:.2f}%, Val Acc={val_acc*100:.2f}%\")\n",
    "    \n",
    "    # Load best model and evaluate on test set\n",
    "    model.load_state_dict(torch.load(f'rnn_agg_{agg_method}_best.pt'))\n",
    "    model.eval()\n",
    "    \n",
    "    test_loss = 0\n",
    "    test_preds = []\n",
    "    test_labels = []\n",
    "    test_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_iter:\n",
    "            text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "            predictions = model(text, text_lengths)\n",
    "            loss = criterion(predictions, labels)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            probs = torch.softmax(predictions, dim=1)\n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            test_preds.extend(preds.cpu().numpy())\n",
    "            test_labels.extend(labels.cpu().numpy())\n",
    "            test_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    # Calculate test metrics\n",
    "    test_acc = accuracy_score(test_labels, test_preds)\n",
    "    test_f1 = f1_score(test_labels, test_preds, average='weighted')\n",
    "    test_loss_avg = test_loss / len(test_iter)\n",
    "    \n",
    "    # Calculate AUC-ROC\n",
    "    try:\n",
    "        test_probs_array = np.array(test_probs)\n",
    "        test_labels_bin = label_binarize(test_labels, classes=range(num_classes))\n",
    "        test_auc = roc_auc_score(test_labels_bin, test_probs_array, average='weighted', multi_class='ovr')\n",
    "    except Exception as e:\n",
    "        print(f\"    Warning: Could not calculate AUC-ROC: {e}\")\n",
    "        test_auc = 0.0\n",
    "    \n",
    "    aggregation_results.append({\n",
    "        'method': agg_method,\n",
    "        'val_acc': best_val_acc,\n",
    "        'test_acc': test_acc,\n",
    "        'test_f1': test_f1,\n",
    "        'test_auc': test_auc,\n",
    "        'test_loss': test_loss_avg\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n>>> Results for {agg_method} aggregation:\")\n",
    "    print(f\"    Validation Acc: {best_val_acc*100:.2f}%\")\n",
    "    print(f\"    Test Acc: {test_acc*100:.2f}%\")\n",
    "    print(f\"    Test F1: {test_f1:.4f}\")\n",
    "    print(f\"    Test AUC-ROC: {test_auc:.4f}\")\n",
    "\n",
    "# Print summary table\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"AGGREGATION METHOD COMPARISON - RESULTS SUMMARY\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "print(f\"\\n>>> Results Summary:\")\n",
    "print(f\"{'Method':<12} {'Val Acc':<10} {'Test Acc':<10} {'Test F1':<10} {'Test AUC':<10}\")\n",
    "print(\"-\" * 55)\n",
    "for result in aggregation_results:\n",
    "    print(f\"{result['method']:<12} {result['val_acc']*100:<10.2f}% {result['test_acc']*100:<10.2f}% \"\n",
    "          f\"{result['test_f1']:<10.4f} {result['test_auc']:<10.4f}\")\n",
    "\n",
    "# Find best aggregation method\n",
    "best_aggregation = max(aggregation_results, key=lambda x: x['val_acc'])\n",
    "\n",
    "print(f\"\\n>>> Best Aggregation Method: {best_aggregation['method'].upper()}\")\n",
    "print(f\"    Validation Accuracy: {best_aggregation['val_acc']*100:.2f}%\")\n",
    "print(f\"    Test Accuracy: {best_aggregation['test_acc']*100:.2f}%\")\n",
    "print(f\"    Test F1 Score: {best_aggregation['test_f1']:.4f}\")\n",
    "print(f\"    Test AUC-ROC: {best_aggregation['test_auc']:.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"AGGREGATION METHOD COMPARISON COMPLETE\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684dfeeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "REGULARIZATION STEP 0: BASELINE (NO REGULARIZATION)\n",
      "================================================================================\n",
      "Using best hyperparameters from tuning:\n",
      "    LR=0.0001, Batch=32, Hidden=128, Optimizer=Adam\n",
      "    Max Epochs=100, Patience=7\n",
      "    Best Aggregation Method: ATTENTION\n",
      "\n",
      "Baseline Configuration:\n",
      "    Dropout: 0.0\n",
      "    Gradient Clipping: 0.0\n",
      "    L1 Lambda: 0.0\n",
      "    L2 Lambda: 0.0\n",
      "\n",
      ">>> Training baseline model...\n",
      "    Epoch 10: Train Acc=94.64%, Val Acc=83.39%\n",
      "    Epoch 20: Train Acc=99.06%, Val Acc=85.50%\n",
      "    Early stopping at epoch 26, best val acc: 85.96%\n",
      "\n",
      ">>> Baseline Results:\n",
      "    Validation Acc: 85.96%\n",
      "    Test Acc: 88.20%\n",
      "    Test F1: 0.8817\n",
      "    Test AUC-ROC: 0.9653\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Regularization: Baseline (No Regularization)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"REGULARIZATION STEP 0: BASELINE (NO REGULARIZATION)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Using best hyperparameters from tuning:\")\n",
    "print(f\"    LR={final_lr}, Batch={best_batch_size}, Hidden={best_hidden_dim}, Optimizer={best_optimizer}\")\n",
    "print(f\"    Max Epochs={MAX_EPOCHS}, Patience={PATIENCE}\")\n",
    "print(f\"    Best Aggregation Method: {best_aggregation['method'].upper()}\")\n",
    "\n",
    "# Baseline configuration\n",
    "baseline_config = {\n",
    "    'dropout': 0.0,\n",
    "    'grad_clip': 0.0,\n",
    "    'l1_lambda': 0.0,\n",
    "    'l2_lambda': 0.0\n",
    "}\n",
    "\n",
    "print(f\"\\nBaseline Configuration:\")\n",
    "print(f\"    Dropout: {baseline_config['dropout']}\")\n",
    "print(f\"    Gradient Clipping: {baseline_config['grad_clip']}\")\n",
    "print(f\"    L1 Lambda: {baseline_config['l1_lambda']}\")\n",
    "print(f\"    L2 Lambda: {baseline_config['l2_lambda']}\")\n",
    "\n",
    "# Create iterators\n",
    "train_iter = data.BucketIterator(\n",
    "    train_data,\n",
    "    batch_size=best_batch_size,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    shuffle=True,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "val_iter = data.BucketIterator(\n",
    "    validation_data,\n",
    "    batch_size=best_batch_size,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    shuffle=False,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "test_iter = data.BucketIterator(\n",
    "    test_data,\n",
    "    batch_size=best_batch_size,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    shuffle=False,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Create model\n",
    "model = RNN_Classifier_Aggregation(\n",
    "    vocab_size=embedding_vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_dim=best_hidden_dim,\n",
    "    output_dim=num_classes,\n",
    "    n_layers=N_LAYERS,\n",
    "    dropout=baseline_config['dropout'],\n",
    "    padding_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "    pretrained_embeddings=pretrained_embeddings,\n",
    "    aggregation=best_aggregation['method']\n",
    ").to(device)\n",
    "\n",
    "# Select optimizer\n",
    "if best_optimizer == 'Adam':\n",
    "    optimizer = optim.Adam(model.parameters(), lr=final_lr, weight_decay=baseline_config['l2_lambda'])\n",
    "elif best_optimizer == 'SGD':\n",
    "    optimizer = optim.SGD(model.parameters(), lr=final_lr, momentum=0.9, weight_decay=baseline_config['l2_lambda'])\n",
    "elif best_optimizer == 'RMSprop':\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=final_lr, weight_decay=baseline_config['l2_lambda'])\n",
    "elif best_optimizer == 'Adagrad':\n",
    "    optimizer = optim.Adagrad(model.parameters(), lr=final_lr, weight_decay=baseline_config['l2_lambda'])\n",
    "\n",
    "# Helper function for L1 regularization\n",
    "def compute_l1_loss(model, l1_lambda):\n",
    "    \"\"\"Compute L1 regularization loss\"\"\"\n",
    "    if l1_lambda > 0:\n",
    "        return l1_lambda * sum(p.abs().sum() for p in model.parameters() if p.requires_grad)\n",
    "    return 0.0\n",
    "\n",
    "# Training loop\n",
    "best_val_acc = 0.0\n",
    "patience_counter = 0\n",
    "\n",
    "print(f\"\\n>>> Training baseline model...\")\n",
    "\n",
    "for epoch in range(MAX_EPOCHS):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_preds = []\n",
    "    train_labels = []\n",
    "    \n",
    "    for batch in train_iter:\n",
    "        text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(text, text_lengths)\n",
    "        \n",
    "        loss = criterion(predictions, labels)\n",
    "        \n",
    "        # Add L1 regularization\n",
    "        if baseline_config['l1_lambda'] > 0:\n",
    "            loss = loss + compute_l1_loss(model, baseline_config['l1_lambda'])\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        if baseline_config['grad_clip'] > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), baseline_config['grad_clip'])\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        preds = torch.argmax(predictions, dim=1)\n",
    "        train_preds.extend(preds.cpu().numpy())\n",
    "        train_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    train_acc = accuracy_score(train_labels, train_preds)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_iter:\n",
    "            text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "            predictions = model(text, text_lengths)\n",
    "            loss = criterion(predictions, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    val_acc = accuracy_score(val_labels, val_preds)\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), 'rnn_reg_baseline_best.pt')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(f\"    Early stopping at epoch {epoch+1}, best val acc: {best_val_acc*100:.2f}%\")\n",
    "            break\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"    Epoch {epoch+1}: Train Acc={train_acc*100:.2f}%, Val Acc={val_acc*100:.2f}%\")\n",
    "\n",
    "# Evaluate on test set\n",
    "model.load_state_dict(torch.load('rnn_reg_baseline_best.pt'))\n",
    "model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "test_preds = []\n",
    "test_labels = []\n",
    "test_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_iter:\n",
    "        text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "        predictions = model(text, text_lengths)\n",
    "        loss = criterion(predictions, labels)\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        probs = torch.softmax(predictions, dim=1)\n",
    "        preds = torch.argmax(predictions, dim=1)\n",
    "        test_preds.extend(preds.cpu().numpy())\n",
    "        test_labels.extend(labels.cpu().numpy())\n",
    "        test_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "test_acc = accuracy_score(test_labels, test_preds)\n",
    "test_f1 = f1_score(test_labels, test_preds, average='weighted')\n",
    "test_loss_avg = test_loss / len(test_iter)\n",
    "\n",
    "try:\n",
    "    test_probs_array = np.array(test_probs)\n",
    "    test_labels_bin = label_binarize(test_labels, classes=range(num_classes))\n",
    "    test_auc = roc_auc_score(test_labels_bin, test_probs_array, average='weighted', multi_class='ovr')\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not calculate AUC-ROC: {e}\")\n",
    "    test_auc = 0.0\n",
    "\n",
    "baseline_results = {\n",
    "    'name': 'baseline',\n",
    "    'dropout': baseline_config['dropout'],\n",
    "    'grad_clip': baseline_config['grad_clip'],\n",
    "    'l1_lambda': baseline_config['l1_lambda'],\n",
    "    'l2_lambda': baseline_config['l2_lambda'],\n",
    "    'val_acc': best_val_acc,\n",
    "    'test_acc': test_acc,\n",
    "    'test_f1': test_f1,\n",
    "    'test_auc': test_auc\n",
    "}\n",
    "\n",
    "print(f\"\\n>>> Baseline Results:\")\n",
    "print(f\"    Validation Acc: {best_val_acc*100:.2f}%\")\n",
    "print(f\"    Test Acc: {test_acc*100:.2f}%\")\n",
    "print(f\"    Test F1: {test_f1:.4f}\")\n",
    "print(f\"    Test AUC-ROC: {test_auc:.4f}\")\n",
    "\n",
    "# Store for next steps\n",
    "best_grad_clip = baseline_config['grad_clip']\n",
    "best_dropout = baseline_config['dropout']\n",
    "best_l1_lambda = baseline_config['l1_lambda']\n",
    "best_l2_lambda = baseline_config['l2_lambda']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a253fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "REGULARIZATION STEP 1: GRADIENT CLIPPING TUNING\n",
      "================================================================================\n",
      "Using baseline settings: dropout=0.0, L1=0.0, L2=0.0\n",
      "\n",
      "Testing gradient clipping values: [0.0, 1.0]\n",
      "\n",
      "================================================================================\n",
      "Testing: Gradient Clipping = 0.0\n",
      "================================================================================\n",
      "    Epoch 10: Train Acc=96.56%, Val Acc=83.85%\n",
      "    Epoch 20: Train Acc=99.38%, Val Acc=86.42%\n",
      "    Early stopping at epoch 27, best val acc: 86.42%\n",
      "    Result: Val Acc=86.42%\n",
      "\n",
      "================================================================================\n",
      "Testing: Gradient Clipping = 1.0\n",
      "================================================================================\n",
      "    Epoch 10: Train Acc=94.48%, Val Acc=83.39%\n",
      "    Epoch 20: Train Acc=98.76%, Val Acc=84.77%\n",
      "    Epoch 30: Train Acc=99.50%, Val Acc=83.49%\n",
      "    Early stopping at epoch 34, best val acc: 85.32%\n",
      "    Result: Val Acc=85.32%\n",
      "\n",
      ">>> Step 1 Results:\n",
      "Grad Clip    Val Acc   \n",
      "-------------------------\n",
      "0.0          86.42     %\n",
      "1.0          85.32     %\n",
      "\n",
      ">>> Best Gradient Clipping: 0.0, Val Acc=86.42%\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Regularization Step 1: Gradient Clipping Tuning\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"REGULARIZATION STEP 1: GRADIENT CLIPPING TUNING\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Using baseline settings: dropout={best_dropout}, L1={best_l1_lambda}, L2={best_l2_lambda}\")\n",
    "\n",
    "# Test different gradient clipping values\n",
    "grad_clip_options = [0.0, 1.0]  # 0.0 = no clipping, 1.0 = clip at 1.0\n",
    "\n",
    "print(f\"\\nTesting gradient clipping values: {grad_clip_options}\")\n",
    "\n",
    "step1_results = []\n",
    "\n",
    "for grad_clip in grad_clip_options:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Testing: Gradient Clipping = {grad_clip}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Create model with baseline settings\n",
    "    model = RNN_Classifier_Aggregation(\n",
    "        vocab_size=embedding_vocab_size,\n",
    "        embedding_dim=embedding_dim,\n",
    "        hidden_dim=best_hidden_dim,\n",
    "        output_dim=num_classes,\n",
    "        n_layers=N_LAYERS,\n",
    "        dropout=best_dropout,\n",
    "        padding_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "        pretrained_embeddings=pretrained_embeddings,\n",
    "        aggregation=best_aggregation['method']\n",
    "    ).to(device)\n",
    "    \n",
    "    # Select optimizer\n",
    "    if best_optimizer == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=final_lr, weight_decay=best_l2_lambda)\n",
    "    elif best_optimizer == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=final_lr, momentum=0.9, weight_decay=best_l2_lambda)\n",
    "    elif best_optimizer == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=final_lr, weight_decay=best_l2_lambda)\n",
    "    elif best_optimizer == 'Adagrad':\n",
    "        optimizer = optim.Adagrad(model.parameters(), lr=final_lr, weight_decay=best_l2_lambda)\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        \n",
    "        for batch in train_iter:\n",
    "            text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(text, text_lengths)\n",
    "            \n",
    "            loss = criterion(predictions, labels)\n",
    "            \n",
    "            if best_l1_lambda > 0:\n",
    "                loss = loss + compute_l1_loss(model, best_l1_lambda)\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            if grad_clip > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        train_acc = accuracy_score(train_labels, train_preds)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_iter:\n",
    "                text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "                predictions = model(text, text_lengths)\n",
    "                loss = criterion(predictions, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                preds = torch.argmax(predictions, dim=1)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_acc = accuracy_score(val_labels, val_preds)\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), f'rnn_reg_step1_gradclip{grad_clip}_best.pt')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(f\"    Early stopping at epoch {epoch+1}, best val acc: {best_val_acc*100:.2f}%\")\n",
    "                break\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"    Epoch {epoch+1}: Train Acc={train_acc*100:.2f}%, Val Acc={val_acc*100:.2f}%\")\n",
    "    \n",
    "    step1_results.append({\n",
    "        'grad_clip': grad_clip,\n",
    "        'val_acc': best_val_acc\n",
    "    })\n",
    "    \n",
    "    print(f\"    Result: Val Acc={best_val_acc*100:.2f}%\")\n",
    "\n",
    "# Find best gradient clipping\n",
    "best_step1 = max(step1_results, key=lambda x: x['val_acc'])\n",
    "best_grad_clip = best_step1['grad_clip']\n",
    "\n",
    "print(f\"\\n>>> Step 1 Results:\")\n",
    "print(f\"{'Grad Clip':<12} {'Val Acc':<10}\")\n",
    "print(\"-\" * 25)\n",
    "for result in step1_results:\n",
    "    print(f\"{result['grad_clip']:<12} {result['val_acc']*100:<10.2f}%\")\n",
    "print(f\"\\n>>> Best Gradient Clipping: {best_grad_clip}, Val Acc={best_step1['val_acc']*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8b7e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "REGULARIZATION STEP 2: DROPOUT TUNING\n",
      "================================================================================\n",
      "Using best from Step 1: grad_clip=0.0\n",
      "Using baseline settings: L1=0.0, L2=0.0\n",
      "\n",
      "Testing dropout values: [0.0, 0.3, 0.5, 0.7]\n",
      "\n",
      "================================================================================\n",
      "Testing: Dropout = 0.0\n",
      "================================================================================\n",
      "    Epoch 10: Train Acc=95.71%, Val Acc=83.39%\n",
      "    Epoch 20: Train Acc=99.24%, Val Acc=84.40%\n",
      "    Early stopping at epoch 24, best val acc: 85.96%\n",
      "    Result: Val Acc=85.96%\n",
      "\n",
      "================================================================================\n",
      "Testing: Dropout = 0.3\n",
      "================================================================================\n",
      "    Epoch 10: Train Acc=93.99%, Val Acc=81.56%\n",
      "    Epoch 20: Train Acc=99.01%, Val Acc=85.05%\n",
      "    Early stopping at epoch 29, best val acc: 85.78%\n",
      "    Result: Val Acc=85.78%\n",
      "\n",
      "================================================================================\n",
      "Testing: Dropout = 0.5\n",
      "================================================================================\n",
      "    Epoch 10: Train Acc=93.56%, Val Acc=83.12%\n",
      "    Epoch 20: Train Acc=98.88%, Val Acc=83.49%\n",
      "    Early stopping at epoch 28, best val acc: 85.87%\n",
      "    Result: Val Acc=85.87%\n",
      "\n",
      "================================================================================\n",
      "Testing: Dropout = 0.7\n",
      "================================================================================\n",
      "    Epoch 10: Train Acc=96.26%, Val Acc=84.22%\n",
      "    Epoch 20: Train Acc=99.50%, Val Acc=86.51%\n",
      "    Epoch 30: Train Acc=99.98%, Val Acc=86.88%\n",
      "    Early stopping at epoch 31, best val acc: 87.16%\n",
      "    Result: Val Acc=87.16%\n",
      "\n",
      ">>> Step 2 Results:\n",
      "Dropout      Val Acc   \n",
      "-------------------------\n",
      "0.0          85.96     %\n",
      "0.3          85.78     %\n",
      "0.5          85.87     %\n",
      "0.7          87.16     %\n",
      "\n",
      ">>> Best Dropout: 0.7, Val Acc=87.16%\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Regularization Step 2: Dropout Tuning\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"REGULARIZATION STEP 2: DROPOUT TUNING\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Using best from Step 1: grad_clip={best_grad_clip}\")\n",
    "print(f\"Using baseline settings: L1={best_l1_lambda}, L2={best_l2_lambda}\")\n",
    "\n",
    "# Test different dropout values\n",
    "dropout_options = [0.0, 0.3, 0.5, 0.7]\n",
    "\n",
    "print(f\"\\nTesting dropout values: {dropout_options}\")\n",
    "\n",
    "step2_results = []\n",
    "\n",
    "for dropout_val in dropout_options:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Testing: Dropout = {dropout_val}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Create model with best grad_clip and current dropout\n",
    "    model = RNN_Classifier_Aggregation(\n",
    "        vocab_size=embedding_vocab_size,\n",
    "        embedding_dim=embedding_dim,\n",
    "        hidden_dim=best_hidden_dim,\n",
    "        output_dim=num_classes,\n",
    "        n_layers=N_LAYERS,\n",
    "        dropout=dropout_val,\n",
    "        padding_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "        pretrained_embeddings=pretrained_embeddings,\n",
    "        aggregation=best_aggregation['method']\n",
    "    ).to(device)\n",
    "    \n",
    "    # Select optimizer\n",
    "    if best_optimizer == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=final_lr, weight_decay=best_l2_lambda)\n",
    "    elif best_optimizer == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=final_lr, momentum=0.9, weight_decay=best_l2_lambda)\n",
    "    elif best_optimizer == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=final_lr, weight_decay=best_l2_lambda)\n",
    "    elif best_optimizer == 'Adagrad':\n",
    "        optimizer = optim.Adagrad(model.parameters(), lr=final_lr, weight_decay=best_l2_lambda)\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        \n",
    "        for batch in train_iter:\n",
    "            text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(text, text_lengths)\n",
    "            \n",
    "            loss = criterion(predictions, labels)\n",
    "            \n",
    "            if best_l1_lambda > 0:\n",
    "                loss = loss + compute_l1_loss(model, best_l1_lambda)\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            if best_grad_clip > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), best_grad_clip)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        train_acc = accuracy_score(train_labels, train_preds)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_iter:\n",
    "                text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "                predictions = model(text, text_lengths)\n",
    "                loss = criterion(predictions, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                preds = torch.argmax(predictions, dim=1)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_acc = accuracy_score(val_labels, val_preds)\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), f'rnn_reg_step2_dropout{dropout_val}_best.pt')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(f\"    Early stopping at epoch {epoch+1}, best val acc: {best_val_acc*100:.2f}%\")\n",
    "                break\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"    Epoch {epoch+1}: Train Acc={train_acc*100:.2f}%, Val Acc={val_acc*100:.2f}%\")\n",
    "    \n",
    "    step2_results.append({\n",
    "        'dropout': dropout_val,\n",
    "        'val_acc': best_val_acc\n",
    "    })\n",
    "    \n",
    "    print(f\"    Result: Val Acc={best_val_acc*100:.2f}%\")\n",
    "\n",
    "# Find best dropout\n",
    "best_step2 = max(step2_results, key=lambda x: x['val_acc'])\n",
    "best_dropout = best_step2['dropout']\n",
    "\n",
    "print(f\"\\n>>> Step 2 Results:\")\n",
    "print(f\"{'Dropout':<12} {'Val Acc':<10}\")\n",
    "print(\"-\" * 25)\n",
    "for result in step2_results:\n",
    "    print(f\"{result['dropout']:<12} {result['val_acc']*100:<10.2f}%\")\n",
    "print(f\"\\n>>> Best Dropout: {best_dropout}, Val Acc={best_step2['val_acc']*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f14317b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "REGULARIZATION STEP 3: L1 REGULARIZATION TUNING\n",
      "================================================================================\n",
      "Using best from Steps 1-2: grad_clip=0.0, dropout=0.7\n",
      "Using baseline setting: L2=0.0\n",
      "\n",
      "Testing L1 lambda values: [0.0, 1e-06, 1e-05, 0.0001]\n",
      "\n",
      "================================================================================\n",
      "Testing: L1 Lambda = 0.0\n",
      "================================================================================\n",
      "    Epoch 10: Train Acc=96.31%, Val Acc=83.39%\n",
      "    Epoch 20: Train Acc=98.85%, Val Acc=85.60%\n",
      "    Early stopping at epoch 24, best val acc: 85.87%\n",
      "    Result: Val Acc=85.87%\n",
      "\n",
      "================================================================================\n",
      "Testing: L1 Lambda = 1e-06\n",
      "================================================================================\n",
      "    Epoch 10: Train Acc=95.78%, Val Acc=84.68%\n",
      "    Early stopping at epoch 17, best val acc: 84.68%\n",
      "    Result: Val Acc=84.68%\n",
      "\n",
      "================================================================================\n",
      "Testing: L1 Lambda = 1e-05\n",
      "================================================================================\n",
      "    Epoch 10: Train Acc=93.83%, Val Acc=82.48%\n",
      "    Early stopping at epoch 19, best val acc: 84.22%\n",
      "    Result: Val Acc=84.22%\n",
      "\n",
      "================================================================================\n",
      "Testing: L1 Lambda = 0.0001\n",
      "================================================================================\n",
      "    Epoch 10: Train Acc=72.05%, Val Acc=67.34%\n",
      "    Epoch 20: Train Acc=91.56%, Val Acc=80.28%\n",
      "    Early stopping at epoch 30, best val acc: 82.48%\n",
      "    Result: Val Acc=82.48%\n",
      "\n",
      ">>> Step 3 Results:\n",
      "L1 Lambda    Val Acc   \n",
      "-------------------------\n",
      "0e+00        85.87     %\n",
      "1e-06        84.68     %\n",
      "1e-05        84.22     %\n",
      "1e-04        82.48     %\n",
      "\n",
      ">>> Best L1 Lambda: 0.0, Val Acc=85.87%\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Regularization Step 3: L1 Regularization Tuning\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"REGULARIZATION STEP 3: L1 REGULARIZATION TUNING\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Using best from Steps 1-2: grad_clip={best_grad_clip}, dropout={best_dropout}\")\n",
    "print(f\"Using baseline setting: L2={best_l2_lambda}\")\n",
    "\n",
    "# Test different L1 lambda values\n",
    "l1_lambda_options = [0.0, 1e-6, 1e-5, 1e-4]\n",
    "\n",
    "print(f\"\\nTesting L1 lambda values: {l1_lambda_options}\")\n",
    "\n",
    "step3_results = []\n",
    "\n",
    "for l1_lambda in l1_lambda_options:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Testing: L1 Lambda = {l1_lambda}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Create model with best grad_clip, dropout, and current L1\n",
    "    model = RNN_Classifier_Aggregation(\n",
    "        vocab_size=embedding_vocab_size,\n",
    "        embedding_dim=embedding_dim,\n",
    "        hidden_dim=best_hidden_dim,\n",
    "        output_dim=num_classes,\n",
    "        n_layers=N_LAYERS,\n",
    "        dropout=best_dropout,\n",
    "        padding_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "        pretrained_embeddings=pretrained_embeddings,\n",
    "        aggregation=best_aggregation['method']\n",
    "    ).to(device)\n",
    "    \n",
    "    # Select optimizer\n",
    "    if best_optimizer == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=final_lr, weight_decay=best_l2_lambda)\n",
    "    elif best_optimizer == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=final_lr, momentum=0.9, weight_decay=best_l2_lambda)\n",
    "    elif best_optimizer == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=final_lr, weight_decay=best_l2_lambda)\n",
    "    elif best_optimizer == 'Adagrad':\n",
    "        optimizer = optim.Adagrad(model.parameters(), lr=final_lr, weight_decay=best_l2_lambda)\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        \n",
    "        for batch in train_iter:\n",
    "            text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(text, text_lengths)\n",
    "            \n",
    "            loss = criterion(predictions, labels)\n",
    "            \n",
    "            if l1_lambda > 0:\n",
    "                loss = loss + compute_l1_loss(model, l1_lambda)\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            if best_grad_clip > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), best_grad_clip)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        train_acc = accuracy_score(train_labels, train_preds)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_iter:\n",
    "                text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "                predictions = model(text, text_lengths)\n",
    "                loss = criterion(predictions, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                preds = torch.argmax(predictions, dim=1)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_acc = accuracy_score(val_labels, val_preds)\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), f'rnn_reg_step3_l1{l1_lambda}_best.pt')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(f\"    Early stopping at epoch {epoch+1}, best val acc: {best_val_acc*100:.2f}%\")\n",
    "                break\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"    Epoch {epoch+1}: Train Acc={train_acc*100:.2f}%, Val Acc={val_acc*100:.2f}%\")\n",
    "    \n",
    "    step3_results.append({\n",
    "        'l1_lambda': l1_lambda,\n",
    "        'val_acc': best_val_acc\n",
    "    })\n",
    "    \n",
    "    print(f\"    Result: Val Acc={best_val_acc*100:.2f}%\")\n",
    "\n",
    "# Find best L1 lambda\n",
    "best_step3 = max(step3_results, key=lambda x: x['val_acc'])\n",
    "best_l1_lambda = best_step3['l1_lambda']\n",
    "\n",
    "print(f\"\\n>>> Step 3 Results:\")\n",
    "print(f\"{'L1 Lambda':<12} {'Val Acc':<10}\")\n",
    "print(\"-\" * 25)\n",
    "for result in step3_results:\n",
    "    print(f\"{result['l1_lambda']:<12.0e} {result['val_acc']*100:<10.2f}%\")\n",
    "print(f\"\\n>>> Best L1 Lambda: {best_l1_lambda}, Val Acc={best_step3['val_acc']*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89324596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "REGULARIZATION STEP 4: L2 REGULARIZATION TUNING\n",
      "================================================================================\n",
      "Using best from Steps 1-3: grad_clip=0.0, dropout=0.7, L1=0.0\n",
      "\n",
      "Testing L2 lambda values: [0.0, 1e-05, 0.0001, 0.001]\n",
      "\n",
      "================================================================================\n",
      "Testing: L2 Lambda = 0.0\n",
      "================================================================================\n",
      "    Epoch 10: Train Acc=95.78%, Val Acc=84.95%\n",
      "    Early stopping at epoch 18, best val acc: 85.78%\n",
      "    Result: Val Acc=85.78%\n",
      "\n",
      "================================================================================\n",
      "Testing: L2 Lambda = 1e-05\n",
      "================================================================================\n",
      "    Epoch 10: Train Acc=94.93%, Val Acc=82.84%\n",
      "    Epoch 20: Train Acc=97.78%, Val Acc=81.01%\n",
      "    Epoch 30: Train Acc=99.75%, Val Acc=84.31%\n",
      "    Early stopping at epoch 36, best val acc: 85.05%\n",
      "    Result: Val Acc=85.05%\n",
      "\n",
      "================================================================================\n",
      "Testing: L2 Lambda = 0.0001\n",
      "================================================================================\n",
      "    Epoch 10: Train Acc=95.76%, Val Acc=82.94%\n",
      "    Epoch 20: Train Acc=99.20%, Val Acc=83.39%\n",
      "    Epoch 30: Train Acc=99.66%, Val Acc=83.21%\n",
      "    Early stopping at epoch 31, best val acc: 84.86%\n",
      "    Result: Val Acc=84.86%\n",
      "\n",
      "================================================================================\n",
      "Testing: L2 Lambda = 0.001\n",
      "================================================================================\n",
      "    Epoch 10: Train Acc=92.02%, Val Acc=81.19%\n",
      "    Epoch 20: Train Acc=97.82%, Val Acc=83.21%\n",
      "    Early stopping at epoch 23, best val acc: 83.76%\n",
      "    Result: Val Acc=83.76%\n",
      "\n",
      ">>> Step 4 Results:\n",
      "L2 Lambda    Val Acc   \n",
      "-------------------------\n",
      "0e+00        85.78     %\n",
      "1e-05        85.05     %\n",
      "1e-04        84.86     %\n",
      "1e-03        83.76     %\n",
      "\n",
      ">>> Best L2 Lambda: 0.0, Val Acc=85.78%\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Regularization Step 4: L2 Regularization Tuning\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"REGULARIZATION STEP 4: L2 REGULARIZATION TUNING\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Using best from Steps 1-3: grad_clip={best_grad_clip}, dropout={best_dropout}, L1={best_l1_lambda}\")\n",
    "\n",
    "# Test different L2 lambda values (via weight_decay)\n",
    "l2_lambda_options = [0.0, 1e-5, 1e-4, 1e-3]\n",
    "\n",
    "print(f\"\\nTesting L2 lambda values: {l2_lambda_options}\")\n",
    "\n",
    "step4_results = []\n",
    "\n",
    "for l2_lambda in l2_lambda_options:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Testing: L2 Lambda = {l2_lambda}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Create model with best grad_clip, dropout, L1, and current L2\n",
    "    model = RNN_Classifier_Aggregation(\n",
    "        vocab_size=embedding_vocab_size,\n",
    "        embedding_dim=embedding_dim,\n",
    "        hidden_dim=best_hidden_dim,\n",
    "        output_dim=num_classes,\n",
    "        n_layers=N_LAYERS,\n",
    "        dropout=best_dropout,\n",
    "        padding_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "        pretrained_embeddings=pretrained_embeddings,\n",
    "        aggregation=best_aggregation['method']\n",
    "    ).to(device)\n",
    "    \n",
    "    # Select optimizer with L2 regularization (weight_decay)\n",
    "    if best_optimizer == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=final_lr, weight_decay=l2_lambda)\n",
    "    elif best_optimizer == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=final_lr, momentum=0.9, weight_decay=l2_lambda)\n",
    "    elif best_optimizer == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=final_lr, weight_decay=l2_lambda)\n",
    "    elif best_optimizer == 'Adagrad':\n",
    "        optimizer = optim.Adagrad(model.parameters(), lr=final_lr, weight_decay=l2_lambda)\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        \n",
    "        for batch in train_iter:\n",
    "            text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(text, text_lengths)\n",
    "            \n",
    "            loss = criterion(predictions, labels)\n",
    "            \n",
    "            if best_l1_lambda > 0:\n",
    "                loss = loss + compute_l1_loss(model, best_l1_lambda)\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            if best_grad_clip > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), best_grad_clip)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        train_acc = accuracy_score(train_labels, train_preds)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_iter:\n",
    "                text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "                predictions = model(text, text_lengths)\n",
    "                loss = criterion(predictions, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                preds = torch.argmax(predictions, dim=1)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_acc = accuracy_score(val_labels, val_preds)\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), f'rnn_reg_step4_l2{l2_lambda}_best.pt')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(f\"    Early stopping at epoch {epoch+1}, best val acc: {best_val_acc*100:.2f}%\")\n",
    "                break\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"    Epoch {epoch+1}: Train Acc={train_acc*100:.2f}%, Val Acc={val_acc*100:.2f}%\")\n",
    "    \n",
    "    step4_results.append({\n",
    "        'l2_lambda': l2_lambda,\n",
    "        'val_acc': best_val_acc\n",
    "    })\n",
    "    \n",
    "    print(f\"    Result: Val Acc={best_val_acc*100:.2f}%\")\n",
    "\n",
    "# Find best L2 lambda\n",
    "best_step4 = max(step4_results, key=lambda x: x['val_acc'])\n",
    "best_l2_lambda = best_step4['l2_lambda']\n",
    "\n",
    "print(f\"\\n>>> Step 4 Results:\")\n",
    "print(f\"{'L2 Lambda':<12} {'Val Acc':<10}\")\n",
    "print(\"-\" * 25)\n",
    "for result in step4_results:\n",
    "    print(f\"{result['l2_lambda']:<12.0e} {result['val_acc']*100:<10.2f}%\")\n",
    "print(f\"\\n>>> Best L2 Lambda: {best_l2_lambda}, Val Acc={best_step4['val_acc']*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a5b338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "REGULARIZATION FINAL: ALL BEST SETTINGS COMBINED\n",
      "================================================================================\n",
      "Best settings from all steps:\n",
      "    Gradient Clipping: 0.0\n",
      "    Dropout: 0.7\n",
      "    L1 Lambda: 0.0\n",
      "    L2 Lambda: 0.0\n",
      "\n",
      ">>> Training final model with all best regularization settings...\n",
      "    Epoch 10: Train Acc=96.91%, Val Acc=86.15%\n",
      "    Early stopping at epoch 20, best val acc: 86.79%\n",
      "\n",
      ">>> Final Combined Results:\n",
      "    Configuration:\n",
      "      - Gradient Clipping: 0.0\n",
      "      - Dropout: 0.7\n",
      "      - L1 Lambda: 0.0\n",
      "      - L2 Lambda: 0.0\n",
      "    Validation Acc: 86.79%\n",
      "    Test Acc: 87.60%\n",
      "    Test F1: 0.8734\n",
      "    Test AUC-ROC: 0.9702\n",
      "\n",
      ">>> Comparison with Baseline:\n",
      "    Baseline Test Acc: 88.20%\n",
      "    Final Regularized Test Acc: 87.60%\n",
      "    Improvement: -0.60% (-0.68% relative)\n",
      "\n",
      ">>> Plotting training curves for best configuration and regularization...\n",
      "    Saved training curves to 'best_config_training_curves.png'\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "REGULARIZATION TUNING COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Regularization Final: All Best Combined\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"REGULARIZATION FINAL: ALL BEST SETTINGS COMBINED\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Best settings from all steps:\")\n",
    "print(f\"    Gradient Clipping: {best_grad_clip}\")\n",
    "print(f\"    Dropout: {best_dropout}\")\n",
    "print(f\"    L1 Lambda: {best_l1_lambda}\")\n",
    "print(f\"    L2 Lambda: {best_l2_lambda}\")\n",
    "\n",
    "# Create model with all best settings\n",
    "model = RNN_Classifier_Aggregation(\n",
    "    vocab_size=embedding_vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_dim=best_hidden_dim,\n",
    "    output_dim=num_classes,\n",
    "    n_layers=N_LAYERS,\n",
    "    dropout=best_dropout,\n",
    "    padding_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "    pretrained_embeddings=pretrained_embeddings,\n",
    "    aggregation=best_aggregation['method']\n",
    ").to(device)\n",
    "\n",
    "# Select optimizer with best L2 (weight_decay)\n",
    "if best_optimizer == 'Adam':\n",
    "    optimizer = optim.Adam(model.parameters(), lr=final_lr, weight_decay=best_l2_lambda)\n",
    "elif best_optimizer == 'SGD':\n",
    "    optimizer = optim.SGD(model.parameters(), lr=final_lr, momentum=0.9, weight_decay=best_l2_lambda)\n",
    "elif best_optimizer == 'RMSprop':\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=final_lr, weight_decay=best_l2_lambda)\n",
    "elif best_optimizer == 'Adagrad':\n",
    "    optimizer = optim.Adagrad(model.parameters(), lr=final_lr, weight_decay=best_l2_lambda)\n",
    "\n",
    "# Training loop\n",
    "best_val_acc = 0.0\n",
    "patience_counter = 0\n",
    "\n",
    "# Store training history for plotting\n",
    "train_losses = []\n",
    "val_accs = []\n",
    "\n",
    "print(f\"\\n>>> Training final model with all best regularization settings...\")\n",
    "\n",
    "for epoch in range(MAX_EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_preds = []\n",
    "    train_labels = []\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch in train_iter:\n",
    "        num_batches += 1\n",
    "        text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(text, text_lengths)\n",
    "        \n",
    "        loss = criterion(predictions, labels)\n",
    "        \n",
    "        if best_l1_lambda > 0:\n",
    "            loss = loss + compute_l1_loss(model, best_l1_lambda)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        if best_grad_clip > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), best_grad_clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        preds = torch.argmax(predictions, dim=1)\n",
    "        train_preds.extend(preds.cpu().numpy())\n",
    "        train_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    train_acc = accuracy_score(train_labels, train_preds)\n",
    "    num_train_batches = len(train_labels) // train_iter.batch_size + (1 if len(train_labels) % train_iter.batch_size != 0 else 0)\n",
    "    train_loss_avg = train_loss / num_train_batches if num_train_batches > 0 else train_loss\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_iter:\n",
    "            text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "            predictions = model(text, text_lengths)\n",
    "            loss = criterion(predictions, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    val_acc = accuracy_score(val_labels, val_preds)\n",
    "    \n",
    "    # Store training history for plotting\n",
    "    train_losses.append(train_loss_avg)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), 'rnn_reg_final_best.pt')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(f\"    Early stopping at epoch {epoch+1}, best val acc: {best_val_acc*100:.2f}%\")\n",
    "            break\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"    Epoch {epoch+1}: Train Acc={train_acc*100:.2f}%, Val Acc={val_acc*100:.2f}%\")\n",
    "\n",
    "# Evaluate on test set\n",
    "model.load_state_dict(torch.load('rnn_reg_final_best.pt'))\n",
    "model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "test_preds = []\n",
    "test_labels = []\n",
    "test_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_iter:\n",
    "        text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "        predictions = model(text, text_lengths)\n",
    "        loss = criterion(predictions, labels)\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        probs = torch.softmax(predictions, dim=1)\n",
    "        preds = torch.argmax(predictions, dim=1)\n",
    "        test_preds.extend(preds.cpu().numpy())\n",
    "        test_labels.extend(labels.cpu().numpy())\n",
    "        test_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "test_acc = accuracy_score(test_labels, test_preds)\n",
    "test_f1 = f1_score(test_labels, test_preds, average='weighted')\n",
    "test_loss_avg = test_loss / len(test_iter)\n",
    "\n",
    "try:\n",
    "    test_probs_array = np.array(test_probs)\n",
    "    test_labels_bin = label_binarize(test_labels, classes=range(num_classes))\n",
    "    test_auc = roc_auc_score(test_labels_bin, test_probs_array, average='weighted', multi_class='ovr')\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not calculate AUC-ROC: {e}\")\n",
    "    test_auc = 0.0\n",
    "\n",
    "final_results = {\n",
    "    'name': 'final_combined',\n",
    "    'dropout': best_dropout,\n",
    "    'grad_clip': best_grad_clip,\n",
    "    'l1_lambda': best_l1_lambda,\n",
    "    'l2_lambda': best_l2_lambda,\n",
    "    'val_acc': best_val_acc,\n",
    "    'test_acc': test_acc,\n",
    "    'test_f1': test_f1,\n",
    "    'test_auc': test_auc\n",
    "}\n",
    "\n",
    "print(f\"\\n>>> Final Combined Results:\")\n",
    "print(f\"    Configuration:\")\n",
    "print(f\"      - Gradient Clipping: {best_grad_clip}\")\n",
    "print(f\"      - Dropout: {best_dropout}\")\n",
    "print(f\"      - L1 Lambda: {best_l1_lambda}\")\n",
    "print(f\"      - L2 Lambda: {best_l2_lambda}\")\n",
    "print(f\"    Validation Acc: {best_val_acc*100:.2f}%\")\n",
    "print(f\"    Test Acc: {test_acc*100:.2f}%\")\n",
    "print(f\"    Test F1: {test_f1:.4f}\")\n",
    "print(f\"    Test AUC-ROC: {test_auc:.4f}\")\n",
    "\n",
    "# Compare with baseline\n",
    "improvement = test_acc - baseline_results['test_acc']\n",
    "improvement_pct = (improvement / baseline_results['test_acc']) * 100 if baseline_results['test_acc'] > 0 else 0\n",
    "\n",
    "print(f\"\\n>>> Comparison with Baseline:\")\n",
    "print(f\"    Baseline Test Acc: {baseline_results['test_acc']*100:.2f}%\")\n",
    "print(f\"    Final Regularized Test Acc: {test_acc*100:.2f}%\")\n",
    "print(f\"    Improvement: {improvement*100:+.2f}% ({improvement_pct:+.2f}% relative)\")\n",
    "\n",
    "# Plot training curves for best configuration and regularization\n",
    "print(f\"\\n>>> Plotting training curves for best configuration and regularization...\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Training Loss vs Epochs\n",
    "epochs = range(1, len(train_losses) + 1)\n",
    "ax1.plot(epochs, train_losses, 'b-', label='Training Loss', linewidth=2, marker='o', markersize=4)\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Training Loss', fontsize=12)\n",
    "ax1.set_title('Training Loss Curve', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xlim(left=1)\n",
    "\n",
    "# Plot 2: Validation Accuracy vs Epochs\n",
    "ax2.plot(epochs, [acc*100 for acc in val_accs], 'r-', label='Validation Accuracy', linewidth=2, marker='s', markersize=4)\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Validation Accuracy (%)', fontsize=12)\n",
    "ax2.set_title('Validation Accuracy Curve', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xlim(left=1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('best_config_training_curves.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"    Saved training curves to 'best_config_training_curves.png'\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"REGULARIZATION TUNING COMPLETE\")\n",
    "print(f\"{'='*80}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f4086d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PART 2(e): TOPIC-WISE ACCURACY EVALUATION\n",
      "================================================================================\n",
      "\n",
      ">>> Using model from regularization tuning...\n",
      "     Using existing model from previous cell\n",
      "    Model vocab size: 8108\n",
      "\n",
      ">>> Evaluating on test set...\n",
      "\n",
      "================================================================================\n",
      "TOPIC-WISE ACCURACY ON TEST SET\n",
      "================================================================================\n",
      "Topic      Accuracy     Correct    Total      % of Test Set  \n",
      "--------------------------------------------------------------------------------\n",
      "ABBR       66.67        6          9          1.80           \n",
      "DESC       100.00       138        138        27.60          \n",
      "ENTY       69.15        65         94         18.80          \n",
      "HUM        87.69        57         65         13.00          \n",
      "LOC        92.59        75         81         16.20          \n",
      "NUM        85.84        97         113        22.60          \n",
      "--------------------------------------------------------------------------------\n",
      "OVERALL    87.60        438        500        100.00         \n",
      "\n",
      "================================================================================\n",
      "DISCUSSION: FACTORS AFFECTING TOPIC-WISE ACCURACY\n",
      "================================================================================\n",
      "\n",
      "1. CLASS IMBALANCE IN TRAINING DATA:\n",
      "Topic      Train Count     Train %      Test Count   Test %      \n",
      "----------------------------------------------------------------------\n",
      "ABBR       69              1.58         9            1.80        \n",
      "DESC       930             21.32        138          27.60       \n",
      "ENTY       1000            22.93        94           18.80       \n",
      "HUM        978             22.42        65           13.00       \n",
      "LOC        668             15.31        81           16.20       \n",
      "NUM        717             16.44        113          22.60       \n",
      "\n",
      "2. KEY OBSERVATIONS AND POTENTIAL CAUSES:\n",
      "\n",
      "   a) Class Imbalance Effect:\n",
      "      - Topics with fewer training examples (e.g., ABBR with only 1.58% of data)\n",
      "        may have lower accuracy due to insufficient learning signal\n",
      "      - The model may be biased toward more frequent classes during training\n",
      "\n",
      "   b) Semantic Complexity:\n",
      "      - Some topics may have more ambiguous or overlapping characteristics\n",
      "      - For example, ABBR (abbreviations) might be confused with other categories\n",
      "        if the context is not clear enough\n",
      "\n",
      "   c) Vocabulary and OOV Rates:\n",
      "      - Topics with higher OOV rates (as seen in Part 1) may have lower accuracy\n",
      "      - ABBR had 9.70% OOV rate (highest), which could contribute to lower performance\n",
      "\n",
      "   d) Question Type Characteristics:\n",
      "      - ENTY (entities) and HUM (humans) are more distinct and may be easier to classify\n",
      "      - DESC (descriptions) might overlap with other categories semantically\n",
      "      - NUM (numeric) questions may have distinctive patterns that aid classification\n",
      "\n",
      "   e) Model Capacity and Representation:\n",
      "      - The RNN may capture certain patterns better than others\n",
      "      - Aggregation method (attention/mean/max/last) may favor certain topic structures\n",
      "\n",
      "3. RECOMMENDATIONS FOR IMPROVEMENT:\n",
      "   - Use class weights in loss function to handle imbalance\n",
      "   - Apply data augmentation or oversampling for minority classes\n",
      "   - Consider focal loss to focus on hard examples\n",
      "   - Fine-tune embeddings specifically for underrepresented topics\n",
      "   - Use ensemble methods combining multiple models\n",
      "\n",
      "================================================================================\n",
      "TOPIC-WISE EVALUATION COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Part 2(e): Topic-wise Accuracy Evaluation on Test Set\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 2(e): TOPIC-WISE ACCURACY EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load the best model from regularization tuning\n",
    "# Use the model from the previous cell (regularization final) if available\n",
    "# Otherwise, load it from the saved checkpoint\n",
    "\n",
    "print(\"\\n>>> Using model from regularization tuning...\")\n",
    "\n",
    "# Check if 'model' variable exists from the previous cell (cell 26)\n",
    "try:\n",
    "    # Try to use the model from the previous cell\n",
    "    if 'model' in locals() or 'model' in globals():\n",
    "        # Verify it's a valid model instance\n",
    "        if hasattr(model, 'embedding') and hasattr(model, 'eval'):\n",
    "            final_model = model\n",
    "            final_model.eval()\n",
    "            saved_vocab_size = final_model.embedding.weight.shape[0]\n",
    "            print(f\"     Using existing model from previous cell\")\n",
    "            print(f\"    Model vocab size: {saved_vocab_size}\")\n",
    "        else:\n",
    "            raise AttributeError(\"Model exists but is not valid\")\n",
    "    else:\n",
    "        raise NameError(\"Model variable not found\")\n",
    "except (NameError, AttributeError):\n",
    "    # Model doesn't exist or is invalid, load from checkpoint\n",
    "    print(\"    Model not found in previous cell, loading from checkpoint...\")\n",
    "    try:\n",
    "        checkpoint = torch.load('weights/rnn_reg_final_best.pt', map_location=device)\n",
    "    except FileNotFoundError:\n",
    "        checkpoint = torch.load('rnn_reg_final_best.pt', map_location=device)\n",
    "    \n",
    "    # Infer configuration from saved state dict\n",
    "    saved_vocab_size = checkpoint['embedding.weight'].shape[0]\n",
    "    saved_hidden_dim = checkpoint['rnn.weight_ih_l0'].shape[0]\n",
    "    has_attention = 'attention.weight' in checkpoint\n",
    "    saved_aggregation = 'attention' if has_attention else 'last'\n",
    "    \n",
    "    # Recreate model\n",
    "    final_model = RNN_Classifier_Aggregation(\n",
    "        vocab_size=saved_vocab_size,\n",
    "        embedding_dim=embedding_dim,\n",
    "        hidden_dim=saved_hidden_dim,\n",
    "        output_dim=num_classes,\n",
    "        n_layers=N_LAYERS,\n",
    "        dropout=best_dropout,\n",
    "        padding_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "        pretrained_embeddings=None,\n",
    "        aggregation=saved_aggregation\n",
    "    ).to(device)\n",
    "    \n",
    "    final_model.load_state_dict(checkpoint, strict=True)\n",
    "    final_model.eval()\n",
    "    print(f\"     Model loaded from checkpoint (vocab_size={saved_vocab_size})\")\n",
    "\n",
    "# Function to evaluate per topic\n",
    "def evaluate_per_topic(model, iterator, device, max_vocab_size=None):\n",
    "    \"\"\"\n",
    "    Evaluate model performance per topic category on the test set.\n",
    "    Returns a dictionary with accuracy for each topic.\n",
    "    \n",
    "    Args:\n",
    "        model: The trained model\n",
    "        iterator: Data iterator for test set\n",
    "        device: Device to run on\n",
    "        max_vocab_size: Maximum valid vocabulary size (to clip token indices)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Dictionary to store correct and total counts per topic\n",
    "    topic_correct = defaultdict(int)\n",
    "    topic_total = defaultdict(int)\n",
    "    \n",
    "    # Get label vocabulary for mapping\n",
    "    label_to_idx = LABEL.vocab.stoi\n",
    "    idx_to_label = LABEL.vocab.itos\n",
    "    \n",
    "    # Get <unk> token index for mapping out-of-range tokens\n",
    "    unk_idx = TEXT.vocab.stoi.get(TEXT.unk_token, 0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            # Process batch (should be on CPU from the iterator)\n",
    "            text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "            \n",
    "            # Clip token indices to valid range if max_vocab_size is specified\n",
    "            # This handles cases where the current vocab is larger than the saved model's vocab\n",
    "            if max_vocab_size is not None:\n",
    "                # Map any indices >= max_vocab_size to <unk> token\n",
    "                text = torch.where(text >= max_vocab_size, \n",
    "                                 torch.tensor(unk_idx, device=text.device, dtype=text.dtype), \n",
    "                                 text)\n",
    "                # Also ensure no negative indices\n",
    "                text = torch.clamp(text, min=0)\n",
    "            \n",
    "            # Move tensors to the actual device after clipping\n",
    "            text = text.to(device)\n",
    "            text_lengths = text_lengths.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            predictions = model(text, text_lengths)\n",
    "            \n",
    "            # Get predicted labels\n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            \n",
    "            # Convert to numpy for easier processing\n",
    "            preds_np = preds.cpu().numpy()\n",
    "            labels_np = labels.cpu().numpy()\n",
    "            \n",
    "            # Count correct and total for each topic\n",
    "            for pred_idx, true_idx in zip(preds_np, labels_np):\n",
    "                true_label = idx_to_label[true_idx]\n",
    "                topic_total[true_label] += 1\n",
    "                \n",
    "                if pred_idx == true_idx:\n",
    "                    topic_correct[true_label] += 1\n",
    "    \n",
    "    # Calculate accuracy per topic\n",
    "    topic_accuracies = {}\n",
    "    for label in sorted(topic_total.keys()):\n",
    "        if topic_total[label] > 0:\n",
    "            acc = topic_correct[label] / topic_total[label]\n",
    "            topic_accuracies[label] = {\n",
    "                'accuracy': acc,\n",
    "                'correct': topic_correct[label],\n",
    "                'total': topic_total[label]\n",
    "            }\n",
    "    \n",
    "    return topic_accuracies\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"\\n>>> Evaluating on test set...\")\n",
    "\n",
    "# Create a CPU iterator to avoid CUDA errors during numericalization with invalid token indices\n",
    "# We'll process on CPU, clip indices, then move to device\n",
    "from torchtext import data\n",
    "cpu_device = torch.device('cpu')\n",
    "test_iter_cpu = data.BucketIterator(\n",
    "    test_data,\n",
    "    batch_size=test_iter.batch_size,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    device=cpu_device,\n",
    "    sort=False,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Pass max_vocab_size to clip token indices to the saved model's vocabulary size\n",
    "topic_accuracies = evaluate_per_topic(final_model, test_iter_cpu, device, max_vocab_size=saved_vocab_size)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TOPIC-WISE ACCURACY ON TEST SET\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Topic':<10} {'Accuracy':<12} {'Correct':<10} {'Total':<10} {'% of Test Set':<15}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Calculate total test samples for percentage calculation\n",
    "total_test_samples = sum(acc['total'] for acc in topic_accuracies.values())\n",
    "\n",
    "for topic in sorted(topic_accuracies.keys()):\n",
    "    acc_info = topic_accuracies[topic]\n",
    "    acc_pct = acc_info['accuracy'] * 100\n",
    "    correct = acc_info['correct']\n",
    "    total = acc_info['total']\n",
    "    pct_of_test = (total / total_test_samples) * 100 if total_test_samples > 0 else 0\n",
    "    \n",
    "    print(f\"{topic:<10} {acc_pct:<12.2f} {correct:<10} {total:<10} {pct_of_test:<15.2f}\")\n",
    "\n",
    "# Calculate overall accuracy\n",
    "overall_correct = sum(acc_info['correct'] for acc_info in topic_accuracies.values())\n",
    "overall_total = sum(acc_info['total'] for acc_info in topic_accuracies.values())\n",
    "overall_acc = overall_correct / overall_total if overall_total > 0 else 0\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'OVERALL':<10} {overall_acc*100:<12.2f} {overall_correct:<10} {overall_total:<10} {'100.00':<15}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Discussion: What may cause differences in accuracies across topics\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DISCUSSION: FACTORS AFFECTING TOPIC-WISE ACCURACY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get training distribution for comparison\n",
    "train_label_counts = Counter([ex.label for ex in train_data.examples])\n",
    "total_train = len(train_data.examples)\n",
    "\n",
    "print(\"\\nCLASS IMBALANCE IN TRAINING DATA:\")\n",
    "print(f\"{'Topic':<10} {'Train Count':<15} {'Train %':<12} {'Test Count':<12} {'Test %':<12}\")\n",
    "print(\"-\" * 70)\n",
    "for topic in sorted(topic_accuracies.keys()):\n",
    "    train_count = train_label_counts.get(topic, 0)\n",
    "    train_pct = (train_count / total_train) * 100 if total_train > 0 else 0\n",
    "    test_count = topic_accuracies[topic]['total']\n",
    "    test_pct = (test_count / total_test_samples) * 100 if total_test_samples > 0 else 0\n",
    "    print(f\"{topic:<10} {train_count:<15} {train_pct:<12.2f} {test_count:<12} {test_pct:<12.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TOPIC-WISE EVALUATION COMPLETE\")\n",
    "print(\"=\"*80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
