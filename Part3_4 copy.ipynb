{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05ac734e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ.setdefault('TORCH_COMPILE_DISABLE', '1')\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "import torch\n",
    "\n",
    "# Method 2: Patch torch._dynamo.disable decorator after import\n",
    "try:\n",
    "    import torch._dynamo\n",
    "    # Patch the disable function to ignore the 'wrapping' parameter\n",
    "    if hasattr(torch._dynamo, 'disable'):\n",
    "        def patched_disable(fn=None, *args, **kwargs):\n",
    "            # Remove problematic 'wrapping' parameter if present\n",
    "            if 'wrapping' in kwargs:\n",
    "                kwargs.pop('wrapping')\n",
    "            if fn is None:\n",
    "                # Decorator usage: @disable\n",
    "                return lambda f: f\n",
    "            # Function usage: disable(fn) or disable(fn, **kwargs)\n",
    "            # Simply return the function unwrapped to avoid recursion\n",
    "            # The original disable was causing issues, so we bypass it entirely\n",
    "            return fn\n",
    "        torch._dynamo.disable = patched_disable\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not patch torch._dynamo: {e}\")\n",
    "    pass  # If patching fails, continue anyway\n",
    "\n",
    "import random, string\n",
    "\n",
    "from torchtext import data , datasets\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "\n",
    "os.environ['GENSIM_DATA_DIR'] = os.path.join(os.getcwd(), 'gensim-data')\n",
    "\n",
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.fasttext import load_facebook_model\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import time, copy\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44e533a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Prepping Data...\n",
      "[+] Test set formed!\n",
      "[+] Train and Validation sets formed!\n",
      "[+] Data prepped successfully!\n",
      "[*] Retrieving pretrained word embeddings...\n",
      "[*] Loading fasttext model...\n",
      "[+] Model loaded!\n",
      "[*] Forming embedding matrix...\n",
      "[+] Embedding matrix formed!\n",
      "[+] Embeddings retrieved successfully!\n",
      "\n",
      "Label distribution in training set:\n",
      "- ABBR: 69 samples (1.58%)\n",
      "- DESC: 930 samples (21.32%)\n",
      "- ENTY: 1000 samples (22.93%)\n",
      "- HUM: 978 samples (22.42%)\n",
      "- LOC: 668 samples (15.31%)\n",
      "- NUM: 717 samples (16.44%)\n",
      "Total samples: 4362, Sum of percentages: 100.00%\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "train_data, validation_data, test_data, LABEL, TEXT, pretrained_embed = data_prep(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c176434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86a01357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of classes: 6\n",
      "Classes: ['ENTY', 'HUM', 'DESC', 'NUM', 'LOC', 'ABBR']\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary for labels\n",
    "LABEL.build_vocab(train_data)\n",
    "num_classes = len(LABEL.vocab)\n",
    "print(f\"\\nNumber of classes: {num_classes}\")\n",
    "print(f\"Classes: {LABEL.vocab.itos}\")\n",
    "\n",
    "# Get pretrained embeddings from Part 1 (frozen embeddings)\n",
    "pretrained_embeddings = pretrained_embed.weight.data\n",
    "\n",
    "# Get embedding dimension and vocab size from the fasttext embedding layer\n",
    "embedding_dim = pretrained_embed.weight.shape[1]\n",
    "embedding_vocab_size = pretrained_embed.weight.shape[0]  # Vocab size from saved embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2597781b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT Vocab Size: 8166\n"
     ]
    }
   ],
   "source": [
    "print(f'TEXT Vocab Size: {len(TEXT.vocab.stoi)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aad6c8e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PART 3.4: TARGETED IMPROVEMENT FOR WEAK TOPICS\n",
      "================================================================================\n",
      "\n",
      "Strategies:\n",
      "  1. Data Augmentation for imbalanced classes (especially ABBR)\n",
      "  2. Positional Embeddings in attention layer\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PART 3.4: Targeted Improvement for Weak Topics\n",
    "# Strategy: Data Augmentation, Positional Embeddings\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 3.4: TARGETED IMPROVEMENT FOR WEAK TOPICS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nStrategies:\")\n",
    "print(\"  1. Data Augmentation for imbalanced classes (especially ABBR)\")\n",
    "print(\"  2. Positional Embeddings in attention layer\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Import required libraries for augmentation\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "try:\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c2db80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Step 2: Implementing Data Augmentation Functions...\n",
      "    ✓ Data augmentation functions ready\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Step 2: Data Augmentation Functions for Imbalanced Classes\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n>>> Step 2: Implementing Data Augmentation Functions...\")\n",
    "\n",
    "def get_synonyms(word):\n",
    "    \"\"\"Get synonyms for a word using WordNet\"\"\"\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonym = lemma.name().replace('_', ' ').lower()\n",
    "            if synonym != word and synonym.isalpha():\n",
    "                synonyms.add(synonym)\n",
    "    return list(synonyms)\n",
    "\n",
    "def synonym_replacement(tokens, n=1):\n",
    "    \"\"\"Replace n random words with their synonyms\"\"\"\n",
    "    new_tokens = tokens.copy()\n",
    "    words_to_replace = [i for i, word in enumerate(tokens) if word.isalpha() and len(word) > 2]\n",
    "    \n",
    "    if len(words_to_replace) == 0:\n",
    "        return tokens\n",
    "    \n",
    "    num_replacements = min(n, len(words_to_replace))\n",
    "    indices_to_replace = random.sample(words_to_replace, num_replacements)\n",
    "    \n",
    "    for idx in indices_to_replace:\n",
    "        synonyms = get_synonyms(tokens[idx])\n",
    "        if synonyms:\n",
    "            new_tokens[idx] = random.choice(synonyms)\n",
    "    \n",
    "    return new_tokens\n",
    "\n",
    "def random_insertion(tokens, n=1):\n",
    "    \"\"\"Randomly insert synonyms of n words\"\"\"\n",
    "    new_tokens = tokens.copy()\n",
    "    \n",
    "    for _ in range(n):\n",
    "        if len(new_tokens) == 0:\n",
    "            break\n",
    "        word = random.choice(new_tokens)\n",
    "        synonyms = get_synonyms(word)\n",
    "        if synonyms:\n",
    "            synonym = random.choice(synonyms)\n",
    "            insert_pos = random.randint(0, len(new_tokens))\n",
    "            new_tokens.insert(insert_pos, synonym)\n",
    "    \n",
    "    return new_tokens\n",
    "\n",
    "def random_deletion(tokens, p=0.1):\n",
    "    \"\"\"Randomly delete words with probability p\"\"\"\n",
    "    if len(tokens) == 1:\n",
    "        return tokens\n",
    "    \n",
    "    new_tokens = []\n",
    "    for token in tokens:\n",
    "        if random.random() > p:\n",
    "            new_tokens.append(token)\n",
    "    \n",
    "    if len(new_tokens) == 0:\n",
    "        return tokens[:1]\n",
    "    \n",
    "    return new_tokens\n",
    "\n",
    "def random_swap(tokens, n=1):\n",
    "    \"\"\"Randomly swap n pairs of words\"\"\"\n",
    "    new_tokens = tokens.copy()\n",
    "    \n",
    "    for _ in range(n):\n",
    "        if len(new_tokens) < 2:\n",
    "            break\n",
    "        idx1, idx2 = random.sample(range(len(new_tokens)), 2)\n",
    "        new_tokens[idx1], new_tokens[idx2] = new_tokens[idx2], new_tokens[idx1]\n",
    "    \n",
    "    return new_tokens\n",
    "\n",
    "def augment_text(text, augmentation_techniques=['synonym', 'insertion', 'deletion', 'swap'], \n",
    "                 num_augmentations=3):\n",
    "    \"\"\"Apply data augmentation to text\"\"\"\n",
    "    augmented_texts = []\n",
    "    \n",
    "    for _ in range(num_augmentations):\n",
    "        aug_text = text.copy()\n",
    "        technique = random.choice(augmentation_techniques)\n",
    "        \n",
    "        if technique == 'synonym' and len(aug_text) > 0:\n",
    "            aug_text = synonym_replacement(aug_text, n=random.randint(1, 2))\n",
    "        elif technique == 'insertion' and len(aug_text) > 0:\n",
    "            aug_text = random_insertion(aug_text, n=random.randint(1, 2))\n",
    "        elif technique == 'deletion' and len(aug_text) > 1:\n",
    "            aug_text = random_deletion(aug_text, p=0.1)\n",
    "        elif technique == 'swap' and len(aug_text) > 1:\n",
    "            aug_text = random_swap(aug_text, n=1)\n",
    "        \n",
    "        augmented_texts.append(aug_text)\n",
    "    \n",
    "    return augmented_texts\n",
    "\n",
    "print(\"    ✓ Data augmentation functions ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8f8ddcf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Step 3: Applying Data Augmentation for Imbalanced Classes...\n",
      "\n",
      "Original label distribution:\n",
      "  ABBR: 69 samples (1.58%)\n",
      "  DESC: 930 samples (21.32%)\n",
      "  ENTY: 1000 samples (22.93%)\n",
      "  HUM: 978 samples (22.42%)\n",
      "  LOC: 668 samples (15.31%)\n",
      "  NUM: 717 samples (16.44%)\n",
      "\n",
      "  Augmenting LOC: 668 -> 768 samples\n",
      "    Generating 100 additional samples...\n",
      "    ✓ Generated 100 augmented samples\n",
      "\n",
      "  Augmenting ENTY: 1000 -> 1500 samples\n",
      "    Generating 500 additional samples...\n",
      "    ✓ Generated 500 augmented samples\n",
      "\n",
      "  Augmenting HUM: 978 -> 1124 samples\n",
      "    Generating 146 additional samples...\n",
      "    ✓ Generated 146 augmented samples\n",
      "\n",
      "  Augmenting ABBR: 69 -> 668 samples\n",
      "    Generating 599 additional samples...\n",
      "    ✓ Generated 599 augmented samples\n",
      "\n",
      "  Augmenting NUM: 717 -> 824 samples\n",
      "    Generating 107 additional samples...\n",
      "    ✓ Generated 107 augmented samples\n",
      "\n",
      "Augmented label distribution:\n",
      "  ABBR: 668 samples (11.49%)\n",
      "  DESC: 930 samples (16.00%)\n",
      "  ENTY: 1500 samples (25.80%)\n",
      "  HUM: 1124 samples (19.33%)\n",
      "  LOC: 768 samples (13.21%)\n",
      "  NUM: 824 samples (14.17%)\n",
      "\n",
      "  Total samples: 4362 -> 5814\n",
      "  ✓ Data augmentation complete\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# Step 3: Apply Data Augmentation for Imbalanced Classes\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n>>> Step 3: Applying Data Augmentation for Imbalanced Classes...\")\n",
    "\n",
    "# Count current label distribution\n",
    "label_counts_p34 = Counter([ex.label for ex in train_data.examples])\n",
    "print(f\"\\nOriginal label distribution:\")\n",
    "for label, count in sorted(label_counts_p34.items()):\n",
    "    print(f\"  {label}: {count} samples ({count/len(train_data.examples)*100:.2f}%)\")\n",
    "\n",
    "# Augmentation targets (boost weaker topics more aggressively)\n",
    "target_counts_p34 = {\n",
    "    'ABBR': 668,   # heavy boost to improve weakest class\n",
    "    'DESC': 930,   # same\n",
    "    'ENTY': 1500,  # same\n",
    "    'HUM': int(978 * 1.15),   # same\n",
    "    'LOC': int(668 * 1.15),    # same\n",
    "    'NUM': int(717 * 1.15)     # same\n",
    "}\n",
    "\n",
    "# Create augmented examples\n",
    "augmented_examples = list(train_data.examples)  # Start with all original examples\n",
    "\n",
    "for label in label_counts_p34.keys():\n",
    "    current_count = label_counts_p34[label]\n",
    "    target_count = target_counts_p34[label]\n",
    "    \n",
    "    if current_count < target_count:\n",
    "        label_examples = [ex for ex in train_data.examples if ex.label == label]\n",
    "        num_augmentations_needed = target_count - current_count\n",
    "        \n",
    "        print(f\"\\n  Augmenting {label}: {current_count} -> {target_count} samples\")\n",
    "        print(f\"    Generating {num_augmentations_needed} additional samples...\")\n",
    "        \n",
    "        augmented_count = 0\n",
    "        while augmented_count < num_augmentations_needed:\n",
    "            original_ex = random.choice(label_examples)\n",
    "            aug_texts = augment_text(original_ex.text, num_augmentations=1)\n",
    "            \n",
    "            for aug_text in aug_texts:\n",
    "                if augmented_count >= num_augmentations_needed:\n",
    "                    break\n",
    "                \n",
    "                new_ex = data.Example.fromlist([aug_text, label], \n",
    "                                               fields=[('text', TEXT), ('label', LABEL)])\n",
    "                augmented_examples.append(new_ex)\n",
    "                augmented_count += 1\n",
    "        \n",
    "        print(f\"    ✓ Generated {augmented_count} augmented samples\")\n",
    "\n",
    "# Create augmented dataset with proper field structure\n",
    "augmented_train_data = data.Dataset(augmented_examples, fields=[('text', TEXT), ('label', LABEL)])\n",
    "\n",
    "# Verify augmented distribution\n",
    "new_label_counts = Counter([ex.label for ex in augmented_examples])\n",
    "print(f\"\\nAugmented label distribution:\")\n",
    "for label, count in sorted(new_label_counts.items()):\n",
    "    print(f\"  {label}: {count} samples ({count/len(augmented_examples)*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\n  Total samples: {len(train_data.examples)} -> {len(augmented_examples)}\")\n",
    "print(f\"  ✓ Data augmentation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "41e244f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Rebuilding TEXT/LABEL vocab with augmented data...\n",
      "[*] Loading fasttext model...\n",
      "[+] Model loaded!\n",
      "[*] Forming embedding matrix...\n",
      "[+] Embedding matrix formed!\n",
      "  ✓ New vocab size: 8464\n",
      "  ✓ Embedding dim: 300\n",
      "  ✓ FastText embedding refreshed for augmented vocabulary\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Step 3b: Refresh vocabulary and embeddings after augmentation\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n>>> Rebuilding TEXT/LABEL vocab with augmented data...\")\n",
    "\n",
    "# Rebuild vocabularies using augmented samples so new tokens get their own ids\n",
    "TEXT.build_vocab(augmented_train_data, min_freq=1)\n",
    "LABEL.build_vocab(augmented_train_data)\n",
    "\n",
    "# Recreate the FastText-initialised embedding layer for the expanded vocab\n",
    "augmented_embedding = embed_prep(TEXT)\n",
    "\n",
    "print(f\"  ✓ New vocab size: {len(TEXT.vocab)}\")\n",
    "print(f\"  ✓ Embedding dim: {augmented_embedding.weight.shape[1]}\")\n",
    "print(\"  ✓ FastText embedding refreshed for augmented vocabulary\")\n",
    "\n",
    "# Downstream code expects `pretrained_embed` / `pretrained_embeddings`\n",
    "pretrained_embed = augmented_embedding\n",
    "pretrained_embeddings = augmented_embedding.weight.data.clone()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9ec4bc0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# augmented_train_data.examples[0].label\n",
    "[(ex.text, ex.label) for ex in augmented_train_data if ex.label not in ['ABBR','DESC','ENTY','HUM','LOC','NUM']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ce04616c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(None,\n",
       "            {'ENTY': 0, 'HUM': 1, 'DESC': 2, 'NUM': 3, 'LOC': 4, 'ABBR': 5})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABEL.vocab.stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a9afaf5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "abbr_aug_ex = [ex for ex in augmented_train_data if ex.label == \"ABBR\"]\n",
    "abbr_ex = [ex for ex in train_data if ex.label == \"ABBR\"]\n",
    "\n",
    "count = 0\n",
    "# for ex in abbr_aug_ex:\n",
    "#     if ex not in abbr_ex:\n",
    "#         print(ex.text)\n",
    "#         print(ex.label)\n",
    "#         count += 1\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "858eb2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label distribution in training set:\n",
      "- ABBR: 668 samples (11.49%)\n",
      "- DESC: 930 samples (16.00%)\n",
      "- ENTY: 1500 samples (25.80%)\n",
      "- HUM: 1124 samples (19.33%)\n",
      "- LOC: 768 samples (13.21%)\n",
      "- NUM: 824 samples (14.17%)\n"
     ]
    }
   ],
   "source": [
    "# Count how many samples per label in the train set\n",
    "label_counts_p34 = Counter([ex.label for ex in augmented_train_data.examples])\n",
    "total_examples_p34 = len(augmented_train_data)\n",
    "\n",
    "print(\"\\nLabel distribution in training set:\")\n",
    "for label, count in sorted(label_counts_p34.items()):\n",
    "    percentage = (count / total_examples_p34) * 100\n",
    "    print(f\"- {label}: {count} samples ({percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2926e83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PART 3.4: SAMPLING STRATEGIES VS DATA AUGMENTATION\n",
      "================================================================================\n",
      "Comparing text augmentation, weighted sampling, and their combination\n",
      "across the Simple RNN baseline (Part 2 best config) and the RNN + BERT hybrid.\n",
      "================================================================================\n",
      ">>> Simple RNN ready (mean aggregation baseline)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PART 3.5: Sampling Strategies vs Data Augmentation\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 3.4: SAMPLING STRATEGIES VS DATA AUGMENTATION\")\n",
    "print(\"=\"*80)\n",
    "print(\"Comparing text augmentation, weighted sampling, and their combination\")\n",
    "print(\"across the Simple RNN baseline (Part 2 best config) and the RNN + BERT hybrid.\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Extended RNN Classifier with multiple aggregation methods\n",
    "class RNN_Classifier_Aggregation(nn.Module):\n",
    "    \"\"\"\n",
    "    RNN for topic classification with multiple aggregation strategies.\n",
    "    Uses pretrained embeddings (learnable/updated during training).\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, \n",
    "                 n_layers=1, dropout=0.0, padding_idx=0, pretrained_embeddings=None,\n",
    "                 aggregation='mean'):\n",
    "        super(RNN_Classifier_Aggregation, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.aggregation = aggregation  # 'last', 'mean', 'max'\n",
    "        \n",
    "        # Embedding layer initialized with pretrained embeddings\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "            self.embedding.weight.requires_grad = True\n",
    "        \n",
    "        # RNN layer\n",
    "        self.rnn = nn.RNN(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=n_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.0  # No dropout in baseline\n",
    "        )\n",
    "                \n",
    "        # Fully connected output layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, text, text_lengths):        \n",
    "        # Embed the input\n",
    "        embedded = self.embedding(text)  # [batch_size, seq_len, embedding_dim]\n",
    "        \n",
    "        # Get dimensions\n",
    "        batch_size = embedded.size(0)\n",
    "        seq_len = embedded.size(1)\n",
    "        \n",
    "        # Handle text_lengths\n",
    "        text_lengths_flat = text_lengths.flatten().cpu().long()\n",
    "        if len(text_lengths_flat) != batch_size:\n",
    "            raise ValueError(\n",
    "                f\"text_lengths size mismatch: got {len(text_lengths_flat)} elements, \"\n",
    "                f\"expected {batch_size}\"\n",
    "            )\n",
    "        \n",
    "        # Clamp lengths\n",
    "        text_lengths_clamped = torch.clamp(text_lengths_flat, min=1, max=seq_len)\n",
    "        \n",
    "        text_lengths_clamped_device = text_lengths_clamped.to(text.device)\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, text_lengths_clamped, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        \n",
    "        # Pass through RNN\n",
    "        packed_output, hidden = self.rnn(packed_embedded)\n",
    "        \n",
    "        # Aggregate word representations to sentence representation\n",
    "        if self.aggregation == 'last':\n",
    "            sentence_repr = hidden[-1]  # [batch_size, hidden_dim]\n",
    "            \n",
    "        elif self.aggregation == 'mean':\n",
    "            output, output_lengths = nn.utils.rnn.pad_packed_sequence(\n",
    "                packed_output, batch_first=True\n",
    "            )\n",
    "            # output: [batch_size, seq_len, hidden_dim]\n",
    "            \n",
    "            # Create mask for padding\n",
    "            mask = torch.arange(seq_len, device=text.device).unsqueeze(0) < text_lengths_clamped_device.unsqueeze(1)\n",
    "            mask = mask.unsqueeze(2).float()  # [batch_size, seq_len, 1]\n",
    "            \n",
    "            # Apply mask and compute mean\n",
    "            masked_output = output * mask\n",
    "            sum_output = masked_output.sum(dim=1)  # [batch_size, hidden_dim]\n",
    "            sentence_repr = sum_output / text_lengths_clamped_device.unsqueeze(1).float()\n",
    "            \n",
    "        elif self.aggregation == 'max':\n",
    "            # Max pooling over all outputs\n",
    "            output, output_lengths = nn.utils.rnn.pad_packed_sequence(\n",
    "                packed_output, batch_first=True\n",
    "            )\n",
    "            # output: [batch_size, seq_len, hidden_dim]\n",
    "            \n",
    "            mask = torch.arange(seq_len, device=text.device).unsqueeze(0) < text_lengths_clamped_device.unsqueeze(1)\n",
    "            mask = mask.unsqueeze(2).float()  # [batch_size, seq_len, 1]\n",
    "            \n",
    "            masked_output = output * mask + (1 - mask) * float('-inf')\n",
    "            sentence_repr, _ = torch.max(masked_output, dim=1)\n",
    "        \n",
    "        # Pass through fully connected layer\n",
    "        output = self.fc(sentence_repr)  # [batch_size, output_dim]\n",
    "        \n",
    "        return output\n",
    "\n",
    "print(\">>> Simple RNN ready (mean aggregation baseline)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dcda8508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================\n",
    "# Helper: Topic-wise evaluation\n",
    "# =========================================================================\n",
    "\n",
    "def evaluate_per_topic_p35(model, iterator, device, text_vocab=None):\n",
    "    \"\"\"Evaluate accuracy per topic on the provided iterator.\"\"\"\n",
    "    model.eval()\n",
    "    topic_correct = defaultdict(int)\n",
    "    topic_total = defaultdict(int)\n",
    "    idx_to_label = LABEL.vocab.itos\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "            if text_vocab is not None:\n",
    "                predictions = model(text, text_lengths, text_vocab=text_vocab)\n",
    "            else:\n",
    "                predictions = model(text, text_lengths)\n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            for pred, label in zip(preds.cpu().numpy(), labels.cpu().numpy()):\n",
    "                topic_name = idx_to_label[label]\n",
    "                topic_total[topic_name] += 1\n",
    "                if pred == label:\n",
    "                    topic_correct[topic_name] += 1\n",
    "\n",
    "    topic_metrics = {}\n",
    "    for topic in sorted(topic_total.keys()):\n",
    "        total = topic_total[topic]\n",
    "        correct = topic_correct[topic]\n",
    "        accuracy = correct / total if total > 0 else 0.0\n",
    "        topic_metrics[topic] = {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"correct\": correct,\n",
    "            \"total\": total,\n",
    "        }\n",
    "    return topic_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0220a7b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Preparing dataset variants for Part 3.4 experiments...\n",
      "  - Original train: 4362 samples\n",
      "      ABBR: 69 (1.58%)\n",
      "      DESC: 930 (21.32%)\n",
      "      ENTY: 1000 (22.93%)\n",
      "      HUM: 978 (22.42%)\n",
      "      LOC: 668 (15.31%)\n",
      "      NUM: 717 (16.44%)\n",
      "  - Augmented train: 5814 samples\n",
      "      ABBR: 668 (11.49%)\n",
      "      DESC: 930 (16.00%)\n",
      "      ENTY: 1500 (25.80%)\n",
      "      HUM: 1124 (19.33%)\n",
      "      LOC: 768 (13.21%)\n",
      "      NUM: 824 (14.17%)\n",
      "  - Weighted-sampled train: 4362 samples\n",
      "      ABBR: 843 (19.33%)\n",
      "      DESC: 547 (12.54%)\n",
      "      ENTY: 884 (20.27%)\n",
      "      HUM: 725 (16.62%)\n",
      "      LOC: 667 (15.29%)\n",
      "      NUM: 696 (15.96%)\n",
      "  - Augmented + weighted train: 5814 samples\n",
      "      ABBR: 1041 (17.91%)\n",
      "      DESC: 769 (13.23%)\n",
      "      ENTY: 1156 (19.88%)\n",
      "      HUM: 934 (16.06%)\n",
      "      LOC: 901 (15.50%)\n",
      "      NUM: 1013 (17.42%)\n",
      "\n",
      ">>> Dataset variants ready. Criterion initialised for upcoming runs.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Dataset Variants & Utilities for Experiments\n",
    "# ============================================================================\n",
    "\n",
    "def describe_dataset(name, dataset):\n",
    "    counts = Counter(ex.label for ex in dataset.examples)\n",
    "    total = len(dataset.examples)\n",
    "    print(f\"  - {name}: {total} samples\")\n",
    "    for label, count in sorted(counts.items()):\n",
    "        print(f\"      {label}: {count} ({count/total*100:.2f}%)\")\n",
    "    return counts\n",
    "\n",
    "\n",
    "# Topic-wise accuracy from latest weighted-sampler evaluation (used to boost weak classes)\n",
    "P35_TOPIC_ACCURACY = {\n",
    "    \"ABBR\": 0.7778,\n",
    "    \"DESC\": 0.9855,\n",
    "    \"ENTY\": 0.7128,\n",
    "    \"HUM\": 0.8769,\n",
    "    \"LOC\": 0.8889,\n",
    "    \"NUM\": 0.8584,\n",
    "}\n",
    "# Convert to difficulty scores (higher when accuracy is lower)\n",
    "P35_TOPIC_DIFFICULTY = {label: max(0.0, 1.0 - acc) for label, acc in P35_TOPIC_ACCURACY.items()}\n",
    "# Global multiplier for difficulty adjustment; tweak to emphasise weak topics more/less\n",
    "P35_DIFFICULTY_SCALE = 2.0\n",
    "\n",
    "\n",
    "def create_weighted_dataset(source_dataset, target_size=None, seed=SEED, difficulty_scale=P35_DIFFICULTY_SCALE):\n",
    "    \"\"\"Mimic WeightedRandomSampler by sampling examples according to class weights, with extra boosts for weak topics.\"\"\"\n",
    "    rng = random.Random(seed)\n",
    "    counts = Counter(ex.label for ex in source_dataset.examples)\n",
    "    total = sum(counts.values())\n",
    "    base_class_weights = {label: total / count for label, count in counts.items()}\n",
    "\n",
    "    class_boosts = {\n",
    "        label: 1.0 + difficulty_scale * P35_TOPIC_DIFFICULTY.get(label, 0.0)\n",
    "        for label in counts.keys()\n",
    "    }\n",
    "\n",
    "    weights = [base_class_weights[ex.label] * class_boosts.get(ex.label, 1.0) for ex in source_dataset.examples]\n",
    "    sample_size = target_size or len(source_dataset.examples)\n",
    "\n",
    "    sampled_examples = rng.choices(source_dataset.examples, weights=weights, k=sample_size)\n",
    "    fields = [('text', TEXT), ('label', LABEL)]\n",
    "    return data.Dataset(sampled_examples, fields=fields)\n",
    "\n",
    "\n",
    "print(\"\\n>>> Preparing dataset variants for Part 3.4 experiments...\")\n",
    "base_counts = describe_dataset(\"Original train\", train_data)\n",
    "aug_counts = describe_dataset(\"Augmented train\", augmented_train_data)\n",
    "\n",
    "weighted_train_data = create_weighted_dataset(train_data)\n",
    "weighted_counts = describe_dataset(\"Weighted-sampled train\", weighted_train_data)\n",
    "\n",
    "augmented_weighted_train_data = create_weighted_dataset(augmented_train_data)\n",
    "aug_weighted_counts = describe_dataset(\"Augmented + weighted train\", augmented_weighted_train_data)\n",
    "\n",
    "p35_datasets = {\n",
    "    \"original\": train_data,\n",
    "    \"augmented\": augmented_train_data,\n",
    "    \"weighted\": weighted_train_data,\n",
    "    \"augmented_weighted\": augmented_weighted_train_data,\n",
    "}\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "p35_results = {\n",
    "    \"simple_rnn_baseline\": {},\n",
    "    \"rnn_bert\": {}\n",
    "}\n",
    "\n",
    "print(\"\\n>>> Dataset variants ready. Criterion initialised for upcoming runs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bda6694d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Simple RNN (mean pooling) experiment runner\n",
    "# ============================================================================\n",
    "\n",
    "def reset_random_seeds(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def build_iterator(dataset, batch_size, shuffle):\n",
    "    return data.BucketIterator(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        sort_key=lambda x: len(x.text),\n",
    "        sort_within_batch=True,\n",
    "        shuffle=shuffle,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "RNN_BASE_HYPERPARM = hyperparam_prep()\n",
    "RNN_BASE_HYPERPARM['HIDDEN_DIM'] *= 2\n",
    "RNN_BASE_HYPERPARM['N_LAYERS'] = 1\n",
    "RNN_BASE_HYPERPARM['SAVE_MODEL'] = True\n",
    "\n",
    "\n",
    "def run_simple_rnn_experiment(dataset_key, description, save_suffix):\n",
    "    if dataset_key not in p35_datasets:\n",
    "        raise ValueError(f\"Unknown dataset key: {dataset_key}\")\n",
    "\n",
    "    reset_random_seeds(SEED)\n",
    "    train_dataset = p35_datasets[dataset_key]\n",
    "\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(f\"Running Simple RNN (mean pooling) experiment: {description}\")\n",
    "    print(\"-\"*80)\n",
    "\n",
    "    train_iter = build_iterator(train_dataset, RNN_BASE_HYPERPARM['BATCH_SIZE'], shuffle=True)\n",
    "    val_iter = build_iterator(validation_data, RNN_BASE_HYPERPARM['BATCH_SIZE'], shuffle=False)\n",
    "    test_iter = build_iterator(test_data, RNN_BASE_HYPERPARM['BATCH_SIZE'], shuffle=False)\n",
    "\n",
    "    model = RNN_Classifier_Aggregation(\n",
    "        vocab_size=len(TEXT.vocab),\n",
    "        embedding_dim=embedding_dim,\n",
    "        hidden_dim=RNN_BASE_HYPERPARM['HIDDEN_DIM'],\n",
    "        output_dim=num_classes,\n",
    "        n_layers=RNN_BASE_HYPERPARM['N_LAYERS'],\n",
    "        dropout=RNN_BASE_HYPERPARM['DROPOUT'],\n",
    "        padding_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "        pretrained_embeddings=pretrained_embeddings,\n",
    "        aggregation=RNN_BASE_HYPERPARM['AGGREGATOR'],\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Select optimizer with best learning rate\n",
    "    if RNN_BASE_HYPERPARM['OPTIMIZER'] == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=RNN_BASE_HYPERPARM['LEARNING_RATE'],\n",
    "                                        weight_decay=RNN_BASE_HYPERPARM['L2_LAMBDA'])\n",
    "    elif RNN_BASE_HYPERPARM['OPTIMIZER'] == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=RNN_BASE_HYPERPARM['LEARNING_RATE'], momentum=0.9,\n",
    "                                        weight_decay=RNN_BASE_HYPERPARM['L2_LAMBDA'])\n",
    "    elif RNN_BASE_HYPERPARM['OPTIMIZER'] == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=RNN_BASE_HYPERPARM['LEARNING_RATE'],\n",
    "                                        weight_decay=RNN_BASE_HYPERPARM['L2_LAMBDA'])\n",
    "    elif RNN_BASE_HYPERPARM['OPTIMIZER'] == 'Adagrad':\n",
    "        optimizer = optim.Adagrad(model.parameters(), lr=RNN_BASE_HYPERPARM['LEARNING_RATE'],\n",
    "                                        weight_decay=RNN_BASE_HYPERPARM['L2_LAMBDA'])\n",
    "\n",
    "    model, history = train_model_with_history(\n",
    "        model,\n",
    "        train_iter,\n",
    "        val_iter,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        RNN_BASE_HYPERPARM['N_EPOCHS'],\n",
    "        device,\n",
    "        num_classes,\n",
    "        RNN_BASE_HYPERPARM['L1_LAMBDA'],\n",
    "        patience=RNN_BASE_HYPERPARM['PATIENCE'],\n",
    "        model_name=f\"Simple RNN ({description})\",\n",
    "    )\n",
    "\n",
    "    test_loss, test_acc, test_f1, test_auc = evaluate_model(\n",
    "        model,\n",
    "        test_iter,\n",
    "        criterion,\n",
    "        device,\n",
    "        f\"Simple RNN ({description})\",\n",
    "        num_classes,\n",
    "    )\n",
    "\n",
    "    topic_metrics = evaluate_per_topic_p35(model, test_iter, device)\n",
    "\n",
    "    model_path = f\"weights/part35_simple_rnn_{save_suffix}.pt\"\n",
    "    if RNN_BASE_HYPERPARM['SAVE_MODEL']:\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    p35_results[\"simple_rnn_baseline\"][save_suffix] = {\n",
    "        \"description\": description,\n",
    "        \"dataset_key\": dataset_key,\n",
    "        \"history\": history,\n",
    "        \"test_metrics\": {\n",
    "            \"loss\": test_loss,\n",
    "            \"accuracy\": test_acc,\n",
    "            \"f1\": test_f1,\n",
    "            \"auc\": test_auc,\n",
    "        },\n",
    "        \"topic_metrics\": topic_metrics,\n",
    "        \"model_path\": model_path if RNN_BASE_HYPERPARM['SAVE_MODEL'] else None,\n",
    "        \"model\": model,\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"history\": history,\n",
    "        \"topic_metrics\": topic_metrics,\n",
    "        \"test_metrics\": {\n",
    "            \"loss\": test_loss,\n",
    "            \"accuracy\": test_acc,\n",
    "            \"f1\": test_f1,\n",
    "            \"auc\": test_auc,\n",
    "        },\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8bff8e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Executing Simple RNN experiments...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Running Simple RNN (mean pooling) experiment: Text Augmentation\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Training Simple RNN (Text Augmentation)\n",
      "    Parameters: 2,959,046\n",
      "    Max epochs: 100, Patience: 10\n",
      "Epoch: 01/100 | Time: 0m 3s\n",
      "\tTrain Loss: 1.7102 | Train Acc: 31.03%\n",
      "\tVal Loss: 1.3900 | Val Acc: 45.14% | Val F1: 0.3828 | Val AUC: 0.7963\n",
      "Epoch: 02/100 | Time: 0m 2s\n",
      "\tTrain Loss: 1.1816 | Train Acc: 59.67%\n",
      "\tVal Loss: 0.9560 | Val Acc: 65.78% | Val F1: 0.6629 | Val AUC: 0.8946\n",
      "Epoch: 03/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.8139 | Train Acc: 77.64%\n",
      "\tVal Loss: 0.7732 | Val Acc: 73.85% | Val F1: 0.7456 | Val AUC: 0.9264\n",
      "Epoch: 04/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.6030 | Train Acc: 85.55%\n",
      "\tVal Loss: 0.6560 | Val Acc: 79.08% | Val F1: 0.7977 | Val AUC: 0.9460\n",
      "Epoch: 05/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.4788 | Train Acc: 90.08%\n",
      "\tVal Loss: 0.5941 | Val Acc: 82.20% | Val F1: 0.8238 | Val AUC: 0.9546\n",
      "Epoch: 06/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.4086 | Train Acc: 91.25%\n",
      "\tVal Loss: 0.5726 | Val Acc: 83.03% | Val F1: 0.8331 | Val AUC: 0.9591\n",
      "Epoch: 07/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.3323 | Train Acc: 93.95%\n",
      "\tVal Loss: 0.5658 | Val Acc: 83.94% | Val F1: 0.8395 | Val AUC: 0.9606\n",
      "Epoch: 08/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.2788 | Train Acc: 95.18%\n",
      "\tVal Loss: 0.5492 | Val Acc: 84.86% | Val F1: 0.8488 | Val AUC: 0.9637\n",
      "Epoch: 09/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.3241 | Train Acc: 92.81%\n",
      "\tVal Loss: 0.5584 | Val Acc: 84.59% | Val F1: 0.8463 | Val AUC: 0.9606\n",
      "Epoch: 10/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.2211 | Train Acc: 96.28%\n",
      "\tVal Loss: 0.5390 | Val Acc: 86.61% | Val F1: 0.8666 | Val AUC: 0.9646\n",
      "Epoch: 11/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.1932 | Train Acc: 97.04%\n",
      "\tVal Loss: 0.5723 | Val Acc: 85.69% | Val F1: 0.8566 | Val AUC: 0.9617\n",
      "Epoch: 12/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.1829 | Train Acc: 96.75%\n",
      "\tVal Loss: 0.5855 | Val Acc: 85.60% | Val F1: 0.8563 | Val AUC: 0.9615\n",
      "Epoch: 13/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.1536 | Train Acc: 97.75%\n",
      "\tVal Loss: 0.5812 | Val Acc: 85.96% | Val F1: 0.8596 | Val AUC: 0.9615\n",
      "Epoch: 14/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.1427 | Train Acc: 98.04%\n",
      "\tVal Loss: 0.6112 | Val Acc: 85.23% | Val F1: 0.8521 | Val AUC: 0.9630\n",
      "Epoch: 15/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.1239 | Train Acc: 98.28%\n",
      "\tVal Loss: 0.6361 | Val Acc: 85.69% | Val F1: 0.8577 | Val AUC: 0.9645\n",
      "Epoch: 16/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.1030 | Train Acc: 98.80%\n",
      "\tVal Loss: 0.6921 | Val Acc: 84.40% | Val F1: 0.8444 | Val AUC: 0.9600\n",
      "Epoch: 17/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.1629 | Train Acc: 97.56%\n",
      "\tVal Loss: 0.6080 | Val Acc: 85.41% | Val F1: 0.8538 | Val AUC: 0.9634\n",
      "Epoch: 18/100 | Time: 0m 5s\n",
      "\tTrain Loss: 0.0846 | Train Acc: 99.19%\n",
      "\tVal Loss: 0.6986 | Val Acc: 86.61% | Val F1: 0.8668 | Val AUC: 0.9606\n",
      "Epoch: 19/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.1568 | Train Acc: 97.39%\n",
      "\tVal Loss: 0.9031 | Val Acc: 84.86% | Val F1: 0.8501 | Val AUC: 0.9542\n",
      "Epoch: 20/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.0921 | Train Acc: 98.90%\n",
      "\tVal Loss: 0.6548 | Val Acc: 86.42% | Val F1: 0.8644 | Val AUC: 0.9613\n",
      "\t>>> Early stopping at epoch 20, best val acc: 86.61%\n",
      "\n",
      ">>> Training completed! Best validation accuracy: 86.61%\n",
      "    Best validation F1: 0.8666\n",
      "    Best validation AUC-ROC: 0.9646\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Running Simple RNN (mean pooling) experiment: Weighted Sampling\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      ">>> Training Simple RNN (Weighted Sampling)\n",
      "    Parameters: 2,959,046\n",
      "    Max epochs: 100, Patience: 10\n",
      "Epoch: 01/100 | Time: 0m 2s\n",
      "\tTrain Loss: 1.7592 | Train Acc: 29.28%\n",
      "\tVal Loss: 1.6622 | Val Acc: 28.99% | Val F1: 0.2464 | Val AUC: 0.7363\n",
      "Epoch: 02/100 | Time: 0m 3s\n",
      "\tTrain Loss: 1.2243 | Train Acc: 57.06%\n",
      "\tVal Loss: 1.1373 | Val Acc: 50.55% | Val F1: 0.4721 | Val AUC: 0.8556\n",
      "Epoch: 03/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.8729 | Train Acc: 73.11%\n",
      "\tVal Loss: 0.9690 | Val Acc: 65.96% | Val F1: 0.6721 | Val AUC: 0.8952\n",
      "Epoch: 04/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.6568 | Train Acc: 81.71%\n",
      "\tVal Loss: 0.7930 | Val Acc: 70.09% | Val F1: 0.7136 | Val AUC: 0.9224\n",
      "Epoch: 05/100 | Time: 0m 2s\n",
      "\tTrain Loss: 0.5237 | Train Acc: 86.13%\n",
      "\tVal Loss: 0.8292 | Val Acc: 69.17% | Val F1: 0.7009 | Val AUC: 0.9225\n",
      "Epoch: 06/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.4372 | Train Acc: 90.19%\n",
      "\tVal Loss: 1.0565 | Val Acc: 66.33% | Val F1: 0.6778 | Val AUC: 0.9113\n",
      "Epoch: 07/100 | Time: 0m 4s\n",
      "\tTrain Loss: 0.3821 | Train Acc: 92.02%\n",
      "\tVal Loss: 0.7736 | Val Acc: 75.23% | Val F1: 0.7652 | Val AUC: 0.9347\n",
      "Epoch: 08/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.4109 | Train Acc: 89.89%\n",
      "\tVal Loss: 0.7304 | Val Acc: 76.24% | Val F1: 0.7688 | Val AUC: 0.9393\n",
      "Epoch: 09/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.2858 | Train Acc: 95.05%\n",
      "\tVal Loss: 0.6765 | Val Acc: 78.44% | Val F1: 0.7950 | Val AUC: 0.9453\n",
      "Epoch: 10/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.2437 | Train Acc: 96.35%\n",
      "\tVal Loss: 0.7144 | Val Acc: 78.07% | Val F1: 0.7912 | Val AUC: 0.9432\n",
      "Epoch: 11/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.2040 | Train Acc: 97.23%\n",
      "\tVal Loss: 0.6807 | Val Acc: 79.36% | Val F1: 0.8009 | Val AUC: 0.9471\n",
      "Epoch: 12/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.1919 | Train Acc: 97.18%\n",
      "\tVal Loss: 0.7817 | Val Acc: 77.89% | Val F1: 0.7896 | Val AUC: 0.9391\n",
      "Epoch: 13/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.2381 | Train Acc: 95.53%\n",
      "\tVal Loss: 0.7938 | Val Acc: 77.16% | Val F1: 0.7744 | Val AUC: 0.9358\n",
      "Epoch: 14/100 | Time: 0m 4s\n",
      "\tTrain Loss: 0.1840 | Train Acc: 97.25%\n",
      "\tVal Loss: 0.7062 | Val Acc: 79.82% | Val F1: 0.8064 | Val AUC: 0.9489\n",
      "Epoch: 15/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.2010 | Train Acc: 97.11%\n",
      "\tVal Loss: 0.8744 | Val Acc: 75.14% | Val F1: 0.7640 | Val AUC: 0.9341\n",
      "Epoch: 16/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.1638 | Train Acc: 97.62%\n",
      "\tVal Loss: 0.7751 | Val Acc: 79.45% | Val F1: 0.8016 | Val AUC: 0.9413\n",
      "Epoch: 17/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.1596 | Train Acc: 97.46%\n",
      "\tVal Loss: 0.7366 | Val Acc: 80.37% | Val F1: 0.8065 | Val AUC: 0.9457\n",
      "Epoch: 18/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.1139 | Train Acc: 98.74%\n",
      "\tVal Loss: 0.7264 | Val Acc: 81.38% | Val F1: 0.8160 | Val AUC: 0.9463\n",
      "Epoch: 19/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.1023 | Train Acc: 99.11%\n",
      "\tVal Loss: 0.7251 | Val Acc: 81.74% | Val F1: 0.8202 | Val AUC: 0.9473\n",
      "Epoch: 20/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.0998 | Train Acc: 98.78%\n",
      "\tVal Loss: 0.7337 | Val Acc: 81.74% | Val F1: 0.8181 | Val AUC: 0.9476\n",
      "Epoch: 21/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.0951 | Train Acc: 98.99%\n",
      "\tVal Loss: 0.7495 | Val Acc: 81.56% | Val F1: 0.8145 | Val AUC: 0.9467\n",
      "Epoch: 22/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.0999 | Train Acc: 98.78%\n",
      "\tVal Loss: 0.8275 | Val Acc: 81.10% | Val F1: 0.8143 | Val AUC: 0.9409\n",
      "Epoch: 23/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.0779 | Train Acc: 99.27%\n",
      "\tVal Loss: 0.7688 | Val Acc: 80.73% | Val F1: 0.8075 | Val AUC: 0.9458\n",
      "Epoch: 24/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.0699 | Train Acc: 99.47%\n",
      "\tVal Loss: 0.7972 | Val Acc: 81.74% | Val F1: 0.8179 | Val AUC: 0.9457\n",
      "Epoch: 25/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.0739 | Train Acc: 99.36%\n",
      "\tVal Loss: 0.9111 | Val Acc: 79.45% | Val F1: 0.7959 | Val AUC: 0.9416\n",
      "Epoch: 26/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.0813 | Train Acc: 98.69%\n",
      "\tVal Loss: 0.8624 | Val Acc: 80.92% | Val F1: 0.8116 | Val AUC: 0.9439\n",
      "Epoch: 27/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.1915 | Train Acc: 96.45%\n",
      "\tVal Loss: 1.1383 | Val Acc: 76.97% | Val F1: 0.7810 | Val AUC: 0.9310\n",
      "Epoch: 28/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.1681 | Train Acc: 96.84%\n",
      "\tVal Loss: 0.7660 | Val Acc: 82.29% | Val F1: 0.8255 | Val AUC: 0.9504\n",
      "Epoch: 29/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.0791 | Train Acc: 98.97%\n",
      "\tVal Loss: 1.0113 | Val Acc: 78.81% | Val F1: 0.7948 | Val AUC: 0.9357\n",
      "Epoch: 30/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.0741 | Train Acc: 98.97%\n",
      "\tVal Loss: 0.8065 | Val Acc: 80.92% | Val F1: 0.8127 | Val AUC: 0.9464\n",
      "Epoch: 31/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.0540 | Train Acc: 99.79%\n",
      "\tVal Loss: 0.8177 | Val Acc: 81.10% | Val F1: 0.8125 | Val AUC: 0.9472\n",
      "Epoch: 32/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.0496 | Train Acc: 99.79%\n",
      "\tVal Loss: 0.8186 | Val Acc: 81.38% | Val F1: 0.8145 | Val AUC: 0.9474\n",
      "Epoch: 33/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.0476 | Train Acc: 99.79%\n",
      "\tVal Loss: 0.9033 | Val Acc: 81.01% | Val F1: 0.8134 | Val AUC: 0.9440\n",
      "Epoch: 34/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.0505 | Train Acc: 99.75%\n",
      "\tVal Loss: 0.8221 | Val Acc: 81.01% | Val F1: 0.8125 | Val AUC: 0.9484\n",
      "Epoch: 35/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.0432 | Train Acc: 99.91%\n",
      "\tVal Loss: 0.8645 | Val Acc: 80.37% | Val F1: 0.8071 | Val AUC: 0.9466\n",
      "Epoch: 36/100 | Time: 0m 4s\n",
      "\tTrain Loss: 0.0429 | Train Acc: 99.84%\n",
      "\tVal Loss: 0.8550 | Val Acc: 81.83% | Val F1: 0.8199 | Val AUC: 0.9488\n",
      "Epoch: 37/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.0405 | Train Acc: 99.91%\n",
      "\tVal Loss: 0.8592 | Val Acc: 82.02% | Val F1: 0.8214 | Val AUC: 0.9487\n",
      "Epoch: 38/100 | Time: 0m 3s\n",
      "\tTrain Loss: 0.0389 | Train Acc: 99.95%\n",
      "\tVal Loss: 0.9215 | Val Acc: 81.01% | Val F1: 0.8119 | Val AUC: 0.9464\n",
      "\t>>> Early stopping at epoch 38, best val acc: 82.29%\n",
      "\n",
      ">>> Training completed! Best validation accuracy: 82.29%\n",
      "    Best validation F1: 0.8255\n",
      "    Best validation AUC-ROC: 0.9504\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Running Simple RNN (mean pooling) experiment: Augmentation + Weighted Sampling\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      ">>> Training Simple RNN (Augmentation + Weighted Sampling)\n",
      "    Parameters: 2,959,046\n",
      "    Max epochs: 100, Patience: 10\n",
      "Epoch: 01/100 | Time: 0m 4s\n",
      "\tTrain Loss: 1.7234 | Train Acc: 32.06%\n",
      "\tVal Loss: 1.3920 | Val Acc: 44.95% | Val F1: 0.4135 | Val AUC: 0.7900\n",
      "Epoch: 02/100 | Time: 0m 4s\n",
      "\tTrain Loss: 1.1013 | Train Acc: 63.55%\n",
      "\tVal Loss: 0.9736 | Val Acc: 63.03% | Val F1: 0.6360 | Val AUC: 0.8977\n",
      "Epoch: 03/100 | Time: 0m 4s\n",
      "\tTrain Loss: 0.7295 | Train Acc: 78.88%\n",
      "\tVal Loss: 0.7923 | Val Acc: 68.90% | Val F1: 0.7001 | Val AUC: 0.9236\n",
      "Epoch: 04/100 | Time: 0m 5s\n",
      "\tTrain Loss: 0.5214 | Train Acc: 86.64%\n",
      "\tVal Loss: 0.7301 | Val Acc: 74.77% | Val F1: 0.7612 | Val AUC: 0.9354\n",
      "Epoch: 05/100 | Time: 0m 5s\n",
      "\tTrain Loss: 0.4265 | Train Acc: 90.13%\n",
      "\tVal Loss: 0.7213 | Val Acc: 77.06% | Val F1: 0.7803 | Val AUC: 0.9404\n",
      "Epoch: 06/100 | Time: 0m 5s\n",
      "\tTrain Loss: 0.3360 | Train Acc: 93.70%\n",
      "\tVal Loss: 0.6558 | Val Acc: 78.53% | Val F1: 0.7968 | Val AUC: 0.9452\n",
      "Epoch: 07/100 | Time: 0m 4s\n",
      "\tTrain Loss: 0.3077 | Train Acc: 93.79%\n",
      "\tVal Loss: 0.6749 | Val Acc: 77.61% | Val F1: 0.7860 | Val AUC: 0.9488\n",
      "Epoch: 08/100 | Time: 0m 4s\n",
      "\tTrain Loss: 0.2465 | Train Acc: 95.99%\n",
      "\tVal Loss: 0.6649 | Val Acc: 80.46% | Val F1: 0.8125 | Val AUC: 0.9468\n",
      "Epoch: 09/100 | Time: 0m 4s\n",
      "\tTrain Loss: 0.2156 | Train Acc: 96.58%\n",
      "\tVal Loss: 0.7531 | Val Acc: 79.36% | Val F1: 0.8043 | Val AUC: 0.9400\n",
      "Epoch: 10/100 | Time: 0m 4s\n",
      "\tTrain Loss: 0.2179 | Train Acc: 96.18%\n",
      "\tVal Loss: 0.6884 | Val Acc: 80.46% | Val F1: 0.8066 | Val AUC: 0.9466\n",
      "Epoch: 11/100 | Time: 0m 4s\n",
      "\tTrain Loss: 0.1793 | Train Acc: 97.06%\n",
      "\tVal Loss: 0.6535 | Val Acc: 83.03% | Val F1: 0.8322 | Val AUC: 0.9531\n",
      "Epoch: 12/100 | Time: 0m 4s\n",
      "\tTrain Loss: 0.1309 | Train Acc: 98.45%\n",
      "\tVal Loss: 0.6632 | Val Acc: 82.84% | Val F1: 0.8298 | Val AUC: 0.9529\n",
      "Epoch: 13/100 | Time: 0m 4s\n",
      "\tTrain Loss: 0.1167 | Train Acc: 98.59%\n",
      "\tVal Loss: 0.6500 | Val Acc: 83.85% | Val F1: 0.8385 | Val AUC: 0.9550\n",
      "Epoch: 14/100 | Time: 0m 5s\n",
      "\tTrain Loss: 0.1036 | Train Acc: 98.76%\n",
      "\tVal Loss: 0.6712 | Val Acc: 83.49% | Val F1: 0.8358 | Val AUC: 0.9546\n",
      "Epoch: 15/100 | Time: 0m 4s\n",
      "\tTrain Loss: 0.0987 | Train Acc: 98.83%\n",
      "\tVal Loss: 0.7244 | Val Acc: 83.49% | Val F1: 0.8376 | Val AUC: 0.9525\n",
      "Epoch: 16/100 | Time: 0m 4s\n",
      "\tTrain Loss: 0.1189 | Train Acc: 98.21%\n",
      "\tVal Loss: 0.8232 | Val Acc: 81.28% | Val F1: 0.8126 | Val AUC: 0.9434\n",
      "Epoch: 17/100 | Time: 0m 4s\n",
      "\tTrain Loss: 0.1168 | Train Acc: 98.13%\n",
      "\tVal Loss: 0.7606 | Val Acc: 83.49% | Val F1: 0.8376 | Val AUC: 0.9505\n",
      "Epoch: 18/100 | Time: 0m 4s\n",
      "\tTrain Loss: 0.0858 | Train Acc: 98.95%\n",
      "\tVal Loss: 0.8018 | Val Acc: 82.29% | Val F1: 0.8262 | Val AUC: 0.9482\n",
      "Epoch: 19/100 | Time: 0m 4s\n",
      "\tTrain Loss: 0.0724 | Train Acc: 99.36%\n",
      "\tVal Loss: 0.7445 | Val Acc: 83.21% | Val F1: 0.8314 | Val AUC: 0.9529\n",
      "Epoch: 20/100 | Time: 0m 4s\n",
      "\tTrain Loss: 0.0632 | Train Acc: 99.62%\n",
      "\tVal Loss: 0.7690 | Val Acc: 83.94% | Val F1: 0.8412 | Val AUC: 0.9530\n",
      "Epoch: 21/100 | Time: 0m 4s\n",
      "\tTrain Loss: 0.0609 | Train Acc: 99.57%\n",
      "\tVal Loss: 0.7900 | Val Acc: 84.77% | Val F1: 0.8499 | Val AUC: 0.9517\n",
      "Epoch: 22/100 | Time: 0m 4s\n",
      "\tTrain Loss: 0.0586 | Train Acc: 99.55%\n",
      "\tVal Loss: 0.8313 | Val Acc: 83.39% | Val F1: 0.8348 | Val AUC: 0.9514\n",
      "Epoch: 23/100 | Time: 0m 4s\n",
      "\tTrain Loss: 0.0755 | Train Acc: 98.80%\n",
      "\tVal Loss: 0.8780 | Val Acc: 81.56% | Val F1: 0.8152 | Val AUC: 0.9477\n",
      "Epoch: 24/100 | Time: 0m 4s\n",
      "\tTrain Loss: 0.0701 | Train Acc: 99.14%\n",
      "\tVal Loss: 0.8558 | Val Acc: 82.84% | Val F1: 0.8336 | Val AUC: 0.9501\n",
      "Epoch: 25/100 | Time: 0m 4s\n",
      "\tTrain Loss: 0.1203 | Train Acc: 97.80%\n",
      "\tVal Loss: 0.8195 | Val Acc: 84.13% | Val F1: 0.8448 | Val AUC: 0.9499\n",
      "Epoch: 26/100 | Time: 0m 4s\n",
      "\tTrain Loss: 0.0595 | Train Acc: 99.40%\n",
      "\tVal Loss: 0.8492 | Val Acc: 84.22% | Val F1: 0.8465 | Val AUC: 0.9505\n",
      "Epoch: 27/100 | Time: 0m 6s\n",
      "\tTrain Loss: 0.0491 | Train Acc: 99.71%\n",
      "\tVal Loss: 0.8942 | Val Acc: 83.94% | Val F1: 0.8429 | Val AUC: 0.9497\n",
      "Epoch: 28/100 | Time: 0m 5s\n",
      "\tTrain Loss: 0.1975 | Train Acc: 95.65%\n",
      "\tVal Loss: 0.8173 | Val Acc: 80.55% | Val F1: 0.8073 | Val AUC: 0.9443\n",
      "Epoch: 29/100 | Time: 0m 4s\n",
      "\tTrain Loss: 0.0857 | Train Acc: 98.54%\n",
      "\tVal Loss: 0.7391 | Val Acc: 84.31% | Val F1: 0.8461 | Val AUC: 0.9535\n",
      "Epoch: 30/100 | Time: 0m 4s\n",
      "\tTrain Loss: 0.0508 | Train Acc: 99.69%\n",
      "\tVal Loss: 0.7506 | Val Acc: 84.22% | Val F1: 0.8451 | Val AUC: 0.9550\n",
      "Epoch: 31/100 | Time: 0m 4s\n",
      "\tTrain Loss: 0.0451 | Train Acc: 99.83%\n",
      "\tVal Loss: 0.7989 | Val Acc: 84.68% | Val F1: 0.8499 | Val AUC: 0.9537\n",
      "\t>>> Early stopping at epoch 31, best val acc: 84.77%\n",
      "\n",
      ">>> Training completed! Best validation accuracy: 84.77%\n",
      "    Best validation F1: 0.8499\n",
      "    Best validation AUC-ROC: 0.9517\n",
      "\n",
      ">>> Simple RNN experiments queued. Run the cells to execute training if needed.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Run Simple RNN + Attention experiments\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n>>> Executing Simple RNN experiments...\")\n",
    "\n",
    "rnn_results_text_aug = run_simple_rnn_experiment(\n",
    "    dataset_key=\"augmented\",\n",
    "    description=\"Text Augmentation\",\n",
    "    save_suffix=\"text_aug\",\n",
    ")\n",
    "\n",
    "rnn_results_weighted = run_simple_rnn_experiment(\n",
    "    dataset_key=\"weighted\",\n",
    "    description=\"Weighted Sampling\",\n",
    "    save_suffix=\"weighted_sampler\",\n",
    ")\n",
    "\n",
    "rnn_results_aug_weighted = run_simple_rnn_experiment(\n",
    "    dataset_key=\"augmented_weighted\",\n",
    "    description=\"Augmentation + Weighted Sampling\",\n",
    "    save_suffix=\"text_aug_weighted\",\n",
    ")\n",
    "\n",
    "print(\"\\n>>> Simple RNN experiments queued. Run the cells to execute training if needed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "426bd88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Topic-wise accuracy for Simple RNN variants\n",
      "\n",
      "Simple RNN (Text Augmentation)\n",
      "------------------------------\n",
      "Topic      Accuracy %   Correct    Total     \n",
      "---------------------------------------------\n",
      "ABBR       77.78        7          9         \n",
      "DESC       97.83        135        138       \n",
      "ENTY       71.28        67         94        \n",
      "HUM        89.23        58         65        \n",
      "LOC        81.48        66         81        \n",
      "NUM        84.07        95         113       \n",
      "---------------------------------------------\n",
      "Topic      0.86         428        500       \n",
      "\n",
      "Simple RNN (Weighted Sampling)\n",
      "------------------------------\n",
      "Topic      Accuracy %   Correct    Total     \n",
      "---------------------------------------------\n",
      "ABBR       77.78        7          9         \n",
      "DESC       97.83        135        138       \n",
      "ENTY       64.89        61         94        \n",
      "HUM        86.15        56         65        \n",
      "LOC        82.72        67         81        \n",
      "NUM        84.07        95         113       \n",
      "---------------------------------------------\n",
      "Topic      0.84         421        500       \n",
      "\n",
      "Simple RNN (Augmentation + Weighted Sampling)\n",
      "---------------------------------------------\n",
      "Topic      Accuracy %   Correct    Total     \n",
      "---------------------------------------------\n",
      "ABBR       77.78        7          9         \n",
      "DESC       96.38        133        138       \n",
      "ENTY       82.98        78         94        \n",
      "HUM        84.62        55         65        \n",
      "LOC        85.19        69         81        \n",
      "NUM        84.07        95         113       \n",
      "---------------------------------------------\n",
      "Topic      0.87         437        500       \n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Topic-wise accuracy summary for Simple RNN experiments\n",
    "# ============================================================================\n",
    "\n",
    "def display_topic_metrics(title, metrics_dict):\n",
    "    print(f\"\\n{title}\")\n",
    "    print(\"-\" * len(title))\n",
    "    header = f\"{'Topic':<10} {'Accuracy %':<12} {'Correct':<10} {'Total':<10}\"\n",
    "    print(header)\n",
    "    print(\"-\" * len(header))\n",
    "    for topic in sorted(metrics_dict.keys()):\n",
    "        stats = metrics_dict[topic]\n",
    "        acc_pct = stats['accuracy'] * 100\n",
    "        print(f\"{topic:<10} {acc_pct:<12.2f} {stats['correct']:<10} {stats['total']:<10}\")\n",
    "    print(\"-\" * len(header))\n",
    "    \n",
    "    total_cor = sum([metrics_dict[topic]['correct'] for topic in metrics_dict])\n",
    "    total_sam = sum([metrics_dict[topic]['total'] for topic in metrics_dict])\n",
    "    total_acc = total_cor / total_sam \n",
    "    print(f\"{'Topic':<10} {total_acc:<12.2f} {total_cor:<10} {total_sam:<10}\")\n",
    "\n",
    "print(\"\\n>>> Topic-wise accuracy for Simple RNN variants\")\n",
    "for run_key, info in p35_results[\"simple_rnn_baseline\"].items():\n",
    "    topic_metrics = info.get(\"topic_metrics\")\n",
    "    if not topic_metrics:\n",
    "        continue\n",
    "    title = f\"Simple RNN ({info['description']})\"\n",
    "    display_topic_metrics(title, topic_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
