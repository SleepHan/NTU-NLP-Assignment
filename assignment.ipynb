{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513c09f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# IMPORTANT: Fix for PyTorch/IPython compatibility issue\n",
    "# This must run BEFORE importing torch to avoid decorator conflicts\n",
    "# This fixes the \"disable() got an unexpected keyword argument 'wrapping'\" error\n",
    "\n",
    "# Method 1: Try to disable dynamo via environment variable (needs to be set before import)\n",
    "os.environ.setdefault('TORCH_COMPILE_DISABLE', '1')\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "import torch\n",
    "\n",
    "# Method 2: Patch torch._dynamo.disable decorator after import\n",
    "try:\n",
    "    import torch._dynamo\n",
    "    # Patch the disable function to ignore the 'wrapping' parameter\n",
    "    if hasattr(torch._dynamo, 'disable'):\n",
    "        def patched_disable(fn=None, *args, **kwargs):\n",
    "            # Remove problematic 'wrapping' parameter if present\n",
    "            if 'wrapping' in kwargs:\n",
    "                kwargs.pop('wrapping')\n",
    "            if fn is None:\n",
    "                # Decorator usage: @disable\n",
    "                return lambda f: f\n",
    "            # Function usage: disable(fn) or disable(fn, **kwargs)\n",
    "            # Simply return the function unwrapped to avoid recursion\n",
    "            # The original disable was causing issues, so we bypass it entirely\n",
    "            return fn\n",
    "        torch._dynamo.disable = patched_disable\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not patch torch._dynamo: {e}\")\n",
    "    pass  # If patching fails, continue anyway\n",
    "\n",
    "import random, string\n",
    "\n",
    "from torchtext import data , datasets\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "\n",
    "os.environ['GENSIM_DATA_DIR'] = os.path.join(os.getcwd(), 'gensim-data')\n",
    "\n",
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.fasttext import load_facebook_model\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import time, copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a560b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4021482c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c413bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Part 0: Dataset Preparation\n",
    "\n",
    "# For tokenization\n",
    "TEXT = data.Field ( tokenize = 'spacy', tokenizer_language = 'en_core_web_sm', include_lengths = True )\n",
    "\n",
    "# For multi - class classification labels\n",
    "LABEL = data.LabelField ()\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Load the TREC dataset\n",
    "# Train / Validation / Test split\n",
    "train_data, test_data = datasets.TREC.splits( TEXT, LABEL, fine_grained = False )\n",
    "\n",
    "train_data, validation_data = train_data.split(\n",
    "    split_ratio=0.8,\n",
    "    stratified=True,\n",
    "    strata_field='label'\n",
    "    random_state= random.seed(42)\n",
    ")\n",
    "print(vars(train_data.examples[0]))\n",
    "\n",
    "\n",
    "# Count how many samples per label in the train set\n",
    "label_counts = Counter([ex.label for ex in train_data.examples])\n",
    "total_examples = len(train_data)\n",
    "\n",
    "print(\"\\nLabel distribution in training set:\")\n",
    "for label, count in sorted(label_counts.items()):\n",
    "    percentage = (count / total_examples) * 100\n",
    "    print(f\"- {label}: {count} samples ({percentage:.2f}%)\")\n",
    "\n",
    "# Optional sanity check: total percentages should sum â‰ˆ 100%\n",
    "total_percentage = sum((count / total_examples) * 100 for count in label_counts.values())\n",
    "print(f\"Total samples: {total_examples}, Sum of percentages: {total_percentage:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d7cf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ex in train_data:\n",
    "    print(f'{ex.label}: {ex.text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442ff1d4",
   "metadata": {},
   "source": [
    "# Part 1: Prepare Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9040ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### a) Size of Vocabulary formed from training data according to tokenization method\n",
    "# Vocabulary size (includes specials like <unk>, <pad>)\n",
    "TEXT.build_vocab(train_data, min_freq=1)\n",
    "vocab_size = len(TEXT.vocab)\n",
    "print(\"Vocabulary Size (with specials):\", vocab_size)\n",
    "\n",
    "vocab_wo_specials = len([w for w in TEXT.vocab.stoi if w not in {TEXT.unk_token, TEXT.pad_token}])\n",
    "print(\"Vocabulary size (no specials):\", vocab_wo_specials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1507b5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### b) How many OOV words exist in your training data?\n",
    "####    What is the number of OOV words for each topic category?\n",
    "# Load Word2Vec model from local file instead of downloading\n",
    "w2v = api.load('word2vec-google-news-300')\n",
    "# w2v = KeyedVectors.load('word2vec-google-news-300.model')\n",
    "w2v_vocab = w2v.key_to_index\n",
    "\n",
    "# Get training vocab tokens (types), excluding specials\n",
    "specials = {TEXT.unk_token, TEXT.pad_token}\n",
    "train_vocab_types = [w for w in TEXT.vocab.stoi.keys() if w not in specials]\n",
    "\n",
    "# Overall OOV types in training vocab\n",
    "oov_types_overall = {w for w in train_vocab_types if w not in w2v_vocab}\n",
    "print(\"Number of OOV word types (overall):\", len(oov_types_overall))\n",
    "\n",
    "# OOV types per label (unique types per category across its sentences)\n",
    "label_to_oov_types = defaultdict(set)\n",
    "label_to_total_types = defaultdict(set)\n",
    "\n",
    "for ex in train_data.examples:\n",
    "    label = ex.label\n",
    "    # Count by unique types per sentence to avoid overcounting repeats\n",
    "    for w in set(ex.text):\n",
    "        label_to_total_types[label].add(w)\n",
    "        if w not in specials and w not in w2v_vocab:\n",
    "            label_to_oov_types[label].add(w)\n",
    "\n",
    "print(\"\\nOOV word types per topic label:\")\n",
    "for label in sorted(label_to_total_types.keys()):\n",
    "    num_oov = len(label_to_oov_types[label])\n",
    "    num_types = len(label_to_total_types[label])\n",
    "    rate = (num_oov / num_types) if num_types > 0 else 0.0\n",
    "    print(f\"- {label}: {num_oov} OOV types (out of {num_types}, rate={rate:.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6a5315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### c) OOV mitigation strategy (No transformer-based language models allowed)\n",
    "# Implement your solution in your source code. Show the corresponding code snippet.\n",
    "# 1. Fast Text Model Implementatation\n",
    "# Load FastText with subword info (pretrained on Wikipedia)\n",
    "# First download is large; cached afterwards\n",
    "\n",
    "# 2. Modelling Unknown (<UNK>) token approach\n",
    "# Make the <unk> vector informative and trainable by initializing it\n",
    "# as the mean of available pretrained vectors.\n",
    "\n",
    "# Loading fasttext model\n",
    "fatter_fasttext_bin = load_facebook_model('crawl-300d-2M-subword/crawl-300d-2M-subword.bin')\n",
    "embedding_dim = fatter_fasttext_bin.wv.vector_size\n",
    "\n",
    "# Build embedding matrix aligned to TEXT.vocab\n",
    "num_tokens = len(TEXT.vocab)\n",
    "emb_matrix = np.zeros((num_tokens, embedding_dim), dtype=np.float32)\n",
    "\n",
    "# torchtext 0.4.0: TEXT.vocab.itos is index->token, stoi is token->index\n",
    "pad_tok = TEXT.pad_token\n",
    "unk_tok = TEXT.unk_token\n",
    "\n",
    "# Getting index of <unk> in vocab\n",
    "unk_index = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "known_vecs = []\n",
    "\n",
    "for idx, token in enumerate(TEXT.vocab.itos):\n",
    "    # Skip specials here; we will set them explicitly below\n",
    "    if token in {pad_tok, unk_tok}:\n",
    "        continue\n",
    "\n",
    "    vec = fatter_fasttext_bin.wv[token]\n",
    "    emb_matrix[idx] = vec\n",
    "    known_vecs.append(vec)\n",
    "\n",
    "if len(known_vecs) > 0:\n",
    "    unk_mean = torch.tensor(np.mean(known_vecs, axis=0), dtype=torch.float32)\n",
    "else:\n",
    "    unk_mean = torch.empty(embedding_dim).uniform_(-0.05, 0.05)\n",
    "with torch.no_grad():\n",
    "    emb_matrix[unk_index] = unk_mean\n",
    "\n",
    "# Create Embedding layer initialized with FastText\n",
    "fatter_embedding = torch.nn.Embedding(num_tokens, embedding_dim, padding_idx=TEXT.vocab.stoi[TEXT.pad_token])\n",
    "fatter_embedding.weight.data.copy_(torch.from_numpy(emb_matrix))\n",
    "\n",
    "torch.save(fatter_embedding, 'embedding_weights_fatter_fasttext.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e7172b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### d) Select the 20 most frequent words from each topic category in the training set (removing\n",
    "# stopwords if necessary). Retrieve their pretrained embeddings (from Word2Vec or GloVe).\n",
    "# Project these embeddings into 2D space (using e.g., t-SNE or Principal Component Analysis).\n",
    "# Plot the points in a scatter plot, color-coded by their topic category. Attach your plot here.\n",
    "# Analyze your findings.\n",
    "\n",
    "# Build per-label token frequency (lowercased, stopwords/punct filtered)\n",
    "label_to_counter = defaultdict(Counter)\n",
    "valid_chars = set(string.ascii_letters)\n",
    "\n",
    "def is_valid_token(tok: str) -> bool:\n",
    "    t = tok.strip(\"'\\\"\")\n",
    "    if len(t) == 0:\n",
    "        return False\n",
    "    # Keep purely alphabetic tokens to avoid punctuation/numbers\n",
    "    return t.isalpha()\n",
    "\n",
    "for ex in train_data.examples:\n",
    "    label = ex.label\n",
    "    for tok in ex.text:\n",
    "        tok_l = tok.lower()\n",
    "        if tok_l in STOP_WORDS:\n",
    "            continue\n",
    "        if not is_valid_token(tok_l):\n",
    "            continue\n",
    "        label_to_counter[label][tok_l] += 1\n",
    "\n",
    "# Select top 20 per label that exist in Word2Vec\n",
    "topk = 20\n",
    "label_to_top_tokens = {}\n",
    "for label, ctr in label_to_counter.items():\n",
    "    selected = []\n",
    "    for tok, _ in ctr.most_common():\n",
    "        if tok in w2v.key_to_index:\n",
    "            selected.append(tok)\n",
    "        if len(selected) >= topk:\n",
    "            break\n",
    "    label_to_top_tokens[label] = selected\n",
    "\n",
    "# Collect embeddings and labels\n",
    "points = []\n",
    "point_labels = []\n",
    "point_words = []\n",
    "for label, toks in label_to_top_tokens.items():\n",
    "    for tok in toks:\n",
    "        vec = w2v.get_vector(tok)\n",
    "        points.append(vec)\n",
    "        point_labels.append(label)\n",
    "        point_words.append(tok)\n",
    "\n",
    "if len(points) > 0:\n",
    "    X = np.vstack(points)\n",
    "\n",
    "    # 2D projections\n",
    "    # tsne_2d = TSNE(n_components=2, random_state=42, init=\"pca\", perplexity=30).fit_transform(X)\n",
    "    # pca_2d = PCA(n_components=2, random_state=42).fit_transform(X)\n",
    "    tsne_2d = TSNE(n_components=2, random_state=SEED, init=\"pca\", perplexity=30).fit_transform(X)\n",
    "    pca_2d = PCA(n_components=2, random_state=SEED).fit_transform(X)\n",
    "\n",
    "    # Assign colors per label\n",
    "    unique_labels = sorted(set(point_labels))\n",
    "    color_map = {lab: plt.cm.tab10(i % 10) for i, lab in enumerate(unique_labels)}\n",
    "\n",
    "    def plot_scatter(Y2, title: str, fname: str):\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        for lab in unique_labels:\n",
    "            idxs = [i for i, l in enumerate(point_labels) if l == lab]\n",
    "            plt.scatter(Y2[idxs, 0], Y2[idxs, 1], c=[color_map[lab]], label=lab, alpha=0.8, s=40)\n",
    "            # Light word annotations (optional; can clutter)\n",
    "            for i in idxs:\n",
    "                plt.annotate(point_words[i], (Y2[i, 0], Y2[i, 1]), fontsize=7, alpha=0.7)\n",
    "        plt.legend(title=\"TREC label\")\n",
    "        plt.title(title)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(fname, dpi=200)\n",
    "        plt.close()\n",
    "\n",
    "    plot_scatter(tsne_2d, \"Top-20 per TREC label (Word2Vec) - t-SNE\", \"trec_top20_tsne.png\")\n",
    "    plot_scatter(pca_2d, \"Top-20 per TREC label (Word2Vec) - PCA\", \"trec_top20_pca.png\")\n",
    "\n",
    "    print(\"Saved plots: trec_top20_tsne.png, trec_top20_pca.png\")\n",
    "    for lab in unique_labels:\n",
    "        print(f\"{lab}: {label_to_top_tokens[lab]}\")\n",
    "else:\n",
    "    print(\"No points collected for visualization. Check filtering or embedding availability.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2dadfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext = torch.load('embedding_weights_fatter_fasttext.pt', weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8aa0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c690c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Part 2: Model Training & Evaluation - RNN\n",
    "\n",
    "# Build vocabulary for labels\n",
    "LABEL.build_vocab(train_data)\n",
    "num_classes = len(LABEL.vocab)\n",
    "print(f\"\\nNumber of classes: {num_classes}\")\n",
    "print(f\"Classes: {LABEL.vocab.itos}\")\n",
    "\n",
    "# Create iterators for batching (inline for easier debugging)\n",
    "# train_iterator = data.BucketIterator(...)\n",
    "# val_iterator = data.BucketIterator(...)\n",
    "# test_iterator = data.BucketIterator(...)\n",
    "# (Used directly in Part 2 execution below)\n",
    "\n",
    "\n",
    "class SimpleRNNClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple RNN for topic classification (Baseline - no dropout).\n",
    "    Uses pretrained embeddings (learnable/updated during training) with OOV mitigation \n",
    "    and aggregates word representations to sentence representation using the last hidden state.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, \n",
    "                 n_layers=1, dropout=0.0, padding_idx=0, pretrained_embeddings=None):\n",
    "        super(SimpleRNNClassifier, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # Embedding layer initialized with pretrained embeddings\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "            # IMPORTANT: Make embeddings learnable (updated during training)\n",
    "            # This allows fine-tuning of embeddings including OOV words handled by FastText\n",
    "            self.embedding.weight.requires_grad = True\n",
    "        \n",
    "        # Simple RNN layer (no dropout in baseline)\n",
    "        self.rnn = nn.RNN(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=n_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.0  # No dropout in baseline\n",
    "        )\n",
    "        \n",
    "        # Removed: Dropout layer (baseline has no regularization)\n",
    "        # self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Fully connected output layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        # text: [batch_size, seq_len]\n",
    "        # text_lengths: [batch_size]\n",
    "        \n",
    "        # Embed the input\n",
    "        embedded = self.embedding(text)  # [batch_size, seq_len, embedding_dim]\n",
    "        \n",
    "        # Get dimensions\n",
    "        batch_size = embedded.size(0)\n",
    "        seq_len = embedded.size(1)\n",
    "        \n",
    "        # Handle text_lengths: ensure it's a 1D tensor with batch_size elements\n",
    "        text_lengths_flat = text_lengths.flatten().cpu().long()\n",
    "        \n",
    "        # text_lengths should have exactly batch_size elements (one length per batch item)\n",
    "        if len(text_lengths_flat) != batch_size:\n",
    "            raise ValueError(\n",
    "                f\"text_lengths size mismatch: got {len(text_lengths_flat)} elements, \"\n",
    "                f\"expected {batch_size} (batch_size). text_lengths.shape={text_lengths.shape}, \"\n",
    "                f\"text.shape={text.shape}, embedded.shape={embedded.shape}\"\n",
    "            )\n",
    "        \n",
    "        # Clamp lengths to be at most the sequence length (safety check)\n",
    "        text_lengths_clamped = torch.clamp(text_lengths_flat, min=1, max=seq_len)\n",
    "        \n",
    "        # Pack the padded sequences for efficient processing\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, text_lengths_clamped, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        \n",
    "        # Pass through RNN\n",
    "        packed_output, hidden = self.rnn(packed_embedded)\n",
    "        # hidden: [n_layers, batch_size, hidden_dim]\n",
    "        \n",
    "        # Use the last hidden state from the last layer\n",
    "        last_hidden = hidden[-1]  # [batch_size, hidden_dim]\n",
    "        \n",
    "        # Removed: Apply dropout (baseline has no regularization)\n",
    "        # last_hidden = self.dropout(last_hidden)\n",
    "        \n",
    "        # Pass through fully connected layer\n",
    "        output = self.fc(last_hidden)  # [batch_size, output_dim]\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "# Utility function for counting parameters\n",
    "def count_parameters(model):\n",
    "    \"\"\"Count trainable parameters\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Training and evaluation functions removed - code is now inline below for easier debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ceba8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(TEXT.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10a0ed9",
   "metadata": {},
   "source": [
    "Training order:\n",
    "1. Word aggregation\n",
    "2. Hyperparameters tuning\n",
    "3. Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3176ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Part 2: Initial Simple RNN Model Training\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 2: SIMPLE RNN MODEL TRAINING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get pretrained embeddings from Part 1 (frozen embeddings)\n",
    "pretrained_embeddings = fasttext.weight.data\n",
    "\n",
    "# Get embedding dimension and vocab size from the fasttext embedding layer\n",
    "embedding_dim = fasttext.weight.shape[1]\n",
    "fasttext_vocab_size = fasttext.weight.shape[0]  # Vocab size from saved embedding\n",
    "\n",
    "# Verify vocab sizes match (they might differ if vocab was rebuilt)\n",
    "print(f\"TEXT.vocab size: {len(TEXT.vocab)}\")\n",
    "print(f\"FastText embedding vocab size: {fasttext_vocab_size}\")\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "HIDDEN_DIM = 256\n",
    "N_LAYERS = 1\n",
    "DROPOUT = 0.0  # Baseline: no dropout\n",
    "N_EPOCHS = 100\n",
    "LEARNING_RATE = 0.001\n",
    "# Removed: PATIENCE = 10  # Baseline: no early stopping\n",
    "\n",
    "# Create data iterators (inline for easier debugging)\n",
    "# Note: Different sequence lengths per batch are normal - BucketIterator groups similar-length sequences\n",
    "train_iterator = data.BucketIterator(\n",
    "    train_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    shuffle=True,  # Shuffle for training\n",
    "    device=device\n",
    ")\n",
    "\n",
    "val_iterator = data.BucketIterator(\n",
    "    validation_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    shuffle=False,  # No shuffle for validation (deterministic)\n",
    "    device=device\n",
    ")\n",
    "\n",
    "test_iterator = data.BucketIterator(\n",
    "    test_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    shuffle=False,  # No shuffle for test (deterministic)\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Initialize simple RNN model (Baseline)\n",
    "# Use vocab size from loaded embedding to match the saved weights exactly\n",
    "model = SimpleRNNClassifier(\n",
    "    vocab_size=fasttext_vocab_size,  # Must match saved embedding vocab size\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    output_dim=num_classes,\n",
    "    n_layers=N_LAYERS,\n",
    "    dropout=0.0,  # Baseline: no dropout\n",
    "    padding_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "    pretrained_embeddings=pretrained_embeddings\n",
    ").to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(f\"\\n>>> Training Baseline RNN Model\")\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  - Hidden Dim: {HIDDEN_DIM}\")\n",
    "print(f\"  - Layers: {N_LAYERS}\")\n",
    "print(f\"  - Dropout: {DROPOUT} (Baseline: no regularization)\")\n",
    "print(f\"  - Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"  - Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  - Epochs: {N_EPOCHS} (no early stopping)\")\n",
    "print(f\"  - Embedding Dim: {embedding_dim} (FastText)\")\n",
    "print(f\"  - Embeddings: LEARNABLE (updated during training)\")\n",
    "print(f\"  - OOV Handling: FastText subword embeddings + trainable <unk> token\")\n",
    "\n",
    "# ============================================================================\n",
    "# Helper function to process batches consistently\n",
    "# ============================================================================\n",
    "\n",
    "def process_batch(batch, debug=False):\n",
    "    \"\"\"\n",
    "    Process a batch from BucketIterator, handling text transpose correctly.\n",
    "    Returns: text, text_lengths, labels (all properly formatted)\n",
    "    \"\"\"\n",
    "    text, text_lengths = batch.text\n",
    "    labels = batch.label\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"DEBUG BATCH - text shape: {text.shape}, text_lengths shape: {text_lengths.shape}, labels shape: {labels.shape}\")\n",
    "    \n",
    "    # torchtext BucketIterator returns text as [seq_len, batch_size] by default\n",
    "    # We need [batch_size, seq_len] for batch_first=True in the model\n",
    "    expected_batch_size = labels.shape[0]\n",
    "    \n",
    "    if text.dim() == 2:\n",
    "        if text.shape[1] == expected_batch_size and len(text_lengths) == expected_batch_size:\n",
    "            # text is [seq_len, batch_size], transpose to [batch_size, seq_len]\n",
    "            text = text.transpose(0, 1)\n",
    "            if debug:\n",
    "                print(f\"DEBUG BATCH - Transposed text to [batch_size, seq_len]: {text.shape}\")\n",
    "        elif text.shape[0] == expected_batch_size and len(text_lengths) == expected_batch_size:\n",
    "            # text is already [batch_size, seq_len]\n",
    "            if debug:\n",
    "                print(f\"DEBUG BATCH - text already in correct format: {text.shape}\")\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Cannot determine text format: text.shape={text.shape}, \"\n",
    "                f\"text_lengths.shape={text_lengths.shape}, labels.shape={labels.shape}\"\n",
    "            )\n",
    "    \n",
    "    # Verify dimensions match\n",
    "    assert text.shape[0] == len(text_lengths) == labels.shape[0], \\\n",
    "        f\"Batch size mismatch: text.shape[0]={text.shape[0]}, len(text_lengths)={len(text_lengths)}, labels.shape[0]={labels.shape[0]}\"\n",
    "    \n",
    "    return text, text_lengths, labels\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "# Ensure deterministic behavior\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Note: PyTorch/IPython compatibility fix is already applied in cell 0\n",
    "# The torch._dynamo.disable decorator has been patched to handle the 'wrapping' parameter\n",
    "\n",
    "# ============================================================================\n",
    "# Training Loop (inline for easier debugging)\n",
    "# ============================================================================\n",
    "\n",
    "best_val_acc = 0\n",
    "# Removed: patience_counter = 0  # Baseline: no early stopping\n",
    "\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "\n",
    "print(f\"\\nStarting training for {N_EPOCHS} epochs...\")  # Removed \"up to\" - no early stopping\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Trainable parameters: {count_parameters(model):,}\")\n",
    "print(f\"Embedding layer learnable: {model.embedding.weight.requires_grad}\")\n",
    "# Removed: print(f\"Early stopping patience: {PATIENCE} epochs\")  # Baseline: no early stopping\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Training for one epoch (inline)\n",
    "    # ========================================================================\n",
    "    model.train()\n",
    "    train_epoch_loss = 0\n",
    "    train_all_preds = []\n",
    "    train_all_labels = []\n",
    "    \n",
    "    batch_idx = 0\n",
    "    for batch in train_iterator:\n",
    "        # Process batch (with debug only for first batch)\n",
    "        text, text_lengths, labels = process_batch(batch, debug=(batch_idx == 0))\n",
    "        batch_idx += 1\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        predictions = model(text, text_lengths)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(predictions, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Removed: Gradient clipping (baseline has no gradient clipping)\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_epoch_loss += loss.item()\n",
    "        \n",
    "        # Store predictions and labels for metrics\n",
    "        preds = torch.argmax(predictions, dim=1)\n",
    "        train_all_preds.extend(preds.cpu().numpy())\n",
    "        train_all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate training accuracy\n",
    "    train_acc = accuracy_score(train_all_labels, train_all_preds)\n",
    "    train_loss = train_epoch_loss / len(train_iterator)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Validation evaluation (inline)\n",
    "    # ========================================================================\n",
    "    model.eval()\n",
    "    val_epoch_loss = 0\n",
    "    val_all_preds = []\n",
    "    val_all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_iterator:\n",
    "            # Process batch consistently with training\n",
    "            text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "            \n",
    "            # Forward pass\n",
    "            predictions = model(text, text_lengths)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(predictions, labels)\n",
    "            val_epoch_loss += loss.item()\n",
    "            \n",
    "            # Store predictions and labels\n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            val_all_preds.extend(preds.cpu().numpy())\n",
    "            val_all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate validation accuracy\n",
    "    val_acc = accuracy_score(val_all_labels, val_all_preds)\n",
    "    val_loss = val_epoch_loss / len(val_iterator)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Logging (without early stopping)\n",
    "    # ========================================================================\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = divmod(end_time - start_time, 60)\n",
    "    \n",
    "    # Store metrics\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02}/{N_EPOCHS} | Time: {int(epoch_mins)}m {int(epoch_secs)}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\tVal Loss: {val_loss:.4f} | Val Acc: {val_acc*100:.2f}%')\n",
    "    \n",
    "    # Track best model (but don't stop early - baseline trains for all epochs)\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        # Save best model\n",
    "        torch.save(model.state_dict(), 'rnn_simple_best.pt')\n",
    "        print(f'\\t>>> New best model saved with Val Acc: {val_acc*100:.2f}%')\n",
    "    # Removed: Early stopping break logic (baseline trains for all epochs)\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(f\"Training completed! Best validation accuracy: {best_val_acc*100:.2f}%\")\n",
    "print(f\"Total epochs trained: {N_EPOCHS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27861988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Validation Set Evaluation (inline) - Evaluate best model on validation set\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VALIDATION SET EVALUATION (Best Model)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load best model and evaluate on validation set\n",
    "model.load_state_dict(torch.load('rnn_simple_best.pt'))\n",
    "\n",
    "model.eval()\n",
    "val_eval_loss = 0\n",
    "val_eval_preds = []\n",
    "val_eval_labels = []\n",
    "val_eval_probs = []  # Store probabilities for AUC-ROC\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_iterator:\n",
    "        text, text_lengths = batch.text\n",
    "        labels = batch.label\n",
    "        \n",
    "        # Process batch consistently\n",
    "        text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "        \n",
    "        # Forward pass\n",
    "        predictions = model(text, text_lengths)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(predictions, labels)\n",
    "        val_eval_loss += loss.item()\n",
    "        \n",
    "        # Store predictions, labels, and probabilities\n",
    "        probs = torch.softmax(predictions, dim=1)\n",
    "        preds = torch.argmax(predictions, dim=1)\n",
    "        val_eval_preds.extend(preds.cpu().numpy())\n",
    "        val_eval_labels.extend(labels.cpu().numpy())\n",
    "        val_eval_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "# Calculate validation metrics\n",
    "val_eval_acc = accuracy_score(val_eval_labels, val_eval_preds)\n",
    "val_eval_f1 = f1_score(val_eval_labels, val_eval_preds, average='weighted')\n",
    "val_eval_loss_final = val_eval_loss / len(val_iterator)\n",
    "\n",
    "# Calculate AUC-ROC (one-vs-rest for multiclass)\n",
    "try:\n",
    "    val_eval_probs_array = np.array(val_eval_probs)\n",
    "    val_eval_labels_bin = label_binarize(val_eval_labels, classes=range(num_classes))\n",
    "    val_eval_auc = roc_auc_score(val_eval_labels_bin, val_eval_probs_array, average='weighted', multi_class='ovr')\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not calculate AUC-ROC: {e}\")\n",
    "    val_eval_auc = 0.0\n",
    "\n",
    "print(f\"\\n>>> Validation Set Results (Best Model):\")\n",
    "print(f\"Validation Loss: {val_eval_loss_final:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_eval_acc*100:.2f}%\")\n",
    "print(f\"Validation F1 Score: {val_eval_f1:.4f}\")\n",
    "print(f\"Validation AUC-ROC: {val_eval_auc:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Test Set Evaluation (inline)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST SET EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "model.eval()\n",
    "test_epoch_loss = 0\n",
    "test_all_preds = []\n",
    "test_all_labels = []\n",
    "test_all_probs = []  # Store probabilities for AUC-ROC\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_iterator:\n",
    "        text, text_lengths = batch.text\n",
    "        labels = batch.label\n",
    "        \n",
    "        # Process batch consistently\n",
    "        text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "        \n",
    "        # Forward pass\n",
    "        predictions = model(text, text_lengths)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(predictions, labels)\n",
    "        test_epoch_loss += loss.item()\n",
    "        \n",
    "        # Store predictions, labels, and probabilities\n",
    "        probs = torch.softmax(predictions, dim=1)\n",
    "        preds = torch.argmax(predictions, dim=1)\n",
    "        test_all_preds.extend(preds.cpu().numpy())\n",
    "        test_all_labels.extend(labels.cpu().numpy())\n",
    "        test_all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "# Calculate test metrics\n",
    "test_acc = accuracy_score(test_all_labels, test_all_preds)\n",
    "test_f1 = f1_score(test_all_labels, test_all_preds, average='weighted')\n",
    "test_loss = test_epoch_loss / len(test_iterator)\n",
    "\n",
    "# Calculate AUC-ROC (one-vs-rest for multiclass)\n",
    "try:\n",
    "    test_all_probs_array = np.array(test_all_probs)\n",
    "    test_all_labels_bin = label_binarize(test_all_labels, classes=range(num_classes))\n",
    "    test_auc = roc_auc_score(test_all_labels_bin, test_all_probs_array, average='weighted', multi_class='ovr')\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not calculate AUC-ROC: {e}\")\n",
    "    test_auc = 0.0\n",
    "\n",
    "print(f\"\\n>>> Test Set Results:\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n",
    "print(f\"Test F1 Score: {test_f1:.4f}\")\n",
    "print(f\"Test AUC-ROC: {test_auc:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 2 INITIAL TRAINING COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ba71f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72a89fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Part 2.2: Sequential Hyperparameter Tuning (One Variable at a Time)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 2.2: SEQUENTIAL HYPERPARAMETER TUNING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# ============================================================================\n",
    "# Step 0: Epoch + Early Stopping Configuration Tuning\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 0: EPOCH + EARLY STOPPING TUNING\")\n",
    "print(\"=\"*80)\n",
    "print(\"Testing different MAX_EPOCHS and PATIENCE configurations\")\n",
    "\n",
    "# Test different epoch and patience configurations\n",
    "max_epochs_options = [100, 200, 300]\n",
    "patience = 10\n",
    "\n",
    "# Use baseline config for testing epoch settings\n",
    "baseline_config = {\n",
    "    'lr': 0.001,\n",
    "    'batch_size': 64,\n",
    "    'hidden_dim': 256,\n",
    "    'optimizer': 'Adam'\n",
    "}\n",
    "\n",
    "step0_configs = []\n",
    "for max_epochs in max_epochs_options:\n",
    "        step0_configs.append({\n",
    "            'config': baseline_config.copy(),\n",
    "            'max_epochs': max_epochs,\n",
    "            'patience': patience\n",
    "        })\n",
    "\n",
    "print(f\"Total combinations to test: {len(step0_configs)}\")\n",
    "print(\"Combinations (Max_Epochs, Patience):\")\n",
    "for idx, ep_config in enumerate(step0_configs, 1):\n",
    "    print(f\"  {idx}. Max_Epochs={ep_config['max_epochs']}, Patience={ep_config['patience']}\")\n",
    "\n",
    "# Helper function to train with specific epoch/patience settings\n",
    "def train_and_evaluate_with_epochs(config, max_epochs, patience, config_name=\"config\"):\n",
    "    \"\"\"Train a model for specific number of epochs WITHOUT early stopping\"\"\"\n",
    "    print(f\"\\n>>> Testing: {config_name}\")\n",
    "    print(f\"    LR={config['lr']}, Batch={config['batch_size']}, Hidden={config['hidden_dim']}, Opt={config['optimizer']}\")\n",
    "    print(f\"    Epochs={max_epochs} (NO early stopping - training for full {max_epochs} epochs)\")\n",
    "    \n",
    "    # Create iterators with specific batch size\n",
    "    train_iter = data.BucketIterator(\n",
    "        train_data,\n",
    "        batch_size=config['batch_size'],\n",
    "        sort_key=lambda x: len(x.text),\n",
    "        sort_within_batch=True,\n",
    "        shuffle=True,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    val_iter = data.BucketIterator(\n",
    "        validation_data,\n",
    "        batch_size=config['batch_size'],\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        sort_key=lambda x: len(x.text),\n",
    "        sort_within_batch=True,\n",
    "        shuffle=False,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Reset random seeds INSIDE function to ensure fresh model for each config\n",
    "    # This is critical to ensure each max_epochs config starts from scratch\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "        torch.cuda.manual_seed_all(SEED)\n",
    "    # Ensure deterministic behavior\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    # Create model with specific hidden dimension\n",
    "    model = SimpleRNNClassifier(\n",
    "        vocab_size=fasttext_vocab_size,\n",
    "        embedding_dim=embedding_dim,\n",
    "        hidden_dim=config['hidden_dim'],\n",
    "        output_dim=num_classes,\n",
    "        n_layers=N_LAYERS,\n",
    "        dropout=0.0,  # Baseline: no dropout\n",
    "        padding_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "        pretrained_embeddings=pretrained_embeddings\n",
    "    ).to(device)\n",
    "    \n",
    "    # Select optimizer\n",
    "    if config['optimizer'] == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config['lr'])\n",
    "    elif config['optimizer'] == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=config['lr'], momentum=0.9)\n",
    "    elif config['optimizer'] == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=config['lr'])\n",
    "    elif config['optimizer'] == 'Adagrad':\n",
    "        optimizer = optim.Adagrad(model.parameters(), lr=config['lr'])\n",
    "    \n",
    "    # Training loop WITHOUT early stopping - train for full num_epochs\n",
    "    best_val_acc = 0.0\n",
    "    best_val_acc_at_epoch = 0\n",
    "    final_val_acc = 0.0\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        \n",
    "        for batch in train_iter:\n",
    "            text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(text, text_lengths)\n",
    "            loss = criterion(predictions, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        train_acc = accuracy_score(train_labels, train_preds)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_iter:\n",
    "                text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "                predictions = model(text, text_lengths)\n",
    "                loss = criterion(predictions, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                preds = torch.argmax(predictions, dim=1)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_acc = accuracy_score(val_labels, val_preds)\n",
    "        \n",
    "        # Track best validation accuracy (but don't stop early)\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_val_acc_at_epoch = epoch + 1\n",
    "        \n",
    "        final_val_acc = val_acc  # Store final epoch's validation accuracy\n",
    "        \n",
    "        # Optional: print progress every 10 epochs or at the end\n",
    "        if (epoch + 1) % 10 == 0 or (epoch + 1) == max_epochs:\n",
    "            print(f\"    Epoch {epoch+1}/{max_epochs}: Train Acc={train_acc*100:.2f}%, Val Acc={val_acc*100:.2f}%\")\n",
    "    \n",
    "    print(f\"    Final Val Acc: {final_val_acc*100:.2f}% | Best Val Acc: {best_val_acc*100:.2f}% (at epoch {best_val_acc_at_epoch}/{max_epochs})\")\n",
    "    return best_val_acc, best_val_acc_at_epoch, max_epochs\n",
    "\n",
    "step0_results = []\n",
    "for idx, ep_config in enumerate(step0_configs):\n",
    "    # Set fixed seed for reproducibility - ensures consistent batch ordering\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "    \n",
    "    val_acc, best_epoch, total_epochs = train_and_evaluate_with_epochs(\n",
    "        ep_config['config'],\n",
    "        ep_config['max_epochs'],\n",
    "        ep_config['patience'],\n",
    "        f\"Step 0 Config {idx+1}/{len(step0_configs)}\"\n",
    "    )\n",
    "    step0_results.append({\n",
    "        'num_epochs': ep_config['max_epochs'],\n",
    "        'val_acc': val_acc,\n",
    "        'best_epoch': best_epoch,\n",
    "        'total_epochs': total_epochs\n",
    "    })\n",
    "\n",
    "# Find best epoch configuration\n",
    "best_step0 = max(step0_results, key=lambda x: x['val_acc'])\n",
    "BEST_EPOCHS = best_step0['num_epochs']\n",
    "\n",
    "# Set appropriate MAX_EPOCHS and PATIENCE for subsequent steps\n",
    "# Use the best number of epochs with some buffer, and set a reasonable patience\n",
    "MAX_EPOCHS = BEST_EPOCHS\n",
    "PATIENCE = 7  # Default patience for early stopping in subsequent steps\n",
    "\n",
    "print(f\"\\n>>> Step 0 Results:\")\n",
    "print(f\"{'#':<4} {'Epochs':<8} {'Val Acc':<10} {'Best At Epoch':<15} {'Total Trained':<15}\")\n",
    "print(\"-\" * 60)\n",
    "for idx, result in enumerate(step0_results, 1):\n",
    "    print(f\"{idx:<4} {result['num_epochs']:<8} {result['val_acc']*100:<10.2f}% {result['best_epoch']:<15} {result['total_epochs']:<15}\")\n",
    "print(f\"\\n>>> Best from Step 0: Epochs={BEST_EPOCHS}, Val Acc={best_step0['val_acc']*100:.2f}%\")\n",
    "print(f\"    Best validation accuracy was achieved at epoch {best_step0['best_epoch']} out of {best_step0['total_epochs']}\")\n",
    "print(f\"\\n>>> Using MAX_EPOCHS={MAX_EPOCHS} and PATIENCE={PATIENCE} for subsequent steps (with early stopping)\")\n",
    "\n",
    "# Helper function to train and evaluate a model configuration\n",
    "def train_and_evaluate(config, config_name=\"config\"):\n",
    "    \"\"\"Train a model with given configuration and return validation accuracy\"\"\"\n",
    "    print(f\"\\n>>> Testing: {config_name}\")\n",
    "    print(f\"    LR={config['lr']}, Batch={config['batch_size']}, Hidden={config['hidden_dim']}, Opt={config['optimizer']}\")\n",
    "    \n",
    "    # Create iterators with specific batch size\n",
    "    train_iter = data.BucketIterator(\n",
    "        train_data,\n",
    "        batch_size=config['batch_size'],\n",
    "        sort_key=lambda x: len(x.text),\n",
    "        sort_within_batch=True,\n",
    "        shuffle=True,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    val_iter = data.BucketIterator(\n",
    "        validation_data,\n",
    "        batch_size=config['batch_size'],\n",
    "        sort_key=lambda x: len(x.text),\n",
    "        sort_within_batch=True,\n",
    "        shuffle=False,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Create model with specific hidden dimension\n",
    "    model = SimpleRNNClassifier(\n",
    "        vocab_size=fasttext_vocab_size,\n",
    "        embedding_dim=embedding_dim,\n",
    "        hidden_dim=config['hidden_dim'],\n",
    "        output_dim=num_classes,\n",
    "        n_layers=N_LAYERS,\n",
    "        dropout=0.0,  # Baseline: no dropout\n",
    "        padding_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "        pretrained_embeddings=pretrained_embeddings\n",
    "    ).to(device)\n",
    "    \n",
    "    # Select optimizer\n",
    "    if config['optimizer'] == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config['lr'])\n",
    "    elif config['optimizer'] == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=config['lr'])\n",
    "    elif config['optimizer'] == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=config['lr'])\n",
    "    elif config['optimizer'] == 'Adagrad':\n",
    "        optimizer = optim.Adagrad(model.parameters(), lr=config['lr'])\n",
    "    \n",
    "    # Training loop with early stopping\n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        \n",
    "        for batch in train_iter:\n",
    "            text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(text, text_lengths)\n",
    "            loss = criterion(predictions, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        train_acc = accuracy_score(train_labels, train_preds)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_iter:\n",
    "                text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "                predictions = model(text, text_lengths)\n",
    "                loss = criterion(predictions, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                preds = torch.argmax(predictions, dim=1)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_acc = accuracy_score(val_labels, val_preds)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(f\"    Early stopping at epoch {epoch+1}, best val acc: {best_val_acc*100:.2f}%\")\n",
    "                break\n",
    "    \n",
    "    print(f\"    Best Val Acc: {best_val_acc*100:.2f}% (stopped at epoch {epoch+1})\")\n",
    "    return best_val_acc, epoch + 1\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8590e566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Step 1: Group 1 - Learning Rate + Batch Size (Test Together)\n",
    "# ============================================================================\n",
    "\n",
    "# Helper function to train and evaluate a model configuration (uses best MAX_EPOCHS and PATIENCE from Step 0)\n",
    "def train_and_evaluate(config, config_name=\"config\"):\n",
    "    \"\"\"Train a model with given configuration and return validation accuracy\"\"\"\n",
    "    print(f\"\\n>>> Testing: {config_name}\")\n",
    "    print(f\"    LR={config['lr']}, Batch={config['batch_size']}, Hidden={config['hidden_dim']}, Opt={config['optimizer']}\")\n",
    "    \n",
    "    # Create iterators with specific batch size\n",
    "    train_iter = data.BucketIterator(\n",
    "        train_data,\n",
    "        batch_size=config['batch_size'],\n",
    "        sort_key=lambda x: len(x.text),\n",
    "        sort_within_batch=True,\n",
    "        shuffle=True,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    val_iter = data.BucketIterator(\n",
    "        validation_data,\n",
    "        batch_size=config['batch_size'],\n",
    "        sort_key=lambda x: len(x.text),\n",
    "        sort_within_batch=True,\n",
    "        shuffle=False,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Create model with specific hidden dimension\n",
    "    model = SimpleRNNClassifier(\n",
    "        vocab_size=fasttext_vocab_size,\n",
    "        embedding_dim=embedding_dim,\n",
    "        hidden_dim=config['hidden_dim'],\n",
    "        output_dim=num_classes,\n",
    "        n_layers=N_LAYERS,\n",
    "        dropout=0.0,  # Baseline: no dropout\n",
    "        padding_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "        pretrained_embeddings=pretrained_embeddings\n",
    "    ).to(device)\n",
    "    \n",
    "    # Select optimizer\n",
    "    if config['optimizer'] == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config['lr'])\n",
    "    elif config['optimizer'] == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=config['lr'], momentum=0.9)\n",
    "    elif config['optimizer'] == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=config['lr'])\n",
    "    elif config['optimizer'] == 'Adagrad':\n",
    "        optimizer = optim.Adagrad(model.parameters(), lr=config['lr'])\n",
    "    \n",
    "    # Training loop with early stopping (using best MAX_EPOCHS and PATIENCE from Step 0)\n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        \n",
    "        for batch in train_iter:\n",
    "            text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(text, text_lengths)\n",
    "            loss = criterion(predictions, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        train_acc = accuracy_score(train_labels, train_preds)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_iter:\n",
    "                text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "                predictions = model(text, text_lengths)\n",
    "                loss = criterion(predictions, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                preds = torch.argmax(predictions, dim=1)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_acc = accuracy_score(val_labels, val_preds)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(f\"    Early stopping at epoch {epoch+1}, best val acc: {best_val_acc*100:.2f}%\")\n",
    "                break\n",
    "    \n",
    "    print(f\"    Best Val Acc: {best_val_acc*100:.2f}% (stopped at epoch {epoch+1})\")\n",
    "    return best_val_acc, epoch + 1\n",
    "    \n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 1: LEARNING RATE + BATCH SIZE TUNING\")\n",
    "print(\"=\"*80)\n",
    "print(\"Testing ALL combinations of LR and Batch Size (they interact)\")\n",
    "\n",
    "# Test all combinations of learning rates and batch sizes\n",
    "learning_rates = [0.01, 0.001, 0.0001]\n",
    "batch_sizes = [32, 64, 128]\n",
    "\n",
    "step1_configs = []\n",
    "for lr in learning_rates:\n",
    "    for batch_size in batch_sizes:\n",
    "        step1_configs.append({\n",
    "            'lr': lr,\n",
    "            'batch_size': batch_size,\n",
    "            'hidden_dim': 256,\n",
    "            'optimizer': 'Adam'\n",
    "        })\n",
    "\n",
    "print(f\"Total combinations to test: {len(step1_configs)}\")\n",
    "print(\"Combinations:\")\n",
    "for idx, config in enumerate(step1_configs, 1):\n",
    "    print(f\"  {idx}. LR={config['lr']}, Batch={config['batch_size']}\")\n",
    "\n",
    "step1_results = []\n",
    "for idx, config in enumerate(step1_configs):\n",
    "    # Set fixed seed for reproducibility\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "    \n",
    "    val_acc, epoch_stopped = train_and_evaluate(config, f\"Step 1 Config {idx+1}/{len(step1_configs)}\")\n",
    "    step1_results.append({\n",
    "        'config': config,\n",
    "        'val_acc': val_acc,\n",
    "        'epoch_stopped': epoch_stopped\n",
    "    })\n",
    "\n",
    "# Find best LR + Batch Size\n",
    "best_step1 = max(step1_results, key=lambda x: x['val_acc'])\n",
    "best_lr = best_step1['config']['lr']\n",
    "best_batch_size = best_step1['config']['batch_size']\n",
    "\n",
    "print(f\"\\n>>> Step 1 Results:\")\n",
    "print(f\"{'#':<4} {'LR':<8} {'Batch':<7} {'Val Acc':<10} {'Epochs':<7}\")\n",
    "print(\"-\" * 40)\n",
    "for idx, result in enumerate(step1_results):\n",
    "    c = result['config']\n",
    "    print(f\"{idx+1:<4} {c['lr']:<8} {c['batch_size']:<7} {result['val_acc']*100:<10.2f}% {result['epoch_stopped']:<7}\")\n",
    "print(f\"\\n>>> Best from Step 1: LR={best_lr}, Batch={best_batch_size}, Val Acc={best_step1['val_acc']*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f576e603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Step 2: Group 2 - Optimizer\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 2: OPTIMIZER\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Using best LR={best_lr} and Batch={best_batch_size} from Step 1\")\n",
    "\n",
    "step2_configs = [\n",
    "    {'lr': best_lr, 'batch_size': best_batch_size, 'hidden_dim': 256, 'optimizer': 'Adam'},\n",
    "    {'lr': best_lr, 'batch_size': best_batch_size, 'hidden_dim': 256, 'optimizer': 'SGD'},\n",
    "    {'lr': best_lr, 'batch_size': best_batch_size, 'hidden_dim': 256, 'optimizer': 'RMSprop'},\n",
    "    {'lr': best_lr, 'batch_size': best_batch_size, 'hidden_dim': 256, 'optimizer': 'Adagrad'},\n",
    "]\n",
    "\n",
    "step2_results = []\n",
    "for idx, config in enumerate(step2_configs):\n",
    "    # Set fixed seed for reproducibility\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "    \n",
    "    val_acc, epoch_stopped = train_and_evaluate(config, f\"Step 2 Config {idx+1}/{len(step2_configs)}\")\n",
    "    step2_results.append({\n",
    "        'config': config,\n",
    "        'val_acc': val_acc,\n",
    "        'epoch_stopped': epoch_stopped\n",
    "    })\n",
    "\n",
    "# Find best Optimizer (and potentially adjusted LR)\n",
    "best_step2 = max(step2_results, key=lambda x: x['val_acc'])\n",
    "best_optimizer = best_step2['config']['optimizer']\n",
    "final_lr = best_step2['config']['lr']  # May be different if SGD needed higher LR\n",
    "\n",
    "print(f\"\\n>>> Step 2 Results:\")\n",
    "print(f\"{'#':<4} {'LR':<8} {'Optimizer':<10} {'Val Acc':<10} {'Epochs':<7}\")\n",
    "print(\"-\" * 45)\n",
    "for idx, result in enumerate(step2_results):\n",
    "    c = result['config']\n",
    "    print(f\"{idx+1:<4} {c['lr']:<8} {c['optimizer']:<10} {result['val_acc']*100:<10.2f}% {result['epoch_stopped']:<7}\")\n",
    "print(f\"\\n>>> Best from Step 2: LR={final_lr}, Optimizer={best_optimizer}, Val Acc={best_step2['val_acc']*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7091121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Step 3: Hidden Dimension (Test Independently)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 3: HIDDEN DIMENSION TUNING\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Using best LR={final_lr}, Batch={best_batch_size}, Optimizer={best_optimizer} from Steps 1-2\")\n",
    "\n",
    "step3_configs = [\n",
    "    {'lr': final_lr, 'batch_size': best_batch_size, 'hidden_dim': 128, 'optimizer': best_optimizer},\n",
    "    {'lr': final_lr, 'batch_size': best_batch_size, 'hidden_dim': 256, 'optimizer': best_optimizer},\n",
    "    {'lr': final_lr, 'batch_size': best_batch_size, 'hidden_dim': 512, 'optimizer': best_optimizer},\n",
    "]\n",
    "\n",
    "step3_results = []\n",
    "for idx, config in enumerate(step3_configs):\n",
    "    # Set fixed seed for reproducibility\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "    \n",
    "    val_acc, epoch_stopped = train_and_evaluate(config, f\"Step 3 Config {idx+1}/{len(step3_configs)}\")\n",
    "    step3_results.append({\n",
    "        'config': config,\n",
    "        'val_acc': val_acc,\n",
    "        'epoch_stopped': epoch_stopped\n",
    "    })\n",
    "\n",
    "# Find best Hidden Dimension\n",
    "best_step3 = max(step3_results, key=lambda x: x['val_acc'])\n",
    "best_hidden_dim = best_step3['config']['hidden_dim']\n",
    "\n",
    "print(f\"\\n>>> Step 3 Results:\")\n",
    "print(f\"{'#':<4} {'Hidden Dim':<12} {'Val Acc':<10} {'Epochs':<7}\")\n",
    "print(\"-\" * 35)\n",
    "for idx, result in enumerate(step3_results):\n",
    "    c = result['config']\n",
    "    print(f\"{idx+1:<4} {c['hidden_dim']:<12} {result['val_acc']*100:<10.2f}% {result['epoch_stopped']:<7}\")\n",
    "print(f\"\\n>>> Best from Step 3: Hidden Dim={best_hidden_dim}, Val Acc={best_step3['val_acc']*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b97f2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Final Best Configuration Summary\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"HYPERPARAMETER TUNING COMPLETE - FINAL BEST CONFIGURATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "final_best_config = {\n",
    "    'lr': final_lr,\n",
    "    'batch_size': best_batch_size,\n",
    "    'hidden_dim': best_hidden_dim,\n",
    "    'optimizer': best_optimizer,\n",
    "    'max_epochs': MAX_EPOCHS,\n",
    "    'patience': PATIENCE\n",
    "}\n",
    "\n",
    "print(f\"\\n>>> Best Configuration Found:\")\n",
    "print(f\"    Learning Rate: {final_best_config['lr']}\")\n",
    "print(f\"    Batch Size: {final_best_config['batch_size']}\")\n",
    "print(f\"    Hidden Dimension: {final_best_config['hidden_dim']}\")\n",
    "print(f\"    Optimizer: {final_best_config['optimizer']}\")\n",
    "print(f\"    Max Epochs: {final_best_config['max_epochs']} (with early stopping, patience={final_best_config['patience']})\")\n",
    "print(f\"    Best Validation Accuracy: {best_step3['val_acc']*100:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SEQUENTIAL HYPERPARAMETER TUNING COMPLETE\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb78d4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Word Aggregation Method Comparison\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"WORD AGGREGATION METHOD COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Using best hyperparameters from tuning:\")\n",
    "print(f\"    LR={final_lr}, Batch={best_batch_size}, Hidden={best_hidden_dim}, Optimizer={best_optimizer}\")\n",
    "print(f\"    Max Epochs={MAX_EPOCHS}, Patience={PATIENCE}\")\n",
    "\n",
    "# Extended RNN Classifier with multiple aggregation methods\n",
    "class RNN_Classifier_Aggregation(nn.Module):\n",
    "    \"\"\"\n",
    "    RNN for topic classification with multiple aggregation strategies.\n",
    "    Uses pretrained embeddings (learnable/updated during training).\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, \n",
    "                 n_layers=1, dropout=0.0, padding_idx=0, pretrained_embeddings=None,\n",
    "                 aggregation='last'):\n",
    "        super(RNN_Classifier_Aggregation, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.aggregation = aggregation  # 'last', 'mean', 'max', 'attention'\n",
    "        \n",
    "        # Embedding layer initialized with pretrained embeddings\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "            # Make embeddings learnable (updated during training)\n",
    "            self.embedding.weight.requires_grad = True\n",
    "        \n",
    "        # RNN layer\n",
    "        self.rnn = nn.RNN(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=n_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.0  # No dropout in baseline\n",
    "        )\n",
    "        \n",
    "        # Attention mechanism for aggregation (only created if needed)\n",
    "        if aggregation == 'attention':\n",
    "            self.attention = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        # Fully connected output layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        # text: [batch_size, seq_len]\n",
    "        # text_lengths: [batch_size]\n",
    "        \n",
    "        # Embed the input\n",
    "        embedded = self.embedding(text)  # [batch_size, seq_len, embedding_dim]\n",
    "        \n",
    "        # Get dimensions\n",
    "        batch_size = embedded.size(0)\n",
    "        seq_len = embedded.size(1)\n",
    "        \n",
    "        # Handle text_lengths\n",
    "        text_lengths_flat = text_lengths.flatten().cpu().long()\n",
    "        if len(text_lengths_flat) != batch_size:\n",
    "            raise ValueError(\n",
    "                f\"text_lengths size mismatch: got {len(text_lengths_flat)} elements, \"\n",
    "                f\"expected {batch_size}\"\n",
    "            )\n",
    "        \n",
    "        # Clamp lengths\n",
    "        text_lengths_clamped = torch.clamp(text_lengths_flat, min=1, max=seq_len)\n",
    "        \n",
    "        # Move to same device as text for mask operations later\n",
    "        # Keep CPU version for pack_padded_sequence (requires CPU)\n",
    "        # Create device version for mask operations later\n",
    "        text_lengths_clamped_device = text_lengths_clamped.to(text.device)\n",
    "        # Pack the padded sequences\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, text_lengths_clamped, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        \n",
    "        # Pass through RNN\n",
    "        packed_output, hidden = self.rnn(packed_embedded)\n",
    "        # hidden: [n_layers, batch_size, hidden_dim]\n",
    "        \n",
    "        # Aggregate word representations to sentence representation\n",
    "        if self.aggregation == 'last':\n",
    "            # Use the last hidden state from the last layer\n",
    "            sentence_repr = hidden[-1]  # [batch_size, hidden_dim]\n",
    "            \n",
    "        elif self.aggregation == 'mean':\n",
    "            # Mean pooling over all outputs (ignoring padding)\n",
    "            # Unpack the sequences first\n",
    "            output, output_lengths = nn.utils.rnn.pad_packed_sequence(\n",
    "                packed_output, batch_first=True\n",
    "            )\n",
    "            # output: [batch_size, seq_len, hidden_dim]\n",
    "            \n",
    "            # Create mask for padding\n",
    "            mask = torch.arange(seq_len, device=text.device).unsqueeze(0) < text_lengths_clamped_device.unsqueeze(1)\n",
    "            mask = mask.unsqueeze(2).float()  # [batch_size, seq_len, 1]\n",
    "            \n",
    "            # Apply mask and compute mean\n",
    "            masked_output = output * mask\n",
    "            sum_output = masked_output.sum(dim=1)  # [batch_size, hidden_dim]\n",
    "            sentence_repr = sum_output / text_lengths_clamped_device.unsqueeze(1).float()\n",
    "            \n",
    "        elif self.aggregation == 'max':\n",
    "            # Max pooling over all outputs\n",
    "            output, output_lengths = nn.utils.rnn.pad_packed_sequence(\n",
    "                packed_output, batch_first=True\n",
    "            )\n",
    "            # output: [batch_size, seq_len, hidden_dim]\n",
    "            \n",
    "            # Create mask for padding (set padding to -inf before max)\n",
    "            mask = torch.arange(seq_len, device=text.device).unsqueeze(0) < text_lengths_clamped_device.unsqueeze(1)\n",
    "            mask = mask.unsqueeze(2).float()  # [batch_size, seq_len, 1]\n",
    "            \n",
    "            masked_output = output * mask + (1 - mask) * float('-inf')\n",
    "            sentence_repr, _ = torch.max(masked_output, dim=1)\n",
    "            \n",
    "        elif self.aggregation == 'attention':\n",
    "            # Attention mechanism\n",
    "            output, output_lengths = nn.utils.rnn.pad_packed_sequence(\n",
    "                packed_output, batch_first=True\n",
    "            )\n",
    "            # output: [batch_size, seq_len, hidden_dim]\n",
    "            \n",
    "            # Compute attention scores\n",
    "            attn_scores = self.attention(output).squeeze(2)  # [batch_size, seq_len]\n",
    "            \n",
    "            # Mask padding positions\n",
    "            mask = torch.arange(seq_len, device=text.device).unsqueeze(0) < text_lengths_clamped_device.unsqueeze(1)\n",
    "            attn_scores = attn_scores.masked_fill(~mask, float('-inf'))\n",
    "            \n",
    "            # Apply softmax\n",
    "            attn_weights = torch.softmax(attn_scores, dim=1).unsqueeze(1)  # [batch_size, 1, seq_len]\n",
    "            \n",
    "            # Weighted sum\n",
    "            sentence_repr = torch.bmm(attn_weights, output).squeeze(1)  # [batch_size, hidden_dim]\n",
    "        \n",
    "        # Pass through fully connected layer\n",
    "        output = self.fc(sentence_repr)  # [batch_size, output_dim]\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Test different aggregation methods\n",
    "aggregation_methods = ['last', 'mean', 'max', 'attention']\n",
    "\n",
    "print(f\"\\nTesting {len(aggregation_methods)} aggregation methods:\")\n",
    "for method in aggregation_methods:\n",
    "    print(f\"  - {method}\")\n",
    "\n",
    "aggregation_results = []\n",
    "\n",
    "for agg_method in aggregation_methods:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Testing Aggregation Method: {agg_method.upper()}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Set fixed seed for reproducibility - ensures consistent batch ordering\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "    \n",
    "    # Create iterators with best batch size\n",
    "    train_iter = data.BucketIterator(\n",
    "        train_data,\n",
    "        batch_size=best_batch_size,\n",
    "        sort_key=lambda x: len(x.text),\n",
    "        sort_within_batch=True,\n",
    "        shuffle=True,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    val_iter = data.BucketIterator(\n",
    "        validation_data,\n",
    "        batch_size=best_batch_size,\n",
    "        sort_key=lambda x: len(x.text),\n",
    "        sort_within_batch=True,\n",
    "        shuffle=False,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    test_iter = data.BucketIterator(\n",
    "        test_data,\n",
    "        batch_size=best_batch_size,\n",
    "        sort_key=lambda x: len(x.text),\n",
    "        sort_within_batch=True,\n",
    "        shuffle=False,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Create model with best hyperparameters and specific aggregation method\n",
    "    model = RNN_Classifier_Aggregation(\n",
    "        vocab_size=fasttext_vocab_size,\n",
    "        embedding_dim=embedding_dim,\n",
    "        hidden_dim=best_hidden_dim,\n",
    "        output_dim=num_classes,\n",
    "        n_layers=N_LAYERS,\n",
    "        dropout=0.0,  # Baseline: no dropout\n",
    "        padding_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "        pretrained_embeddings=pretrained_embeddings,\n",
    "        aggregation=agg_method\n",
    "    ).to(device)\n",
    "    \n",
    "    # Select optimizer with best learning rate\n",
    "    if best_optimizer == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=final_lr)\n",
    "    elif best_optimizer == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=final_lr, momentum=0.9)\n",
    "    elif best_optimizer == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=final_lr)\n",
    "    elif best_optimizer == 'Adagrad':\n",
    "        optimizer = optim.Adagrad(model.parameters(), lr=final_lr)\n",
    "    \n",
    "    # Training loop with early stopping\n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    print(f\"\\n>>> Training model with {agg_method} aggregation...\")\n",
    "    \n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        \n",
    "        for batch in train_iter:\n",
    "            text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(text, text_lengths)\n",
    "            loss = criterion(predictions, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        train_acc = accuracy_score(train_labels, train_preds)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_iter:\n",
    "                text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "                predictions = model(text, text_lengths)\n",
    "                loss = criterion(predictions, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                preds = torch.argmax(predictions, dim=1)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_acc = accuracy_score(val_labels, val_preds)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            # Save best model for this aggregation method\n",
    "            torch.save(model.state_dict(), f'rnn_agg_{agg_method}_best.pt')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(f\"    Early stopping at epoch {epoch+1}, best val acc: {best_val_acc*100:.2f}%\")\n",
    "                break\n",
    "        \n",
    "        # Print progress every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"    Epoch {epoch+1}: Train Acc={train_acc*100:.2f}%, Val Acc={val_acc*100:.2f}%\")\n",
    "    \n",
    "    # Load best model and evaluate on test set\n",
    "    model.load_state_dict(torch.load(f'rnn_agg_{agg_method}_best.pt'))\n",
    "    model.eval()\n",
    "    \n",
    "    test_loss = 0\n",
    "    test_preds = []\n",
    "    test_labels = []\n",
    "    test_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_iter:\n",
    "            text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "            predictions = model(text, text_lengths)\n",
    "            loss = criterion(predictions, labels)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            probs = torch.softmax(predictions, dim=1)\n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            test_preds.extend(preds.cpu().numpy())\n",
    "            test_labels.extend(labels.cpu().numpy())\n",
    "            test_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    # Calculate test metrics\n",
    "    test_acc = accuracy_score(test_labels, test_preds)\n",
    "    test_f1 = f1_score(test_labels, test_preds, average='weighted')\n",
    "    test_loss_avg = test_loss / len(test_iter)\n",
    "    \n",
    "    # Calculate AUC-ROC\n",
    "    try:\n",
    "        test_probs_array = np.array(test_probs)\n",
    "        test_labels_bin = label_binarize(test_labels, classes=range(num_classes))\n",
    "        test_auc = roc_auc_score(test_labels_bin, test_probs_array, average='weighted', multi_class='ovr')\n",
    "    except Exception as e:\n",
    "        print(f\"    Warning: Could not calculate AUC-ROC: {e}\")\n",
    "        test_auc = 0.0\n",
    "    \n",
    "    aggregation_results.append({\n",
    "        'method': agg_method,\n",
    "        'val_acc': best_val_acc,\n",
    "        'test_acc': test_acc,\n",
    "        'test_f1': test_f1,\n",
    "        'test_auc': test_auc,\n",
    "        'test_loss': test_loss_avg\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n>>> Results for {agg_method} aggregation:\")\n",
    "    print(f\"    Validation Acc: {best_val_acc*100:.2f}%\")\n",
    "    print(f\"    Test Acc: {test_acc*100:.2f}%\")\n",
    "    print(f\"    Test F1: {test_f1:.4f}\")\n",
    "    print(f\"    Test AUC-ROC: {test_auc:.4f}\")\n",
    "\n",
    "# Print summary table\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"AGGREGATION METHOD COMPARISON - RESULTS SUMMARY\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "print(f\"\\n>>> Results Summary:\")\n",
    "print(f\"{'Method':<12} {'Val Acc':<10} {'Test Acc':<10} {'Test F1':<10} {'Test AUC':<10}\")\n",
    "print(\"-\" * 55)\n",
    "for result in aggregation_results:\n",
    "    print(f\"{result['method']:<12} {result['val_acc']*100:<10.2f}% {result['test_acc']*100:<10.2f}% \"\n",
    "          f\"{result['test_f1']:<10.4f} {result['test_auc']:<10.4f}\")\n",
    "\n",
    "# Find best aggregation method\n",
    "best_aggregation = max(aggregation_results, key=lambda x: x['val_acc'])\n",
    "\n",
    "print(f\"\\n>>> Best Aggregation Method: {best_aggregation['method'].upper()}\")\n",
    "print(f\"    Validation Accuracy: {best_aggregation['val_acc']*100:.2f}%\")\n",
    "print(f\"    Test Accuracy: {best_aggregation['test_acc']*100:.2f}%\")\n",
    "print(f\"    Test F1 Score: {best_aggregation['test_f1']:.4f}\")\n",
    "print(f\"    Test AUC-ROC: {best_aggregation['test_auc']:.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"AGGREGATION METHOD COMPARISON COMPLETE\")\n",
    "print(f\"{'='*80}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77f7d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Regularization: Baseline (No Regularization)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"REGULARIZATION STEP 0: BASELINE (NO REGULARIZATION)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Using best hyperparameters from tuning:\")\n",
    "print(f\"    LR={final_lr}, Batch={best_batch_size}, Hidden={best_hidden_dim}, Optimizer={best_optimizer}\")\n",
    "print(f\"    Max Epochs={MAX_EPOCHS}, Patience={PATIENCE}\")\n",
    "print(f\"    Best Aggregation Method: {best_aggregation['method'].upper()}\")\n",
    "\n",
    "# Baseline configuration\n",
    "baseline_config = {\n",
    "    'dropout': 0.0,\n",
    "    'grad_clip': 0.0,\n",
    "    'l1_lambda': 0.0,\n",
    "    'l2_lambda': 0.0\n",
    "}\n",
    "\n",
    "print(f\"\\nBaseline Configuration:\")\n",
    "print(f\"    Dropout: {baseline_config['dropout']}\")\n",
    "print(f\"    Gradient Clipping: {baseline_config['grad_clip']}\")\n",
    "print(f\"    L1 Lambda: {baseline_config['l1_lambda']}\")\n",
    "print(f\"    L2 Lambda: {baseline_config['l2_lambda']}\")\n",
    "\n",
    "# Create iterators\n",
    "train_iter = data.BucketIterator(\n",
    "    train_data,\n",
    "    batch_size=best_batch_size,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    shuffle=True,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "val_iter = data.BucketIterator(\n",
    "    validation_data,\n",
    "    batch_size=best_batch_size,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    shuffle=False,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "test_iter = data.BucketIterator(\n",
    "    test_data,\n",
    "    batch_size=best_batch_size,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    shuffle=False,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Create model\n",
    "model = RNN_Classifier_Aggregation(\n",
    "    vocab_size=fasttext_vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_dim=best_hidden_dim,\n",
    "    output_dim=num_classes,\n",
    "    n_layers=N_LAYERS,\n",
    "    dropout=baseline_config['dropout'],\n",
    "    padding_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "    pretrained_embeddings=pretrained_embeddings,\n",
    "    aggregation=best_aggregation['method']\n",
    ").to(device)\n",
    "\n",
    "# Select optimizer\n",
    "if best_optimizer == 'Adam':\n",
    "    optimizer = optim.Adam(model.parameters(), lr=final_lr, weight_decay=baseline_config['l2_lambda'])\n",
    "elif best_optimizer == 'SGD':\n",
    "    optimizer = optim.SGD(model.parameters(), lr=final_lr, momentum=0.9, weight_decay=baseline_config['l2_lambda'])\n",
    "elif best_optimizer == 'RMSprop':\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=final_lr, weight_decay=baseline_config['l2_lambda'])\n",
    "elif best_optimizer == 'Adagrad':\n",
    "    optimizer = optim.Adagrad(model.parameters(), lr=final_lr, weight_decay=baseline_config['l2_lambda'])\n",
    "\n",
    "# Helper function for L1 regularization\n",
    "def compute_l1_loss(model, l1_lambda):\n",
    "    \"\"\"Compute L1 regularization loss\"\"\"\n",
    "    if l1_lambda > 0:\n",
    "        return l1_lambda * sum(p.abs().sum() for p in model.parameters() if p.requires_grad)\n",
    "    return 0.0\n",
    "\n",
    "# Training loop\n",
    "best_val_acc = 0.0\n",
    "patience_counter = 0\n",
    "\n",
    "print(f\"\\n>>> Training baseline model...\")\n",
    "\n",
    "for epoch in range(MAX_EPOCHS):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_preds = []\n",
    "    train_labels = []\n",
    "    \n",
    "    for batch in train_iter:\n",
    "        text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(text, text_lengths)\n",
    "        \n",
    "        loss = criterion(predictions, labels)\n",
    "        \n",
    "        # Add L1 regularization\n",
    "        if baseline_config['l1_lambda'] > 0:\n",
    "            loss = loss + compute_l1_loss(model, baseline_config['l1_lambda'])\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        if baseline_config['grad_clip'] > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), baseline_config['grad_clip'])\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        preds = torch.argmax(predictions, dim=1)\n",
    "        train_preds.extend(preds.cpu().numpy())\n",
    "        train_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    train_acc = accuracy_score(train_labels, train_preds)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_iter:\n",
    "            text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "            predictions = model(text, text_lengths)\n",
    "            loss = criterion(predictions, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    val_acc = accuracy_score(val_labels, val_preds)\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), 'rnn_reg_baseline_best.pt')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(f\"    Early stopping at epoch {epoch+1}, best val acc: {best_val_acc*100:.2f}%\")\n",
    "            break\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"    Epoch {epoch+1}: Train Acc={train_acc*100:.2f}%, Val Acc={val_acc*100:.2f}%\")\n",
    "\n",
    "# Evaluate on test set\n",
    "model.load_state_dict(torch.load('rnn_reg_baseline_best.pt'))\n",
    "model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "test_preds = []\n",
    "test_labels = []\n",
    "test_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_iter:\n",
    "        text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "        predictions = model(text, text_lengths)\n",
    "        loss = criterion(predictions, labels)\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        probs = torch.softmax(predictions, dim=1)\n",
    "        preds = torch.argmax(predictions, dim=1)\n",
    "        test_preds.extend(preds.cpu().numpy())\n",
    "        test_labels.extend(labels.cpu().numpy())\n",
    "        test_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "test_acc = accuracy_score(test_labels, test_preds)\n",
    "test_f1 = f1_score(test_labels, test_preds, average='weighted')\n",
    "test_loss_avg = test_loss / len(test_iter)\n",
    "\n",
    "try:\n",
    "    test_probs_array = np.array(test_probs)\n",
    "    test_labels_bin = label_binarize(test_labels, classes=range(num_classes))\n",
    "    test_auc = roc_auc_score(test_labels_bin, test_probs_array, average='weighted', multi_class='ovr')\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not calculate AUC-ROC: {e}\")\n",
    "    test_auc = 0.0\n",
    "\n",
    "baseline_results = {\n",
    "    'name': 'baseline',\n",
    "    'dropout': baseline_config['dropout'],\n",
    "    'grad_clip': baseline_config['grad_clip'],\n",
    "    'l1_lambda': baseline_config['l1_lambda'],\n",
    "    'l2_lambda': baseline_config['l2_lambda'],\n",
    "    'val_acc': best_val_acc,\n",
    "    'test_acc': test_acc,\n",
    "    'test_f1': test_f1,\n",
    "    'test_auc': test_auc\n",
    "}\n",
    "\n",
    "print(f\"\\n>>> Baseline Results:\")\n",
    "print(f\"    Validation Acc: {best_val_acc*100:.2f}%\")\n",
    "print(f\"    Test Acc: {test_acc*100:.2f}%\")\n",
    "print(f\"    Test F1: {test_f1:.4f}\")\n",
    "print(f\"    Test AUC-ROC: {test_auc:.4f}\")\n",
    "\n",
    "# Store for next steps\n",
    "best_grad_clip = baseline_config['grad_clip']  # Will be updated in Step 1\n",
    "best_dropout = baseline_config['dropout']  # Will be updated in Step 2\n",
    "best_l1_lambda = baseline_config['l1_lambda']  # Will be updated in Step 3\n",
    "best_l2_lambda = baseline_config['l2_lambda']  # Will be updated in Step 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1903f6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Regularization Step 1: Gradient Clipping Tuning\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"REGULARIZATION STEP 1: GRADIENT CLIPPING TUNING\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Using baseline settings: dropout={best_dropout}, L1={best_l1_lambda}, L2={best_l2_lambda}\")\n",
    "\n",
    "# Test different gradient clipping values\n",
    "grad_clip_options = [0.0, 1.0]  # 0.0 = no clipping, 1.0 = clip at 1.0\n",
    "\n",
    "print(f\"\\nTesting gradient clipping values: {grad_clip_options}\")\n",
    "\n",
    "step1_results = []\n",
    "\n",
    "for grad_clip in grad_clip_options:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Testing: Gradient Clipping = {grad_clip}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Create model with baseline settings\n",
    "    model = RNN_Classifier_Aggregation(\n",
    "        vocab_size=fasttext_vocab_size,\n",
    "        embedding_dim=embedding_dim,\n",
    "        hidden_dim=best_hidden_dim,\n",
    "        output_dim=num_classes,\n",
    "        n_layers=N_LAYERS,\n",
    "        dropout=best_dropout,\n",
    "        padding_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "        pretrained_embeddings=pretrained_embeddings,\n",
    "        aggregation=best_aggregation['method']\n",
    "    ).to(device)\n",
    "    \n",
    "    # Select optimizer\n",
    "    if best_optimizer == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=final_lr, weight_decay=best_l2_lambda)\n",
    "    elif best_optimizer == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=final_lr, momentum=0.9, weight_decay=best_l2_lambda)\n",
    "    elif best_optimizer == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=final_lr, weight_decay=best_l2_lambda)\n",
    "    elif best_optimizer == 'Adagrad':\n",
    "        optimizer = optim.Adagrad(model.parameters(), lr=final_lr, weight_decay=best_l2_lambda)\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        \n",
    "        for batch in train_iter:\n",
    "            text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(text, text_lengths)\n",
    "            \n",
    "            loss = criterion(predictions, labels)\n",
    "            \n",
    "            if best_l1_lambda > 0:\n",
    "                loss = loss + compute_l1_loss(model, best_l1_lambda)\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            if grad_clip > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        train_acc = accuracy_score(train_labels, train_preds)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_iter:\n",
    "                text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "                predictions = model(text, text_lengths)\n",
    "                loss = criterion(predictions, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                preds = torch.argmax(predictions, dim=1)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_acc = accuracy_score(val_labels, val_preds)\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), f'rnn_reg_step1_gradclip{grad_clip}_best.pt')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(f\"    Early stopping at epoch {epoch+1}, best val acc: {best_val_acc*100:.2f}%\")\n",
    "                break\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"    Epoch {epoch+1}: Train Acc={train_acc*100:.2f}%, Val Acc={val_acc*100:.2f}%\")\n",
    "    \n",
    "    step1_results.append({\n",
    "        'grad_clip': grad_clip,\n",
    "        'val_acc': best_val_acc\n",
    "    })\n",
    "    \n",
    "    print(f\"    Result: Val Acc={best_val_acc*100:.2f}%\")\n",
    "\n",
    "# Find best gradient clipping\n",
    "best_step1 = max(step1_results, key=lambda x: x['val_acc'])\n",
    "best_grad_clip = best_step1['grad_clip']\n",
    "\n",
    "print(f\"\\n>>> Step 1 Results:\")\n",
    "print(f\"{'Grad Clip':<12} {'Val Acc':<10}\")\n",
    "print(\"-\" * 25)\n",
    "for result in step1_results:\n",
    "    print(f\"{result['grad_clip']:<12} {result['val_acc']*100:<10.2f}%\")\n",
    "print(f\"\\n>>> Best Gradient Clipping: {best_grad_clip}, Val Acc={best_step1['val_acc']*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fccda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Regularization Step 2: Dropout Tuning\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"REGULARIZATION STEP 2: DROPOUT TUNING\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Using best from Step 1: grad_clip={best_grad_clip}\")\n",
    "print(f\"Using baseline settings: L1={best_l1_lambda}, L2={best_l2_lambda}\")\n",
    "\n",
    "# Test different dropout values\n",
    "dropout_options = [0.0, 0.3, 0.5, 0.7]\n",
    "\n",
    "print(f\"\\nTesting dropout values: {dropout_options}\")\n",
    "\n",
    "step2_results = []\n",
    "\n",
    "for dropout_val in dropout_options:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Testing: Dropout = {dropout_val}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Create model with best grad_clip and current dropout\n",
    "    model = RNN_Classifier_Aggregation(\n",
    "        vocab_size=fasttext_vocab_size,\n",
    "        embedding_dim=embedding_dim,\n",
    "        hidden_dim=best_hidden_dim,\n",
    "        output_dim=num_classes,\n",
    "        n_layers=N_LAYERS,\n",
    "        dropout=dropout_val,\n",
    "        padding_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "        pretrained_embeddings=pretrained_embeddings,\n",
    "        aggregation=best_aggregation['method']\n",
    "    ).to(device)\n",
    "    \n",
    "    # Select optimizer\n",
    "    if best_optimizer == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=final_lr, weight_decay=best_l2_lambda)\n",
    "    elif best_optimizer == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=final_lr, momentum=0.9, weight_decay=best_l2_lambda)\n",
    "    elif best_optimizer == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=final_lr, weight_decay=best_l2_lambda)\n",
    "    elif best_optimizer == 'Adagrad':\n",
    "        optimizer = optim.Adagrad(model.parameters(), lr=final_lr, weight_decay=best_l2_lambda)\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        \n",
    "        for batch in train_iter:\n",
    "            text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(text, text_lengths)\n",
    "            \n",
    "            loss = criterion(predictions, labels)\n",
    "            \n",
    "            if best_l1_lambda > 0:\n",
    "                loss = loss + compute_l1_loss(model, best_l1_lambda)\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            if best_grad_clip > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), best_grad_clip)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        train_acc = accuracy_score(train_labels, train_preds)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_iter:\n",
    "                text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "                predictions = model(text, text_lengths)\n",
    "                loss = criterion(predictions, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                preds = torch.argmax(predictions, dim=1)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_acc = accuracy_score(val_labels, val_preds)\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), f'rnn_reg_step2_dropout{dropout_val}_best.pt')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(f\"    Early stopping at epoch {epoch+1}, best val acc: {best_val_acc*100:.2f}%\")\n",
    "                break\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"    Epoch {epoch+1}: Train Acc={train_acc*100:.2f}%, Val Acc={val_acc*100:.2f}%\")\n",
    "    \n",
    "    step2_results.append({\n",
    "        'dropout': dropout_val,\n",
    "        'val_acc': best_val_acc\n",
    "    })\n",
    "    \n",
    "    print(f\"    Result: Val Acc={best_val_acc*100:.2f}%\")\n",
    "\n",
    "# Find best dropout\n",
    "best_step2 = max(step2_results, key=lambda x: x['val_acc'])\n",
    "best_dropout = best_step2['dropout']\n",
    "\n",
    "print(f\"\\n>>> Step 2 Results:\")\n",
    "print(f\"{'Dropout':<12} {'Val Acc':<10}\")\n",
    "print(\"-\" * 25)\n",
    "for result in step2_results:\n",
    "    print(f\"{result['dropout']:<12} {result['val_acc']*100:<10.2f}%\")\n",
    "print(f\"\\n>>> Best Dropout: {best_dropout}, Val Acc={best_step2['val_acc']*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0afecca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Regularization Step 3: L1 Regularization Tuning\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"REGULARIZATION STEP 3: L1 REGULARIZATION TUNING\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Using best from Steps 1-2: grad_clip={best_grad_clip}, dropout={best_dropout}\")\n",
    "print(f\"Using baseline setting: L2={best_l2_lambda}\")\n",
    "\n",
    "# Test different L1 lambda values\n",
    "l1_lambda_options = [0.0, 1e-6, 1e-5, 1e-4]\n",
    "\n",
    "print(f\"\\nTesting L1 lambda values: {l1_lambda_options}\")\n",
    "\n",
    "step3_results = []\n",
    "\n",
    "for l1_lambda in l1_lambda_options:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Testing: L1 Lambda = {l1_lambda}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Create model with best grad_clip, dropout, and current L1\n",
    "    model = RNN_Classifier_Aggregation(\n",
    "        vocab_size=fasttext_vocab_size,\n",
    "        embedding_dim=embedding_dim,\n",
    "        hidden_dim=best_hidden_dim,\n",
    "        output_dim=num_classes,\n",
    "        n_layers=N_LAYERS,\n",
    "        dropout=best_dropout,\n",
    "        padding_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "        pretrained_embeddings=pretrained_embeddings,\n",
    "        aggregation=best_aggregation['method']\n",
    "    ).to(device)\n",
    "    \n",
    "    # Select optimizer\n",
    "    if best_optimizer == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=final_lr, weight_decay=best_l2_lambda)\n",
    "    elif best_optimizer == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=final_lr, momentum=0.9, weight_decay=best_l2_lambda)\n",
    "    elif best_optimizer == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=final_lr, weight_decay=best_l2_lambda)\n",
    "    elif best_optimizer == 'Adagrad':\n",
    "        optimizer = optim.Adagrad(model.parameters(), lr=final_lr, weight_decay=best_l2_lambda)\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        \n",
    "        for batch in train_iter:\n",
    "            text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(text, text_lengths)\n",
    "            \n",
    "            loss = criterion(predictions, labels)\n",
    "            \n",
    "            if l1_lambda > 0:\n",
    "                loss = loss + compute_l1_loss(model, l1_lambda)\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            if best_grad_clip > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), best_grad_clip)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        train_acc = accuracy_score(train_labels, train_preds)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_iter:\n",
    "                text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "                predictions = model(text, text_lengths)\n",
    "                loss = criterion(predictions, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                preds = torch.argmax(predictions, dim=1)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_acc = accuracy_score(val_labels, val_preds)\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), f'rnn_reg_step3_l1{l1_lambda}_best.pt')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(f\"    Early stopping at epoch {epoch+1}, best val acc: {best_val_acc*100:.2f}%\")\n",
    "                break\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"    Epoch {epoch+1}: Train Acc={train_acc*100:.2f}%, Val Acc={val_acc*100:.2f}%\")\n",
    "    \n",
    "    step3_results.append({\n",
    "        'l1_lambda': l1_lambda,\n",
    "        'val_acc': best_val_acc\n",
    "    })\n",
    "    \n",
    "    print(f\"    Result: Val Acc={best_val_acc*100:.2f}%\")\n",
    "\n",
    "# Find best L1 lambda\n",
    "best_step3 = max(step3_results, key=lambda x: x['val_acc'])\n",
    "best_l1_lambda = best_step3['l1_lambda']\n",
    "\n",
    "print(f\"\\n>>> Step 3 Results:\")\n",
    "print(f\"{'L1 Lambda':<12} {'Val Acc':<10}\")\n",
    "print(\"-\" * 25)\n",
    "for result in step3_results:\n",
    "    print(f\"{result['l1_lambda']:<12.0e} {result['val_acc']*100:<10.2f}%\")\n",
    "print(f\"\\n>>> Best L1 Lambda: {best_l1_lambda}, Val Acc={best_step3['val_acc']*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71abaf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Regularization Step 4: L2 Regularization Tuning\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"REGULARIZATION STEP 4: L2 REGULARIZATION TUNING\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Using best from Steps 1-3: grad_clip={best_grad_clip}, dropout={best_dropout}, L1={best_l1_lambda}\")\n",
    "\n",
    "# Test different L2 lambda values (via weight_decay)\n",
    "l2_lambda_options = [0.0, 1e-5, 1e-4, 1e-3]\n",
    "\n",
    "print(f\"\\nTesting L2 lambda values: {l2_lambda_options}\")\n",
    "\n",
    "step4_results = []\n",
    "\n",
    "for l2_lambda in l2_lambda_options:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Testing: L2 Lambda = {l2_lambda}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Create model with best grad_clip, dropout, L1, and current L2\n",
    "    model = RNN_Classifier_Aggregation(\n",
    "        vocab_size=fasttext_vocab_size,\n",
    "        embedding_dim=embedding_dim,\n",
    "        hidden_dim=best_hidden_dim,\n",
    "        output_dim=num_classes,\n",
    "        n_layers=N_LAYERS,\n",
    "        dropout=best_dropout,\n",
    "        padding_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "        pretrained_embeddings=pretrained_embeddings,\n",
    "        aggregation=best_aggregation['method']\n",
    "    ).to(device)\n",
    "    \n",
    "    # Select optimizer with L2 regularization (weight_decay)\n",
    "    if best_optimizer == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=final_lr, weight_decay=l2_lambda)\n",
    "    elif best_optimizer == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=final_lr, momentum=0.9, weight_decay=l2_lambda)\n",
    "    elif best_optimizer == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=final_lr, weight_decay=l2_lambda)\n",
    "    elif best_optimizer == 'Adagrad':\n",
    "        optimizer = optim.Adagrad(model.parameters(), lr=final_lr, weight_decay=l2_lambda)\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        \n",
    "        for batch in train_iter:\n",
    "            text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(text, text_lengths)\n",
    "            \n",
    "            loss = criterion(predictions, labels)\n",
    "            \n",
    "            if best_l1_lambda > 0:\n",
    "                loss = loss + compute_l1_loss(model, best_l1_lambda)\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            if best_grad_clip > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), best_grad_clip)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        train_acc = accuracy_score(train_labels, train_preds)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_iter:\n",
    "                text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "                predictions = model(text, text_lengths)\n",
    "                loss = criterion(predictions, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                preds = torch.argmax(predictions, dim=1)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_acc = accuracy_score(val_labels, val_preds)\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), f'rnn_reg_step4_l2{l2_lambda}_best.pt')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(f\"    Early stopping at epoch {epoch+1}, best val acc: {best_val_acc*100:.2f}%\")\n",
    "                break\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"    Epoch {epoch+1}: Train Acc={train_acc*100:.2f}%, Val Acc={val_acc*100:.2f}%\")\n",
    "    \n",
    "    step4_results.append({\n",
    "        'l2_lambda': l2_lambda,\n",
    "        'val_acc': best_val_acc\n",
    "    })\n",
    "    \n",
    "    print(f\"    Result: Val Acc={best_val_acc*100:.2f}%\")\n",
    "\n",
    "# Find best L2 lambda\n",
    "best_step4 = max(step4_results, key=lambda x: x['val_acc'])\n",
    "best_l2_lambda = best_step4['l2_lambda']\n",
    "\n",
    "print(f\"\\n>>> Step 4 Results:\")\n",
    "print(f\"{'L2 Lambda':<12} {'Val Acc':<10}\")\n",
    "print(\"-\" * 25)\n",
    "for result in step4_results:\n",
    "    print(f\"{result['l2_lambda']:<12.0e} {result['val_acc']*100:<10.2f}%\")\n",
    "print(f\"\\n>>> Best L2 Lambda: {best_l2_lambda}, Val Acc={best_step4['val_acc']*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f3c33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Regularization Final: All Best Combined\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"REGULARIZATION FINAL: ALL BEST SETTINGS COMBINED\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Best settings from all steps:\")\n",
    "print(f\"    Gradient Clipping: {best_grad_clip}\")\n",
    "print(f\"    Dropout: {best_dropout}\")\n",
    "print(f\"    L1 Lambda: {best_l1_lambda}\")\n",
    "print(f\"    L2 Lambda: {best_l2_lambda}\")\n",
    "\n",
    "# Create model with all best settings\n",
    "model = RNN_Classifier_Aggregation(\n",
    "    vocab_size=fasttext_vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_dim=best_hidden_dim,\n",
    "    output_dim=num_classes,\n",
    "    n_layers=N_LAYERS,\n",
    "    dropout=best_dropout,\n",
    "    padding_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "    pretrained_embeddings=pretrained_embeddings,\n",
    "    aggregation=best_aggregation['method']\n",
    ").to(device)\n",
    "\n",
    "# Select optimizer with best L2 (weight_decay)\n",
    "if best_optimizer == 'Adam':\n",
    "    optimizer = optim.Adam(model.parameters(), lr=final_lr, weight_decay=best_l2_lambda)\n",
    "elif best_optimizer == 'SGD':\n",
    "    optimizer = optim.SGD(model.parameters(), lr=final_lr, momentum=0.9, weight_decay=best_l2_lambda)\n",
    "elif best_optimizer == 'RMSprop':\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=final_lr, weight_decay=best_l2_lambda)\n",
    "elif best_optimizer == 'Adagrad':\n",
    "    optimizer = optim.Adagrad(model.parameters(), lr=final_lr, weight_decay=best_l2_lambda)\n",
    "\n",
    "# Training loop\n",
    "best_val_acc = 0.0\n",
    "patience_counter = 0\n",
    "\n",
    "# Store training history for plotting\n",
    "train_losses = []\n",
    "val_accs = []\n",
    "\n",
    "print(f\"\\n>>> Training final model with all best regularization settings...\")\n",
    "\n",
    "for epoch in range(MAX_EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_preds = []\n",
    "    train_labels = []\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch in train_iter:\n",
    "        num_batches += 1\n",
    "        text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(text, text_lengths)\n",
    "        \n",
    "        loss = criterion(predictions, labels)\n",
    "        \n",
    "        if best_l1_lambda > 0:\n",
    "            loss = loss + compute_l1_loss(model, best_l1_lambda)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        if best_grad_clip > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), best_grad_clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        preds = torch.argmax(predictions, dim=1)\n",
    "        train_preds.extend(preds.cpu().numpy())\n",
    "        train_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    train_acc = accuracy_score(train_labels, train_preds)\n",
    "    # Calculate average training loss (train_loss is sum over all batches)\n",
    "    num_train_batches = len(train_labels) // train_iter.batch_size + (1 if len(train_labels) % train_iter.batch_size != 0 else 0)\n",
    "    train_loss_avg = train_loss / num_train_batches if num_train_batches > 0 else train_loss\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_iter:\n",
    "            text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "            predictions = model(text, text_lengths)\n",
    "            loss = criterion(predictions, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    val_acc = accuracy_score(val_labels, val_preds)\n",
    "    \n",
    "    # Store training history for plotting\n",
    "    train_losses.append(train_loss_avg)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), 'rnn_reg_final_best.pt')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(f\"    Early stopping at epoch {epoch+1}, best val acc: {best_val_acc*100:.2f}%\")\n",
    "            break\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"    Epoch {epoch+1}: Train Acc={train_acc*100:.2f}%, Val Acc={val_acc*100:.2f}%\")\n",
    "\n",
    "# Evaluate on test set\n",
    "model.load_state_dict(torch.load('rnn_reg_final_best.pt'))\n",
    "model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "test_preds = []\n",
    "test_labels = []\n",
    "test_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_iter:\n",
    "        text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "        predictions = model(text, text_lengths)\n",
    "        loss = criterion(predictions, labels)\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        probs = torch.softmax(predictions, dim=1)\n",
    "        preds = torch.argmax(predictions, dim=1)\n",
    "        test_preds.extend(preds.cpu().numpy())\n",
    "        test_labels.extend(labels.cpu().numpy())\n",
    "        test_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "test_acc = accuracy_score(test_labels, test_preds)\n",
    "test_f1 = f1_score(test_labels, test_preds, average='weighted')\n",
    "test_loss_avg = test_loss / len(test_iter)\n",
    "\n",
    "try:\n",
    "    test_probs_array = np.array(test_probs)\n",
    "    test_labels_bin = label_binarize(test_labels, classes=range(num_classes))\n",
    "    test_auc = roc_auc_score(test_labels_bin, test_probs_array, average='weighted', multi_class='ovr')\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not calculate AUC-ROC: {e}\")\n",
    "    test_auc = 0.0\n",
    "\n",
    "final_results = {\n",
    "    'name': 'final_combined',\n",
    "    'dropout': best_dropout,\n",
    "    'grad_clip': best_grad_clip,\n",
    "    'l1_lambda': best_l1_lambda,\n",
    "    'l2_lambda': best_l2_lambda,\n",
    "    'val_acc': best_val_acc,\n",
    "    'test_acc': test_acc,\n",
    "    'test_f1': test_f1,\n",
    "    'test_auc': test_auc\n",
    "}\n",
    "\n",
    "print(f\"\\n>>> Final Combined Results:\")\n",
    "print(f\"    Configuration:\")\n",
    "print(f\"      - Gradient Clipping: {best_grad_clip}\")\n",
    "print(f\"      - Dropout: {best_dropout}\")\n",
    "print(f\"      - L1 Lambda: {best_l1_lambda}\")\n",
    "print(f\"      - L2 Lambda: {best_l2_lambda}\")\n",
    "print(f\"    Validation Acc: {best_val_acc*100:.2f}%\")\n",
    "print(f\"    Test Acc: {test_acc*100:.2f}%\")\n",
    "print(f\"    Test F1: {test_f1:.4f}\")\n",
    "print(f\"    Test AUC-ROC: {test_auc:.4f}\")\n",
    "\n",
    "# Compare with baseline\n",
    "improvement = test_acc - baseline_results['test_acc']\n",
    "improvement_pct = (improvement / baseline_results['test_acc']) * 100 if baseline_results['test_acc'] > 0 else 0\n",
    "\n",
    "print(f\"\\n>>> Comparison with Baseline:\")\n",
    "print(f\"    Baseline Test Acc: {baseline_results['test_acc']*100:.2f}%\")\n",
    "print(f\"    Final Regularized Test Acc: {test_acc*100:.2f}%\")\n",
    "print(f\"    Improvement: {improvement*100:+.2f}% ({improvement_pct:+.2f}% relative)\")\n",
    "\n",
    "# Plot training curves for best configuration and regularization\n",
    "print(f\"\\n>>> Plotting training curves for best configuration and regularization...\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Training Loss vs Epochs\n",
    "epochs = range(1, len(train_losses) + 1)\n",
    "ax1.plot(epochs, train_losses, 'b-', label='Training Loss', linewidth=2, marker='o', markersize=4)\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Training Loss', fontsize=12)\n",
    "ax1.set_title('Training Loss Curve', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xlim(left=1)\n",
    "\n",
    "# Plot 2: Validation Accuracy vs Epochs\n",
    "ax2.plot(epochs, [acc*100 for acc in val_accs], 'r-', label='Validation Accuracy', linewidth=2, marker='s', markersize=4)\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Validation Accuracy (%)', fontsize=12)\n",
    "ax2.set_title('Validation Accuracy Curve', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xlim(left=1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('best_config_training_curves.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"    Saved training curves to 'best_config_training_curves.png'\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"REGULARIZATION TUNING COMPLETE\")\n",
    "print(f\"{'='*80}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e084ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Part 2(e): Topic-wise Accuracy Evaluation on Test Set\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 2(e): TOPIC-WISE ACCURACY EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load the best model from regularization tuning\n",
    "# Use the model from the previous cell (regularization final) if available\n",
    "# Otherwise, load it from the saved checkpoint\n",
    "\n",
    "print(\"\\n>>> Using model from regularization tuning...\")\n",
    "\n",
    "# Check if 'model' variable exists from the previous cell (cell 26)\n",
    "try:\n",
    "    # Try to use the model from the previous cell\n",
    "    if 'model' in locals() or 'model' in globals():\n",
    "        # Verify it's a valid model instance\n",
    "        if hasattr(model, 'embedding') and hasattr(model, 'eval'):\n",
    "            final_model = model\n",
    "            final_model.eval()\n",
    "            saved_vocab_size = final_model.embedding.weight.shape[0]\n",
    "            print(f\"    âœ“ Using existing model from previous cell\")\n",
    "            print(f\"    Model vocab size: {saved_vocab_size}\")\n",
    "        else:\n",
    "            raise AttributeError(\"Model exists but is not valid\")\n",
    "    else:\n",
    "        raise NameError(\"Model variable not found\")\n",
    "except (NameError, AttributeError):\n",
    "    # Model doesn't exist or is invalid, load from checkpoint\n",
    "    print(\"    Model not found in previous cell, loading from checkpoint...\")\n",
    "    try:\n",
    "        checkpoint = torch.load('weights/rnn_reg_final_best.pt', map_location=device)\n",
    "    except FileNotFoundError:\n",
    "        checkpoint = torch.load('rnn_reg_final_best.pt', map_location=device)\n",
    "    \n",
    "    # Infer configuration from saved state dict\n",
    "    saved_vocab_size = checkpoint['embedding.weight'].shape[0]\n",
    "    saved_hidden_dim = checkpoint['rnn.weight_ih_l0'].shape[0]\n",
    "    has_attention = 'attention.weight' in checkpoint\n",
    "    saved_aggregation = 'attention' if has_attention else 'last'\n",
    "    \n",
    "    # Recreate model\n",
    "    final_model = RNN_Classifier_Aggregation(\n",
    "        vocab_size=saved_vocab_size,\n",
    "        embedding_dim=embedding_dim,\n",
    "        hidden_dim=saved_hidden_dim,\n",
    "        output_dim=num_classes,\n",
    "        n_layers=N_LAYERS,\n",
    "        dropout=best_dropout,\n",
    "        padding_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "        pretrained_embeddings=None,\n",
    "        aggregation=saved_aggregation\n",
    "    ).to(device)\n",
    "    \n",
    "    final_model.load_state_dict(checkpoint, strict=True)\n",
    "    final_model.eval()\n",
    "    print(f\"    âœ“ Model loaded from checkpoint (vocab_size={saved_vocab_size})\")\n",
    "\n",
    "# Function to evaluate per topic\n",
    "def evaluate_per_topic(model, iterator, device, max_vocab_size=None):\n",
    "    \"\"\"\n",
    "    Evaluate model performance per topic category on the test set.\n",
    "    Returns a dictionary with accuracy for each topic.\n",
    "    \n",
    "    Args:\n",
    "        model: The trained model\n",
    "        iterator: Data iterator for test set\n",
    "        device: Device to run on\n",
    "        max_vocab_size: Maximum valid vocabulary size (to clip token indices)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Dictionary to store correct and total counts per topic\n",
    "    topic_correct = defaultdict(int)\n",
    "    topic_total = defaultdict(int)\n",
    "    \n",
    "    # Get label vocabulary for mapping\n",
    "    label_to_idx = LABEL.vocab.stoi\n",
    "    idx_to_label = LABEL.vocab.itos\n",
    "    \n",
    "    # Get <unk> token index for mapping out-of-range tokens\n",
    "    unk_idx = TEXT.vocab.stoi.get(TEXT.unk_token, 0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            # Process batch (should be on CPU from the iterator)\n",
    "            text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "            \n",
    "            # Clip token indices to valid range if max_vocab_size is specified\n",
    "            # This handles cases where the current vocab is larger than the saved model's vocab\n",
    "            if max_vocab_size is not None:\n",
    "                # Map any indices >= max_vocab_size to <unk> token\n",
    "                text = torch.where(text >= max_vocab_size, \n",
    "                                 torch.tensor(unk_idx, device=text.device, dtype=text.dtype), \n",
    "                                 text)\n",
    "                # Also ensure no negative indices\n",
    "                text = torch.clamp(text, min=0)\n",
    "            \n",
    "            # Move tensors to the actual device after clipping\n",
    "            text = text.to(device)\n",
    "            text_lengths = text_lengths.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            predictions = model(text, text_lengths)\n",
    "            \n",
    "            # Get predicted labels\n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            \n",
    "            # Convert to numpy for easier processing\n",
    "            preds_np = preds.cpu().numpy()\n",
    "            labels_np = labels.cpu().numpy()\n",
    "            \n",
    "            # Count correct and total for each topic\n",
    "            for pred_idx, true_idx in zip(preds_np, labels_np):\n",
    "                true_label = idx_to_label[true_idx]\n",
    "                topic_total[true_label] += 1\n",
    "                \n",
    "                if pred_idx == true_idx:\n",
    "                    topic_correct[true_label] += 1\n",
    "    \n",
    "    # Calculate accuracy per topic\n",
    "    topic_accuracies = {}\n",
    "    for label in sorted(topic_total.keys()):\n",
    "        if topic_total[label] > 0:\n",
    "            acc = topic_correct[label] / topic_total[label]\n",
    "            topic_accuracies[label] = {\n",
    "                'accuracy': acc,\n",
    "                'correct': topic_correct[label],\n",
    "                'total': topic_total[label]\n",
    "            }\n",
    "    \n",
    "    return topic_accuracies\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"\\n>>> Evaluating on test set...\")\n",
    "\n",
    "# Create a CPU iterator to avoid CUDA errors during numericalization with invalid token indices\n",
    "# We'll process on CPU, clip indices, then move to device\n",
    "from torchtext import data\n",
    "cpu_device = torch.device('cpu')\n",
    "test_iter_cpu = data.BucketIterator(\n",
    "    test_data,\n",
    "    batch_size=test_iter.batch_size,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    device=cpu_device,\n",
    "    sort=False,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Pass max_vocab_size to clip token indices to the saved model's vocabulary size\n",
    "topic_accuracies = evaluate_per_topic(final_model, test_iter_cpu, device, max_vocab_size=saved_vocab_size)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TOPIC-WISE ACCURACY ON TEST SET\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Topic':<10} {'Accuracy':<12} {'Correct':<10} {'Total':<10} {'% of Test Set':<15}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Calculate total test samples for percentage calculation\n",
    "total_test_samples = sum(acc['total'] for acc in topic_accuracies.values())\n",
    "\n",
    "for topic in sorted(topic_accuracies.keys()):\n",
    "    acc_info = topic_accuracies[topic]\n",
    "    acc_pct = acc_info['accuracy'] * 100\n",
    "    correct = acc_info['correct']\n",
    "    total = acc_info['total']\n",
    "    pct_of_test = (total / total_test_samples) * 100 if total_test_samples > 0 else 0\n",
    "    \n",
    "    print(f\"{topic:<10} {acc_pct:<12.2f} {correct:<10} {total:<10} {pct_of_test:<15.2f}\")\n",
    "\n",
    "# Calculate overall accuracy\n",
    "overall_correct = sum(acc_info['correct'] for acc_info in topic_accuracies.values())\n",
    "overall_total = sum(acc_info['total'] for acc_info in topic_accuracies.values())\n",
    "overall_acc = overall_correct / overall_total if overall_total > 0 else 0\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'OVERALL':<10} {overall_acc*100:<12.2f} {overall_correct:<10} {overall_total:<10} {'100.00':<15}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Discussion: What may cause differences in accuracies across topics\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DISCUSSION: FACTORS AFFECTING TOPIC-WISE ACCURACY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get training distribution for comparison\n",
    "train_label_counts = Counter([ex.label for ex in train_data.examples])\n",
    "total_train = len(train_data.examples)\n",
    "\n",
    "print(\"\\n1. CLASS IMBALANCE IN TRAINING DATA:\")\n",
    "print(f\"{'Topic':<10} {'Train Count':<15} {'Train %':<12} {'Test Count':<12} {'Test %':<12}\")\n",
    "print(\"-\" * 70)\n",
    "for topic in sorted(topic_accuracies.keys()):\n",
    "    train_count = train_label_counts.get(topic, 0)\n",
    "    train_pct = (train_count / total_train) * 100 if total_train > 0 else 0\n",
    "    test_count = topic_accuracies[topic]['total']\n",
    "    test_pct = (test_count / total_test_samples) * 100 if total_test_samples > 0 else 0\n",
    "    print(f\"{topic:<10} {train_count:<15} {train_pct:<12.2f} {test_count:<12} {test_pct:<12.2f}\")\n",
    "\n",
    "print(\"\\n2. KEY OBSERVATIONS AND POTENTIAL CAUSES:\")\n",
    "print(\"\\n   a) Class Imbalance Effect:\")\n",
    "print(\"      - Topics with fewer training examples (e.g., ABBR with only 1.58% of data)\")\n",
    "print(\"        may have lower accuracy due to insufficient learning signal\")\n",
    "print(\"      - The model may be biased toward more frequent classes during training\")\n",
    "\n",
    "print(\"\\n   b) Semantic Complexity:\")\n",
    "print(\"      - Some topics may have more ambiguous or overlapping characteristics\")\n",
    "print(\"      - For example, ABBR (abbreviations) might be confused with other categories\")\n",
    "print(\"        if the context is not clear enough\")\n",
    "\n",
    "print(\"\\n   c) Vocabulary and OOV Rates:\")\n",
    "print(\"      - Topics with higher OOV rates (as seen in Part 1) may have lower accuracy\")\n",
    "print(\"      - ABBR had 9.70% OOV rate (highest), which could contribute to lower performance\")\n",
    "\n",
    "print(\"\\n   d) Question Type Characteristics:\")\n",
    "print(\"      - ENTY (entities) and HUM (humans) are more distinct and may be easier to classify\")\n",
    "print(\"      - DESC (descriptions) might overlap with other categories semantically\")\n",
    "print(\"      - NUM (numeric) questions may have distinctive patterns that aid classification\")\n",
    "\n",
    "print(\"\\n   e) Model Capacity and Representation:\")\n",
    "print(\"      - The RNN may capture certain patterns better than others\")\n",
    "print(\"      - Aggregation method (attention/mean/max/last) may favor certain topic structures\")\n",
    "\n",
    "print(\"\\n3. RECOMMENDATIONS FOR IMPROVEMENT:\")\n",
    "print(\"   - Use class weights in loss function to handle imbalance\")\n",
    "print(\"   - Apply data augmentation or oversampling for minority classes\")\n",
    "print(\"   - Consider focal loss to focus on hard examples\")\n",
    "print(\"   - Fine-tune embeddings specifically for underrepresented topics\")\n",
    "print(\"   - Use ensemble methods combining multiple models\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TOPIC-WISE EVALUATION COMPLETE\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ba54e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Part 3.1: biLSTM and biGRU Models\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 3.1: biLSTM AND biGRU MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# biLSTM Model Class\n",
    "class BiLSTMClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Bidirectional LSTM for topic classification.\n",
    "    Uses pretrained embeddings (learnable/updated during training) with OOV mitigation.\n",
    "    Incorporates recurrent computations in both directions and supports multiple layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, \n",
    "                 n_layers=2, dropout=0.5, padding_idx=0, pretrained_embeddings=None):\n",
    "        super(BiLSTMClassifier, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.bidirectional = True\n",
    "        \n",
    "        # Embedding layer initialized with pretrained embeddings\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "            # Make embeddings learnable (updated during training)\n",
    "            self.embedding.weight.requires_grad = True\n",
    "        \n",
    "        # Bidirectional LSTM layer with multiple layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=n_layers,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if n_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Fully connected output layer\n",
    "        # For bidirectional LSTM, output dimension is hidden_dim * 2\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        # text: [batch_size, seq_len]\n",
    "        # text_lengths: [batch_size]\n",
    "        \n",
    "        # Embed the input\n",
    "        embedded = self.embedding(text)  # [batch_size, seq_len, embedding_dim]\n",
    "        \n",
    "        # Get dimensions\n",
    "        batch_size = embedded.size(0)\n",
    "        seq_len = embedded.size(1)\n",
    "        \n",
    "        # Handle text_lengths\n",
    "        text_lengths_flat = text_lengths.flatten().cpu().long()\n",
    "        if len(text_lengths_flat) != batch_size:\n",
    "            raise ValueError(\n",
    "                f\"text_lengths size mismatch: got {len(text_lengths_flat)} elements, \"\n",
    "                f\"expected {batch_size}\"\n",
    "            )\n",
    "        \n",
    "        # Clamp lengths\n",
    "        text_lengths_clamped = torch.clamp(text_lengths_flat, min=1, max=seq_len)\n",
    "        \n",
    "        # Pack the padded sequences\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, text_lengths_clamped, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        \n",
    "        # Pass through bidirectional LSTM\n",
    "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
    "        # hidden: [n_layers * 2, batch_size, hidden_dim] (2 for bidirectional)\n",
    "        # For bidirectional LSTM, we concatenate forward and backward hidden states\n",
    "        # from the last layer\n",
    "        forward_hidden = hidden[-2]  # [batch_size, hidden_dim]\n",
    "        backward_hidden = hidden[-1]  # [batch_size, hidden_dim]\n",
    "        last_hidden = torch.cat([forward_hidden, backward_hidden], dim=1)  # [batch_size, hidden_dim * 2]\n",
    "        \n",
    "        # Apply dropout\n",
    "        last_hidden = self.dropout(last_hidden)\n",
    "        \n",
    "        # Pass through fully connected layer\n",
    "        output = self.fc(last_hidden)  # [batch_size, output_dim]\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "# biGRU Model Class\n",
    "class BiGRUClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Bidirectional GRU for topic classification.\n",
    "    Uses pretrained embeddings (learnable/updated during training) with OOV mitigation.\n",
    "    Incorporates recurrent computations in both directions and supports multiple layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, \n",
    "                 n_layers=2, dropout=0.5, padding_idx=0, pretrained_embeddings=None):\n",
    "        super(BiGRUClassifier, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.bidirectional = True\n",
    "        \n",
    "        # Embedding layer initialized with pretrained embeddings\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "            # Make embeddings learnable (updated during training)\n",
    "            self.embedding.weight.requires_grad = True\n",
    "        \n",
    "        # Bidirectional GRU layer with multiple layers\n",
    "        self.gru = nn.GRU(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=n_layers,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if n_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Fully connected output layer\n",
    "        # For bidirectional GRU, output dimension is hidden_dim * 2\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        # text: [batch_size, seq_len]\n",
    "        # text_lengths: [batch_size]\n",
    "        \n",
    "        # Embed the input\n",
    "        embedded = self.embedding(text)  # [batch_size, seq_len, embedding_dim]\n",
    "        \n",
    "        # Get dimensions\n",
    "        batch_size = embedded.size(0)\n",
    "        seq_len = embedded.size(1)\n",
    "        \n",
    "        # Handle text_lengths\n",
    "        text_lengths_flat = text_lengths.flatten().cpu().long()\n",
    "        if len(text_lengths_flat) != batch_size:\n",
    "            raise ValueError(\n",
    "                f\"text_lengths size mismatch: got {len(text_lengths_flat)} elements, \"\n",
    "                f\"expected {batch_size}\"\n",
    "            )\n",
    "        \n",
    "        # Clamp lengths\n",
    "        text_lengths_clamped = torch.clamp(text_lengths_flat, min=1, max=seq_len)\n",
    "        \n",
    "        # Pack the padded sequences\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, text_lengths_clamped, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        \n",
    "        # Pass through bidirectional GRU\n",
    "        packed_output, hidden = self.gru(packed_embedded)\n",
    "        # hidden: [n_layers * 2, batch_size, hidden_dim] (2 for bidirectional)\n",
    "        # For bidirectional GRU, we concatenate forward and backward hidden states\n",
    "        # from the last layer\n",
    "        forward_hidden = hidden[-2]  # [batch_size, hidden_dim]\n",
    "        backward_hidden = hidden[-1]  # [batch_size, hidden_dim]\n",
    "        last_hidden = torch.cat([forward_hidden, backward_hidden], dim=1)  # [batch_size, hidden_dim * 2]\n",
    "        \n",
    "        # Apply dropout\n",
    "        last_hidden = self.dropout(last_hidden)\n",
    "        \n",
    "        # Pass through fully connected layer\n",
    "        output = self.fc(last_hidden)  # [batch_size, output_dim]\n",
    "        \n",
    "        return output\n",
    "\n",
    "print(\">>> Model classes created: BiLSTMClassifier and BiGRUClassifier\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ac7c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function with history tracking for plotting curves\n",
    "def train_model_with_history(model, train_iterator, val_iterator, optimizer, criterion, \n",
    "                             n_epochs, device, patience=10, model_name=\"model\"):\n",
    "    \"\"\"\n",
    "    Train the model with early stopping and track training history.\n",
    "    Returns: model, training_history dictionary\n",
    "    \"\"\"\n",
    "    best_val_acc = 0.0\n",
    "    best_val_f1 = 0.0\n",
    "    best_val_auc = 0.0\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    # History tracking\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "    val_f1s = []  # Add F1 tracking\n",
    "    val_aucs = []  # Add AUROC tracking\n",
    "    \n",
    "    print(f\"\\n>>> Training {model_name}\")\n",
    "    print(f\"    Parameters: {count_parameters(model):,}\")\n",
    "    print(f\"    Max epochs: {n_epochs}, Patience: {patience}\")\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        \n",
    "        for batch in train_iterator:\n",
    "            text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(text, text_lengths)\n",
    "            loss = criterion(predictions, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_iterator)\n",
    "        train_acc = accuracy_score(train_labels, train_preds)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        val_probs = []  # Add probabilities for AUROC\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_iterator:\n",
    "                text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "                predictions = model(text, text_lengths)\n",
    "                loss = criterion(predictions, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                probs = torch.softmax(predictions, dim=1)  # Get probabilities\n",
    "                preds = torch.argmax(predictions, dim=1)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "                val_probs.extend(probs.cpu().numpy())  # Store probabilities\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_iterator)\n",
    "        val_acc = accuracy_score(val_labels, val_preds)\n",
    "        val_f1 = f1_score(val_labels, val_preds, average='weighted')  # Calculate F1\n",
    "        \n",
    "        # Calculate AUC-ROC\n",
    "        try:\n",
    "            val_probs_array = np.array(val_probs)\n",
    "            val_labels_bin = label_binarize(val_labels, classes=range(num_classes))\n",
    "            val_auc = roc_auc_score(val_labels_bin, val_probs_array, average='weighted', multi_class='ovr')\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not calculate AUC-ROC for {model_name} at epoch {epoch+1}: {e}\")\n",
    "            val_auc = 0.0\n",
    "        \n",
    "        # Store history\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "        val_f1s.append(val_f1)  # Store F1\n",
    "        val_aucs.append(val_auc)  # Store AUROC\n",
    "        \n",
    "        # Early stopping and model saving (using accuracy for early stopping)\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_val_f1 = val_f1\n",
    "            best_val_auc = val_auc\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict().copy()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        end_time = time.time()\n",
    "        epoch_mins, epoch_secs = divmod(end_time - start_time, 60)\n",
    "        \n",
    "        print(f'Epoch: {epoch+1:02}/{n_epochs} | Time: {int(epoch_mins)}m {int(epoch_secs)}s')\n",
    "        print(f'\\tTrain Loss: {avg_train_loss:.4f} | Train Acc: {train_acc*100:.2f}%')\n",
    "        print(f'\\tVal Loss: {avg_val_loss:.4f} | Val Acc: {val_acc*100:.2f}% | Val F1: {val_f1:.4f} | Val AUC: {val_auc:.4f}')\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f'\\t>>> Early stopping at epoch {epoch+1}, best val acc: {best_val_acc*100:.2f}%')\n",
    "            break\n",
    "    \n",
    "    # Load best model state\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    print(f'\\n>>> Training completed! Best validation accuracy: {best_val_acc*100:.2f}%')\n",
    "    print(f'    Best validation F1: {best_val_f1:.4f}')\n",
    "    print(f'    Best validation AUC-ROC: {best_val_auc:.4f}')\n",
    "    \n",
    "    # Return history dictionary\n",
    "    history = {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'val_accs': val_accs,\n",
    "        'val_f1s': val_f1s,  # Add F1 history\n",
    "        'val_aucs': val_aucs,  # Add AUROC history\n",
    "        'best_val_acc': best_val_acc,\n",
    "        'best_val_f1': best_val_f1,  # Add best F1\n",
    "        'best_val_auc': best_val_auc,  # Add best AUROC\n",
    "        'epochs_trained': len(train_losses)\n",
    "    }\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "print(\">>> Training function with history tracking created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac34c4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Train biLSTM Model\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING biLSTM MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Use best hyperparameters from Part 2 (or reasonable defaults)\n",
    "# Based on Part 2 results, using: LR=0.0001, Batch=32, Hidden=512, but we'll use Hidden=256 for biLSTM\n",
    "# since bidirectional doubles the effective hidden size\n",
    "BILSTM_HIDDEN_DIM = 256  # Effective hidden size will be 512 (256*2) due to bidirectional\n",
    "BILSTM_N_LAYERS = 2  # Stack multiple layers\n",
    "BILSTM_DROPOUT = 0.5\n",
    "BILSTM_BATCH_SIZE = 64\n",
    "BILSTM_LEARNING_RATE = 0.001\n",
    "BILSTM_N_EPOCHS = 100\n",
    "BILSTM_PATIENCE = 10\n",
    "\n",
    "# Set seed for reproducibility\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "# Create iterators\n",
    "train_iter_bilstm = data.BucketIterator(\n",
    "    train_data,\n",
    "    batch_size=BILSTM_BATCH_SIZE,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    shuffle=True,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "val_iter_bilstm = data.BucketIterator(\n",
    "    validation_data,\n",
    "    batch_size=BILSTM_BATCH_SIZE,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    shuffle=False,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "test_iter_bilstm = data.BucketIterator(\n",
    "    test_data,\n",
    "    batch_size=BILSTM_BATCH_SIZE,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    shuffle=False,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Create biLSTM model\n",
    "bilstm_model = BiLSTMClassifier(\n",
    "    vocab_size=fasttext_vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_dim=BILSTM_HIDDEN_DIM,\n",
    "    output_dim=num_classes,\n",
    "    n_layers=BILSTM_N_LAYERS,\n",
    "    dropout=BILSTM_DROPOUT,\n",
    "    padding_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "    pretrained_embeddings=pretrained_embeddings\n",
    ").to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "bilstm_optimizer = optim.Adam(bilstm_model.parameters(), lr=BILSTM_LEARNING_RATE)\n",
    "\n",
    "print(f\"\\n>>> biLSTM Model Configuration:\")\n",
    "print(f\"    Hidden Dim: {BILSTM_HIDDEN_DIM} (effective: {BILSTM_HIDDEN_DIM * 2} due to bidirectional)\")\n",
    "print(f\"    Layers: {BILSTM_N_LAYERS}\")\n",
    "print(f\"    Dropout: {BILSTM_DROPOUT}\")\n",
    "print(f\"    Learning Rate: {BILSTM_LEARNING_RATE}\")\n",
    "print(f\"    Batch Size: {BILSTM_BATCH_SIZE}\")\n",
    "print(f\"    Max Epochs: {BILSTM_N_EPOCHS}, Patience: {BILSTM_PATIENCE}\")\n",
    "\n",
    "# Train biLSTM\n",
    "bilstm_model, bilstm_history = train_model_with_history(\n",
    "    bilstm_model, train_iter_bilstm, val_iter_bilstm, bilstm_optimizer, criterion,\n",
    "    BILSTM_N_EPOCHS, device, patience=BILSTM_PATIENCE, model_name=\"biLSTM\"\n",
    ")\n",
    "\n",
    "# Save best model\n",
    "torch.save(bilstm_model.state_dict(), 'weights/bilstm_best.pt')\n",
    "print(f\"\\n>>> biLSTM model saved to 'weights/bilstm_best.pt'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd47f564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Train biGRU Model\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING biGRU MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Use same hyperparameters as biLSTM for fair comparison\n",
    "BIGRU_HIDDEN_DIM = 256  # Effective hidden size will be 512 (256*2) due to bidirectional\n",
    "BIGRU_N_LAYERS = 2  # Stack multiple layers\n",
    "BIGRU_DROPOUT = 0.5\n",
    "BIGRU_BATCH_SIZE = 64\n",
    "BIGRU_LEARNING_RATE = 0.001\n",
    "BIGRU_N_EPOCHS = 100\n",
    "BIGRU_PATIENCE = 10\n",
    "\n",
    "# Set seed for reproducibility\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "# Create iterators\n",
    "train_iter_bigru = data.BucketIterator(\n",
    "    train_data,\n",
    "    batch_size=BIGRU_BATCH_SIZE,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    shuffle=True,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "val_iter_bigru = data.BucketIterator(\n",
    "    validation_data,\n",
    "    batch_size=BIGRU_BATCH_SIZE,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    shuffle=False,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "test_iter_bigru = data.BucketIterator(\n",
    "    test_data,\n",
    "    batch_size=BIGRU_BATCH_SIZE,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    shuffle=False,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Create biGRU model\n",
    "bigru_model = BiGRUClassifier(\n",
    "    vocab_size=fasttext_vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_dim=BIGRU_HIDDEN_DIM,\n",
    "    output_dim=num_classes,\n",
    "    n_layers=BIGRU_N_LAYERS,\n",
    "    dropout=BIGRU_DROPOUT,\n",
    "    padding_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "    pretrained_embeddings=pretrained_embeddings\n",
    ").to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "bigru_optimizer = optim.Adam(bigru_model.parameters(), lr=BIGRU_LEARNING_RATE)\n",
    "\n",
    "print(f\"\\n>>> biGRU Model Configuration:\")\n",
    "print(f\"    Hidden Dim: {BIGRU_HIDDEN_DIM} (effective: {BIGRU_HIDDEN_DIM * 2} due to bidirectional)\")\n",
    "print(f\"    Layers: {BIGRU_N_LAYERS}\")\n",
    "print(f\"    Dropout: {BIGRU_DROPOUT}\")\n",
    "print(f\"    Learning Rate: {BIGRU_LEARNING_RATE}\")\n",
    "print(f\"    Batch Size: {BIGRU_BATCH_SIZE}\")\n",
    "print(f\"    Max Epochs: {BIGRU_N_EPOCHS}, Patience: {BIGRU_PATIENCE}\")\n",
    "\n",
    "# Train biGRU\n",
    "bigru_model, bigru_history = train_model_with_history(\n",
    "    bigru_model, train_iter_bigru, val_iter_bigru, bigru_optimizer, criterion,\n",
    "    BIGRU_N_EPOCHS, device, patience=BIGRU_PATIENCE, model_name=\"biGRU\"\n",
    ")\n",
    "\n",
    "# Save best model\n",
    "torch.save(bigru_model.state_dict(), 'weights/bigru_best.pt')\n",
    "print(f\"\\n>>> biGRU model saved to 'weights/bigru_best.pt'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f763e2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Plot Training Curves for biLSTM and biGRU\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PLOTTING TRAINING CURVES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create figure with subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Training Loss Curves\n",
    "axes[0, 0].plot(bilstm_history['train_losses'], label='biLSTM Train Loss', marker='o', linewidth=2)\n",
    "axes[0, 0].plot(bigru_history['train_losses'], label='biGRU Train Loss', marker='s', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Training Loss', fontsize=12)\n",
    "axes[0, 0].set_title('Training Loss Curves', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].legend(fontsize=11)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Validation Accuracy Curves\n",
    "axes[0, 1].plot([acc*100 for acc in bilstm_history['val_accs']], label='biLSTM Val Acc', marker='o', linewidth=2)\n",
    "axes[0, 1].plot([acc*100 for acc in bigru_history['val_accs']], label='biGRU Val Acc', marker='s', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Validation Accuracy (%)', fontsize=12)\n",
    "axes[0, 1].set_title('Validation Accuracy Curves', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].legend(fontsize=11)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: biLSTM Training and Validation Loss\n",
    "axes[1, 0].plot(bilstm_history['train_losses'], label='biLSTM Train Loss', marker='o', linewidth=2)\n",
    "axes[1, 0].plot(bilstm_history['val_losses'], label='biLSTM Val Loss', marker='s', linewidth=2)\n",
    "axes[1, 0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Loss', fontsize=12)\n",
    "axes[1, 0].set_title('biLSTM: Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].legend(fontsize=11)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: biGRU Training and Validation Loss\n",
    "axes[1, 1].plot(bigru_history['train_losses'], label='biGRU Train Loss', marker='o', linewidth=2)\n",
    "axes[1, 1].plot(bigru_history['val_losses'], label='biGRU Val Loss', marker='s', linewidth=2)\n",
    "axes[1, 1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Loss', fontsize=12)\n",
    "axes[1, 1].set_title('biGRU: Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].legend(fontsize=11)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('part3_1_bilstm_bigru_training_curves.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"    Saved training curves to 'part3_1_bilstm_bigru_training_curves.png'\")\n",
    "plt.show()\n",
    "\n",
    "# Also create separate plots for Part 3.1 requirement (training loss and validation accuracy)\n",
    "fig2, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Training Loss Curve\n",
    "ax1.plot(bilstm_history['train_losses'], label='biLSTM', marker='o', linewidth=2)\n",
    "ax1.plot(bigru_history['train_losses'], label='biGRU', marker='s', linewidth=2)\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Training Loss', fontsize=12)\n",
    "ax1.set_title('Training Loss Curve (Part 3.1)', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Validation Accuracy Curve\n",
    "ax2.plot([acc*100 for acc in bilstm_history['val_accs']], label='biLSTM', marker='o', linewidth=2)\n",
    "ax2.plot([acc*100 for acc in bigru_history['val_accs']], label='biGRU', marker='s', linewidth=2)\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Validation Accuracy (%)', fontsize=12)\n",
    "ax2.set_title('Validation Accuracy Curve (Part 3.1)', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('part3_1_training_curves.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"    Saved Part 3.1 curves to 'part3_1_training_curves.png'\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94912510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Evaluate biLSTM and biGRU on Test Set\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST SET EVALUATION - biLSTM AND biGRU\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def evaluate_model(model, iterator, criterion, device, model_name):\n",
    "    \"\"\"Evaluate model on test set and return metrics\"\"\"\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_preds = []\n",
    "    test_labels = []\n",
    "    test_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "            predictions = model(text, text_lengths)\n",
    "            loss = criterion(predictions, labels)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            probs = torch.softmax(predictions, dim=1)\n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            test_preds.extend(preds.cpu().numpy())\n",
    "            test_labels.extend(labels.cpu().numpy())\n",
    "            test_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    avg_test_loss = test_loss / len(iterator)\n",
    "    test_acc = accuracy_score(test_labels, test_preds)\n",
    "    test_f1 = f1_score(test_labels, test_preds, average='weighted')\n",
    "    \n",
    "    # Calculate AUC-ROC\n",
    "    try:\n",
    "        test_probs_array = np.array(test_probs)\n",
    "        test_labels_bin = label_binarize(test_labels, classes=range(num_classes))\n",
    "        test_auc = roc_auc_score(test_labels_bin, test_probs_array, average='weighted', multi_class='ovr')\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not calculate AUC-ROC for {model_name}: {e}\")\n",
    "        test_auc = 0.0\n",
    "    \n",
    "    return avg_test_loss, test_acc, test_f1, test_auc\n",
    "\n",
    "# Evaluate biLSTM\n",
    "print(\"\\n>>> Evaluating biLSTM on test set...\")\n",
    "bilstm_test_loss, bilstm_test_acc, bilstm_test_f1, bilstm_test_auc = evaluate_model(\n",
    "    bilstm_model, test_iter_bilstm, criterion, device, \"biLSTM\"\n",
    ")\n",
    "\n",
    "print(f\"\\n>>> biLSTM Test Set Results:\")\n",
    "print(f\"    Test Loss: {bilstm_test_loss:.4f}\")\n",
    "print(f\"    Test Accuracy: {bilstm_test_acc*100:.2f}%\")\n",
    "print(f\"    Test F1 Score: {bilstm_test_f1:.4f}\")\n",
    "print(f\"    Test AUC-ROC: {bilstm_test_auc:.4f}\")\n",
    "\n",
    "# Evaluate biGRU\n",
    "print(\"\\n>>> Evaluating biGRU on test set...\")\n",
    "bigru_test_loss, bigru_test_acc, bigru_test_f1, bigru_test_auc = evaluate_model(\n",
    "    bigru_model, test_iter_bigru, criterion, device, \"biGRU\"\n",
    ")\n",
    "\n",
    "print(f\"\\n>>> biGRU Test Set Results:\")\n",
    "print(f\"    Test Loss: {bigru_test_loss:.4f}\")\n",
    "print(f\"    Test Accuracy: {bigru_test_acc*100:.2f}%\")\n",
    "print(f\"    Test F1 Score: {bigru_test_f1:.4f}\")\n",
    "print(f\"    Test AUC-ROC: {bigru_test_auc:.4f}\")\n",
    "\n",
    "# Summary comparison\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 3.1 SUMMARY - biLSTM vs biGRU\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Model':<10} {'Test Accuracy':<15} {'Test F1':<12} {'Val Acc (Best)':<15}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'biLSTM':<10} {bilstm_test_acc*100:<15.2f}% {bilstm_test_f1:<12.4f} {bilstm_history['best_val_acc']*100:<15.2f}%\")\n",
    "print(f\"{'biGRU':<10} {bigru_test_acc*100:<15.2f}% {bigru_test_f1:<12.4f} {bigru_history['best_val_acc']*100:<15.2f}%\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n>>> Part 3.1 Complete!\")\n",
    "print(\"    - biLSTM and biGRU models trained with bidirectional and multiple layers\")\n",
    "print(\"    - Training curves plotted and saved\")\n",
    "print(\"    - Test set accuracies reported above\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3041ab71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Part 3.2: Convolutional Neural Network (CNN) Model\n",
    "# ============================================================================\n",
    "# Replace simple RNN with CNN to produce sentence representations and perform topic classification\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 3.2: CONVOLUTIONAL NEURAL NETWORK (CNN) MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "class CNNClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN for text classification using multiple filter sizes (n-gram features)\n",
    "    Uses convolutional layers to extract local features and max pooling to create sentence representation\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, output_dim, \n",
    "                 num_filters=100, filter_sizes=[3, 4, 5], dropout=0.5, \n",
    "                 padding_idx=0, pretrained_embeddings=None):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_filters = num_filters\n",
    "        self.filter_sizes = filter_sizes\n",
    "        \n",
    "        # Embedding layer initialized with pretrained embeddings\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "            # Make embeddings learnable (updated during training)\n",
    "            self.embedding.weight.requires_grad = True\n",
    "        \n",
    "        # Convolutional layers with different filter sizes\n",
    "        # Each filter size captures different n-gram patterns (e.g., 3-grams, 4-grams, 5-grams)\n",
    "        # Use (filter_size - 1) // 2 for \"same\" padding (consistent with hybrid model)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(\n",
    "                in_channels=embedding_dim,\n",
    "                out_channels=num_filters,\n",
    "                kernel_size=filter_size,\n",
    "                padding=(filter_size - 1) // 2  # Proper \"same\" padding\n",
    "            )\n",
    "            for filter_size in filter_sizes\n",
    "        ])\n",
    "        \n",
    "        # Dropout layers for regularization\n",
    "        self.conv_dropout = nn.Dropout(dropout)  # Dropout after CNN conv outputs\n",
    "        self.dropout = nn.Dropout(dropout)  # Dropout before FC layer\n",
    "        \n",
    "        # Fully connected output layer\n",
    "        # Input size is num_filters * len(filter_sizes) (concatenated features from all filters)\n",
    "        self.fc = nn.Linear(num_filters * len(filter_sizes), output_dim)\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        # text: [batch_size, seq_len]\n",
    "        # text_lengths: [batch_size] (not used in CNN, but kept for interface compatibility)\n",
    "        \n",
    "        # Embed the input\n",
    "        embedded = self.embedding(text)  # [batch_size, seq_len, embedding_dim]\n",
    "        \n",
    "        # CNN expects input as [batch_size, channels, seq_len]\n",
    "        # So we need to transpose: [batch_size, embedding_dim, seq_len]\n",
    "        embedded = embedded.permute(0, 2, 1)  # [batch_size, embedding_dim, seq_len]\n",
    "        \n",
    "        # Apply each convolutional filter and max pooling\n",
    "        conv_outputs = []\n",
    "        for conv in self.convs:\n",
    "            # Convolution: [batch_size, num_filters, seq_len]\n",
    "            conv_out = conv(embedded)\n",
    "            # Apply ReLU activation\n",
    "            conv_out = torch.relu(conv_out)\n",
    "            # Apply dropout to conv outputs\n",
    "            conv_out = self.conv_dropout(conv_out)\n",
    "            conv_outputs.append(conv_out)\n",
    "        \n",
    "        # Ensure all conv outputs have the same sequence length (consistent with hybrid model)\n",
    "        # Different filter sizes with padding can produce slightly different lengths\n",
    "        min_seq_len = min(conv_out.size(2) for conv_out in conv_outputs)\n",
    "        conv_outputs = [conv_out[:, :, :min_seq_len] for conv_out in conv_outputs]\n",
    "        \n",
    "        # Apply max pooling over time dimension (seq_len)\n",
    "        # This extracts the most important feature for each filter\n",
    "        pooled_outputs = []\n",
    "        for conv_out in conv_outputs:\n",
    "            pooled = torch.max(conv_out, dim=2)[0]  # [batch_size, num_filters]\n",
    "            pooled_outputs.append(pooled)\n",
    "        \n",
    "        # Concatenate features from all filter sizes\n",
    "        # This combines information from different n-gram patterns\n",
    "        concatenated = torch.cat(pooled_outputs, dim=1)  # [batch_size, num_filters * len(filter_sizes)]\n",
    "        \n",
    "        # Apply dropout before FC layer\n",
    "        concatenated = self.dropout(concatenated)\n",
    "        \n",
    "        # Pass through fully connected layer\n",
    "        output = self.fc(concatenated)  # [batch_size, output_dim]\n",
    "        \n",
    "        return output\n",
    "\n",
    "print(\">>> CNNClassifier model class created\")\n",
    "print(\"    - Uses multiple convolutional filters with different sizes (3, 4, 5)\")\n",
    "print(\"    - Captures n-gram features at different scales\")\n",
    "print(\"    - Max pooling to create sentence representation\")\n",
    "print(\"    - Concatenates features from all filter sizes\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38f6aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Train CNN Model\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING CNN MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Set seed for reproducibility\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Hyperparameters for CNN\n",
    "CNN_NUM_FILTERS = 100  # Number of filters per filter size\n",
    "CNN_FILTER_SIZES = [3, 4, 5]  # Different n-gram sizes to capture\n",
    "CNN_DROPOUT = 0.5\n",
    "CNN_BATCH_SIZE = 64\n",
    "CNN_LEARNING_RATE = 0.001\n",
    "CNN_N_EPOCHS = 100\n",
    "CNN_PATIENCE = 10\n",
    "\n",
    "# Create data iterators\n",
    "train_iter_cnn = data.BucketIterator(\n",
    "    train_data,\n",
    "    batch_size=CNN_BATCH_SIZE,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    shuffle=True,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "val_iter_cnn = data.BucketIterator(\n",
    "    validation_data,\n",
    "    batch_size=CNN_BATCH_SIZE,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    shuffle=False,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "test_iter_cnn = data.BucketIterator(\n",
    "    test_data,\n",
    "    batch_size=CNN_BATCH_SIZE,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    shuffle=False,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Create CNN model\n",
    "cnn_model = CNNClassifier(\n",
    "    vocab_size=fasttext_vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    output_dim=num_classes,\n",
    "    num_filters=CNN_NUM_FILTERS,\n",
    "    filter_sizes=CNN_FILTER_SIZES,\n",
    "    dropout=CNN_DROPOUT,\n",
    "    padding_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "    pretrained_embeddings=pretrained_embeddings\n",
    ").to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "cnn_optimizer = optim.Adam(cnn_model.parameters(), lr=CNN_LEARNING_RATE)\n",
    "\n",
    "print(f\"\\n>>> CNN Model Configuration:\")\n",
    "print(f\"    Number of Filters: {CNN_NUM_FILTERS} per filter size\")\n",
    "print(f\"    Filter Sizes: {CNN_FILTER_SIZES} (captures {', '.join([f'{s}-grams' for s in CNN_FILTER_SIZES])})\")\n",
    "print(f\"    Total Feature Size: {CNN_NUM_FILTERS * len(CNN_FILTER_SIZES)}\")\n",
    "print(f\"    Dropout: {CNN_DROPOUT}\")\n",
    "print(f\"    Learning Rate: {CNN_LEARNING_RATE}\")\n",
    "print(f\"    Batch Size: {CNN_BATCH_SIZE}\")\n",
    "print(f\"    Max Epochs: {CNN_N_EPOCHS}, Patience: {CNN_PATIENCE}\")\n",
    "\n",
    "# Train CNN model\n",
    "cnn_model, cnn_history = train_model_with_history(\n",
    "    cnn_model, train_iter_cnn, val_iter_cnn, cnn_optimizer, criterion,\n",
    "    CNN_N_EPOCHS, device, patience=CNN_PATIENCE, model_name=\"CNN\"\n",
    ")\n",
    "\n",
    "# Save best model\n",
    "torch.save(cnn_model.state_dict(), 'weights/cnn_best.pt')\n",
    "print(f\"\\n>>> CNN model saved to 'weights/cnn_best.pt'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11de7721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Evaluate CNN Model on Test Set\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST SET EVALUATION - CNN\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Evaluate CNN\n",
    "print(\"\\n>>> Evaluating CNN on test set...\")\n",
    "cnn_test_loss, cnn_test_acc, cnn_test_f1, cnn_test_auc = evaluate_model(\n",
    "    cnn_model, test_iter_cnn, criterion, device, \"CNN\"\n",
    ")\n",
    "\n",
    "print(f\"\\n>>> CNN Test Set Results:\")\n",
    "print(f\"    Test Loss: {cnn_test_loss:.4f}\")\n",
    "print(f\"    Test Accuracy: {cnn_test_acc*100:.2f}%\")\n",
    "print(f\"    Test F1 Score: {cnn_test_f1:.4f}\")\n",
    "print(f\"    Test AUC-ROC: {cnn_test_auc:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf75219c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Part 3.2 Summary - CNN Model\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 3.2 SUMMARY - CNN MODEL\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Model':<15} {'Val Acc':<12} {'Val F1':<12} {'Val AUC':<12} {'Test Acc':<12} {'Test F1':<12} {'Test AUC':<12}\")\n",
    "print(\"-\" * 90)\n",
    "print(f\"{'CNN':<15} {cnn_history['best_val_acc']*100:<12.2f}% {cnn_history['best_val_f1']:<12.4f} {cnn_history['best_val_auc']:<12.4f} {cnn_test_acc*100:<12.2f}% {cnn_test_f1:<12.4f} {cnn_test_auc:<12.4f}\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "print(\"\\n>>> Part 3.2 Complete!\")\n",
    "print(\"    - CNN model implemented with multiple filter sizes (3, 4, 5)\")\n",
    "print(\"    - Captures n-gram features at different scales\")\n",
    "print(\"    - Max pooling to create sentence representation\")\n",
    "print(\"    - All metrics (Accuracy, F1, AUC-ROC) reported for validation and test sets\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b9c677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Plot Training Curves for CNN\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PLOTTING TRAINING CURVES - CNN\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create figure with subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Training Loss Curves (Comparison: biLSTM, biGRU, CNN)\n",
    "axes[0, 0].plot(bilstm_history['train_losses'], label='biLSTM Train Loss', marker='o', linewidth=2)\n",
    "axes[0, 0].plot(bigru_history['train_losses'], label='biGRU Train Loss', marker='s', linewidth=2)\n",
    "axes[0, 0].plot(cnn_history['train_losses'], label='CNN Train Loss', marker='^', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Training Loss', fontsize=12)\n",
    "axes[0, 0].set_title('Training Loss Curves (Part 3.1 & 3.2)', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].legend(fontsize=11)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Validation Accuracy Curves (Comparison: biLSTM, biGRU, CNN)\n",
    "axes[0, 1].plot([acc*100 for acc in bilstm_history['val_accs']], label='biLSTM Val Acc', marker='o', linewidth=2)\n",
    "axes[0, 1].plot([acc*100 for acc in bigru_history['val_accs']], label='biGRU Val Acc', marker='s', linewidth=2)\n",
    "axes[0, 1].plot([acc*100 for acc in cnn_history['val_accs']], label='CNN Val Acc', marker='^', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Validation Accuracy (%)', fontsize=12)\n",
    "axes[0, 1].set_title('Validation Accuracy Curves (Part 3.1 & 3.2)', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].legend(fontsize=11)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: CNN Training and Validation Loss\n",
    "axes[1, 0].plot(cnn_history['train_losses'], label='CNN Train Loss', marker='o', linewidth=2)\n",
    "axes[1, 0].plot(cnn_history['val_losses'], label='CNN Val Loss', marker='s', linewidth=2)\n",
    "axes[1, 0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Loss', fontsize=12)\n",
    "axes[1, 0].set_title('CNN: Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].legend(fontsize=11)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: CNN Training and Validation Accuracy\n",
    "axes[1, 1].plot([acc*100 for acc in cnn_history['val_accs']], label='CNN Val Acc', marker='o', linewidth=2, color='green')\n",
    "axes[1, 1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Validation Accuracy (%)', fontsize=12)\n",
    "axes[1, 1].set_title('CNN: Validation Accuracy Curve', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].legend(fontsize=11)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('part3_2_cnn_training_curves.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"    Saved training curves to 'part3_2_cnn_training_curves.png'\")\n",
    "plt.show()  # Display the plot in notebook\n",
    "\n",
    "# Also create separate plots for Part 3.2 requirement (training loss and validation accuracy)\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Training Loss Curve\n",
    "ax1.plot(cnn_history['train_losses'], label='CNN Train Loss', marker='o', linewidth=2, color='blue')\n",
    "ax1.plot(cnn_history['val_losses'], label='CNN Val Loss', marker='s', linewidth=2, color='red')\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Loss', fontsize=12)\n",
    "ax1.set_title('Training Loss Curve (Part 3.2)', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Validation Accuracy Curve\n",
    "ax2.plot([acc*100 for acc in cnn_history['val_accs']], label='CNN Val Acc', marker='o', linewidth=2, color='green')\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Validation Accuracy (%)', fontsize=12)\n",
    "ax2.set_title('Validation Accuracy Curve (Part 3.2)', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('part3_2_training_curves.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"    Saved Part 3.2 curves to 'part3_2_training_curves.png'\")\n",
    "plt.show()  # Display the plot in notebook\n",
    "\n",
    "print(\"\\n>>> Training curves plotted and saved for CNN model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04034b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Part 3.1 & 3.2 Comparison: BiLSTM vs BiGRU vs CNN\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARISON: biLSTM vs biGRU vs CNN\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Model':<15} {'Val Acc':<12} {'Val F1':<12} {'Val AUC':<12} {'Test Acc':<12} {'Test F1':<12} {'Test AUC':<12}\")\n",
    "print(\"-\" * 90)\n",
    "print(f\"{'biLSTM':<15} {bilstm_history['best_val_acc']*100:<12.2f}% {bilstm_history['best_val_f1']:<12.4f} {bilstm_history['best_val_auc']:<12.4f} {bilstm_test_acc*100:<12.2f}% {bilstm_test_f1:<12.4f} {bilstm_test_auc:<12.4f}\")\n",
    "print(f\"{'biGRU':<15} {bigru_history['best_val_acc']*100:<12.2f}% {bigru_history['best_val_f1']:<12.4f} {bigru_history['best_val_auc']:<12.4f} {bigru_test_acc*100:<12.2f}% {bigru_test_f1:<12.4f} {bigru_test_auc:<12.4f}\")\n",
    "print(f\"{'CNN':<15} {cnn_history['best_val_acc']*100:<12.2f}% {cnn_history['best_val_f1']:<12.4f} {cnn_history['best_val_auc']:<12.4f} {cnn_test_acc*100:<12.2f}% {cnn_test_f1:<12.4f} {cnn_test_auc:<12.4f}\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "# Find best model based on test accuracy\n",
    "models_comparison = {\n",
    "    'biLSTM': {\n",
    "        'val_acc': bilstm_history['best_val_acc'],\n",
    "        'val_f1': bilstm_history['best_val_f1'],\n",
    "        'val_auc': bilstm_history['best_val_auc'],\n",
    "        'test_acc': bilstm_test_acc,\n",
    "        'test_f1': bilstm_test_f1,\n",
    "        'test_auc': bilstm_test_auc\n",
    "    },\n",
    "    'biGRU': {\n",
    "        'val_acc': bigru_history['best_val_acc'],\n",
    "        'val_f1': bigru_history['best_val_f1'],\n",
    "        'val_auc': bigru_history['best_val_auc'],\n",
    "        'test_acc': bigru_test_acc,\n",
    "        'test_f1': bigru_test_f1,\n",
    "        'test_auc': bigru_test_auc\n",
    "    },\n",
    "    'CNN': {\n",
    "        'val_acc': cnn_history['best_val_acc'],\n",
    "        'val_f1': cnn_history['best_val_f1'],\n",
    "        'val_auc': cnn_history['best_val_auc'],\n",
    "        'test_acc': cnn_test_acc,\n",
    "        'test_f1': cnn_test_f1,\n",
    "        'test_auc': cnn_test_auc\n",
    "    }\n",
    "}\n",
    "\n",
    "best_model_test_acc = max(models_comparison.items(), key=lambda x: x[1]['test_acc'])\n",
    "best_model_val_acc = max(models_comparison.items(), key=lambda x: x[1]['val_acc'])\n",
    "\n",
    "print(f\"\\n>>> Best Model by Test Accuracy: {best_model_test_acc[0]}\")\n",
    "print(f\"    Test Accuracy: {best_model_test_acc[1]['test_acc']*100:.2f}%\")\n",
    "print(f\"    Test F1: {best_model_test_acc[1]['test_f1']:.4f}\")\n",
    "print(f\"    Test AUC-ROC: {best_model_test_acc[1]['test_auc']:.4f}\")\n",
    "\n",
    "print(f\"\\n>>> Best Model by Validation Accuracy: {best_model_val_acc[0]}\")\n",
    "print(f\"    Validation Accuracy: {best_model_val_acc[1]['val_acc']*100:.2f}%\")\n",
    "print(f\"    Validation F1: {best_model_val_acc[1]['val_f1']:.4f}\")\n",
    "print(f\"    Validation AUC-ROC: {best_model_val_acc[1]['val_auc']:.4f}\")\n",
    "\n",
    "print(\"\\n>>> Model Architecture Comparison:\")\n",
    "print(\"    - biLSTM: Bidirectional LSTM with 2 layers, captures sequential dependencies in both directions\")\n",
    "print(\"    - biGRU: Bidirectional GRU with 2 layers, similar to biLSTM but with fewer parameters\")\n",
    "print(\"    - CNN: Convolutional layers with multiple filter sizes (3, 4, 5), captures n-gram patterns\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cd59cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # c) \n",
    "\n",
    "# 1: (hybrid model) rnn + attention layer\n",
    "# 2: (hybrid model) cnn + bilstm + attention layer\n",
    "# 3: (hybrid model) bigru + pretrained transformer (berto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e0141c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Part 3.3: RNN + Attention Layer Model\n",
    "# ============================================================================\n",
    "# This implements a more sophisticated attention mechanism (additive/Bahdanau-style)\n",
    "# compared to the simple linear attention used in Part 2\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 3.3: RNN + ATTENTION LAYER MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "class RNNWithAttentionClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    RNN with Additive Attention Mechanism (Bahdanau-style)\n",
    "    This is a more sophisticated attention mechanism than simple linear attention.\n",
    "    Uses a feed-forward network to compute attention scores.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, \n",
    "                 n_layers=2, dropout=0.5, padding_idx=0, pretrained_embeddings=None,\n",
    "                 attention_dim=None):\n",
    "        super(RNNWithAttentionClassifier, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.attention_dim = attention_dim if attention_dim else hidden_dim\n",
    "        \n",
    "        # Embedding layer initialized with pretrained embeddings\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "            # Make embeddings learnable (updated during training)\n",
    "            self.embedding.weight.requires_grad = True\n",
    "        \n",
    "        # Bidirectional LSTM layer (using LSTM for better performance)\n",
    "        self.rnn = nn.LSTM(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=n_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,  # Bidirectional for better context\n",
    "            dropout=dropout if n_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Additive Attention Mechanism (Bahdanau-style)\n",
    "        # Attention uses a feed-forward network instead of simple linear layer\n",
    "        # This is more expressive and can learn complex attention patterns\n",
    "        self.attention_linear1 = nn.Linear(hidden_dim * 2, self.attention_dim)  # *2 for bidirectional\n",
    "        self.attention_linear2 = nn.Linear(self.attention_dim, 1)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Fully connected output layer\n",
    "        # Input is hidden_dim * 2 (bidirectional) after attention\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        # text: [batch_size, seq_len]\n",
    "        # text_lengths: [batch_size]\n",
    "        \n",
    "        # Embed the input\n",
    "        embedded = self.embedding(text)  # [batch_size, seq_len, embedding_dim]\n",
    "        \n",
    "        # Pack sequences for efficient RNN processing\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, text_lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        \n",
    "        # Pass through bidirectional LSTM\n",
    "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
    "        # hidden: [n_layers * 2, batch_size, hidden_dim] (2 for bidirectional)\n",
    "        \n",
    "        # Unpack the sequences\n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(\n",
    "            packed_output, batch_first=True\n",
    "        )\n",
    "        # output: [batch_size, seq_len, hidden_dim * 2]\n",
    "        \n",
    "        # Apply Additive Attention Mechanism\n",
    "        # Step 1: Compute attention scores using feed-forward network\n",
    "        # This is more sophisticated than simple linear attention\n",
    "        attention_scores = self.attention_linear1(output)  # [batch_size, seq_len, attention_dim]\n",
    "        attention_scores = self.tanh(attention_scores)  # Apply tanh activation\n",
    "        attention_scores = self.attention_linear2(attention_scores).squeeze(2)  # [batch_size, seq_len]\n",
    "        \n",
    "        # Step 2: Mask padding positions\n",
    "        batch_size, seq_len = output.size(0), output.size(1)\n",
    "        mask = torch.arange(seq_len, device=text.device).unsqueeze(0) < text_lengths.unsqueeze(1)\n",
    "        attention_scores = attention_scores.masked_fill(~mask, float('-inf'))\n",
    "        \n",
    "        # Step 3: Apply softmax to get attention weights\n",
    "        attention_weights = torch.softmax(attention_scores, dim=1).unsqueeze(2)  # [batch_size, seq_len, 1]\n",
    "        \n",
    "        # Step 4: Compute weighted sum of RNN outputs\n",
    "        # This gives us a context vector that focuses on important parts of the sequence\n",
    "        context_vector = torch.sum(attention_weights * output, dim=1)  # [batch_size, hidden_dim * 2]\n",
    "        \n",
    "        # Apply dropout\n",
    "        context_vector = self.dropout(context_vector)\n",
    "        \n",
    "        # Pass through fully connected layer\n",
    "        output = self.fc(context_vector)  # [batch_size, output_dim]\n",
    "        \n",
    "        return output\n",
    "\n",
    "print(\">>> RNNWithAttentionClassifier model class created\")\n",
    "print(\"    - Uses bidirectional LSTM for encoding\")\n",
    "print(\"    - Implements additive attention mechanism (Bahdanau-style)\")\n",
    "print(\"    - More sophisticated than simple linear attention\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e78698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Train RNN + Attention Model\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING RNN + ATTENTION MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Set seed for reproducibility\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Hyperparameters (using similar settings to BiLSTM/BiGRU for fair comparison)\n",
    "RNN_ATTN_HIDDEN_DIM = 256  # Effective hidden size will be 512 (256*2) due to bidirectional\n",
    "RNN_ATTN_N_LAYERS = 2\n",
    "RNN_ATTN_DROPOUT = 0.5\n",
    "RNN_ATTN_BATCH_SIZE = 64\n",
    "RNN_ATTN_LEARNING_RATE = 0.001\n",
    "RNN_ATTN_N_EPOCHS = 100\n",
    "RNN_ATTN_PATIENCE = 10\n",
    "RNN_ATTN_ATTENTION_DIM = 256  # Dimension for attention mechanism\n",
    "\n",
    "# Create data iterators\n",
    "train_iter_rnn_attn = data.BucketIterator(\n",
    "    train_data,\n",
    "    batch_size=RNN_ATTN_BATCH_SIZE,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    shuffle=True,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "val_iter_rnn_attn = data.BucketIterator(\n",
    "    validation_data,\n",
    "    batch_size=RNN_ATTN_BATCH_SIZE,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    shuffle=False,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "test_iter_rnn_attn = data.BucketIterator(\n",
    "    test_data,\n",
    "    batch_size=RNN_ATTN_BATCH_SIZE,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    shuffle=False,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Create RNN + Attention model\n",
    "rnn_attn_model = RNNWithAttentionClassifier(\n",
    "    vocab_size=fasttext_vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_dim=RNN_ATTN_HIDDEN_DIM,\n",
    "    output_dim=num_classes,\n",
    "    n_layers=RNN_ATTN_N_LAYERS,\n",
    "    dropout=RNN_ATTN_DROPOUT,\n",
    "    padding_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "    pretrained_embeddings=pretrained_embeddings,\n",
    "    attention_dim=RNN_ATTN_ATTENTION_DIM\n",
    ").to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "rnn_attn_optimizer = optim.Adam(rnn_attn_model.parameters(), lr=RNN_ATTN_LEARNING_RATE)\n",
    "\n",
    "print(f\"\\n>>> RNN + Attention Model Configuration:\")\n",
    "print(f\"    Hidden Dim: {RNN_ATTN_HIDDEN_DIM} (effective: {RNN_ATTN_HIDDEN_DIM * 2} due to bidirectional)\")\n",
    "print(f\"    Layers: {RNN_ATTN_N_LAYERS}\")\n",
    "print(f\"    Dropout: {RNN_ATTN_DROPOUT}\")\n",
    "print(f\"    Attention Dim: {RNN_ATTN_ATTENTION_DIM}\")\n",
    "print(f\"    Learning Rate: {RNN_ATTN_LEARNING_RATE}\")\n",
    "print(f\"    Batch Size: {RNN_ATTN_BATCH_SIZE}\")\n",
    "print(f\"    Max Epochs: {RNN_ATTN_N_EPOCHS}, Patience: {RNN_ATTN_PATIENCE}\")\n",
    "\n",
    "# Train RNN + Attention model\n",
    "rnn_attn_model, rnn_attn_history = train_model_with_history(\n",
    "    rnn_attn_model, train_iter_rnn_attn, val_iter_rnn_attn, rnn_attn_optimizer, criterion,\n",
    "    RNN_ATTN_N_EPOCHS, device, patience=RNN_ATTN_PATIENCE, model_name=\"RNN+Attention\"\n",
    ")\n",
    "\n",
    "# Save best model\n",
    "torch.save(rnn_attn_model.state_dict(), 'weights/rnn_attention_best.pt')\n",
    "print(f\"\\n>>> RNN + Attention model saved to 'weights/rnn_attention_best.pt'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88985e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Evaluate RNN + Attention Model on Test Set\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST SET EVALUATION - RNN + ATTENTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Evaluate RNN + Attention\n",
    "print(\"\\n>>> Evaluating RNN + Attention on test set...\")\n",
    "rnn_attn_test_loss, rnn_attn_test_acc, rnn_attn_test_f1, rnn_attn_test_auc = evaluate_model(\n",
    "    rnn_attn_model, test_iter_rnn_attn, criterion, device, \"RNN+Attention\"\n",
    ")\n",
    "\n",
    "print(f\"\\n>>> RNN + Attention Test Set Results:\")\n",
    "print(f\"    Test Loss: {rnn_attn_test_loss:.4f}\")\n",
    "print(f\"    Test Accuracy: {rnn_attn_test_acc*100:.2f}%\")\n",
    "print(f\"    Test F1 Score: {rnn_attn_test_f1:.4f}\")\n",
    "print(f\"    Test AUC-ROC: {rnn_attn_test_auc:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a71b271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Part 3.3 Summary - Compare RNN+Attention with BiLSTM and BiGRU\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 3.3 SUMMARY - RNN+ATTENTION vs biLSTM vs biGRU vs CNN\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Model':<15} {'Val Acc':<12} {'Val F1':<12} {'Val AUC':<12} {'Test Acc':<12} {'Test F1':<12} {'Test AUC':<12}\")\n",
    "print(\"-\" * 90)\n",
    "print(f\"{'biLSTM':<15} {bilstm_history['best_val_acc']*100:<12.2f}% {bilstm_history['best_val_f1']:<12.4f} {bilstm_history['best_val_auc']:<12.4f} {bilstm_test_acc*100:<12.2f}% {bilstm_test_f1:<12.4f} {bilstm_test_auc:<12.4f}\")\n",
    "print(f\"{'biGRU':<15} {bigru_history['best_val_acc']*100:<12.2f}% {bigru_history['best_val_f1']:<12.4f} {bigru_history['best_val_auc']:<12.4f} {bigru_test_acc*100:<12.2f}% {bigru_test_f1:<12.4f} {bigru_test_auc:<12.4f}\")\n",
    "print(f\"{'CNN':<15} {cnn_history['best_val_acc']*100:<12.2f}% {cnn_history['best_val_f1']:<12.4f} {cnn_history['best_val_auc']:<12.4f} {cnn_test_acc*100:<12.2f}% {cnn_test_f1:<12.4f} {cnn_test_auc:<12.4f}\")\n",
    "print(f\"{'RNN+Attention':<15} {rnn_attn_history['best_val_acc']*100:<12.2f}% {rnn_attn_history['best_val_f1']:<12.4f} {rnn_attn_history['best_val_auc']:<12.4f} {rnn_attn_test_acc*100:<12.2f}% {rnn_attn_test_f1:<12.4f} {rnn_attn_test_auc:<12.4f}\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "print(\"\\n>>> Part 3.3 Complete!\")\n",
    "print(\"    - RNN + Attention model implemented with additive attention mechanism\")\n",
    "print(\"    - More sophisticated than simple linear attention (uses feed-forward network)\")\n",
    "print(\"    - Comparison with BiLSTM, BiGRU, and CNN models shown above\")\n",
    "print(\"    - All metrics (Accuracy, F1, AUC-ROC) reported for validation and test sets\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be35f3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Plot Training Curves for RNN + Attention\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PLOTTING TRAINING CURVES - RNN + ATTENTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create figure with subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Training Loss Curves (Comparison: biLSTM, biGRU, CNN, RNN+Attention)\n",
    "axes[0, 0].plot(bilstm_history['train_losses'], label='biLSTM Train Loss', marker='o', linewidth=2)\n",
    "axes[0, 0].plot(bigru_history['train_losses'], label='biGRU Train Loss', marker='s', linewidth=2)\n",
    "axes[0, 0].plot(cnn_history['train_losses'], label='CNN Train Loss', marker='^', linewidth=2)\n",
    "axes[0, 0].plot(rnn_attn_history['train_losses'], label='RNN+Attention Train Loss', marker='d', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Training Loss', fontsize=12)\n",
    "axes[0, 0].set_title('Training Loss Curves (All Models)', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].legend(fontsize=11)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Validation Accuracy Curves (Comparison: biLSTM, biGRU, CNN, RNN+Attention)\n",
    "axes[0, 1].plot([acc*100 for acc in bilstm_history['val_accs']], label='biLSTM Val Acc', marker='o', linewidth=2)\n",
    "axes[0, 1].plot([acc*100 for acc in bigru_history['val_accs']], label='biGRU Val Acc', marker='s', linewidth=2)\n",
    "axes[0, 1].plot([acc*100 for acc in cnn_history['val_accs']], label='CNN Val Acc', marker='^', linewidth=2)\n",
    "axes[0, 1].plot([acc*100 for acc in rnn_attn_history['val_accs']], label='RNN+Attention Val Acc', marker='d', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Validation Accuracy (%)', fontsize=12)\n",
    "axes[0, 1].set_title('Validation Accuracy Curves (All Models)', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].legend(fontsize=11)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: RNN+Attention Training and Validation Loss\n",
    "axes[1, 0].plot(rnn_attn_history['train_losses'], label='RNN+Attention Train Loss', marker='o', linewidth=2)\n",
    "axes[1, 0].plot(rnn_attn_history['val_losses'], label='RNN+Attention Val Loss', marker='s', linewidth=2)\n",
    "axes[1, 0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Loss', fontsize=12)\n",
    "axes[1, 0].set_title('RNN+Attention: Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].legend(fontsize=11)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: RNN+Attention Validation Accuracy\n",
    "axes[1, 1].plot([acc*100 for acc in rnn_attn_history['val_accs']], label='RNN+Attention Val Acc', marker='o', linewidth=2, color='purple')\n",
    "axes[1, 1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Validation Accuracy (%)', fontsize=12)\n",
    "axes[1, 1].set_title('RNN+Attention: Validation Accuracy Curve', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].legend(fontsize=11)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('part3_3_rnn_attention_training_curves.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"    Saved training curves to 'part3_3_rnn_attention_training_curves.png'\")\n",
    "plt.show()  # Display the plot in notebook\n",
    "\n",
    "# Also create separate plots for Part 3.3 requirement (training loss and validation accuracy)\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Training Loss Curve\n",
    "ax1.plot(rnn_attn_history['train_losses'], label='RNN+Attention Train Loss', marker='o', linewidth=2, color='blue')\n",
    "ax1.plot(rnn_attn_history['val_losses'], label='RNN+Attention Val Loss', marker='s', linewidth=2, color='red')\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Loss', fontsize=12)\n",
    "ax1.set_title('Training Loss Curve (Part 3.3)', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Validation Accuracy Curve\n",
    "ax2.plot([acc*100 for acc in rnn_attn_history['val_accs']], label='RNN+Attention Val Acc', marker='o', linewidth=2, color='purple')\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Validation Accuracy (%)', fontsize=12)\n",
    "ax2.set_title('Validation Accuracy Curve (Part 3.3)', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('part3_3_training_curves.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"    Saved Part 3.3 curves to 'part3_3_training_curves.png'\")\n",
    "plt.show()  # Display the plot in notebook\n",
    "\n",
    "print(\"\\n>>> Training curves plotted and saved for RNN + Attention model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06177db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Part 3.3: CNN + BiLSTM + Attention Model\n",
    "# ============================================================================\n",
    "# Hybrid model combining CNN, BiLSTM, and Attention mechanisms\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 3.3: CNN + biLSTM + ATTENTION MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "class CNNBiLSTMAttentionClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Modified Hybrid model with label-specific representations:\n",
    "    - CNN: Extracts n-gram features and produces label-specific latent representations\n",
    "    - BiLSTM: Processes label-specific representations instead of full sentence\n",
    "    - Attention: Focuses on relevant label-specific features\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, output_dim, \n",
    "                 num_filters=100, filter_sizes=[3, 4, 5],\n",
    "                 hidden_dim=256, n_layers=2, dropout=0.5,\n",
    "                 padding_idx=0, pretrained_embeddings=None,\n",
    "                 attention_dim=None, label_latent_dim=None):\n",
    "        super(CNNBiLSTMAttentionClassifier, self).__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_filters = num_filters\n",
    "        self.filter_sizes = filter_sizes\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.output_dim = output_dim\n",
    "        self.attention_dim = attention_dim if attention_dim else hidden_dim\n",
    "        # Dimension for label-specific latent representations\n",
    "        self.label_latent_dim = label_latent_dim if label_latent_dim else hidden_dim\n",
    "        \n",
    "        # Embedding layer initialized with pretrained embeddings\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "            # Make embeddings learnable (updated during training)\n",
    "            self.embedding.weight.requires_grad = True\n",
    "        \n",
    "        # CNN layers to extract n-gram features\n",
    "        # Use (filter_size - 1) // 2 for \"same\" padding\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(\n",
    "                in_channels=embedding_dim,\n",
    "                out_channels=num_filters,\n",
    "                kernel_size=filter_size,\n",
    "                padding=(filter_size - 1) // 2\n",
    "            )\n",
    "            for filter_size in filter_sizes\n",
    "        ])\n",
    "        \n",
    "        # CNN produces label-specific latent representations\n",
    "        # Max pooling over sequence to get sentence-level CNN features\n",
    "        cnn_output_dim = num_filters * len(filter_sizes)\n",
    "        # Project CNN features to label-specific latent space (one per class)\n",
    "        # This creates label-specific sentence representations\n",
    "        self.cnn_label_projections = nn.ModuleList([\n",
    "            nn.Linear(cnn_output_dim, self.label_latent_dim)\n",
    "            for _ in range(output_dim)\n",
    "        ])\n",
    "        \n",
    "        # Dropout layers for regularization\n",
    "        self.cnn_dropout = nn.Dropout(dropout)  # Dropout after CNN features\n",
    "        self.bilstm_dropout = nn.Dropout(dropout)  # Dropout after BiLSTM outputs\n",
    "        self.dropout = nn.Dropout(dropout)  # Dropout after attention\n",
    "        \n",
    "        # Bidirectional LSTM processes label-specific representations\n",
    "        # Input: label_latent_dim (each label-specific representation)\n",
    "        # Sequence length: output_dim (one representation per class)\n",
    "        self.bilstm = nn.LSTM(\n",
    "            self.label_latent_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=n_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout if n_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Additive Attention Mechanism over label-specific BiLSTM outputs\n",
    "        self.attention_linear1 = nn.Linear(hidden_dim * 2, self.attention_dim)  # *2 for bidirectional\n",
    "        self.attention_linear2 = nn.Linear(self.attention_dim, 1)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "        # Fully connected output layer\n",
    "        # Input: hidden_dim * 2 (bidirectional) after attention\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        # text: [batch_size, seq_len]\n",
    "        # text_lengths: [batch_size]\n",
    "        \n",
    "        batch_size = text.size(0)\n",
    "        \n",
    "        # Step 1: Embed the input\n",
    "        embedded = self.embedding(text)  # [batch_size, seq_len, embedding_dim]\n",
    "        \n",
    "        # Step 2: Apply CNN to extract n-gram features\n",
    "        # CNN expects [batch_size, channels, seq_len]\n",
    "        embedded_cnn = embedded.permute(0, 2, 1)  # [batch_size, embedding_dim, seq_len]\n",
    "        \n",
    "        conv_outputs = []\n",
    "        for conv in self.convs:\n",
    "            # Convolution: [batch_size, num_filters, seq_len]\n",
    "            conv_out = conv(embedded_cnn)\n",
    "            conv_out = torch.relu(conv_out)\n",
    "            conv_outputs.append(conv_out)\n",
    "        \n",
    "        # Ensure all conv outputs have the same sequence length\n",
    "        min_seq_len = min(conv_out.size(2) for conv_out in conv_outputs)\n",
    "        conv_outputs = [conv_out[:, :, :min_seq_len] for conv_out in conv_outputs]\n",
    "        \n",
    "        # Concatenate CNN features from all filter sizes\n",
    "        # [batch_size, num_filters * len(filter_sizes), seq_len]\n",
    "        cnn_features = torch.cat(conv_outputs, dim=1)\n",
    "        \n",
    "        # Step 3: Produce label-specific latent representations\n",
    "        # Max pooling over sequence to get sentence-level CNN features\n",
    "        # [batch_size, num_filters * len(filter_sizes), seq_len] -> [batch_size, num_filters * len(filter_sizes)]\n",
    "        cnn_sentence_features = torch.max(cnn_features, dim=2)[0]  # [batch_size, cnn_output_dim]\n",
    "        \n",
    "        # Apply dropout to CNN features\n",
    "        cnn_sentence_features = self.cnn_dropout(cnn_sentence_features)\n",
    "        \n",
    "        # Project CNN features to label-specific latent representations\n",
    "        # Each projection creates a label-specific sentence representation\n",
    "        label_specific_reprs = []\n",
    "        for label_proj in self.cnn_label_projections:\n",
    "            label_repr = label_proj(cnn_sentence_features)  # [batch_size, label_latent_dim]\n",
    "            label_specific_reprs.append(label_repr)\n",
    "        \n",
    "        # Stack label-specific representations to form a sequence\n",
    "        # [batch_size, output_dim, label_latent_dim]\n",
    "        label_sequence = torch.stack(label_specific_reprs, dim=1)\n",
    "        \n",
    "        # Step 4: Pass label-specific representations through BiLSTM\n",
    "        # BiLSTM processes the sequence of label-specific representations\n",
    "        # label_sequence: [batch_size, output_dim, label_latent_dim]\n",
    "        # Create lengths tensor (all sequences have length = output_dim)\n",
    "        label_lengths = torch.full((batch_size,), self.output_dim, dtype=torch.long, device=text.device)\n",
    "        \n",
    "        # Pack sequences for efficient processing\n",
    "        packed_label_seq = nn.utils.rnn.pack_padded_sequence(\n",
    "            label_sequence, label_lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        \n",
    "        packed_output, (hidden, cell) = self.bilstm(packed_label_seq)\n",
    "        \n",
    "        # Unpack sequences\n",
    "        bilstm_output, output_lengths = nn.utils.rnn.pad_packed_sequence(\n",
    "            packed_output, batch_first=True\n",
    "        )\n",
    "        # bilstm_output: [batch_size, output_dim, hidden_dim * 2]\n",
    "        \n",
    "        # Apply dropout to BiLSTM outputs\n",
    "        bilstm_output = self.bilstm_dropout(bilstm_output)\n",
    "        \n",
    "        # Step 5: Apply Attention Mechanism over label-specific BiLSTM outputs\n",
    "        # Compute attention scores using feed-forward network\n",
    "        attention_scores = self.attention_linear1(bilstm_output)  # [batch_size, output_dim, attention_dim]\n",
    "        attention_scores = self.tanh(attention_scores)\n",
    "        attention_scores = self.attention_linear2(attention_scores).squeeze(2)  # [batch_size, output_dim]\n",
    "        \n",
    "        # Apply softmax to get attention weights over label-specific representations\n",
    "        attention_weights = torch.softmax(attention_scores, dim=1).unsqueeze(2)  # [batch_size, output_dim, 1]\n",
    "        \n",
    "        # Compute weighted sum of label-specific BiLSTM outputs\n",
    "        context_vector = torch.sum(attention_weights * bilstm_output, dim=1)  # [batch_size, hidden_dim * 2]\n",
    "        \n",
    "        # Apply dropout\n",
    "        context_vector = self.dropout(context_vector)\n",
    "        \n",
    "        # Step 6: Pass through fully connected layer\n",
    "        output = self.fc(context_vector)  # [batch_size, output_dim]\n",
    "        \n",
    "        return output\n",
    "\n",
    "print(\">>> CNNBiLSTMAttentionClassifier model class created\")\n",
    "print(\"    - Combines CNN (n-gram features), BiLSTM (sequential dependencies), and Attention\")\n",
    "print(\"    - CNN extracts local patterns, BiLSTM captures long-range dependencies\")\n",
    "print(\"    - Attention mechanism focuses on important parts of the sequence\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a846e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Train CNN + BiLSTM + Attention Model\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING CNN + biLSTM + ATTENTION MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Set seed for reproducibility\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Hyperparameters for CNN + BiLSTM + Attention\n",
    "HYBRID_NUM_FILTERS = 100\n",
    "HYBRID_FILTER_SIZES = [3, 4, 5]\n",
    "HYBRID_HIDDEN_DIM = 256  # Effective hidden size will be 512 (256*2) due to bidirectional\n",
    "HYBRID_N_LAYERS = 2\n",
    "HYBRID_DROPOUT = 0.5\n",
    "HYBRID_BATCH_SIZE = 64\n",
    "HYBRID_LEARNING_RATE = 0.001\n",
    "HYBRID_N_EPOCHS = 100\n",
    "HYBRID_PATIENCE = 10\n",
    "HYBRID_ATTENTION_DIM = 256\n",
    "\n",
    "# Create data iterators\n",
    "train_iter_hybrid = data.BucketIterator(\n",
    "    train_data,\n",
    "    batch_size=HYBRID_BATCH_SIZE,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    shuffle=True,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "val_iter_hybrid = data.BucketIterator(\n",
    "    validation_data,\n",
    "    batch_size=HYBRID_BATCH_SIZE,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    shuffle=False,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "test_iter_hybrid = data.BucketIterator(\n",
    "    test_data,\n",
    "    batch_size=HYBRID_BATCH_SIZE,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    shuffle=False,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Create CNN + BiLSTM + Attention model\n",
    "hybrid_model = CNNBiLSTMAttentionClassifier(\n",
    "    vocab_size=fasttext_vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    output_dim=num_classes,\n",
    "    num_filters=HYBRID_NUM_FILTERS,\n",
    "    filter_sizes=HYBRID_FILTER_SIZES,\n",
    "    hidden_dim=HYBRID_HIDDEN_DIM,\n",
    "    n_layers=HYBRID_N_LAYERS,\n",
    "    dropout=HYBRID_DROPOUT,\n",
    "    padding_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "    pretrained_embeddings=pretrained_embeddings,\n",
    "    attention_dim=HYBRID_ATTENTION_DIM\n",
    ").to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "hybrid_optimizer = optim.Adam(hybrid_model.parameters(), lr=HYBRID_LEARNING_RATE)\n",
    "\n",
    "print(f\"\\n>>> CNN + BiLSTM + Attention Model Configuration:\")\n",
    "print(f\"    CNN Filters: {HYBRID_NUM_FILTERS} per filter size\")\n",
    "print(f\"    CNN Filter Sizes: {HYBRID_FILTER_SIZES}\")\n",
    "print(f\"    BiLSTM Hidden Dim: {HYBRID_HIDDEN_DIM} (effective: {HYBRID_HIDDEN_DIM * 2} due to bidirectional)\")\n",
    "print(f\"    BiLSTM Layers: {HYBRID_N_LAYERS}\")\n",
    "print(f\"    Attention Dim: {HYBRID_ATTENTION_DIM}\")\n",
    "print(f\"    Dropout: {HYBRID_DROPOUT}\")\n",
    "print(f\"    Learning Rate: {HYBRID_LEARNING_RATE}\")\n",
    "print(f\"    Batch Size: {HYBRID_BATCH_SIZE}\")\n",
    "print(f\"    Max Epochs: {HYBRID_N_EPOCHS}, Patience: {HYBRID_PATIENCE}\")\n",
    "\n",
    "# Train hybrid model\n",
    "hybrid_model, hybrid_history = train_model_with_history(\n",
    "    hybrid_model, train_iter_hybrid, val_iter_hybrid, hybrid_optimizer, criterion,\n",
    "    HYBRID_N_EPOCHS, device, patience=HYBRID_PATIENCE, model_name=\"CNN+BiLSTM+Attention\"\n",
    ")\n",
    "\n",
    "# Save best model\n",
    "torch.save(hybrid_model.state_dict(), 'weights/cnn_bilstm_attention_best.pt')\n",
    "print(f\"\\n>>> CNN + BiLSTM + Attention model saved to 'weights/cnn_bilstm_attention_best.pt'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6d8580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Evaluate CNN + BiLSTM + Attention Model on Test Set\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST SET EVALUATION - CNN + biLSTM + ATTENTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Evaluate hybrid model\n",
    "print(\"\\n>>> Evaluating CNN + BiLSTM + Attention on test set...\")\n",
    "hybrid_test_loss, hybrid_test_acc, hybrid_test_f1, hybrid_test_auc = evaluate_model(\n",
    "    hybrid_model, test_iter_hybrid, criterion, device, \"CNN+BiLSTM+Attention\"\n",
    ")\n",
    "\n",
    "print(f\"\\n>>> CNN + BiLSTM + Attention Test Set Results:\")\n",
    "print(f\"    Test Loss: {hybrid_test_loss:.4f}\")\n",
    "print(f\"    Test Accuracy: {hybrid_test_acc*100:.2f}%\")\n",
    "print(f\"    Test F1 Score: {hybrid_test_f1:.4f}\")\n",
    "print(f\"    Test AUC-ROC: {hybrid_test_auc:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d98889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Part 3.3 Updated Summary - All Models Including CNN + BiLSTM + Attention\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 3.3 SUMMARY - ALL MODELS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Model':<20} {'Val Acc':<12} {'Val F1':<12} {'Val AUC':<12} {'Test Acc':<12} {'Test F1':<12} {'Test AUC':<12}\")\n",
    "print(\"-\" * 100)\n",
    "print(f\"{'biLSTM':<20} {bilstm_history['best_val_acc']*100:<12.2f}% {bilstm_history['best_val_f1']:<12.4f} {bilstm_history['best_val_auc']:<12.4f} {bilstm_test_acc*100:<12.2f}% {bilstm_test_f1:<12.4f} {bilstm_test_auc:<12.4f}\")\n",
    "print(f\"{'biGRU':<20} {bigru_history['best_val_acc']*100:<12.2f}% {bigru_history['best_val_f1']:<12.4f} {bigru_history['best_val_auc']:<12.4f} {bigru_test_acc*100:<12.2f}% {bigru_test_f1:<12.4f} {bigru_test_auc:<12.4f}\")\n",
    "print(f\"{'CNN':<20} {cnn_history['best_val_acc']*100:<12.2f}% {cnn_history['best_val_f1']:<12.4f} {cnn_history['best_val_auc']:<12.4f} {cnn_test_acc*100:<12.2f}% {cnn_test_f1:<12.4f} {cnn_test_auc:<12.4f}\")\n",
    "print(f\"{'RNN+Attention':<20} {rnn_attn_history['best_val_acc']*100:<12.2f}% {rnn_attn_history['best_val_f1']:<12.4f} {rnn_attn_history['best_val_auc']:<12.4f} {rnn_attn_test_acc*100:<12.2f}% {rnn_attn_test_f1:<12.4f} {rnn_attn_test_auc:<12.4f}\")\n",
    "print(f\"{'CNN+BiLSTM+Attn':<20} {hybrid_history['best_val_acc']*100:<12.2f}% {hybrid_history['best_val_f1']:<12.4f} {hybrid_history['best_val_auc']:<12.4f} {hybrid_test_acc*100:<12.2f}% {hybrid_test_f1:<12.4f} {hybrid_test_auc:<12.4f}\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Find best model based on test accuracy\n",
    "all_models_comparison = {\n",
    "    'biLSTM': {\n",
    "        'val_acc': bilstm_history['best_val_acc'],\n",
    "        'val_f1': bilstm_history['best_val_f1'],\n",
    "        'val_auc': bilstm_history['best_val_auc'],\n",
    "        'test_acc': bilstm_test_acc,\n",
    "        'test_f1': bilstm_test_f1,\n",
    "        'test_auc': bilstm_test_auc\n",
    "    },\n",
    "    'biGRU': {\n",
    "        'val_acc': bigru_history['best_val_acc'],\n",
    "        'val_f1': bigru_history['best_val_f1'],\n",
    "        'val_auc': bigru_history['best_val_auc'],\n",
    "        'test_acc': bigru_test_acc,\n",
    "        'test_f1': bigru_test_f1,\n",
    "        'test_auc': bigru_test_auc\n",
    "    },\n",
    "    'CNN': {\n",
    "        'val_acc': cnn_history['best_val_acc'],\n",
    "        'val_f1': cnn_history['best_val_f1'],\n",
    "        'val_auc': cnn_history['best_val_auc'],\n",
    "        'test_acc': cnn_test_acc,\n",
    "        'test_f1': cnn_test_f1,\n",
    "        'test_auc': cnn_test_auc\n",
    "    },\n",
    "    'RNN+Attention': {\n",
    "        'val_acc': rnn_attn_history['best_val_acc'],\n",
    "        'val_f1': rnn_attn_history['best_val_f1'],\n",
    "        'val_auc': rnn_attn_history['best_val_auc'],\n",
    "        'test_acc': rnn_attn_test_acc,\n",
    "        'test_f1': rnn_attn_test_f1,\n",
    "        'test_auc': rnn_attn_test_auc\n",
    "    },\n",
    "    'CNN+BiLSTM+Attention': {\n",
    "        'val_acc': hybrid_history['best_val_acc'],\n",
    "        'val_f1': hybrid_history['best_val_f1'],\n",
    "        'val_auc': hybrid_history['best_val_auc'],\n",
    "        'test_acc': hybrid_test_acc,\n",
    "        'test_f1': hybrid_test_f1,\n",
    "        'test_auc': hybrid_test_auc\n",
    "    }\n",
    "}\n",
    "\n",
    "best_model_test_acc = max(all_models_comparison.items(), key=lambda x: x[1]['test_acc'])\n",
    "best_model_val_acc = max(all_models_comparison.items(), key=lambda x: x[1]['val_acc'])\n",
    "\n",
    "print(f\"\\n>>> Best Model by Test Accuracy: {best_model_test_acc[0]}\")\n",
    "print(f\"    Test Accuracy: {best_model_test_acc[1]['test_acc']*100:.2f}%\")\n",
    "print(f\"    Test F1: {best_model_test_acc[1]['test_f1']:.4f}\")\n",
    "print(f\"    Test AUC-ROC: {best_model_test_acc[1]['test_auc']:.4f}\")\n",
    "\n",
    "print(f\"\\n>>> Best Model by Validation Accuracy: {best_model_val_acc[0]}\")\n",
    "print(f\"    Validation Accuracy: {best_model_val_acc[1]['val_acc']*100:.2f}%\")\n",
    "print(f\"    Validation F1: {best_model_val_acc[1]['val_f1']:.4f}\")\n",
    "print(f\"    Validation AUC-ROC: {best_model_val_acc[1]['val_auc']:.4f}\")\n",
    "\n",
    "print(\"\\n>>> Part 3.3 Complete!\")\n",
    "print(\"    - RNN + Attention: Additive attention mechanism with bidirectional LSTM\")\n",
    "print(\"    - CNN + BiLSTM + Attention: Hybrid model combining CNN, BiLSTM, and attention\")\n",
    "print(\"    - All models compared above with validation and test metrics\")\n",
    "print(\"    - All metrics (Accuracy, F1, AUC-ROC) reported for validation and test sets\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188a0722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Plot Training Curves for CNN + BiLSTM + Attention\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PLOTTING TRAINING CURVES - CNN + biLSTM + ATTENTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create figure with subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Training Loss Curves (All Models)\n",
    "axes[0, 0].plot(bilstm_history['train_losses'], label='biLSTM Train Loss', marker='o', linewidth=2)\n",
    "axes[0, 0].plot(bigru_history['train_losses'], label='biGRU Train Loss', marker='s', linewidth=2)\n",
    "axes[0, 0].plot(cnn_history['train_losses'], label='CNN Train Loss', marker='^', linewidth=2)\n",
    "axes[0, 0].plot(rnn_attn_history['train_losses'], label='RNN+Attention Train Loss', marker='d', linewidth=2)\n",
    "axes[0, 0].plot(hybrid_history['train_losses'], label='CNN+BiLSTM+Attn Train Loss', marker='*', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Training Loss', fontsize=12)\n",
    "axes[0, 0].set_title('Training Loss Curves (All Models)', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].legend(fontsize=10)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Validation Accuracy Curves (All Models)\n",
    "axes[0, 1].plot([acc*100 for acc in bilstm_history['val_accs']], label='biLSTM Val Acc', marker='o', linewidth=2)\n",
    "axes[0, 1].plot([acc*100 for acc in bigru_history['val_accs']], label='biGRU Val Acc', marker='s', linewidth=2)\n",
    "axes[0, 1].plot([acc*100 for acc in cnn_history['val_accs']], label='CNN Val Acc', marker='^', linewidth=2)\n",
    "axes[0, 1].plot([acc*100 for acc in rnn_attn_history['val_accs']], label='RNN+Attention Val Acc', marker='d', linewidth=2)\n",
    "axes[0, 1].plot([acc*100 for acc in hybrid_history['val_accs']], label='CNN+BiLSTM+Attn Val Acc', marker='*', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Validation Accuracy (%)', fontsize=12)\n",
    "axes[0, 1].set_title('Validation Accuracy Curves (All Models)', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].legend(fontsize=10)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: CNN+BiLSTM+Attention Training and Validation Loss\n",
    "axes[1, 0].plot(hybrid_history['train_losses'], label='CNN+BiLSTM+Attn Train Loss', marker='o', linewidth=2)\n",
    "axes[1, 0].plot(hybrid_history['val_losses'], label='CNN+BiLSTM+Attn Val Loss', marker='s', linewidth=2)\n",
    "axes[1, 0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Loss', fontsize=12)\n",
    "axes[1, 0].set_title('CNN+BiLSTM+Attention: Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].legend(fontsize=11)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: CNN+BiLSTM+Attention Validation Accuracy\n",
    "axes[1, 1].plot([acc*100 for acc in hybrid_history['val_accs']], label='CNN+BiLSTM+Attn Val Acc', marker='o', linewidth=2, color='orange')\n",
    "axes[1, 1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Validation Accuracy (%)', fontsize=12)\n",
    "axes[1, 1].set_title('CNN+BiLSTM+Attention: Validation Accuracy Curve', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].legend(fontsize=11)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('part3_3_cnn_bilstm_attention_training_curves.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"    Saved training curves to 'part3_3_cnn_bilstm_attention_training_curves.png'\")\n",
    "plt.show()  # Display the plot in notebook\n",
    "\n",
    "# Also create separate plots for Part 3.3 requirement (training loss and validation accuracy)\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Training Loss Curve\n",
    "ax1.plot(hybrid_history['train_losses'], label='CNN+BiLSTM+Attn Train Loss', marker='o', linewidth=2, color='blue')\n",
    "ax1.plot(hybrid_history['val_losses'], label='CNN+BiLSTM+Attn Val Loss', marker='s', linewidth=2, color='red')\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Loss', fontsize=12)\n",
    "ax1.set_title('Training Loss Curve (CNN+BiLSTM+Attention)', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Validation Accuracy Curve\n",
    "ax2.plot([acc*100 for acc in hybrid_history['val_accs']], label='CNN+BiLSTM+Attn Val Acc', marker='o', linewidth=2, color='orange')\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Validation Accuracy (%)', fontsize=12)\n",
    "ax2.set_title('Validation Accuracy Curve (CNN+BiLSTM+Attention)', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('part3_3_hybrid_training_curves.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"    Saved Part 3.3 hybrid curves to 'part3_3_hybrid_training_curves.png'\")\n",
    "plt.show()  # Display the plot in notebook\n",
    "\n",
    "print(\"\\n>>> Training curves plotted and saved for CNN + BiLSTM + Attention model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a72627",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6b4059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Part 3.3: RNN + Pretrained BERT Model\n",
    "# ============================================================================\n",
    "# Hybrid model combining pretrained BERT embeddings with RNN\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 3.3: RNN + PRETRAINED BERT MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Import transformers for BERT\n",
    "try:\n",
    "    from transformers import BertModel, BertTokenizer\n",
    "    BERT_AVAILABLE = True\n",
    "    print(\">>> Transformers library available\")\n",
    "except ImportError:\n",
    "    print(\">>> Warning: transformers library not found. Installing...\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"transformers\"])\n",
    "    from transformers import BertModel, BertTokenizer\n",
    "    BERT_AVAILABLE = True\n",
    "    print(\">>> Transformers library installed\")\n",
    "\n",
    "class RNNBertClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    RNN with Pretrained BERT embeddings\n",
    "    Uses BERT to get contextualized embeddings, then passes through BiLSTM with attention\n",
    "    \"\"\"\n",
    "    def __init__(self, output_dim, hidden_dim=256, n_layers=2, dropout=0.5,\n",
    "                 bert_model_name='distilbert-base-uncased', freeze_bert=False):\n",
    "        super(RNNBertClassifier, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.freeze_bert = freeze_bert\n",
    "        \n",
    "        # Load pretrained BERT model and tokenizer\n",
    "        self.bert_tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "        self.bert_model = BertModel.from_pretrained(bert_model_name)\n",
    "        \n",
    "        # Freeze BERT parameters if specified\n",
    "        if freeze_bert:\n",
    "            for param in self.bert_model.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # BERT output dimension (768 for bert-base-uncased)\n",
    "        bert_output_dim = self.bert_model.config.hidden_size\n",
    "        \n",
    "        # Bidirectional LSTM to process BERT embeddings\n",
    "        self.bilstm = nn.LSTM(\n",
    "            bert_output_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=n_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout if n_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Additive Attention Mechanism (Bahdanau-style)\n",
    "        self.attention_linear1 = nn.Linear(hidden_dim * 2, hidden_dim)  # *2 for bidirectional\n",
    "        self.attention_linear2 = nn.Linear(hidden_dim, 1)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Fully connected output layer\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        \n",
    "    def forward(self, text, text_lengths, text_vocab=None):\n",
    "        # text: [batch_size, seq_len] - token indices from torchtext\n",
    "        # text_lengths: [batch_size] - actual sequence lengths\n",
    "        # text_vocab: TEXT.vocab.itos to convert indices to tokens\n",
    "        \n",
    "        batch_size = text.size(0)\n",
    "        seq_len = text.size(1)\n",
    "        device = text.device\n",
    "        \n",
    "        # Convert token indices back to text strings using vocab\n",
    "        text_list = []\n",
    "        for i in range(batch_size):\n",
    "            actual_len = text_lengths[i].item() if isinstance(text_lengths[i], torch.Tensor) else text_lengths[i]\n",
    "            tokens = []\n",
    "            for j in range(min(actual_len, seq_len)):\n",
    "                token_idx = text[i, j].item()\n",
    "                if text_vocab is not None and token_idx < len(text_vocab):\n",
    "                    token = text_vocab[token_idx]\n",
    "                    # Skip special tokens\n",
    "                    if token not in ['<pad>', '<unk>', '<sos>', '<eos>']:\n",
    "                        tokens.append(token)\n",
    "                else:\n",
    "                    # Fallback if vocab not provided\n",
    "                    tokens.append(str(token_idx))\n",
    "            # Join tokens to form sentence\n",
    "            sentence = \" \".join(tokens)\n",
    "            text_list.append(sentence)\n",
    "        \n",
    "        # Tokenize with BERT\n",
    "        encoded = self.bert_tokenizer(\n",
    "            text_list,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors='pt'\n",
    "        ).to(device)\n",
    "        \n",
    "        # Get BERT embeddings\n",
    "        with torch.set_grad_enabled(not self.freeze_bert):\n",
    "            bert_outputs = self.bert_model(**encoded)\n",
    "            bert_embeddings = bert_outputs.last_hidden_state  # [batch_size, seq_len, 768]\n",
    "        \n",
    "        # Get actual sequence lengths from BERT tokenizer\n",
    "        bert_lengths = encoded['attention_mask'].sum(dim=1).cpu()\n",
    "\n",
    "\n",
    "        # Get BERT embeddings\n",
    "        with torch.set_grad_enabled(not self.freeze_bert):\n",
    "            bert_outputs = self.bert_model(**encoded)\n",
    "            bert_embeddings = bert_outputs.last_hidden_state  # [batch_size, seq_len, 768]\n",
    "\n",
    "        # Align lengths\n",
    "        bert_lengths = encoded['attention_mask'].sum(dim=1)\n",
    "        max_len = bert_embeddings.size(1)\n",
    "        bert_lengths = bert_lengths.clamp(max=max_len).cpu()\n",
    "\n",
    "        # Pack safely\n",
    "        packed_bert = nn.utils.rnn.pack_padded_sequence(\n",
    "        bert_embeddings, bert_lengths, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "\n",
    "        \n",
    "        # Pack sequences for efficient RNN processing\n",
    "        packed_bert = nn.utils.rnn.pack_padded_sequence(\n",
    "            bert_embeddings, bert_lengths, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        \n",
    "        # Pass through bidirectional LSTM\n",
    "        packed_output, (hidden, cell) = self.bilstm(packed_bert)\n",
    "        \n",
    "        # Unpack sequences\n",
    "        bilstm_output, output_lengths = nn.utils.rnn.pad_packed_sequence(\n",
    "            packed_output, batch_first=True\n",
    "        )\n",
    "        # bilstm_output: [batch_size, seq_len, hidden_dim * 2]\n",
    "        \n",
    "        # Apply Attention Mechanism\n",
    "        attention_scores = self.attention_linear1(bilstm_output)  # [batch_size, seq_len, hidden_dim]\n",
    "        attention_scores = self.tanh(attention_scores)\n",
    "        attention_scores = self.attention_linear2(attention_scores).squeeze(2)  # [batch_size, seq_len]\n",
    "        \n",
    "        # Mask padding positions\n",
    "        batch_size_attn, seq_len_attn = bilstm_output.size(0), bilstm_output.size(1)\n",
    "        mask = torch.arange(seq_len_attn, device=device).unsqueeze(0) < bert_lengths.unsqueeze(1).to(device)\n",
    "        attention_scores = attention_scores.masked_fill(~mask, float('-inf'))\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        attention_weights = torch.softmax(attention_scores, dim=1).unsqueeze(2)  # [batch_size, seq_len, 1]\n",
    "        \n",
    "        # Compute weighted sum\n",
    "        context_vector = torch.sum(attention_weights * bilstm_output, dim=1)  # [batch_size, hidden_dim * 2]\n",
    "        \n",
    "        # Apply dropout\n",
    "        context_vector = self.dropout(context_vector)\n",
    "        \n",
    "        # Pass through fully connected layer\n",
    "        output = self.fc(context_vector)  # [batch_size, output_dim]\n",
    "        \n",
    "        return output\n",
    "\n",
    "print(\">>> RNNBertClassifier model class created\")\n",
    "print(\"    - Uses pretrained BERT (bert-base-uncased) for contextualized embeddings\")\n",
    "print(\"    - Passes BERT embeddings through bidirectional LSTM\")\n",
    "print(\"    - Applies attention mechanism to focus on important parts\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1708baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to process batch for BERT model\n",
    "def process_batch_bert(batch, text_vocab, debug=False):\n",
    "    text, text_lengths = batch.text\n",
    "    labels = batch.label\n",
    "\n",
    "    # Convert to proper format\n",
    "    if isinstance(text, tuple):\n",
    "        text, text_lengths = text\n",
    "\n",
    "    # >>> ADD THIS LINE <<<\n",
    "    text = text.transpose(0, 1)  # (seq_len, batch_size) â†’ (batch_size, seq_len)\n",
    "\n",
    "    # Ensure text_lengths is 1D\n",
    "    if text_lengths.dim() > 1:\n",
    "        text_lengths = text_lengths.squeeze()\n",
    "\n",
    "    text = text.to(device)\n",
    "    text_lengths = text_lengths.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    if debug:\n",
    "        print(f\"DEBUG BERT BATCH - text shape: {text.shape}, text_lengths shape: {text_lengths.shape}, labels shape: {labels.shape}\")\n",
    "    \n",
    "    return text, text_lengths, labels, text_vocab\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086bb270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Train RNN + BERT Model\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING RNN + BERT MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Set seed for reproducibility\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Hyperparameters for RNN + BERT\n",
    "BERT_HIDDEN_DIM = 256  # Effective hidden size will be 512 (256*2) due to bidirectional\n",
    "BERT_N_LAYERS = 2\n",
    "BERT_DROPOUT = 0.5\n",
    "BERT_BATCH_SIZE = 32  # Smaller batch size due to BERT's memory requirements\n",
    "BERT_LEARNING_RATE = 2e-5  # Lower learning rate for fine-tuning BERT\n",
    "BERT_N_EPOCHS = 50  # Fewer epochs as BERT converges faster\n",
    "BERT_PATIENCE = 7\n",
    "BERT_FREEZE = False  # Fine-tune BERT (set to True to freeze BERT)\n",
    "\n",
    "# Create data iterators\n",
    "train_iter_bert = data.BucketIterator(\n",
    "    train_data,\n",
    "    batch_size=BERT_BATCH_SIZE,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    shuffle=True,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "val_iter_bert = data.BucketIterator(\n",
    "    validation_data,\n",
    "    batch_size=BERT_BATCH_SIZE,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    shuffle=False,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "test_iter_bert = data.BucketIterator(\n",
    "    test_data,\n",
    "    batch_size=BERT_BATCH_SIZE,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    shuffle=False,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Create RNN + BERT model\n",
    "bert_model = RNNBertClassifier(\n",
    "    output_dim=num_classes,\n",
    "    hidden_dim=BERT_HIDDEN_DIM,\n",
    "    n_layers=BERT_N_LAYERS,\n",
    "    dropout=BERT_DROPOUT,\n",
    "    bert_model_name='bert-base-uncased',\n",
    "    freeze_bert=BERT_FREEZE\n",
    ").to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "# Use different learning rates for BERT and other layers\n",
    "bert_params = list(bert_model.bert_model.parameters())\n",
    "other_params = [p for n, p in bert_model.named_parameters() if 'bert_model' not in n]\n",
    "\n",
    "bert_optimizer = optim.AdamW([\n",
    "    {'params': bert_params, 'lr': BERT_LEARNING_RATE},\n",
    "    {'params': other_params, 'lr': BERT_LEARNING_RATE * 10}  # Higher LR for new layers\n",
    "], weight_decay=0.01)\n",
    "\n",
    "print(f\"\\n>>> RNN + BERT Model Configuration:\")\n",
    "print(f\"    BERT Model: bert-base-uncased\")\n",
    "print(f\"    BERT Frozen: {BERT_FREEZE}\")\n",
    "print(f\"    BiLSTM Hidden Dim: {BERT_HIDDEN_DIM} (effective: {BERT_HIDDEN_DIM * 2} due to bidirectional)\")\n",
    "print(f\"    BiLSTM Layers: {BERT_N_LAYERS}\")\n",
    "print(f\"    Dropout: {BERT_DROPOUT}\")\n",
    "print(f\"    Learning Rate: {BERT_LEARNING_RATE} (BERT), {BERT_LEARNING_RATE * 10} (other layers)\")\n",
    "print(f\"    Batch Size: {BERT_BATCH_SIZE}\")\n",
    "print(f\"    Max Epochs: {BERT_N_EPOCHS}, Patience: {BERT_PATIENCE}\")\n",
    "\n",
    "# Modified training function for BERT\n",
    "def train_model_with_history_bert(model, train_iterator, val_iterator, optimizer, criterion, \n",
    "                                   n_epochs, device, patience=10, model_name=\"model\", text_vocab=None):\n",
    "    \"\"\"\n",
    "    Train the model with early stopping and track training history (BERT version)\n",
    "    \"\"\"\n",
    "    best_val_acc = 0.0\n",
    "    best_val_f1 = 0.0\n",
    "    best_val_auc = 0.0\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    # History tracking\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "    val_f1s = []\n",
    "    val_aucs = []\n",
    "    \n",
    "    print(f\"\\n>>> Training {model_name}\")\n",
    "    print(f\"    Parameters: {count_parameters(model):,}\")\n",
    "    print(f\"    Max epochs: {n_epochs}, Patience: {patience}\")\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        \n",
    "        for batch in train_iterator:\n",
    "            text, text_lengths, labels, _ = process_batch_bert(batch, text_vocab, debug=False)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(text, text_lengths, text_vocab=text_vocab)\n",
    "            loss = criterion(predictions, labels)\n",
    "            loss.backward()\n",
    "            # Gradient clipping for BERT\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_iterator)\n",
    "        train_acc = accuracy_score(train_labels, train_preds)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        val_probs = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_iterator:\n",
    "                text, text_lengths, labels, _ = process_batch_bert(batch, text_vocab, debug=False)\n",
    "                predictions = model(text, text_lengths, text_vocab=text_vocab)\n",
    "                loss = criterion(predictions, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                probs = torch.softmax(predictions, dim=1)\n",
    "                preds = torch.argmax(predictions, dim=1)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "                val_probs.extend(probs.cpu().numpy())\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_iterator)\n",
    "        val_acc = accuracy_score(val_labels, val_preds)\n",
    "        val_f1 = f1_score(val_labels, val_preds, average='weighted')\n",
    "        \n",
    "        # Calculate AUC-ROC\n",
    "        try:\n",
    "            val_probs_array = np.array(val_probs)\n",
    "            val_labels_bin = label_binarize(val_labels, classes=range(num_classes))\n",
    "            val_auc = roc_auc_score(val_labels_bin, val_probs_array, average='weighted', multi_class='ovr')\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not calculate AUC-ROC for {model_name} at epoch {epoch+1}: {e}\")\n",
    "            val_auc = 0.0\n",
    "        \n",
    "        # Store history\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "        val_f1s.append(val_f1)\n",
    "        val_aucs.append(val_auc)\n",
    "        \n",
    "        # Early stopping and model saving\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_val_f1 = val_f1\n",
    "            best_val_auc = val_auc\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict().copy()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        end_time = time.time()\n",
    "        epoch_mins, epoch_secs = divmod(end_time - start_time, 60)\n",
    "        \n",
    "        print(f'Epoch: {epoch+1:02}/{n_epochs} | Time: {int(epoch_mins)}m {int(epoch_secs)}s')\n",
    "        print(f'\\tTrain Loss: {avg_train_loss:.4f} | Train Acc: {train_acc*100:.2f}%')\n",
    "        print(f'\\tVal Loss: {avg_val_loss:.4f} | Val Acc: {val_acc*100:.2f}% | Val F1: {val_f1:.4f} | Val AUC: {val_auc:.4f}')\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f'\\t>>> Early stopping at epoch {epoch+1}, best val acc: {best_val_acc*100:.2f}%')\n",
    "            break\n",
    "    \n",
    "    # Load best model state\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    print(f'\\n>>> Training completed! Best validation accuracy: {best_val_acc*100:.2f}%')\n",
    "    print(f'    Best validation F1: {best_val_f1:.4f}')\n",
    "    print(f'    Best validation AUC-ROC: {best_val_auc:.4f}')\n",
    "    \n",
    "    # Return history dictionary\n",
    "    history = {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'val_accs': val_accs,\n",
    "        'val_f1s': val_f1s,\n",
    "        'val_aucs': val_aucs,\n",
    "        'best_val_acc': best_val_acc,\n",
    "        'best_val_f1': best_val_f1,\n",
    "        'best_val_auc': best_val_auc,\n",
    "        'epochs_trained': len(train_losses)\n",
    "    }\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# Train RNN + BERT model\n",
    "bert_model, bert_history = train_model_with_history_bert(\n",
    "    bert_model, train_iter_bert, val_iter_bert, bert_optimizer, criterion,\n",
    "    BERT_N_EPOCHS, device, patience=BERT_PATIENCE, model_name=\"RNN+BERT\", text_vocab=TEXT.vocab.itos\n",
    ")\n",
    "\n",
    "# Save best model\n",
    "torch.save(bert_model.state_dict(), 'weights/rnn_bert_best.pt')\n",
    "print(f\"\\n>>> RNN + BERT model saved to 'weights/rnn_bert_best.pt'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c18dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Evaluate RNN + BERT Model on Test Set\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST SET EVALUATION - RNN + BERT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Modified evaluate function for BERT\n",
    "def evaluate_model_bert(model, iterator, criterion, device, model_name, text_vocab=None):\n",
    "    \"\"\"Evaluate model on test set and return metrics (BERT version)\"\"\"\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_preds = []\n",
    "    test_labels = []\n",
    "    test_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text, text_lengths, labels, _ = process_batch_bert(batch, text_vocab, debug=False)\n",
    "            predictions = model(text, text_lengths, text_vocab=text_vocab)\n",
    "            loss = criterion(predictions, labels)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            probs = torch.softmax(predictions, dim=1)\n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            test_preds.extend(preds.cpu().numpy())\n",
    "            test_labels.extend(labels.cpu().numpy())\n",
    "            test_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    avg_test_loss = test_loss / len(iterator)\n",
    "    test_acc = accuracy_score(test_labels, test_preds)\n",
    "    test_f1 = f1_score(test_labels, test_preds, average='weighted')\n",
    "    \n",
    "    # Calculate AUC-ROC\n",
    "    try:\n",
    "        test_probs_array = np.array(test_probs)\n",
    "        test_labels_bin = label_binarize(test_labels, classes=range(num_classes))\n",
    "        test_auc = roc_auc_score(test_labels_bin, test_probs_array, average='weighted', multi_class='ovr')\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not calculate AUC-ROC for {model_name}: {e}\")\n",
    "        test_auc = 0.0\n",
    "    \n",
    "    return avg_test_loss, test_acc, test_f1, test_auc\n",
    "\n",
    "# Evaluate RNN + BERT\n",
    "print(\"\\n>>> Evaluating RNN + BERT on test set...\")\n",
    "bert_test_loss, bert_test_acc, bert_test_f1, bert_test_auc = evaluate_model_bert(\n",
    "    bert_model, test_iter_bert, criterion, device, \"RNN+BERT\", text_vocab=TEXT.vocab.itos\n",
    ")\n",
    "\n",
    "print(f\"\\n>>> RNN + BERT Test Set Results:\")\n",
    "print(f\"    Test Loss: {bert_test_loss:.4f}\")\n",
    "print(f\"    Test Accuracy: {bert_test_acc*100:.2f}%\")\n",
    "print(f\"    Test F1 Score: {bert_test_f1:.4f}\")\n",
    "print(f\"    Test AUC-ROC: {bert_test_auc:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12be395b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Part 3.3 Final Summary - All Models Including RNN + BERT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 3.3 FINAL SUMMARY - ALL MODELS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Model':<20} {'Val Acc':<12} {'Val F1':<12} {'Val AUC':<12} {'Test Acc':<12} {'Test F1':<12} {'Test AUC':<12}\")\n",
    "print(\"-\" * 100)\n",
    "print(f\"{'biLSTM':<20} {bilstm_history['best_val_acc']*100:<12.2f}% {bilstm_history['best_val_f1']:<12.4f} {bilstm_history['best_val_auc']:<12.4f} {bilstm_test_acc*100:<12.2f}% {bilstm_test_f1:<12.4f} {bilstm_test_auc:<12.4f}\")\n",
    "print(f\"{'biGRU':<20} {bigru_history['best_val_acc']*100:<12.2f}% {bigru_history['best_val_f1']:<12.4f} {bigru_history['best_val_auc']:<12.4f} {bigru_test_acc*100:<12.2f}% {bigru_test_f1:<12.4f} {bigru_test_auc:<12.4f}\")\n",
    "print(f\"{'CNN':<20} {cnn_history['best_val_acc']*100:<12.2f}% {cnn_history['best_val_f1']:<12.4f} {cnn_history['best_val_auc']:<12.4f} {cnn_test_acc*100:<12.2f}% {cnn_test_f1:<12.4f} {cnn_test_auc:<12.4f}\")\n",
    "print(f\"{'RNN+Attention':<20} {rnn_attn_history['best_val_acc']*100:<12.2f}% {rnn_attn_history['best_val_f1']:<12.4f} {rnn_attn_history['best_val_auc']:<12.4f} {rnn_attn_test_acc*100:<12.2f}% {rnn_attn_test_f1:<12.4f} {rnn_attn_test_auc:<12.4f}\")\n",
    "print(f\"{'CNN+BiLSTM+Attn':<20} {hybrid_history['best_val_acc']*100:<12.2f}% {hybrid_history['best_val_f1']:<12.4f} {hybrid_history['best_val_auc']:<12.4f} {hybrid_test_acc*100:<12.2f}% {hybrid_test_f1:<12.4f} {hybrid_test_auc:<12.4f}\")\n",
    "print(f\"{'RNN+BERT':<20} {bert_history['best_val_acc']*100:<12.2f}% {bert_history['best_val_f1']:<12.4f} {bert_history['best_val_auc']:<12.4f} {bert_test_acc*100:<12.2f}% {bert_test_f1:<12.4f} {bert_test_auc:<12.4f}\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Find best model based on test accuracy\n",
    "all_models_final = {\n",
    "    'biLSTM': {\n",
    "        'val_acc': bilstm_history['best_val_acc'],\n",
    "        'val_f1': bilstm_history['best_val_f1'],\n",
    "        'val_auc': bilstm_history['best_val_auc'],\n",
    "        'test_acc': bilstm_test_acc,\n",
    "        'test_f1': bilstm_test_f1,\n",
    "        'test_auc': bilstm_test_auc\n",
    "    },\n",
    "    'biGRU': {\n",
    "        'val_acc': bigru_history['best_val_acc'],\n",
    "        'val_f1': bigru_history['best_val_f1'],\n",
    "        'val_auc': bigru_history['best_val_auc'],\n",
    "        'test_acc': bigru_test_acc,\n",
    "        'test_f1': bigru_test_f1,\n",
    "        'test_auc': bigru_test_auc\n",
    "    },\n",
    "    'CNN': {\n",
    "        'val_acc': cnn_history['best_val_acc'],\n",
    "        'val_f1': cnn_history['best_val_f1'],\n",
    "        'val_auc': cnn_history['best_val_auc'],\n",
    "        'test_acc': cnn_test_acc,\n",
    "        'test_f1': cnn_test_f1,\n",
    "        'test_auc': cnn_test_auc\n",
    "    },\n",
    "    'RNN+Attention': {\n",
    "        'val_acc': rnn_attn_history['best_val_acc'],\n",
    "        'val_f1': rnn_attn_history['best_val_f1'],\n",
    "        'val_auc': rnn_attn_history['best_val_auc'],\n",
    "        'test_acc': rnn_attn_test_acc,\n",
    "        'test_f1': rnn_attn_test_f1,\n",
    "        'test_auc': rnn_attn_test_auc\n",
    "    },\n",
    "    'CNN+BiLSTM+Attention': {\n",
    "        'val_acc': hybrid_history['best_val_acc'],\n",
    "        'val_f1': hybrid_history['best_val_f1'],\n",
    "        'val_auc': hybrid_history['best_val_auc'],\n",
    "        'test_acc': hybrid_test_acc,\n",
    "        'test_f1': hybrid_test_f1,\n",
    "        'test_auc': hybrid_test_auc\n",
    "    },\n",
    "    'RNN+BERT': {\n",
    "        'val_acc': bert_history['best_val_acc'],\n",
    "        'val_f1': bert_history['best_val_f1'],\n",
    "        'val_auc': bert_history['best_val_auc'],\n",
    "        'test_acc': bert_test_acc,\n",
    "        'test_f1': bert_test_f1,\n",
    "        'test_auc': bert_test_auc\n",
    "    }\n",
    "}\n",
    "\n",
    "best_model_test_acc = max(all_models_final.items(), key=lambda x: x[1]['test_acc'])\n",
    "best_model_val_acc = max(all_models_final.items(), key=lambda x: x[1]['val_acc'])\n",
    "\n",
    "print(f\"\\n>>> Best Model by Test Accuracy: {best_model_test_acc[0]}\")\n",
    "print(f\"    Test Accuracy: {best_model_test_acc[1]['test_acc']*100:.2f}%\")\n",
    "print(f\"    Test F1: {best_model_test_acc[1]['test_f1']:.4f}\")\n",
    "print(f\"    Test AUC-ROC: {best_model_test_acc[1]['test_auc']:.4f}\")\n",
    "\n",
    "print(f\"\\n>>> Best Model by Validation Accuracy: {best_model_val_acc[0]}\")\n",
    "print(f\"    Validation Accuracy: {best_model_val_acc[1]['val_acc']*100:.2f}%\")\n",
    "print(f\"    Validation F1: {best_model_val_acc[1]['val_f1']:.4f}\")\n",
    "print(f\"    Validation AUC-ROC: {best_model_val_acc[1]['val_auc']:.4f}\")\n",
    "\n",
    "print(\"\\n>>> Part 3.3 Complete!\")\n",
    "print(\"    - RNN + Attention: Additive attention mechanism with bidirectional LSTM\")\n",
    "print(\"    - CNN + BiLSTM + Attention: Hybrid model combining CNN, BiLSTM, and attention\")\n",
    "print(\"    - RNN + BERT: Pretrained BERT embeddings with bidirectional LSTM and attention\")\n",
    "print(\"    - All models compared above with validation and test metrics\")\n",
    "print(\"    - All metrics (Accuracy, F1, AUC-ROC) reported for validation and test sets\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f951e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Plot Training Curves for RNN + BERT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PLOTTING TRAINING CURVES - RNN + BERT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create figure with subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Training Loss Curves (All Models)\n",
    "axes[0, 0].plot(bilstm_history['train_losses'], label='biLSTM Train Loss', marker='o', linewidth=2)\n",
    "axes[0, 0].plot(bigru_history['train_losses'], label='biGRU Train Loss', marker='s', linewidth=2)\n",
    "axes[0, 0].plot(cnn_history['train_losses'], label='CNN Train Loss', marker='^', linewidth=2)\n",
    "axes[0, 0].plot(rnn_attn_history['train_losses'], label='RNN+Attention Train Loss', marker='d', linewidth=2)\n",
    "axes[0, 0].plot(hybrid_history['train_losses'], label='CNN+BiLSTM+Attn Train Loss', marker='*', linewidth=2)\n",
    "axes[0, 0].plot(bert_history['train_losses'], label='RNN+BERT Train Loss', marker='x', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Training Loss', fontsize=12)\n",
    "axes[0, 0].set_title('Training Loss Curves (All Models)', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].legend(fontsize=9)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Validation Accuracy Curves (All Models)\n",
    "axes[0, 1].plot([acc*100 for acc in bilstm_history['val_accs']], label='biLSTM Val Acc', marker='o', linewidth=2)\n",
    "axes[0, 1].plot([acc*100 for acc in bigru_history['val_accs']], label='biGRU Val Acc', marker='s', linewidth=2)\n",
    "axes[0, 1].plot([acc*100 for acc in cnn_history['val_accs']], label='CNN Val Acc', marker='^', linewidth=2)\n",
    "axes[0, 1].plot([acc*100 for acc in rnn_attn_history['val_accs']], label='RNN+Attention Val Acc', marker='d', linewidth=2)\n",
    "axes[0, 1].plot([acc*100 for acc in hybrid_history['val_accs']], label='CNN+BiLSTM+Attn Val Acc', marker='*', linewidth=2)\n",
    "axes[0, 1].plot([acc*100 for acc in bert_history['val_accs']], label='RNN+BERT Val Acc', marker='x', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Validation Accuracy (%)', fontsize=12)\n",
    "axes[0, 1].set_title('Validation Accuracy Curves (All Models)', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].legend(fontsize=9)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: RNN+BERT Training and Validation Loss\n",
    "axes[1, 0].plot(bert_history['train_losses'], label='RNN+BERT Train Loss', marker='o', linewidth=2)\n",
    "axes[1, 0].plot(bert_history['val_losses'], label='RNN+BERT Val Loss', marker='s', linewidth=2)\n",
    "axes[1, 0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Loss', fontsize=12)\n",
    "axes[1, 0].set_title('RNN+BERT: Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].legend(fontsize=11)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: RNN+BERT Validation Accuracy\n",
    "axes[1, 1].plot([acc*100 for acc in bert_history['val_accs']], label='RNN+BERT Val Acc', marker='o', linewidth=2, color='brown')\n",
    "axes[1, 1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Validation Accuracy (%)', fontsize=12)\n",
    "axes[1, 1].set_title('RNN+BERT: Validation Accuracy Curve', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].legend(fontsize=11)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('part3_3_rnn_bert_training_curves.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"    Saved training curves to 'part3_3_rnn_bert_training_curves.png'\")\n",
    "plt.show()  # Display the plot in notebook\n",
    "\n",
    "# Also create separate plots for Part 3.3 requirement (training loss and validation accuracy)\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Training Loss Curve\n",
    "ax1.plot(bert_history['train_losses'], label='RNN+BERT Train Loss', marker='o', linewidth=2, color='blue')\n",
    "ax1.plot(bert_history['val_losses'], label='RNN+BERT Val Loss', marker='s', linewidth=2, color='red')\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Loss', fontsize=12)\n",
    "ax1.set_title('Training Loss Curve (RNN+BERT)', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Validation Accuracy Curve\n",
    "ax2.plot([acc*100 for acc in bert_history['val_accs']], label='RNN+BERT Val Acc', marker='o', linewidth=2, color='brown')\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Validation Accuracy (%)', fontsize=12)\n",
    "ax2.set_title('Validation Accuracy Curve (RNN+BERT)', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('part3_3_bert_training_curves.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"    Saved Part 3.3 BERT curves to 'part3_3_bert_training_curves.png'\")\n",
    "plt.show()  # Display the plot in notebook\n",
    "\n",
    "print(\"\\n>>> Training curves plotted and saved for RNN + BERT model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1f31be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfebdb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# c) \n",
    "\n",
    "# 1: (hybrid model) rnn + attention layer\n",
    "# 2: (hybrid model) cnn + bilstm + attention layer\n",
    "# 3: (hybrid model) bigru + pretrained transformer (berto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21396841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PART 3.4: Targeted Improvement for Weak Topics\n",
    "# Strategy: Data Augmentation, Positional Embeddings\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 3.4: TARGETED IMPROVEMENT FOR WEAK TOPICS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nStrategies:\")\n",
    "print(\"  1. Data Augmentation for imbalanced classes (especially ABBR)\")\n",
    "print(\"  2. Positional Embeddings in attention layer\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Import required libraries for augmentation\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "try:\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9259d813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FastText model\n",
    "print(\"\\n    Loading FastText model...\")\n",
    "fatter_fasttext_bin = load_facebook_model('crawl-300d-2M-subword/crawl-300d-2M-subword.bin')\n",
    "print(\"    âœ“ FastText model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ed5cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Step 1: Prepare Enhanced Embeddings with <mask> Token\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n>>> Step 1: Preparing Enhanced Embeddings with <mask> Token...\")\n",
    "\n",
    "# TEXT = data.Field ( tokenize = 'spacy', tokenizer_language = 'en_core_web_sm', include_lengths = True )\n",
    "\n",
    "# # For multi - class classification labels\n",
    "# LABEL = data.LabelField ()\n",
    "\n",
    "# SEED = 42\n",
    "# random.seed(SEED)\n",
    "# np.random.seed(SEED)\n",
    "# torch.manual_seed(SEED)\n",
    "\n",
    "# # Load the TREC dataset\n",
    "# # Train / Validation / Test split\n",
    "# train_data, test_data = datasets.TREC.splits( TEXT, LABEL, fine_grained = False )\n",
    "\n",
    "# train_data, validation_data = train_data.split(\n",
    "#     split_ratio=0.8,\n",
    "#     stratified=True,\n",
    "#     strata_field='label'\n",
    "# )\n",
    "# print(vars(train_data.examples[0]))\n",
    "\n",
    "TEXT_P34 = data.Field()\n",
    "LABEL_P34 = data.LabelField()\n",
    "\n",
    "train_data_p34 = data.Dataset(train_data.examples, train_data.fields)\n",
    "TEXT_P34 = copy.deepcopy(TEXT)\n",
    "LABEL_P34 = copy.deepcopy(LABEL)\n",
    "\n",
    "# Add <mask> token to vocabulary\n",
    "# mask_token = '<mask>'\n",
    "# if mask_token not in TEXT_P34.vocab.stoi:\n",
    "#     mask_index = len(TEXT_P34.vocab.stoi)\n",
    "#     TEXT_P34.vocab.stoi[mask_token] = mask_index\n",
    "#     TEXT_P34.vocab.itos.append(mask_token)\n",
    "#     print(f\"    âœ“ Added <mask> token at index {mask_index}\")\n",
    "# else:\n",
    "#     mask_index = TEXT_P34.vocab.stoi[mask_token]\n",
    "#     print(f\"    âœ“ <mask> token already exists at index {mask_index}\")\n",
    "\n",
    "#TODO: Add loading of fasttext back here\n",
    "\n",
    "pretrained_embeddings_p34 = None\n",
    "embedding_dim_p34 = fatter_fasttext_bin.wv.vector_size\n",
    "\n",
    "# Build embedding matrix with <mask> token\n",
    "num_tokens_p34 = len(TEXT_P34.vocab.stoi)\n",
    "emb_matrix_p34 = np.zeros((num_tokens_p34, embedding_dim_p34), dtype=np.float32)\n",
    "\n",
    "pad_tok = TEXT_P34.pad_token\n",
    "unk_tok = TEXT_P34.unk_token\n",
    "pad_index = TEXT_P34.vocab.stoi[pad_tok]\n",
    "unk_index = TEXT_P34.vocab.stoi[unk_tok]\n",
    "\n",
    "known_vecs_p34 = []\n",
    "mask_candidates = []\n",
    "\n",
    "print(\"    Processing vocabulary tokens...\")\n",
    "for idx, token in enumerate(TEXT_P34.vocab.itos):\n",
    "    # if token in {pad_tok, unk_tok, mask_token}:\n",
    "    if token in {pad_tok, unk_tok}:\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        vec = fatter_fasttext_bin.wv[token]\n",
    "        emb_matrix_p34[idx] = vec\n",
    "        known_vecs_p34.append(vec)\n",
    "        if len(mask_candidates) < 1000:\n",
    "            mask_candidates.append(vec)\n",
    "    except KeyError:\n",
    "        try:\n",
    "            vec = fatter_fasttext_bin.wv[token.lower()]\n",
    "            emb_matrix_p34[idx] = vec\n",
    "            known_vecs_p34.append(vec)\n",
    "            if len(mask_candidates) < 1000:\n",
    "                mask_candidates.append(vec)\n",
    "        except KeyError:\n",
    "            emb_matrix_p34[idx] = np.random.uniform(-0.05, 0.05, embedding_dim_p34).astype(np.float32)\n",
    "\n",
    "# Initialize special tokens\n",
    "if len(known_vecs_p34) > 0:\n",
    "    unk_mean_p34 = np.mean(known_vecs_p34, axis=0)\n",
    "    emb_matrix_p34[unk_index] = unk_mean_p34\n",
    "    print(f\"    âœ“ <unk> initialized as mean of {len(known_vecs_p34)} vectors\")\n",
    "\n",
    "# if len(mask_candidates) > 0:\n",
    "#     mask_mean_p34 = np.mean(mask_candidates, axis=0)\n",
    "#     emb_matrix_p34[mask_index] = mask_mean_p34\n",
    "#     print(f\"    âœ“ <mask> initialized as mean of {len(mask_candidates)} vectors\")\n",
    "# else:\n",
    "#     emb_matrix_p34[mask_index] = emb_matrix_p34[unk_index]\n",
    "\n",
    "emb_matrix_p34[pad_index] = np.zeros(embedding_dim_p34, dtype=np.float32)\n",
    "\n",
    "# Create and save embedding layer\n",
    "p34_embedding = torch.nn.Embedding(\n",
    "    num_tokens_p34, \n",
    "    embedding_dim_p34, \n",
    "    padding_idx=pad_index\n",
    ")\n",
    "p34_embedding.weight.data.copy_(torch.from_numpy(emb_matrix_p34))\n",
    "p34_embedding.weight.requires_grad = True\n",
    "\n",
    "torch.save(p34_embedding, 'embedding_weights_part34_enhanced.pt')\n",
    "pretrained_embeddings_p34 = p34_embedding.weight.data.clone()\n",
    "\n",
    "print(f\"    âœ“ Embeddings saved (shape: {pretrained_embeddings_p34.shape})\")\n",
    "print(f\"    âœ“ Ready for Part 3.4 training\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb52911",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(TEXT.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b2a68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Step 2: Data Augmentation Functions for Imbalanced Classes\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n>>> Step 2: Implementing Data Augmentation Functions...\")\n",
    "\n",
    "def get_synonyms(word):\n",
    "    \"\"\"Get synonyms for a word using WordNet\"\"\"\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonym = lemma.name().replace('_', ' ').lower()\n",
    "            if synonym != word and synonym.isalpha():\n",
    "                synonyms.add(synonym)\n",
    "    return list(synonyms)\n",
    "\n",
    "def synonym_replacement(tokens, n=1):\n",
    "    \"\"\"Replace n random words with their synonyms\"\"\"\n",
    "    new_tokens = tokens.copy()\n",
    "    words_to_replace = [i for i, word in enumerate(tokens) if word.isalpha() and len(word) > 2]\n",
    "    \n",
    "    if len(words_to_replace) == 0:\n",
    "        return tokens\n",
    "    \n",
    "    num_replacements = min(n, len(words_to_replace))\n",
    "    indices_to_replace = random.sample(words_to_replace, num_replacements)\n",
    "    \n",
    "    for idx in indices_to_replace:\n",
    "        synonyms = get_synonyms(tokens[idx])\n",
    "        if synonyms:\n",
    "            new_tokens[idx] = random.choice(synonyms)\n",
    "    \n",
    "    return new_tokens\n",
    "\n",
    "def random_insertion(tokens, n=1):\n",
    "    \"\"\"Randomly insert synonyms of n words\"\"\"\n",
    "    new_tokens = tokens.copy()\n",
    "    \n",
    "    for _ in range(n):\n",
    "        if len(new_tokens) == 0:\n",
    "            break\n",
    "        word = random.choice(new_tokens)\n",
    "        synonyms = get_synonyms(word)\n",
    "        if synonyms:\n",
    "            synonym = random.choice(synonyms)\n",
    "            insert_pos = random.randint(0, len(new_tokens))\n",
    "            new_tokens.insert(insert_pos, synonym)\n",
    "    \n",
    "    return new_tokens\n",
    "\n",
    "def random_deletion(tokens, p=0.1):\n",
    "    \"\"\"Randomly delete words with probability p\"\"\"\n",
    "    if len(tokens) == 1:\n",
    "        return tokens\n",
    "    \n",
    "    new_tokens = []\n",
    "    for token in tokens:\n",
    "        if random.random() > p:\n",
    "            new_tokens.append(token)\n",
    "    \n",
    "    if len(new_tokens) == 0:\n",
    "        return tokens[:1]\n",
    "    \n",
    "    return new_tokens\n",
    "\n",
    "def random_swap(tokens, n=1):\n",
    "    \"\"\"Randomly swap n pairs of words\"\"\"\n",
    "    new_tokens = tokens.copy()\n",
    "    \n",
    "    for _ in range(n):\n",
    "        if len(new_tokens) < 2:\n",
    "            break\n",
    "        idx1, idx2 = random.sample(range(len(new_tokens)), 2)\n",
    "        new_tokens[idx1], new_tokens[idx2] = new_tokens[idx2], new_tokens[idx1]\n",
    "    \n",
    "    return new_tokens\n",
    "\n",
    "def augment_text(text, augmentation_techniques=['synonym', 'insertion', 'deletion', 'swap'], \n",
    "                 num_augmentations=3):\n",
    "    \"\"\"Apply data augmentation to text\"\"\"\n",
    "    augmented_texts = []\n",
    "    \n",
    "    for _ in range(num_augmentations):\n",
    "        aug_text = text.copy()\n",
    "        technique = random.choice(augmentation_techniques)\n",
    "        \n",
    "        if technique == 'synonym' and len(aug_text) > 0:\n",
    "            aug_text = synonym_replacement(aug_text, n=random.randint(1, 2))\n",
    "        elif technique == 'insertion' and len(aug_text) > 0:\n",
    "            aug_text = random_insertion(aug_text, n=random.randint(1, 2))\n",
    "        elif technique == 'deletion' and len(aug_text) > 1:\n",
    "            aug_text = random_deletion(aug_text, p=0.1)\n",
    "        elif technique == 'swap' and len(aug_text) > 1:\n",
    "            aug_text = random_swap(aug_text, n=1)\n",
    "        \n",
    "        augmented_texts.append(aug_text)\n",
    "    \n",
    "    return augmented_texts\n",
    "\n",
    "print(\"    âœ“ Data augmentation functions ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea5f924",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Step 3: Apply Data Augmentation for Imbalanced Classes\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n>>> Step 3: Applying Data Augmentation for Imbalanced Classes...\")\n",
    "\n",
    "# Count current label distribution\n",
    "label_counts_p34 = Counter([ex.label for ex in train_data.examples])\n",
    "print(f\"\\nOriginal label distribution:\")\n",
    "for label, count in sorted(label_counts_p34.items()):\n",
    "    print(f\"  {label}: {count} samples ({count/len(train_data.examples)*100:.2f}%)\")\n",
    "\n",
    "# Target counts: Augment ABBR significantly, keep others similar\n",
    "target_counts_p34 = {\n",
    "    'ABBR': 500,   # Augment from 69 to 500 (~7x)\n",
    "    'DESC': 930,   # Keep original\n",
    "    'ENTY': 1000,  # Keep original\n",
    "    'HUM': 978,    # Keep original\n",
    "    'LOC': 668,    # Keep original\n",
    "    'NUM': 717     # Keep original\n",
    "}\n",
    "\n",
    "# Create augmented examples\n",
    "augmented_examples = list(train_data.examples)  # Start with all original examples\n",
    "\n",
    "for label in label_counts_p34.keys():\n",
    "    current_count = label_counts_p34[label]\n",
    "    target_count = target_counts_p34[label]\n",
    "    \n",
    "    if current_count < target_count:\n",
    "        label_examples = [ex for ex in train_data.examples if ex.label == label]\n",
    "        num_augmentations_needed = target_count - current_count\n",
    "        \n",
    "        print(f\"\\n  Augmenting {label}: {current_count} -> {target_count} samples\")\n",
    "        print(f\"    Generating {num_augmentations_needed} additional samples...\")\n",
    "        \n",
    "        augmented_count = 0\n",
    "        while augmented_count < num_augmentations_needed:\n",
    "            original_ex = random.choice(label_examples)\n",
    "            aug_texts = augment_text(original_ex.text, num_augmentations=1)\n",
    "            \n",
    "            for aug_text in aug_texts:\n",
    "                if augmented_count >= num_augmentations_needed:\n",
    "                    break\n",
    "                \n",
    "                new_ex = data.Example.fromlist([aug_text, label], \n",
    "                                               fields=[('text', TEXT_P34), ('label', LABEL_P34)])\n",
    "                augmented_examples.append(new_ex)\n",
    "                augmented_count += 1\n",
    "        \n",
    "        print(f\"    âœ“ Generated {augmented_count} augmented samples\")\n",
    "\n",
    "# Create augmented dataset with proper field structure\n",
    "augmented_train_data = data.Dataset(augmented_examples, fields=[('text', TEXT_P34), ('label', LABEL_P34)])\n",
    "\n",
    "# Verify augmented distribution\n",
    "new_label_counts = Counter([ex.label for ex in augmented_examples])\n",
    "print(f\"\\nAugmented label distribution:\")\n",
    "for label, count in sorted(new_label_counts.items()):\n",
    "    print(f\"  {label}: {count} samples ({count/len(augmented_examples)*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\n  Total samples: {len(train_data.examples)} -> {len(augmented_examples)}\")\n",
    "print(f\"  âœ“ Data augmentation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0e1c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_train_data.examples[0].label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc672cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_P34.vocab.stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0685fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "abbr_aug_ex = [ex for ex in augmented_train_data if ex.label == \"ABBR\"]\n",
    "abbr_ex = [ex for ex in train_data if ex.label == \"ABBR\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e804f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "[(ex.text, ex.label) for ex in augmented_train_data if ex.label not in ['ABBR','DESC','ENTY','HUM','LOC','NUM']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d882b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for ex in abbr_aug_ex:\n",
    "    if ex not in abbr_ex:\n",
    "        print(ex.text)\n",
    "        print(ex.label)\n",
    "        count += 1\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31e503f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many samples per label in the train set\n",
    "label_counts_p34 = Counter([ex.label for ex in augmented_train_data.examples])\n",
    "total_examples_p34 = len(augmented_train_data)\n",
    "\n",
    "print(\"\\nLabel distribution in training set:\")\n",
    "for label, count in sorted(label_counts_p34.items()):\n",
    "    percentage = (count / total_examples_p34) * 100\n",
    "    print(f\"- {label}: {count} samples ({percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20d97c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Step 5: RNN Model with Positional Embeddings in Attention Layer\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n>>> Step 5: Creating RNN Model with Positional Embeddings...\")\n",
    "\n",
    "class RNNWithPositionalAttentionClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    RNN with Attention and Positional Embeddings\n",
    "    Adds positional information to help the model understand word order\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, \n",
    "                 n_layers=2, dropout=0.5, padding_idx=0, pretrained_embeddings=None,\n",
    "                 attention_dim=None, max_seq_length=100, use_positional_emb=True):\n",
    "        super(RNNWithPositionalAttentionClassifier, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.attention_dim = attention_dim if attention_dim else hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.use_positional_emb = use_positional_emb\n",
    "        \n",
    "        # Word Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "            self.embedding.weight.requires_grad = True\n",
    "        \n",
    "        # Positional Embedding layer (learnable)\n",
    "        if self.use_positional_emb:\n",
    "            self.positional_embedding = nn.Embedding(max_seq_length, embedding_dim)\n",
    "            # Initialize with sinusoidal pattern\n",
    "            self._init_positional_embeddings()\n",
    "        \n",
    "        # Bidirectional LSTM layer\n",
    "        self.rnn = nn.LSTM(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=n_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout if n_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Attention Mechanism with positional awareness\n",
    "        self.attention_linear1 = nn.Linear(hidden_dim * 2, self.attention_dim)\n",
    "        self.attention_linear2 = nn.Linear(self.attention_dim, 1)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Fully connected output layer\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        \n",
    "    def _init_positional_embeddings(self):\n",
    "        \"\"\"Initialize positional embeddings with sinusoidal pattern\"\"\"\n",
    "        pe = torch.zeros(self.max_seq_length, self.embedding_dim)\n",
    "        position = torch.arange(0, self.max_seq_length).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, self.embedding_dim, 2).float() * \n",
    "                            -(np.log(10000.0) / self.embedding_dim))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Set as parameter data (no gradient for positional embeddings initially, but can be learned)\n",
    "        with torch.no_grad():\n",
    "            self.positional_embedding.weight.data = pe\n",
    "    \n",
    "    def forward(self, text, text_lengths):\n",
    "        # text: [batch_size, seq_len]\n",
    "        batch_size, seq_len = text.size()\n",
    "        \n",
    "        # Word Embeddings\n",
    "        word_embeddings = self.embedding(text)  # [batch_size, seq_len, embedding_dim]\n",
    "        \n",
    "        # Positional Embeddings\n",
    "        if self.use_positional_emb:\n",
    "            positions = torch.arange(0, seq_len, device=text.device).unsqueeze(0).expand(batch_size, -1)\n",
    "            positions = torch.clamp(positions, 0, self.max_seq_length - 1)\n",
    "            pos_embeddings = self.positional_embedding(positions)  # [batch_size, seq_len, embedding_dim]\n",
    "            \n",
    "            # Combine word and positional embeddings\n",
    "            embedded = word_embeddings + pos_embeddings\n",
    "        else:\n",
    "            embedded = word_embeddings\n",
    "        \n",
    "        # Pack sequences for efficient RNN processing\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, text_lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        \n",
    "        # Pass through bidirectional LSTM\n",
    "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
    "        \n",
    "        # Unpack the sequences\n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(\n",
    "            packed_output, batch_first=True\n",
    "        )\n",
    "        # output: [batch_size, seq_len, hidden_dim * 2]\n",
    "        \n",
    "        # Apply Attention Mechanism with positional information\n",
    "        attention_scores = self.attention_linear1(output)  # [batch_size, seq_len, attention_dim]\n",
    "        attention_scores = self.tanh(attention_scores)\n",
    "        attention_scores = self.attention_linear2(attention_scores).squeeze(2)  # [batch_size, seq_len]\n",
    "        \n",
    "        # Mask padding positions\n",
    "        mask = torch.arange(seq_len, device=text.device).unsqueeze(0) < text_lengths.unsqueeze(1)\n",
    "        attention_scores = attention_scores.masked_fill(~mask, float('-inf'))\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        attention_weights = torch.softmax(attention_scores, dim=1).unsqueeze(2)  # [batch_size, seq_len, 1]\n",
    "        \n",
    "        # Compute weighted sum of RNN outputs\n",
    "        context_vector = torch.sum(attention_weights * output, dim=1)  # [batch_size, hidden_dim * 2]\n",
    "        \n",
    "        # Apply dropout\n",
    "        context_vector = self.dropout(context_vector)\n",
    "        \n",
    "        # Pass through fully connected layer\n",
    "        output = self.fc(context_vector)  # [batch_size, output_dim]\n",
    "        \n",
    "        return output\n",
    "\n",
    "print(\"    âœ“ RNNWithPositionalAttentionClassifier model created\")\n",
    "print(\"    Features:\")\n",
    "print(\"      - Word embeddings (FastText pretrained)\")\n",
    "print(\"      - Positional embeddings (sinusoidal pattern)\")\n",
    "print(\"      - Bidirectional LSTM\")\n",
    "print(\"      - Attention mechanism with positional awareness\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75411bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Step 6: Create Iterators\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n>>> Step 6: Creating Iterators...\")\n",
    "\n",
    "P34_BATCH_SIZE = 64\n",
    "\n",
    "# Create iterators\n",
    "train_iter_p34 = data.BucketIterator(\n",
    "    augmented_train_data,\n",
    "    batch_size=P34_BATCH_SIZE,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    shuffle=True,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Validation and test iterators\n",
    "val_iter_p34 = data.BucketIterator(\n",
    "    validation_data,\n",
    "    batch_size=P34_BATCH_SIZE,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    shuffle=False,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "test_iter_p34 = data.BucketIterator(\n",
    "    test_data,\n",
    "    batch_size=P34_BATCH_SIZE,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    shuffle=False,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"    âœ“ Training iterator: {len(augmented_train_data)} samples\")\n",
    "print(f\"    âœ“ Validation iterator: {len(validation_data)} samples\")\n",
    "print(f\"    âœ“ Test iterator: {len(test_data)} samples\")\n",
    "print(f\"    âœ“ Batch size: {P34_BATCH_SIZE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee16638",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_P34.vocab.stoi['<mask>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce061dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Step 7: Train Part 3.4 Model\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n>>> Step 7: Training Part 3.4 Enhanced Model...\")\n",
    "\n",
    "# Model hyperparameters\n",
    "P34_EMBEDDING_DIM = embedding_dim_p34\n",
    "P34_HIDDEN_DIM = 256\n",
    "P34_N_LAYERS = 2\n",
    "P34_DROPOUT = 0.5\n",
    "P34_ATTENTION_DIM = 256\n",
    "P34_MAX_SEQ_LENGTH = 100\n",
    "P34_LEARNING_RATE = 0.001\n",
    "P34_N_EPOCHS = 50\n",
    "P34_PATIENCE = 10\n",
    "P34_NUM_CLASSES = len(LABEL_P34.vocab)\n",
    "\n",
    "# Create model\n",
    "p34_model = RNNWithPositionalAttentionClassifier(\n",
    "    vocab_size=len(TEXT_P34.vocab.stoi),\n",
    "    embedding_dim=P34_EMBEDDING_DIM,\n",
    "    hidden_dim=P34_HIDDEN_DIM,\n",
    "    output_dim=P34_NUM_CLASSES,\n",
    "    n_layers=P34_N_LAYERS,\n",
    "    dropout=P34_DROPOUT,\n",
    "    padding_idx=TEXT_P34.vocab.stoi[TEXT_P34.pad_token],\n",
    "    pretrained_embeddings=pretrained_embeddings_p34,\n",
    "    attention_dim=P34_ATTENTION_DIM,\n",
    "    max_seq_length=P34_MAX_SEQ_LENGTH,\n",
    "    use_positional_emb=True\n",
    ").to(device)\n",
    "\n",
    "print(f\"\\n>>> Part 3.4 Model Configuration:\")\n",
    "print(f\"    Embedding Dim: {P34_EMBEDDING_DIM}\")\n",
    "print(f\"    Hidden Dim: {P34_HIDDEN_DIM}\")\n",
    "print(f\"    Layers: {P34_N_LAYERS}\")\n",
    "print(f\"    Dropout: {P34_DROPOUT}\")\n",
    "print(f\"    Attention Dim: {P34_ATTENTION_DIM}\")\n",
    "print(f\"    Max Seq Length: {P34_MAX_SEQ_LENGTH}\")\n",
    "print(f\"    Positional Embeddings: Enabled\")\n",
    "print(f\"    Parameters: {sum(p.numel() for p in p34_model.parameters()):,}\")\n",
    "\n",
    "# Loss function and optimizer\n",
    "p34_criterion = nn.CrossEntropyLoss()\n",
    "p34_optimizer = optim.Adam(p34_model.parameters(), lr=P34_LEARNING_RATE)\n",
    "\n",
    "# Training function (using existing train_model_with_history if available, otherwise create one)\n",
    "def process_batch_p34(batch):\n",
    "    \"\"\"Process batch for Part 3.4\"\"\"\n",
    "    text, text_lengths = batch.text\n",
    "    labels = batch.label\n",
    "    return text, text_lengths, labels\n",
    "\n",
    "# Train the model\n",
    "print(\"\\n>>> Starting training...\")\n",
    "p34_model.train()\n",
    "best_val_acc = 0.0\n",
    "best_val_f1 = 0.0\n",
    "best_val_auc = 0.0\n",
    "patience_counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "train_losses_p34 = []\n",
    "val_losses_p34 = []\n",
    "val_accs_p34 = []\n",
    "val_f1s_p34 = []\n",
    "val_aucs_p34 = []\n",
    "\n",
    "for epoch in range(P34_N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Training phase\n",
    "    p34_model.train()\n",
    "    train_loss = 0\n",
    "    train_preds = []\n",
    "    train_labels = []\n",
    "    \n",
    "    for batch in train_iter_p34:\n",
    "        text, text_lengths, labels = process_batch_p34(batch)\n",
    "        p34_optimizer.zero_grad()\n",
    "        predictions = p34_model(text, text_lengths)\n",
    "        loss = p34_criterion(predictions, labels)\n",
    "        loss.backward()\n",
    "        p34_optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        preds = torch.argmax(predictions, dim=1)\n",
    "        train_preds.extend(preds.cpu().numpy())\n",
    "        train_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_iter_p34)\n",
    "    train_acc = accuracy_score(train_labels, train_preds)\n",
    "    \n",
    "    # Validation phase\n",
    "    p34_model.eval()\n",
    "    val_loss = 0\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "    val_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_iter_p34:\n",
    "            text, text_lengths, labels = process_batch_p34(batch)\n",
    "            predictions = p34_model(text, text_lengths)\n",
    "            loss = p34_criterion(predictions, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            probs = torch.softmax(predictions, dim=1)\n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "            val_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_iter_p34)\n",
    "    val_acc = accuracy_score(val_labels, val_preds)\n",
    "    val_f1 = f1_score(val_labels, val_preds, average='weighted')\n",
    "    \n",
    "    # Calculate AUC-ROC\n",
    "    try:\n",
    "        val_probs_array = np.array(val_probs)\n",
    "        val_labels_bin = label_binarize(val_labels, classes=range(P34_NUM_CLASSES))\n",
    "        val_auc = roc_auc_score(val_labels_bin, val_probs_array, average='weighted', multi_class='ovr')\n",
    "    except:\n",
    "        val_auc = 0.0\n",
    "    \n",
    "    # Store history\n",
    "    train_losses_p34.append(avg_train_loss)\n",
    "    val_losses_p34.append(avg_val_loss)\n",
    "    val_accs_p34.append(val_acc)\n",
    "    val_f1s_p34.append(val_f1)\n",
    "    val_aucs_p34.append(val_auc)\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_val_f1 = val_f1\n",
    "        best_val_auc = val_auc\n",
    "        patience_counter = 0\n",
    "        best_model_state = p34_model.state_dict().copy()\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = divmod(end_time - start_time, 60)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02}/{P34_N_EPOCHS} | Time: {int(epoch_mins)}m {int(epoch_secs)}s')\n",
    "    print(f'\\tTrain Loss: {avg_train_loss:.4f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\tVal Loss: {avg_val_loss:.4f} | Val Acc: {val_acc*100:.2f}% | Val F1: {val_f1:.4f} | Val AUC: {val_auc:.4f}')\n",
    "    \n",
    "    if patience_counter >= P34_PATIENCE:\n",
    "        print(f'\\t>>> Early stopping at epoch {epoch+1}, best val acc: {best_val_acc*100:.2f}%')\n",
    "        break\n",
    "\n",
    "# Load best model state\n",
    "if best_model_state is not None:\n",
    "    p34_model.load_state_dict(best_model_state)\n",
    "\n",
    "# Save model\n",
    "torch.save(p34_model.state_dict(), 'weights/part34_enhanced_best.pt')\n",
    "print(f\"\\n>>> Part 3.4 model saved to 'weights/part34_enhanced_best.pt'\")\n",
    "\n",
    "p34_history = {\n",
    "    'train_losses': train_losses_p34,\n",
    "    'val_losses': val_losses_p34,\n",
    "    'val_accs': val_accs_p34,\n",
    "    'val_f1s': val_f1s_p34,\n",
    "    'val_aucs': val_aucs_p34,\n",
    "    'best_val_acc': best_val_acc,\n",
    "    'best_val_f1': best_val_f1,\n",
    "    'best_val_auc': best_val_auc\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e31053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Step 8: Evaluate Part 3.4 Model on Test Set\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n>>> Step 8: Evaluating Part 3.4 Model on Test Set...\")\n",
    "\n",
    "p34_model.eval()\n",
    "test_loss_p34 = 0\n",
    "test_preds_p34 = []\n",
    "test_labels_p34 = []\n",
    "test_probs_p34 = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_iter_p34:\n",
    "        text, text_lengths, labels = process_batch_p34(batch)\n",
    "        predictions = p34_model(text, text_lengths)\n",
    "        loss = p34_criterion(predictions, labels)\n",
    "        test_loss_p34 += loss.item()\n",
    "        \n",
    "        probs = torch.softmax(predictions, dim=1)\n",
    "        preds = torch.argmax(predictions, dim=1)\n",
    "        test_preds_p34.extend(preds.cpu().numpy())\n",
    "        test_labels_p34.extend(labels.cpu().numpy())\n",
    "        test_probs_p34.extend(probs.cpu().numpy())\n",
    "\n",
    "test_loss_p34 = test_loss_p34 / len(test_iter_p34)\n",
    "test_acc_p34 = accuracy_score(test_labels_p34, test_preds_p34)\n",
    "test_f1_p34 = f1_score(test_labels_p34, test_preds_p34, average='weighted')\n",
    "\n",
    "try:\n",
    "    test_probs_array_p34 = np.array(test_probs_p34)\n",
    "    test_labels_bin_p34 = label_binarize(test_labels_p34, classes=range(P34_NUM_CLASSES))\n",
    "    test_auc_p34 = roc_auc_score(test_labels_bin_p34, test_probs_array_p34, average='weighted', multi_class='ovr')\n",
    "except:\n",
    "    test_auc_p34 = 0.0\n",
    "\n",
    "print(f\"\\n>>> Part 3.4 Test Set Results:\")\n",
    "print(f\"    Test Loss: {test_loss_p34:.4f}\")\n",
    "print(f\"    Test Accuracy: {test_acc_p34*100:.2f}%\")\n",
    "print(f\"    Test F1 Score: {test_f1_p34:.4f}\")\n",
    "print(f\"    Test AUC-ROC: {test_auc_p34:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbe2025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Step 9: Topic-wise Accuracy Evaluation and Comparison\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n>>> Step 9: Topic-wise Accuracy Evaluation...\")\n",
    "\n",
    "def evaluate_per_topic_p34(model, iterator, device):\n",
    "    \"\"\"Evaluate model performance per topic category\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    topic_correct = defaultdict(int)\n",
    "    topic_total = defaultdict(int)\n",
    "    \n",
    "    idx_to_label = LABEL_P34.vocab.itos\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text, text_lengths, labels = process_batch_p34(batch)\n",
    "            predictions = model(text, text_lengths)\n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            \n",
    "            for pred, label in zip(preds.cpu().numpy(), labels.cpu().numpy()):\n",
    "                topic_name = idx_to_label[label]\n",
    "                topic_total[topic_name] += 1\n",
    "                if pred == label:\n",
    "                    topic_correct[topic_name] += 1\n",
    "    \n",
    "    # Calculate accuracy per topic\n",
    "    topic_accuracies = {}\n",
    "    for topic in sorted(topic_total.keys()):\n",
    "        acc = topic_correct[topic] / topic_total[topic] if topic_total[topic] > 0 else 0\n",
    "        topic_accuracies[topic] = acc\n",
    "        print(f'  {topic}: {topic_correct[topic]}/{topic_total[topic]} = {acc*100:.2f}%')\n",
    "    \n",
    "    return topic_accuracies\n",
    "\n",
    "print(\"\\n>>> Part 3.4 Enhanced Model - Topic-wise Accuracy:\")\n",
    "p34_topic_acc = evaluate_per_topic_p34(p34_model, test_iter_p34, device)\n",
    "\n",
    "# Note: To compare with Part 2(e), you would need to load and evaluate the baseline model\n",
    "# For now, we'll display Part 3.4 results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 3.4 TOPIC-WISE ACCURACY SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Topic':<10} {'Accuracy':<12} {'Correct':<10} {'Total':<10}\")\n",
    "print(\"-\" * 50)\n",
    "for topic in sorted(p34_topic_acc.keys()):\n",
    "    # Get counts from evaluation\n",
    "    # (We'd need to modify evaluate_per_topic_p34 to return counts, or re-evaluate)\n",
    "    acc = p34_topic_acc[topic]\n",
    "    print(f\"{topic:<10} {acc*100:<12.2f}%\")\n",
    "\n",
    "print(\"\\n>>> Note: Compare these results with Part 2(e) baseline for improvement analysis\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffaf48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Step 10: Plot Training Curves for Part 3.4\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n>>> Step 10: Plotting Training Curves...\")\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Training Loss Curve\n",
    "ax1.plot(p34_history['train_losses'], label='Part 3.4 Train Loss', marker='o', linewidth=2, color='blue')\n",
    "ax1.plot(p34_history['val_losses'], label='Part 3.4 Val Loss', marker='s', linewidth=2, color='red')\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Loss', fontsize=12)\n",
    "ax1.set_title('Training Loss Curve (Part 3.4)', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Validation Accuracy Curve\n",
    "ax2.plot([acc*100 for acc in p34_history['val_accs']], label='Part 3.4 Val Acc', marker='o', linewidth=2, color='purple')\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Validation Accuracy (%)', fontsize=12)\n",
    "ax2.set_title('Validation Accuracy Curve (Part 3.4)', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('part3_4_training_curves.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"    Saved training curves to 'part3_4_training_curves.png'\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n>>> Training curves plotted and saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2120eb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PART 3.4 SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 3.4: TARGETED IMPROVEMENT FOR WEAK TOPICS - SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n>>> Strategies Implemented:\")\n",
    "print(\"  1. Data Augmentation for Imbalanced Classes\")\n",
    "print(\"     - ABBR: 69 -> 500 samples (~7x augmentation)\")\n",
    "print(\"     - Used synonym replacement, random insertion, deletion, and swap\")\n",
    "print(\"     - Other classes maintained original size\")\n",
    "print(\"\\n  2. Positional Embeddings in Attention Layer\")\n",
    "print(\"     - Sinusoidal positional embeddings added to word embeddings\")\n",
    "print(\"     - Helps model understand word order and sequence structure\")\n",
    "print(\"     - Integrated into attention mechanism\")\n",
    "\n",
    "print(\"\\n>>> Model Configuration:\")\n",
    "print(f\"  - Embedding Dim: {P34_EMBEDDING_DIM}\")\n",
    "print(f\"  - Hidden Dim: {P34_HIDDEN_DIM}\")\n",
    "print(f\"  - Layers: {P34_N_LAYERS} (bidirectional LSTM)\")\n",
    "print(f\"  - Dropout: {P34_DROPOUT}\")\n",
    "print(f\"  - Attention Dim: {P34_ATTENTION_DIM}\")\n",
    "print(f\"  - Max Seq Length: {P34_MAX_SEQ_LENGTH}\")\n",
    "print(f\"  - Learning Rate: {P34_LEARNING_RATE}\")\n",
    "print(f\"  - Batch Size: {P34_BATCH_SIZE}\")\n",
    "\n",
    "print(\"\\n>>> Results:\")\n",
    "print(f\"  - Best Val Accuracy: {p34_history['best_val_acc']*100:.2f}%\")\n",
    "print(f\"  - Best Val F1: {p34_history['best_val_f1']:.4f}\")\n",
    "print(f\"  - Best Val AUC: {p34_history['best_val_auc']:.4f}\")\n",
    "print(f\"  - Test Accuracy: {test_acc_p34*100:.2f}%\")\n",
    "print(f\"  - Test F1: {test_f1_p34:.4f}\")\n",
    "print(f\"  - Test AUC: {test_auc_p34:.4f}\")\n",
    "\n",
    "print(\"\\n>>> Topic-wise Accuracy (Test Set):\")\n",
    "for topic in sorted(p34_topic_acc.keys()):\n",
    "    print(f\"  - {topic}: {p34_topic_acc[topic]*100:.2f}%\")\n",
    "\n",
    "print(\"\\n>>> Expected Improvements:\")\n",
    "print(\"  - ABBR accuracy should improve significantly due to:\")\n",
    "print(\"    * 7x data augmentation (69 -> 500 samples)\")\n",
    "print(\"    * Positional embeddings helping with question structure\")\n",
    "print(\"\\n  - Other topics should maintain or slightly improve performance\")\n",
    "\n",
    "print(\"\\n>>> Files Saved:\")\n",
    "print(\"  - Model: weights/part34_enhanced_best.pt\")\n",
    "print(\"  - Embeddings: embedding_weights_part34_enhanced.pt\")\n",
    "print(\"  - Training Curves: part3_4_training_curves.png\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 3.4 COMPLETE\")\n",
    "print(\"=\"*80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
