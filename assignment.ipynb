{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "513c09f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# IMPORTANT: Fix for PyTorch/IPython compatibility issue\n",
    "# This must run BEFORE importing torch to avoid decorator conflicts\n",
    "# This fixes the \"disable() got an unexpected keyword argument 'wrapping'\" error\n",
    "\n",
    "# Method 1: Try to disable dynamo via environment variable (needs to be set before import)\n",
    "os.environ.setdefault('TORCH_COMPILE_DISABLE', '1')\n",
    "\n",
    "import torch\n",
    "\n",
    "# Method 2: Patch torch._dynamo.disable decorator after import\n",
    "try:\n",
    "    import torch._dynamo\n",
    "    # Patch the disable function to ignore the 'wrapping' parameter\n",
    "    if hasattr(torch._dynamo, 'disable'):\n",
    "        original_disable = torch._dynamo.disable\n",
    "        def patched_disable(fn=None, *args, **kwargs):\n",
    "            # Remove problematic 'wrapping' parameter if present\n",
    "            if 'wrapping' in kwargs:\n",
    "                kwargs.pop('wrapping')\n",
    "            if fn is None:\n",
    "                # Decorator usage: @disable\n",
    "                return lambda f: f\n",
    "            # Function usage: disable(fn) or disable(fn, **kwargs)\n",
    "            try:\n",
    "                return original_disable(fn, *args, **kwargs)\n",
    "            except TypeError:\n",
    "                # If original still fails, just return the function unwrapped\n",
    "                return fn\n",
    "        torch._dynamo.disable = patched_disable\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not patch torch._dynamo: {e}\")\n",
    "    pass  # If patching fails, continue anyway\n",
    "\n",
    "import random, string\n",
    "\n",
    "from torchtext import data , datasets\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "\n",
    "os.environ['GENSIM_DATA_DIR'] = os.path.join(os.getcwd(), 'gensim-data')\n",
    "\n",
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.fasttext import load_facebook_model\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c413bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['What', 'does', 'Elysium', 'mean', '?'], 'label': 'DESC'}\n",
      "\n",
      "Label distribution in training set:\n",
      "- ABBR: 69 samples (1.58%)\n",
      "- DESC: 930 samples (21.32%)\n",
      "- ENTY: 1000 samples (22.93%)\n",
      "- HUM: 978 samples (22.42%)\n",
      "- LOC: 668 samples (15.31%)\n",
      "- NUM: 717 samples (16.44%)\n",
      "Total samples: 4362, Sum of percentages: 100.00%\n"
     ]
    }
   ],
   "source": [
    "### Part 0: Dataset Preparation\n",
    "\n",
    "# For tokenization\n",
    "TEXT = data.Field ( tokenize = 'spacy', tokenizer_language = 'en_core_web_sm', include_lengths = True )\n",
    "\n",
    "# For multi - class classification labels\n",
    "LABEL = data.LabelField ()\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Load the TREC dataset\n",
    "# Train / Validation / Test split\n",
    "train_data, test_data = datasets.TREC.splits( TEXT, LABEL, fine_grained = False )\n",
    "\n",
    "train_data, validation_data = train_data.split(\n",
    "    split_ratio=0.8,\n",
    "    stratified=True,\n",
    "    strata_field='label',\n",
    "    random_state=random.seed(42)\n",
    ")\n",
    "print(vars(train_data.examples[0]))\n",
    "\n",
    "\n",
    "# Count how many samples per label in the train set\n",
    "label_counts = Counter([ex.label for ex in train_data.examples])\n",
    "total_examples = len(train_data)\n",
    "\n",
    "print(\"\\nLabel distribution in training set:\")\n",
    "for label, count in sorted(label_counts.items()):\n",
    "    percentage = (count / total_examples) * 100\n",
    "    print(f\"- {label}: {count} samples ({percentage:.2f}%)\")\n",
    "\n",
    "# Optional sanity check: total percentages should sum â‰ˆ 100%\n",
    "total_percentage = sum((count / total_examples) * 100 for count in label_counts.values())\n",
    "print(f\"Total samples: {total_examples}, Sum of percentages: {total_percentage:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442ff1d4",
   "metadata": {},
   "source": [
    "# Part 1: Prepare Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e9040ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size (with specials): 8143\n",
      "Vocabulary size (no specials): 8141\n"
     ]
    }
   ],
   "source": [
    "#### a) Size of Vocabulary formed from training data according to tokenization method\n",
    "# Vocabulary size (includes specials like <unk>, <pad>)\n",
    "TEXT.build_vocab(train_data, min_freq=1)\n",
    "vocab_size = len(TEXT.vocab)\n",
    "print(\"Vocabulary Size (with specials):\", vocab_size)\n",
    "\n",
    "vocab_wo_specials = len([w for w in TEXT.vocab.stoi if w not in {TEXT.unk_token, TEXT.pad_token}])\n",
    "print(\"Vocabulary size (no specials):\", vocab_wo_specials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1507b5a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of OOV word types (overall): 419\n",
      "\n",
      "OOV word types per topic label:\n",
      "- ABBR: 13 OOV types (out of 134, rate=9.70%)\n",
      "- DESC: 118 OOV types (out of 2278, rate=5.18%)\n",
      "- ENTY: 153 OOV types (out of 2991, rate=5.12%)\n",
      "- HUM: 146 OOV types (out of 3053, rate=4.78%)\n",
      "- LOC: 86 OOV types (out of 1795, rate=4.79%)\n",
      "- NUM: 76 OOV types (out of 1911, rate=3.98%)\n"
     ]
    }
   ],
   "source": [
    "#### b) How many OOV words exist in your training data?\n",
    "####    What is the number of OOV words for each topic category?\n",
    "# Load Word2Vec model from local file instead of downloading\n",
    "# w2v = api.load('word2vec-google-news-300')\n",
    "w2v = KeyedVectors.load('word2vec-google-news-300.model')\n",
    "w2v_vocab = w2v.key_to_index\n",
    "\n",
    "# Get training vocab tokens (types), excluding specials\n",
    "specials = {TEXT.unk_token, TEXT.pad_token}\n",
    "train_vocab_types = [w for w in TEXT.vocab.stoi.keys() if w not in specials]\n",
    "\n",
    "# Overall OOV types in training vocab\n",
    "oov_types_overall = {w for w in train_vocab_types if w not in w2v_vocab}\n",
    "print(\"Number of OOV word types (overall):\", len(oov_types_overall))\n",
    "\n",
    "# OOV types per label (unique types per category across its sentences)\n",
    "label_to_oov_types = defaultdict(set)\n",
    "label_to_total_types = defaultdict(set)\n",
    "\n",
    "for ex in train_data.examples:\n",
    "    label = ex.label\n",
    "    # Count by unique types per sentence to avoid overcounting repeats\n",
    "    for w in set(ex.text):\n",
    "        label_to_total_types[label].add(w)\n",
    "        if w not in specials and w not in w2v_vocab:\n",
    "            label_to_oov_types[label].add(w)\n",
    "\n",
    "print(\"\\nOOV word types per topic label:\")\n",
    "for label in sorted(label_to_total_types.keys()):\n",
    "    num_oov = len(label_to_oov_types[label])\n",
    "    num_types = len(label_to_total_types[label])\n",
    "    rate = (num_oov / num_types) if num_types > 0 else 0.0\n",
    "    print(f\"- {label}: {num_oov} OOV types (out of {num_types}, rate={rate:.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e6a5315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### c) OOV mitigation strategy (No transformer-based language models allowed)\n",
    "# Implement your solution in your source code. Show the corresponding code snippet.\n",
    "# 1. Fast Text Model Implementatation\n",
    "# Load FastText with subword info (pretrained on Wikipedia)\n",
    "# First download is large; cached afterwards\n",
    "\n",
    "# 2. Modelling Unknown (<UNK>) token approach\n",
    "# Make the <unk> vector informative and trainable by initializing it\n",
    "# as the mean of available pretrained vectors.\n",
    "\n",
    "# Loading fasttext model\n",
    "fatter_fasttext_bin = load_facebook_model('crawl-300d-2M-subword/crawl-300d-2M-subword.bin')\n",
    "embedding_dim = fatter_fasttext_bin.wv.vector_size\n",
    "\n",
    "# Build embedding matrix aligned to TEXT.vocab\n",
    "num_tokens = len(TEXT.vocab)\n",
    "emb_matrix = np.zeros((num_tokens, embedding_dim), dtype=np.float32)\n",
    "\n",
    "# torchtext 0.4.0: TEXT.vocab.itos is index->token, stoi is token->index\n",
    "pad_tok = TEXT.pad_token\n",
    "unk_tok = TEXT.unk_token\n",
    "\n",
    "# Getting index of <unk> in vocab\n",
    "unk_index = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "known_vecs = []\n",
    "\n",
    "for idx, token in enumerate(TEXT.vocab.itos):\n",
    "    # Skip specials here; we will set them explicitly below\n",
    "    if token in {pad_tok, unk_tok}:\n",
    "        continue\n",
    "\n",
    "    vec = fatter_fasttext_bin.wv[token]\n",
    "    emb_matrix[idx] = vec\n",
    "    known_vecs.append(vec)\n",
    "\n",
    "if len(known_vecs) > 0:\n",
    "    unk_mean = torch.tensor(np.mean(known_vecs, axis=0), dtype=torch.float32)\n",
    "else:\n",
    "    unk_mean = torch.empty(embedding_dim).uniform_(-0.05, 0.05)\n",
    "with torch.no_grad():\n",
    "    emb_matrix[unk_index] = unk_mean\n",
    "\n",
    "# Create Embedding layer initialized with FastText\n",
    "fatter_embedding = torch.nn.Embedding(num_tokens, embedding_dim, padding_idx=TEXT.vocab.stoi[TEXT.pad_token])\n",
    "fatter_embedding.weight.data.copy_(torch.from_numpy(emb_matrix))\n",
    "\n",
    "torch.save(fatter_embedding, 'embedding_weights_fatter_fasttext.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1e7172b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved plots: trec_top20_tsne.png, trec_top20_pca.png\n",
      "ABBR: ['stand', 'abbreviation', 'mean', 'acronym', 'national', 'bureau', 'investigation', 'e', 'letters', 'computer', 'cnn', 'equation', 'zip', 'cpr', 'abbreviated', 'form', 'nasdaq', 'c', 'sids', 'btu']\n",
      "DESC: ['mean', 'origin', 'difference', 'word', 'find', 'come', 'work', 'people', 'term', 'meaning', 'causes', 'like', 't', 'school', 'definition', 'called', 'time', 'happened', 'famous', 'computer']\n",
      "ENTY: ['fear', 'kind', 'called', 'world', 'color', 'book', 'film', 'best', 'war', 'play', 'term', 'movie', 'tv', 'sport', 'animal', 'novel', 'drink', 'word', 'game', 'known']\n",
      "HUM: ['president', 'wrote', 'company', 'world', 'famous', 'invented', 'tv', 'team', 'baseball', 'new', 'character', 'said', 'actor', 'known', 'won', 'star', 'american', 'portrayed', 'movie', 'woman']\n",
      "LOC: ['country', 'city', 'world', 'state', 'find', 'largest', 'located', 'capital', 'river', 'countries', 'highest', 'information', 'live', 'american', 'island', 'states', 'mountain', 'boasts', 'park', 'cities']\n",
      "NUM: ['year', 'long', 'people', 'day', 'average', 'number', 'world', 'population', 'american', 'old', 'war', 'years', 'game', 'live', 'earth', 'cost', 'time', 'date', 'states', 'born']\n"
     ]
    }
   ],
   "source": [
    "#### d) Select the 20 most frequent words from each topic category in the training set (removing\n",
    "# stopwords if necessary). Retrieve their pretrained embeddings (from Word2Vec or GloVe).\n",
    "# Project these embeddings into 2D space (using e.g., t-SNE or Principal Component Analysis).\n",
    "# Plot the points in a scatter plot, color-coded by their topic category. Attach your plot here.\n",
    "# Analyze your findings.\n",
    "\n",
    "# Build per-label token frequency (lowercased, stopwords/punct filtered)\n",
    "label_to_counter = defaultdict(Counter)\n",
    "valid_chars = set(string.ascii_letters)\n",
    "\n",
    "def is_valid_token(tok: str) -> bool:\n",
    "    t = tok.strip(\"'\\\"\")\n",
    "    if len(t) == 0:\n",
    "        return False\n",
    "    # Keep purely alphabetic tokens to avoid punctuation/numbers\n",
    "    return t.isalpha()\n",
    "\n",
    "for ex in train_data.examples:\n",
    "    label = ex.label\n",
    "    for tok in ex.text:\n",
    "        tok_l = tok.lower()\n",
    "        if tok_l in STOP_WORDS:\n",
    "            continue\n",
    "        if not is_valid_token(tok_l):\n",
    "            continue\n",
    "        label_to_counter[label][tok_l] += 1\n",
    "\n",
    "# Select top 20 per label that exist in Word2Vec\n",
    "topk = 20\n",
    "label_to_top_tokens = {}\n",
    "for label, ctr in label_to_counter.items():\n",
    "    selected = []\n",
    "    for tok, _ in ctr.most_common():\n",
    "        if tok in w2v.key_to_index:\n",
    "            selected.append(tok)\n",
    "        if len(selected) >= topk:\n",
    "            break\n",
    "    label_to_top_tokens[label] = selected\n",
    "\n",
    "# Collect embeddings and labels\n",
    "points = []\n",
    "point_labels = []\n",
    "point_words = []\n",
    "for label, toks in label_to_top_tokens.items():\n",
    "    for tok in toks:\n",
    "        vec = w2v.get_vector(tok)\n",
    "        points.append(vec)\n",
    "        point_labels.append(label)\n",
    "        point_words.append(tok)\n",
    "\n",
    "if len(points) > 0:\n",
    "    X = np.vstack(points)\n",
    "\n",
    "    # 2D projections\n",
    "    tsne_2d = TSNE(n_components=2, random_state=42, init=\"pca\", perplexity=30).fit_transform(X)\n",
    "    pca_2d = PCA(n_components=2, random_state=42).fit_transform(X)\n",
    "\n",
    "    # Assign colors per label\n",
    "    unique_labels = sorted(set(point_labels))\n",
    "    color_map = {lab: plt.cm.tab10(i % 10) for i, lab in enumerate(unique_labels)}\n",
    "\n",
    "    def plot_scatter(Y2, title: str, fname: str):\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        for lab in unique_labels:\n",
    "            idxs = [i for i, l in enumerate(point_labels) if l == lab]\n",
    "            plt.scatter(Y2[idxs, 0], Y2[idxs, 1], c=[color_map[lab]], label=lab, alpha=0.8, s=40)\n",
    "            # Light word annotations (optional; can clutter)\n",
    "            for i in idxs:\n",
    "                plt.annotate(point_words[i], (Y2[i, 0], Y2[i, 1]), fontsize=7, alpha=0.7)\n",
    "        plt.legend(title=\"TREC label\")\n",
    "        plt.title(title)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(fname, dpi=200)\n",
    "        plt.close()\n",
    "\n",
    "    plot_scatter(tsne_2d, \"Top-20 per TREC label (Word2Vec) - t-SNE\", \"trec_top20_tsne.png\")\n",
    "    plot_scatter(pca_2d, \"Top-20 per TREC label (Word2Vec) - PCA\", \"trec_top20_pca.png\")\n",
    "\n",
    "    print(\"Saved plots: trec_top20_tsne.png, trec_top20_pca.png\")\n",
    "    for lab in unique_labels:\n",
    "        print(f\"{lab}: {label_to_top_tokens[lab]}\")\n",
    "else:\n",
    "    print(\"No points collected for visualization. Check filtering or embedding availability.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc2dadfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext = torch.load('embedding_weights_fatter_fasttext.pt', weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d8aa0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61c690c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of classes: 6\n",
      "Classes: ['ENTY', 'HUM', 'DESC', 'NUM', 'LOC', 'ABBR']\n"
     ]
    }
   ],
   "source": [
    "### Part 2: Model Training & Evaluation - RNN\n",
    "\n",
    "# Build vocabulary for labels\n",
    "LABEL.build_vocab(train_data)\n",
    "num_classes = len(LABEL.vocab)\n",
    "print(f\"\\nNumber of classes: {num_classes}\")\n",
    "print(f\"Classes: {LABEL.vocab.itos}\")\n",
    "\n",
    "# Create iterators for batching (inline for easier debugging)\n",
    "# train_iterator = data.BucketIterator(...)\n",
    "# val_iterator = data.BucketIterator(...)\n",
    "# test_iterator = data.BucketIterator(...)\n",
    "# (Used directly in Part 2 execution below)\n",
    "\n",
    "\n",
    "class SimpleRNNClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple RNN for topic classification (Baseline - no dropout).\n",
    "    Uses pretrained embeddings (learnable/updated during training) with OOV mitigation \n",
    "    and aggregates word representations to sentence representation using the last hidden state.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, \n",
    "                 n_layers=1, dropout=0.0, padding_idx=0, pretrained_embeddings=None):\n",
    "        super(SimpleRNNClassifier, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # Embedding layer initialized with pretrained embeddings\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "            # IMPORTANT: Make embeddings learnable (updated during training)\n",
    "            # This allows fine-tuning of embeddings including OOV words handled by FastText\n",
    "            self.embedding.weight.requires_grad = True\n",
    "        \n",
    "        # Simple RNN layer (no dropout in baseline)\n",
    "        self.rnn = nn.RNN(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=n_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.0  # No dropout in baseline\n",
    "        )\n",
    "        \n",
    "        # Removed: Dropout layer (baseline has no regularization)\n",
    "        # self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Fully connected output layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        # text: [batch_size, seq_len]\n",
    "        # text_lengths: [batch_size]\n",
    "        \n",
    "        # Embed the input\n",
    "        embedded = self.embedding(text)  # [batch_size, seq_len, embedding_dim]\n",
    "        \n",
    "        # Get dimensions\n",
    "        batch_size = embedded.size(0)\n",
    "        seq_len = embedded.size(1)\n",
    "        \n",
    "        # Handle text_lengths: ensure it's a 1D tensor with batch_size elements\n",
    "        text_lengths_flat = text_lengths.flatten().cpu().long()\n",
    "        \n",
    "        # text_lengths should have exactly batch_size elements (one length per batch item)\n",
    "        if len(text_lengths_flat) != batch_size:\n",
    "            raise ValueError(\n",
    "                f\"text_lengths size mismatch: got {len(text_lengths_flat)} elements, \"\n",
    "                f\"expected {batch_size} (batch_size). text_lengths.shape={text_lengths.shape}, \"\n",
    "                f\"text.shape={text.shape}, embedded.shape={embedded.shape}\"\n",
    "            )\n",
    "        \n",
    "        # Clamp lengths to be at most the sequence length (safety check)\n",
    "        text_lengths_clamped = torch.clamp(text_lengths_flat, min=1, max=seq_len)\n",
    "        \n",
    "        # Pack the padded sequences for efficient processing\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, text_lengths_clamped, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        \n",
    "        # Pass through RNN\n",
    "        packed_output, hidden = self.rnn(packed_embedded)\n",
    "        # hidden: [n_layers, batch_size, hidden_dim]\n",
    "        \n",
    "        # Use the last hidden state from the last layer\n",
    "        last_hidden = hidden[-1]  # [batch_size, hidden_dim]\n",
    "        \n",
    "        # Removed: Apply dropout (baseline has no regularization)\n",
    "        # last_hidden = self.dropout(last_hidden)\n",
    "        \n",
    "        # Pass through fully connected layer\n",
    "        output = self.fc(last_hidden)  # [batch_size, output_dim]\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "# Utility function for counting parameters\n",
    "def count_parameters(model):\n",
    "    \"\"\"Count trainable parameters\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Training and evaluation functions removed - code is now inline below for easier debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38ceba8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8143\n"
     ]
    }
   ],
   "source": [
    "print(len(TEXT.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10a0ed9",
   "metadata": {},
   "source": [
    "Training order:\n",
    "1. Word aggregation\n",
    "2. Hyperparameters tuning\n",
    "3. Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a3176ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PART 2: SIMPLE RNN MODEL TRAINING\n",
      "================================================================================\n",
      "TEXT.vocab size: 8143\n",
      "FastText embedding vocab size: 8143\n",
      "\n",
      ">>> Training Baseline RNN Model\n",
      "Configuration:\n",
      "  - Hidden Dim: 256\n",
      "  - Layers: 1\n",
      "  - Dropout: 0.0 (Baseline: no regularization)\n",
      "  - Learning Rate: 0.001\n",
      "  - Batch Size: 64\n",
      "  - Epochs: 100 (no early stopping)\n",
      "  - Embedding Dim: 300 (FastText)\n",
      "  - Embeddings: LEARNABLE (updated during training)\n",
      "  - OOV Handling: FastText subword embeddings + trainable <unk> token\n",
      "\n",
      "Starting training for 100 epochs...\n",
      "Device: cuda\n",
      "Trainable parameters: 2,587,290\n",
      "Embedding layer learnable: True\n",
      "--------------------------------------------------------------------------------\n",
      "DEBUG BATCH - text shape: torch.Size([8, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 8])\n",
      "Epoch: 01/100 | Time: 0m 8s\n",
      "\tTrain Loss: 1.5678 | Train Acc: 32.49%\n",
      "\tVal Loss: 1.3797 | Val Acc: 43.39%\n",
      "\t>>> New best model saved with Val Acc: 43.39%\n",
      "DEBUG BATCH - text shape: torch.Size([14, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 14])\n",
      "Epoch: 02/100 | Time: 0m 1s\n",
      "\tTrain Loss: 1.1718 | Train Acc: 55.71%\n",
      "\tVal Loss: 1.0334 | Val Acc: 58.26%\n",
      "\t>>> New best model saved with Val Acc: 58.26%\n",
      "DEBUG BATCH - text shape: torch.Size([8, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 8])\n",
      "Epoch: 03/100 | Time: 0m 1s\n",
      "\tTrain Loss: 0.7242 | Train Acc: 76.32%\n",
      "\tVal Loss: 1.0805 | Val Acc: 61.74%\n",
      "\t>>> New best model saved with Val Acc: 61.74%\n",
      "DEBUG BATCH - text shape: torch.Size([14, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 14])\n",
      "Epoch: 04/100 | Time: 0m 1s\n",
      "\tTrain Loss: 0.3571 | Train Acc: 88.84%\n",
      "\tVal Loss: 1.2623 | Val Acc: 62.57%\n",
      "\t>>> New best model saved with Val Acc: 62.57%\n",
      "DEBUG BATCH - text shape: torch.Size([10, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 10])\n",
      "Epoch: 05/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0999 | Train Acc: 97.27%\n",
      "\tVal Loss: 1.4163 | Val Acc: 60.37%\n",
      "DEBUG BATCH - text shape: torch.Size([16, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 16])\n",
      "Epoch: 06/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0225 | Train Acc: 99.75%\n",
      "\tVal Loss: 1.2260 | Val Acc: 66.88%\n",
      "\t>>> New best model saved with Val Acc: 66.88%\n",
      "DEBUG BATCH - text shape: torch.Size([5, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 5])\n",
      "Epoch: 07/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0075 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.2816 | Val Acc: 68.26%\n",
      "\t>>> New best model saved with Val Acc: 68.26%\n",
      "DEBUG BATCH - text shape: torch.Size([11, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 11])\n",
      "Epoch: 08/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0042 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.3269 | Val Acc: 67.89%\n",
      "DEBUG BATCH - text shape: torch.Size([17, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 17])\n",
      "Epoch: 09/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0030 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.3673 | Val Acc: 67.71%\n",
      "DEBUG BATCH - text shape: torch.Size([11, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 11])\n",
      "Epoch: 10/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0022 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.3843 | Val Acc: 67.98%\n",
      "DEBUG BATCH - text shape: torch.Size([6, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 6])\n",
      "Epoch: 11/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0018 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.4150 | Val Acc: 68.44%\n",
      "\t>>> New best model saved with Val Acc: 68.44%\n",
      "DEBUG BATCH - text shape: torch.Size([11, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 11])\n",
      "Epoch: 12/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0015 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.4456 | Val Acc: 68.35%\n",
      "DEBUG BATCH - text shape: torch.Size([5, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 5])\n",
      "Epoch: 13/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0012 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.4665 | Val Acc: 68.26%\n",
      "DEBUG BATCH - text shape: torch.Size([10, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 10])\n",
      "Epoch: 14/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0010 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.4866 | Val Acc: 68.17%\n",
      "DEBUG BATCH - text shape: torch.Size([6, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 6])\n",
      "Epoch: 15/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0009 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.5035 | Val Acc: 68.44%\n",
      "DEBUG BATCH - text shape: torch.Size([8, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 8])\n",
      "Epoch: 16/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0008 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.5198 | Val Acc: 67.98%\n",
      "DEBUG BATCH - text shape: torch.Size([9, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 9])\n",
      "Epoch: 17/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0007 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.5453 | Val Acc: 68.35%\n",
      "DEBUG BATCH - text shape: torch.Size([37, 10]), text_lengths shape: torch.Size([10]), labels shape: torch.Size([10])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([10, 37])\n",
      "Epoch: 18/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0006 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.5548 | Val Acc: 68.07%\n",
      "DEBUG BATCH - text shape: torch.Size([14, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 14])\n",
      "Epoch: 19/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0005 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.5740 | Val Acc: 67.89%\n",
      "DEBUG BATCH - text shape: torch.Size([16, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 16])\n",
      "Epoch: 20/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0005 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.5912 | Val Acc: 67.89%\n",
      "DEBUG BATCH - text shape: torch.Size([10, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 10])\n",
      "Epoch: 21/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0004 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.6032 | Val Acc: 67.89%\n",
      "DEBUG BATCH - text shape: torch.Size([17, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 17])\n",
      "Epoch: 22/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0004 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.6206 | Val Acc: 67.89%\n",
      "DEBUG BATCH - text shape: torch.Size([8, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 8])\n",
      "Epoch: 23/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0004 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.6304 | Val Acc: 67.71%\n",
      "DEBUG BATCH - text shape: torch.Size([13, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 13])\n",
      "Epoch: 24/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0003 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.6427 | Val Acc: 67.89%\n",
      "DEBUG BATCH - text shape: torch.Size([11, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 11])\n",
      "Epoch: 25/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0003 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.6553 | Val Acc: 67.89%\n",
      "DEBUG BATCH - text shape: torch.Size([6, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 6])\n",
      "Epoch: 26/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0003 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.6675 | Val Acc: 67.71%\n",
      "DEBUG BATCH - text shape: torch.Size([11, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 11])\n",
      "Epoch: 27/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0003 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.6764 | Val Acc: 67.89%\n",
      "DEBUG BATCH - text shape: torch.Size([14, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 14])\n",
      "Epoch: 28/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0002 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.6914 | Val Acc: 67.98%\n",
      "DEBUG BATCH - text shape: torch.Size([7, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 7])\n",
      "Epoch: 29/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0002 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.7034 | Val Acc: 68.07%\n",
      "DEBUG BATCH - text shape: torch.Size([13, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 13])\n",
      "Epoch: 30/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0002 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.7118 | Val Acc: 68.17%\n",
      "DEBUG BATCH - text shape: torch.Size([11, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 11])\n",
      "Epoch: 31/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0002 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.7227 | Val Acc: 67.98%\n",
      "DEBUG BATCH - text shape: torch.Size([10, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 10])\n",
      "Epoch: 32/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0002 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.7337 | Val Acc: 67.98%\n",
      "DEBUG BATCH - text shape: torch.Size([15, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 15])\n",
      "Epoch: 33/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0002 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.7433 | Val Acc: 67.89%\n",
      "DEBUG BATCH - text shape: torch.Size([9, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 9])\n",
      "Epoch: 34/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0002 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.7541 | Val Acc: 67.89%\n",
      "DEBUG BATCH - text shape: torch.Size([8, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 8])\n",
      "Epoch: 35/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.7625 | Val Acc: 67.80%\n",
      "DEBUG BATCH - text shape: torch.Size([8, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 8])\n",
      "Epoch: 36/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.7737 | Val Acc: 67.98%\n",
      "DEBUG BATCH - text shape: torch.Size([9, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 9])\n",
      "Epoch: 37/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.7822 | Val Acc: 67.98%\n",
      "DEBUG BATCH - text shape: torch.Size([16, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 16])\n",
      "Epoch: 38/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.7922 | Val Acc: 67.71%\n",
      "DEBUG BATCH - text shape: torch.Size([14, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 14])\n",
      "Epoch: 39/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.8002 | Val Acc: 67.89%\n",
      "DEBUG BATCH - text shape: torch.Size([5, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 5])\n",
      "Epoch: 40/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.8107 | Val Acc: 67.80%\n",
      "DEBUG BATCH - text shape: torch.Size([8, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 8])\n",
      "Epoch: 41/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.8164 | Val Acc: 67.98%\n",
      "DEBUG BATCH - text shape: torch.Size([10, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 10])\n",
      "Epoch: 42/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.8274 | Val Acc: 67.98%\n",
      "DEBUG BATCH - text shape: torch.Size([5, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 5])\n",
      "Epoch: 43/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.8377 | Val Acc: 67.80%\n",
      "DEBUG BATCH - text shape: torch.Size([12, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 12])\n",
      "Epoch: 44/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.8421 | Val Acc: 68.17%\n",
      "DEBUG BATCH - text shape: torch.Size([14, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 14])\n",
      "Epoch: 45/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.8517 | Val Acc: 68.17%\n",
      "DEBUG BATCH - text shape: torch.Size([8, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 8])\n",
      "Epoch: 46/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.8594 | Val Acc: 68.17%\n",
      "DEBUG BATCH - text shape: torch.Size([8, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 8])\n",
      "Epoch: 47/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.8670 | Val Acc: 68.17%\n",
      "DEBUG BATCH - text shape: torch.Size([9, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 9])\n",
      "Epoch: 48/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.8755 | Val Acc: 67.98%\n",
      "DEBUG BATCH - text shape: torch.Size([5, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 5])\n",
      "Epoch: 49/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.8862 | Val Acc: 67.80%\n",
      "DEBUG BATCH - text shape: torch.Size([10, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 10])\n",
      "Epoch: 50/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.8896 | Val Acc: 68.07%\n",
      "DEBUG BATCH - text shape: torch.Size([6, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 6])\n",
      "Epoch: 51/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.9014 | Val Acc: 67.98%\n",
      "DEBUG BATCH - text shape: torch.Size([12, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 12])\n",
      "Epoch: 52/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.9097 | Val Acc: 67.98%\n",
      "DEBUG BATCH - text shape: torch.Size([7, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 7])\n",
      "Epoch: 53/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.9155 | Val Acc: 67.98%\n",
      "DEBUG BATCH - text shape: torch.Size([6, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 6])\n",
      "Epoch: 54/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0001 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.9217 | Val Acc: 67.98%\n",
      "DEBUG BATCH - text shape: torch.Size([9, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 9])\n",
      "Epoch: 55/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.9307 | Val Acc: 68.07%\n",
      "DEBUG BATCH - text shape: torch.Size([8, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 8])\n",
      "Epoch: 56/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.9358 | Val Acc: 67.89%\n",
      "DEBUG BATCH - text shape: torch.Size([22, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 22])\n",
      "Epoch: 57/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.9449 | Val Acc: 67.98%\n",
      "DEBUG BATCH - text shape: torch.Size([13, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 13])\n",
      "Epoch: 58/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.9517 | Val Acc: 67.80%\n",
      "DEBUG BATCH - text shape: torch.Size([9, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 9])\n",
      "Epoch: 59/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.9585 | Val Acc: 68.07%\n",
      "DEBUG BATCH - text shape: torch.Size([11, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 11])\n",
      "Epoch: 60/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.9676 | Val Acc: 68.07%\n",
      "DEBUG BATCH - text shape: torch.Size([5, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 5])\n",
      "Epoch: 61/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.9745 | Val Acc: 67.98%\n",
      "DEBUG BATCH - text shape: torch.Size([5, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 5])\n",
      "Epoch: 62/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.9835 | Val Acc: 68.17%\n",
      "DEBUG BATCH - text shape: torch.Size([13, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 13])\n",
      "Epoch: 63/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.9888 | Val Acc: 68.07%\n",
      "DEBUG BATCH - text shape: torch.Size([14, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 14])\n",
      "Epoch: 64/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 1.9937 | Val Acc: 68.07%\n",
      "DEBUG BATCH - text shape: torch.Size([8, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 8])\n",
      "Epoch: 65/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.0002 | Val Acc: 67.98%\n",
      "DEBUG BATCH - text shape: torch.Size([10, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 10])\n",
      "Epoch: 66/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.0082 | Val Acc: 68.07%\n",
      "DEBUG BATCH - text shape: torch.Size([18, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 18])\n",
      "Epoch: 67/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.0179 | Val Acc: 67.80%\n",
      "DEBUG BATCH - text shape: torch.Size([7, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 7])\n",
      "Epoch: 68/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.0192 | Val Acc: 68.07%\n",
      "DEBUG BATCH - text shape: torch.Size([10, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 10])\n",
      "Epoch: 69/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.0284 | Val Acc: 67.89%\n",
      "DEBUG BATCH - text shape: torch.Size([13, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 13])\n",
      "Epoch: 70/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.0353 | Val Acc: 67.80%\n",
      "DEBUG BATCH - text shape: torch.Size([4, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 4])\n",
      "Epoch: 71/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.0394 | Val Acc: 68.07%\n",
      "DEBUG BATCH - text shape: torch.Size([10, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 10])\n",
      "Epoch: 72/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.0477 | Val Acc: 67.98%\n",
      "DEBUG BATCH - text shape: torch.Size([14, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 14])\n",
      "Epoch: 73/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.0551 | Val Acc: 67.98%\n",
      "DEBUG BATCH - text shape: torch.Size([8, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 8])\n",
      "Epoch: 74/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.0614 | Val Acc: 68.07%\n",
      "DEBUG BATCH - text shape: torch.Size([17, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 17])\n",
      "Epoch: 75/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.0720 | Val Acc: 67.89%\n",
      "DEBUG BATCH - text shape: torch.Size([12, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 12])\n",
      "Epoch: 76/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.0738 | Val Acc: 67.89%\n",
      "DEBUG BATCH - text shape: torch.Size([9, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 9])\n",
      "Epoch: 77/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.0810 | Val Acc: 68.07%\n",
      "DEBUG BATCH - text shape: torch.Size([7, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 7])\n",
      "Epoch: 78/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.0887 | Val Acc: 67.89%\n",
      "DEBUG BATCH - text shape: torch.Size([9, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 9])\n",
      "Epoch: 79/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.0936 | Val Acc: 68.07%\n",
      "DEBUG BATCH - text shape: torch.Size([12, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 12])\n",
      "Epoch: 80/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.1006 | Val Acc: 67.98%\n",
      "DEBUG BATCH - text shape: torch.Size([8, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 8])\n",
      "Epoch: 81/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.1100 | Val Acc: 67.98%\n",
      "DEBUG BATCH - text shape: torch.Size([8, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 8])\n",
      "Epoch: 82/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.1139 | Val Acc: 68.17%\n",
      "DEBUG BATCH - text shape: torch.Size([7, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 7])\n",
      "Epoch: 83/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.1176 | Val Acc: 67.98%\n",
      "DEBUG BATCH - text shape: torch.Size([12, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 12])\n",
      "Epoch: 84/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.1257 | Val Acc: 68.07%\n",
      "DEBUG BATCH - text shape: torch.Size([37, 10]), text_lengths shape: torch.Size([10]), labels shape: torch.Size([10])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([10, 37])\n",
      "Epoch: 85/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.1345 | Val Acc: 67.89%\n",
      "DEBUG BATCH - text shape: torch.Size([11, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 11])\n",
      "Epoch: 86/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.1394 | Val Acc: 67.61%\n",
      "DEBUG BATCH - text shape: torch.Size([10, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 10])\n",
      "Epoch: 87/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.1453 | Val Acc: 67.71%\n",
      "DEBUG BATCH - text shape: torch.Size([7, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 7])\n",
      "Epoch: 88/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.1503 | Val Acc: 67.61%\n",
      "DEBUG BATCH - text shape: torch.Size([6, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 6])\n",
      "Epoch: 89/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.1590 | Val Acc: 67.61%\n",
      "DEBUG BATCH - text shape: torch.Size([9, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 9])\n",
      "Epoch: 90/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.1629 | Val Acc: 67.52%\n",
      "DEBUG BATCH - text shape: torch.Size([7, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 7])\n",
      "Epoch: 91/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.1679 | Val Acc: 67.52%\n",
      "DEBUG BATCH - text shape: torch.Size([11, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 11])\n",
      "Epoch: 92/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.1780 | Val Acc: 67.52%\n",
      "DEBUG BATCH - text shape: torch.Size([11, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 11])\n",
      "Epoch: 93/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.1828 | Val Acc: 67.52%\n",
      "DEBUG BATCH - text shape: torch.Size([8, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 8])\n",
      "Epoch: 94/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.1871 | Val Acc: 67.61%\n",
      "DEBUG BATCH - text shape: torch.Size([12, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 12])\n",
      "Epoch: 95/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.1955 | Val Acc: 67.61%\n",
      "DEBUG BATCH - text shape: torch.Size([7, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 7])\n",
      "Epoch: 96/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.2002 | Val Acc: 67.61%\n",
      "DEBUG BATCH - text shape: torch.Size([9, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 9])\n",
      "Epoch: 97/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.2059 | Val Acc: 67.71%\n",
      "DEBUG BATCH - text shape: torch.Size([10, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 10])\n",
      "Epoch: 98/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.2105 | Val Acc: 67.61%\n",
      "DEBUG BATCH - text shape: torch.Size([12, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 12])\n",
      "Epoch: 99/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.2195 | Val Acc: 67.52%\n",
      "DEBUG BATCH - text shape: torch.Size([10, 64]), text_lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n",
      "DEBUG BATCH - Transposed text to [batch_size, seq_len]: torch.Size([64, 10])\n",
      "Epoch: 100/100 | Time: 0m 1s\n",
      "\tTrain Loss: 0.0000 | Train Acc: 100.00%\n",
      "\tVal Loss: 2.2241 | Val Acc: 67.52%\n",
      "--------------------------------------------------------------------------------\n",
      "Training completed! Best validation accuracy: 68.44%\n",
      "Total epochs trained: 100\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Part 2: Initial Simple RNN Model Training\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 2: SIMPLE RNN MODEL TRAINING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get pretrained embeddings from Part 1 (frozen embeddings)\n",
    "pretrained_embeddings = fasttext.weight.data\n",
    "\n",
    "# Get embedding dimension and vocab size from the fasttext embedding layer\n",
    "embedding_dim = fasttext.weight.shape[1]\n",
    "fasttext_vocab_size = fasttext.weight.shape[0]  # Vocab size from saved embedding\n",
    "\n",
    "# Verify vocab sizes match (they might differ if vocab was rebuilt)\n",
    "print(f\"TEXT.vocab size: {len(TEXT.vocab)}\")\n",
    "print(f\"FastText embedding vocab size: {fasttext_vocab_size}\")\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "HIDDEN_DIM = 256\n",
    "N_LAYERS = 1\n",
    "DROPOUT = 0.0  # Baseline: no dropout\n",
    "N_EPOCHS = 100\n",
    "LEARNING_RATE = 0.001\n",
    "# Removed: PATIENCE = 10  # Baseline: no early stopping\n",
    "\n",
    "# Create data iterators (inline for easier debugging)\n",
    "# Note: Different sequence lengths per batch are normal - BucketIterator groups similar-length sequences\n",
    "train_iterator = data.BucketIterator(\n",
    "    train_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    shuffle=True,  # Shuffle for training\n",
    "    device=device\n",
    ")\n",
    "\n",
    "val_iterator = data.BucketIterator(\n",
    "    validation_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    shuffle=False,  # No shuffle for validation (deterministic)\n",
    "    device=device\n",
    ")\n",
    "\n",
    "test_iterator = data.BucketIterator(\n",
    "    test_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    shuffle=False,  # No shuffle for test (deterministic)\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Initialize simple RNN model (Baseline)\n",
    "# Use vocab size from loaded embedding to match the saved weights exactly\n",
    "model = SimpleRNNClassifier(\n",
    "    vocab_size=fasttext_vocab_size,  # Must match saved embedding vocab size\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    output_dim=num_classes,\n",
    "    n_layers=N_LAYERS,\n",
    "    dropout=0.0,  # Baseline: no dropout\n",
    "    padding_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "    pretrained_embeddings=pretrained_embeddings\n",
    ").to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(f\"\\n>>> Training Baseline RNN Model\")\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  - Hidden Dim: {HIDDEN_DIM}\")\n",
    "print(f\"  - Layers: {N_LAYERS}\")\n",
    "print(f\"  - Dropout: {DROPOUT} (Baseline: no regularization)\")\n",
    "print(f\"  - Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"  - Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  - Epochs: {N_EPOCHS} (no early stopping)\")\n",
    "print(f\"  - Embedding Dim: {embedding_dim} (FastText)\")\n",
    "print(f\"  - Embeddings: LEARNABLE (updated during training)\")\n",
    "print(f\"  - OOV Handling: FastText subword embeddings + trainable <unk> token\")\n",
    "\n",
    "# ============================================================================\n",
    "# Helper function to process batches consistently\n",
    "# ============================================================================\n",
    "\n",
    "def process_batch(batch, debug=False):\n",
    "    \"\"\"\n",
    "    Process a batch from BucketIterator, handling text transpose correctly.\n",
    "    Returns: text, text_lengths, labels (all properly formatted)\n",
    "    \"\"\"\n",
    "    text, text_lengths = batch.text\n",
    "    labels = batch.label\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"DEBUG BATCH - text shape: {text.shape}, text_lengths shape: {text_lengths.shape}, labels shape: {labels.shape}\")\n",
    "    \n",
    "    # torchtext BucketIterator returns text as [seq_len, batch_size] by default\n",
    "    # We need [batch_size, seq_len] for batch_first=True in the model\n",
    "    expected_batch_size = labels.shape[0]\n",
    "    \n",
    "    if text.dim() == 2:\n",
    "        if text.shape[1] == expected_batch_size and len(text_lengths) == expected_batch_size:\n",
    "            # text is [seq_len, batch_size], transpose to [batch_size, seq_len]\n",
    "            text = text.transpose(0, 1)\n",
    "            if debug:\n",
    "                print(f\"DEBUG BATCH - Transposed text to [batch_size, seq_len]: {text.shape}\")\n",
    "        elif text.shape[0] == expected_batch_size and len(text_lengths) == expected_batch_size:\n",
    "            # text is already [batch_size, seq_len]\n",
    "            if debug:\n",
    "                print(f\"DEBUG BATCH - text already in correct format: {text.shape}\")\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Cannot determine text format: text.shape={text.shape}, \"\n",
    "                f\"text_lengths.shape={text_lengths.shape}, labels.shape={labels.shape}\"\n",
    "            )\n",
    "    \n",
    "    # Verify dimensions match\n",
    "    assert text.shape[0] == len(text_lengths) == labels.shape[0], \\\n",
    "        f\"Batch size mismatch: text.shape[0]={text.shape[0]}, len(text_lengths)={len(text_lengths)}, labels.shape[0]={labels.shape[0]}\"\n",
    "    \n",
    "    return text, text_lengths, labels\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "# Ensure deterministic behavior\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Note: PyTorch/IPython compatibility fix is already applied in cell 0\n",
    "# The torch._dynamo.disable decorator has been patched to handle the 'wrapping' parameter\n",
    "\n",
    "# ============================================================================\n",
    "# Training Loop (inline for easier debugging)\n",
    "# ============================================================================\n",
    "\n",
    "best_val_acc = 0\n",
    "# Removed: patience_counter = 0  # Baseline: no early stopping\n",
    "\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "\n",
    "print(f\"\\nStarting training for {N_EPOCHS} epochs...\")  # Removed \"up to\" - no early stopping\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Trainable parameters: {count_parameters(model):,}\")\n",
    "print(f\"Embedding layer learnable: {model.embedding.weight.requires_grad}\")\n",
    "# Removed: print(f\"Early stopping patience: {PATIENCE} epochs\")  # Baseline: no early stopping\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Training for one epoch (inline)\n",
    "    # ========================================================================\n",
    "    model.train()\n",
    "    train_epoch_loss = 0\n",
    "    train_all_preds = []\n",
    "    train_all_labels = []\n",
    "    \n",
    "    batch_idx = 0\n",
    "    for batch in train_iterator:\n",
    "        # Process batch (with debug only for first batch)\n",
    "        text, text_lengths, labels = process_batch(batch, debug=(batch_idx == 0))\n",
    "        batch_idx += 1\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        predictions = model(text, text_lengths)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(predictions, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Removed: Gradient clipping (baseline has no gradient clipping)\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_epoch_loss += loss.item()\n",
    "        \n",
    "        # Store predictions and labels for metrics\n",
    "        preds = torch.argmax(predictions, dim=1)\n",
    "        train_all_preds.extend(preds.cpu().numpy())\n",
    "        train_all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate training accuracy\n",
    "    train_acc = accuracy_score(train_all_labels, train_all_preds)\n",
    "    train_loss = train_epoch_loss / len(train_iterator)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Validation evaluation (inline)\n",
    "    # ========================================================================\n",
    "    model.eval()\n",
    "    val_epoch_loss = 0\n",
    "    val_all_preds = []\n",
    "    val_all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_iterator:\n",
    "            # Process batch consistently with training\n",
    "            text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "            \n",
    "            # Forward pass\n",
    "            predictions = model(text, text_lengths)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(predictions, labels)\n",
    "            val_epoch_loss += loss.item()\n",
    "            \n",
    "            # Store predictions and labels\n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            val_all_preds.extend(preds.cpu().numpy())\n",
    "            val_all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate validation accuracy\n",
    "    val_acc = accuracy_score(val_all_labels, val_all_preds)\n",
    "    val_loss = val_epoch_loss / len(val_iterator)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Logging (without early stopping)\n",
    "    # ========================================================================\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = divmod(end_time - start_time, 60)\n",
    "    \n",
    "    # Store metrics\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02}/{N_EPOCHS} | Time: {int(epoch_mins)}m {int(epoch_secs)}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\tVal Loss: {val_loss:.4f} | Val Acc: {val_acc*100:.2f}%')\n",
    "    \n",
    "    # Track best model (but don't stop early - baseline trains for all epochs)\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        # Save best model\n",
    "        torch.save(model.state_dict(), 'rnn_simple_best.pt')\n",
    "        print(f'\\t>>> New best model saved with Val Acc: {val_acc*100:.2f}%')\n",
    "    # Removed: Early stopping break logic (baseline trains for all epochs)\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(f\"Training completed! Best validation accuracy: {best_val_acc*100:.2f}%\")\n",
    "print(f\"Total epochs trained: {N_EPOCHS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27861988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "VALIDATION SET EVALUATION (Best Model)\n",
      "================================================================================\n",
      "\n",
      ">>> Validation Set Results (Best Model):\n",
      "Validation Loss: 1.4150\n",
      "Validation Accuracy: 68.44%\n",
      "Validation F1 Score: 0.6817\n",
      "Validation AUC-ROC: 0.8923\n",
      "\n",
      "================================================================================\n",
      "TEST SET EVALUATION\n",
      "================================================================================\n",
      "\n",
      ">>> Test Set Results:\n",
      "Test Loss: 0.9124\n",
      "Test Accuracy: 75.40%\n",
      "Test F1 Score: 0.7497\n",
      "Test AUC-ROC: 0.9185\n",
      "\n",
      "================================================================================\n",
      "PART 2 INITIAL TRAINING COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Validation Set Evaluation (inline) - Evaluate best model on validation set\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VALIDATION SET EVALUATION (Best Model)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load best model and evaluate on validation set\n",
    "model.load_state_dict(torch.load('rnn_simple_best.pt'))\n",
    "\n",
    "model.eval()\n",
    "val_eval_loss = 0\n",
    "val_eval_preds = []\n",
    "val_eval_labels = []\n",
    "val_eval_probs = []  # Store probabilities for AUC-ROC\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_iterator:\n",
    "        text, text_lengths = batch.text\n",
    "        labels = batch.label\n",
    "        \n",
    "        # Process batch consistently\n",
    "        text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "        \n",
    "        # Forward pass\n",
    "        predictions = model(text, text_lengths)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(predictions, labels)\n",
    "        val_eval_loss += loss.item()\n",
    "        \n",
    "        # Store predictions, labels, and probabilities\n",
    "        probs = torch.softmax(predictions, dim=1)\n",
    "        preds = torch.argmax(predictions, dim=1)\n",
    "        val_eval_preds.extend(preds.cpu().numpy())\n",
    "        val_eval_labels.extend(labels.cpu().numpy())\n",
    "        val_eval_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "# Calculate validation metrics\n",
    "val_eval_acc = accuracy_score(val_eval_labels, val_eval_preds)\n",
    "val_eval_f1 = f1_score(val_eval_labels, val_eval_preds, average='weighted')\n",
    "val_eval_loss_final = val_eval_loss / len(val_iterator)\n",
    "\n",
    "# Calculate AUC-ROC (one-vs-rest for multiclass)\n",
    "try:\n",
    "    val_eval_probs_array = np.array(val_eval_probs)\n",
    "    val_eval_labels_bin = label_binarize(val_eval_labels, classes=range(num_classes))\n",
    "    val_eval_auc = roc_auc_score(val_eval_labels_bin, val_eval_probs_array, average='weighted', multi_class='ovr')\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not calculate AUC-ROC: {e}\")\n",
    "    val_eval_auc = 0.0\n",
    "\n",
    "print(f\"\\n>>> Validation Set Results (Best Model):\")\n",
    "print(f\"Validation Loss: {val_eval_loss_final:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_eval_acc*100:.2f}%\")\n",
    "print(f\"Validation F1 Score: {val_eval_f1:.4f}\")\n",
    "print(f\"Validation AUC-ROC: {val_eval_auc:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Test Set Evaluation (inline)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST SET EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "model.eval()\n",
    "test_epoch_loss = 0\n",
    "test_all_preds = []\n",
    "test_all_labels = []\n",
    "test_all_probs = []  # Store probabilities for AUC-ROC\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_iterator:\n",
    "        text, text_lengths = batch.text\n",
    "        labels = batch.label\n",
    "        \n",
    "        # Process batch consistently\n",
    "        text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "        \n",
    "        # Forward pass\n",
    "        predictions = model(text, text_lengths)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(predictions, labels)\n",
    "        test_epoch_loss += loss.item()\n",
    "        \n",
    "        # Store predictions, labels, and probabilities\n",
    "        probs = torch.softmax(predictions, dim=1)\n",
    "        preds = torch.argmax(predictions, dim=1)\n",
    "        test_all_preds.extend(preds.cpu().numpy())\n",
    "        test_all_labels.extend(labels.cpu().numpy())\n",
    "        test_all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "# Calculate test metrics\n",
    "test_acc = accuracy_score(test_all_labels, test_all_preds)\n",
    "test_f1 = f1_score(test_all_labels, test_all_preds, average='weighted')\n",
    "test_loss = test_epoch_loss / len(test_iterator)\n",
    "\n",
    "# Calculate AUC-ROC (one-vs-rest for multiclass)\n",
    "try:\n",
    "    test_all_probs_array = np.array(test_all_probs)\n",
    "    test_all_labels_bin = label_binarize(test_all_labels, classes=range(num_classes))\n",
    "    test_auc = roc_auc_score(test_all_labels_bin, test_all_probs_array, average='weighted', multi_class='ovr')\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not calculate AUC-ROC: {e}\")\n",
    "    test_auc = 0.0\n",
    "\n",
    "print(f\"\\n>>> Test Set Results:\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n",
    "print(f\"Test F1 Score: {test_f1:.4f}\")\n",
    "print(f\"Test AUC-ROC: {test_auc:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 2 INITIAL TRAINING COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ba71f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c72a89fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PART 2.2: SEQUENTIAL HYPERPARAMETER TUNING\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "STEP 0: EPOCH + EARLY STOPPING TUNING\n",
      "================================================================================\n",
      "Testing different MAX_EPOCHS and PATIENCE configurations\n",
      "Total combinations to test: 3\n",
      "Combinations (Max_Epochs, Patience):\n",
      "  1. Max_Epochs=100, Patience=10\n",
      "  2. Max_Epochs=250, Patience=10\n",
      "  3. Max_Epochs=500, Patience=10\n",
      "\n",
      ">>> Testing: Step 0 Config 1/3\n",
      "    LR=0.001, Batch=64, Hidden=256, Opt=Adam\n",
      "    Epochs=100 (NO early stopping - training for full 100 epochs)\n",
      "    Epoch 10/100: Train Acc=100.00%, Val Acc=78.53%\n",
      "    Epoch 20/100: Train Acc=100.00%, Val Acc=79.17%\n",
      "    Epoch 30/100: Train Acc=100.00%, Val Acc=79.27%\n",
      "    Epoch 40/100: Train Acc=100.00%, Val Acc=79.36%\n",
      "    Epoch 50/100: Train Acc=100.00%, Val Acc=79.27%\n",
      "    Epoch 60/100: Train Acc=100.00%, Val Acc=79.36%\n",
      "    Epoch 70/100: Train Acc=100.00%, Val Acc=79.36%\n",
      "    Epoch 80/100: Train Acc=100.00%, Val Acc=79.17%\n",
      "    Epoch 90/100: Train Acc=100.00%, Val Acc=79.17%\n",
      "    Epoch 100/100: Train Acc=100.00%, Val Acc=79.17%\n",
      "    Final Val Acc: 79.17% | Best Val Acc: 79.54% (at epoch 38/100)\n",
      "\n",
      ">>> Testing: Step 0 Config 2/3\n",
      "    LR=0.001, Batch=64, Hidden=256, Opt=Adam\n",
      "    Epochs=250 (NO early stopping - training for full 250 epochs)\n",
      "    Epoch 10/250: Train Acc=100.00%, Val Acc=78.53%\n",
      "    Epoch 20/250: Train Acc=100.00%, Val Acc=79.17%\n",
      "    Epoch 30/250: Train Acc=100.00%, Val Acc=79.27%\n",
      "    Epoch 40/250: Train Acc=100.00%, Val Acc=79.36%\n",
      "    Epoch 50/250: Train Acc=100.00%, Val Acc=79.27%\n",
      "    Epoch 60/250: Train Acc=100.00%, Val Acc=79.36%\n",
      "    Epoch 70/250: Train Acc=100.00%, Val Acc=79.36%\n",
      "    Epoch 80/250: Train Acc=100.00%, Val Acc=79.17%\n",
      "    Epoch 90/250: Train Acc=100.00%, Val Acc=79.17%\n",
      "    Epoch 100/250: Train Acc=100.00%, Val Acc=79.17%\n",
      "    Epoch 110/250: Train Acc=100.00%, Val Acc=78.99%\n",
      "    Epoch 120/250: Train Acc=100.00%, Val Acc=78.90%\n",
      "    Epoch 130/250: Train Acc=100.00%, Val Acc=79.17%\n",
      "    Epoch 140/250: Train Acc=100.00%, Val Acc=79.36%\n",
      "    Epoch 150/250: Train Acc=100.00%, Val Acc=79.27%\n",
      "    Epoch 160/250: Train Acc=100.00%, Val Acc=79.45%\n",
      "    Epoch 170/250: Train Acc=100.00%, Val Acc=79.45%\n",
      "    Epoch 180/250: Train Acc=100.00%, Val Acc=79.45%\n",
      "    Epoch 190/250: Train Acc=100.00%, Val Acc=79.54%\n",
      "    Epoch 200/250: Train Acc=100.00%, Val Acc=79.45%\n",
      "    Epoch 210/250: Train Acc=100.00%, Val Acc=79.45%\n",
      "    Epoch 220/250: Train Acc=100.00%, Val Acc=79.45%\n",
      "    Epoch 230/250: Train Acc=100.00%, Val Acc=79.63%\n",
      "    Epoch 240/250: Train Acc=100.00%, Val Acc=79.36%\n",
      "    Epoch 250/250: Train Acc=100.00%, Val Acc=79.36%\n",
      "    Final Val Acc: 79.36% | Best Val Acc: 79.63% (at epoch 203/250)\n",
      "\n",
      ">>> Testing: Step 0 Config 3/3\n",
      "    LR=0.001, Batch=64, Hidden=256, Opt=Adam\n",
      "    Epochs=500 (NO early stopping - training for full 500 epochs)\n",
      "    Epoch 10/500: Train Acc=100.00%, Val Acc=78.53%\n",
      "    Epoch 20/500: Train Acc=100.00%, Val Acc=79.17%\n",
      "    Epoch 30/500: Train Acc=100.00%, Val Acc=79.27%\n",
      "    Epoch 40/500: Train Acc=100.00%, Val Acc=79.36%\n",
      "    Epoch 50/500: Train Acc=100.00%, Val Acc=79.27%\n",
      "    Epoch 60/500: Train Acc=100.00%, Val Acc=79.36%\n",
      "    Epoch 70/500: Train Acc=100.00%, Val Acc=79.36%\n",
      "    Epoch 80/500: Train Acc=100.00%, Val Acc=79.17%\n",
      "    Epoch 90/500: Train Acc=100.00%, Val Acc=79.17%\n",
      "    Epoch 100/500: Train Acc=100.00%, Val Acc=79.17%\n",
      "    Epoch 110/500: Train Acc=100.00%, Val Acc=78.99%\n",
      "    Epoch 120/500: Train Acc=100.00%, Val Acc=78.90%\n",
      "    Epoch 130/500: Train Acc=100.00%, Val Acc=79.17%\n",
      "    Epoch 140/500: Train Acc=100.00%, Val Acc=79.36%\n",
      "    Epoch 150/500: Train Acc=100.00%, Val Acc=79.27%\n",
      "    Epoch 160/500: Train Acc=100.00%, Val Acc=79.45%\n",
      "    Epoch 170/500: Train Acc=100.00%, Val Acc=79.45%\n",
      "    Epoch 180/500: Train Acc=100.00%, Val Acc=79.45%\n",
      "    Epoch 190/500: Train Acc=100.00%, Val Acc=79.54%\n",
      "    Epoch 200/500: Train Acc=100.00%, Val Acc=79.45%\n",
      "    Epoch 210/500: Train Acc=100.00%, Val Acc=79.45%\n",
      "    Epoch 220/500: Train Acc=100.00%, Val Acc=79.45%\n",
      "    Epoch 230/500: Train Acc=100.00%, Val Acc=79.63%\n",
      "    Epoch 240/500: Train Acc=100.00%, Val Acc=79.36%\n",
      "    Epoch 250/500: Train Acc=100.00%, Val Acc=79.36%\n",
      "    Epoch 260/500: Train Acc=100.00%, Val Acc=79.36%\n",
      "    Epoch 270/500: Train Acc=100.00%, Val Acc=79.63%\n",
      "    Epoch 280/500: Train Acc=100.00%, Val Acc=79.82%\n",
      "    Epoch 290/500: Train Acc=100.00%, Val Acc=79.82%\n",
      "    Epoch 300/500: Train Acc=100.00%, Val Acc=79.72%\n",
      "    Epoch 310/500: Train Acc=100.00%, Val Acc=79.63%\n",
      "    Epoch 320/500: Train Acc=100.00%, Val Acc=79.91%\n",
      "    Epoch 330/500: Train Acc=90.55%, Val Acc=38.99%\n",
      "    Epoch 340/500: Train Acc=100.00%, Val Acc=36.06%\n",
      "    Epoch 350/500: Train Acc=100.00%, Val Acc=36.15%\n",
      "    Epoch 360/500: Train Acc=100.00%, Val Acc=36.15%\n",
      "    Epoch 370/500: Train Acc=100.00%, Val Acc=35.96%\n",
      "    Epoch 380/500: Train Acc=100.00%, Val Acc=35.69%\n",
      "    Epoch 390/500: Train Acc=100.00%, Val Acc=35.78%\n",
      "    Epoch 400/500: Train Acc=100.00%, Val Acc=35.78%\n",
      "    Epoch 410/500: Train Acc=100.00%, Val Acc=35.96%\n",
      "    Epoch 420/500: Train Acc=100.00%, Val Acc=35.60%\n",
      "    Epoch 430/500: Train Acc=100.00%, Val Acc=35.50%\n",
      "    Epoch 440/500: Train Acc=100.00%, Val Acc=35.32%\n",
      "    Epoch 450/500: Train Acc=100.00%, Val Acc=34.86%\n",
      "    Epoch 460/500: Train Acc=100.00%, Val Acc=34.68%\n",
      "    Epoch 470/500: Train Acc=100.00%, Val Acc=34.86%\n",
      "    Epoch 480/500: Train Acc=100.00%, Val Acc=34.86%\n",
      "    Epoch 490/500: Train Acc=100.00%, Val Acc=35.05%\n",
      "    Epoch 500/500: Train Acc=100.00%, Val Acc=34.86%\n",
      "    Final Val Acc: 34.86% | Best Val Acc: 80.00% (at epoch 288/500)\n",
      "\n",
      ">>> Step 0 Results:\n",
      "#    Epochs   Val Acc    Best At Epoch   Total Trained  \n",
      "------------------------------------------------------------\n",
      "1    100      79.54     % 38              100            \n",
      "2    250      79.63     % 203             250            \n",
      "3    500      80.00     % 288             500            \n",
      "\n",
      ">>> Best from Step 0: Epochs=500, Val Acc=80.00%\n",
      "    Best validation accuracy was achieved at epoch 288 out of 500\n",
      "\n",
      ">>> Using MAX_EPOCHS=500 and PATIENCE=7 for subsequent steps (with early stopping)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Part 2.2: Sequential Hyperparameter Tuning (One Variable at a Time)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 2.2: SEQUENTIAL HYPERPARAMETER TUNING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# ============================================================================\n",
    "# Step 0: Epoch + Early Stopping Configuration Tuning\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 0: EPOCH + EARLY STOPPING TUNING\")\n",
    "print(\"=\"*80)\n",
    "print(\"Testing different MAX_EPOCHS and PATIENCE configurations\")\n",
    "\n",
    "# Test different epoch and patience configurations\n",
    "max_epochs_options = [100, 250, 500]\n",
    "patience = 10\n",
    "\n",
    "# Use baseline config for testing epoch settings\n",
    "baseline_config = {\n",
    "    'lr': 0.001,\n",
    "    'batch_size': 64,\n",
    "    'hidden_dim': 256,\n",
    "    'optimizer': 'Adam'\n",
    "}\n",
    "\n",
    "step0_configs = []\n",
    "for max_epochs in max_epochs_options:\n",
    "        step0_configs.append({\n",
    "            'config': baseline_config.copy(),\n",
    "            'max_epochs': max_epochs,\n",
    "            'patience': patience\n",
    "        })\n",
    "\n",
    "print(f\"Total combinations to test: {len(step0_configs)}\")\n",
    "print(\"Combinations (Max_Epochs, Patience):\")\n",
    "for idx, ep_config in enumerate(step0_configs, 1):\n",
    "    print(f\"  {idx}. Max_Epochs={ep_config['max_epochs']}, Patience={ep_config['patience']}\")\n",
    "\n",
    "# Helper function to train with specific epoch/patience settings\n",
    "def train_and_evaluate_with_epochs(config, max_epochs, patience, config_name=\"config\"):\n",
    "    \"\"\"Train a model for specific number of epochs WITHOUT early stopping\"\"\"\n",
    "    print(f\"\\n>>> Testing: {config_name}\")\n",
    "    print(f\"    LR={config['lr']}, Batch={config['batch_size']}, Hidden={config['hidden_dim']}, Opt={config['optimizer']}\")\n",
    "    print(f\"    Epochs={max_epochs} (NO early stopping - training for full {max_epochs} epochs)\")\n",
    "    \n",
    "    # Create iterators with specific batch size\n",
    "    train_iter = data.BucketIterator(\n",
    "        train_data,\n",
    "        batch_size=config['batch_size'],\n",
    "        sort_key=lambda x: len(x.text),\n",
    "        sort_within_batch=True,\n",
    "        shuffle=True,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    val_iter = data.BucketIterator(\n",
    "        validation_data,\n",
    "        batch_size=config['batch_size'],\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        sort_key=lambda x: len(x.text),\n",
    "        sort_within_batch=True,\n",
    "        shuffle=False,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Reset random seeds INSIDE function to ensure fresh model for each config\n",
    "    # This is critical to ensure each max_epochs config starts from scratch\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "        torch.cuda.manual_seed_all(SEED)\n",
    "    # Ensure deterministic behavior\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    # Create model with specific hidden dimension\n",
    "    model = SimpleRNNClassifier(\n",
    "        vocab_size=fasttext_vocab_size,\n",
    "        embedding_dim=embedding_dim,\n",
    "        hidden_dim=config['hidden_dim'],\n",
    "        output_dim=num_classes,\n",
    "        n_layers=N_LAYERS,\n",
    "        dropout=0.0,  # Baseline: no dropout\n",
    "        padding_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "        pretrained_embeddings=pretrained_embeddings\n",
    "    ).to(device)\n",
    "    \n",
    "    # Select optimizer\n",
    "    if config['optimizer'] == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config['lr'])\n",
    "    elif config['optimizer'] == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=config['lr'], momentum=0.9)\n",
    "    elif config['optimizer'] == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=config['lr'])\n",
    "    elif config['optimizer'] == 'Adagrad':\n",
    "        optimizer = optim.Adagrad(model.parameters(), lr=config['lr'])\n",
    "    \n",
    "    # Training loop WITHOUT early stopping - train for full num_epochs\n",
    "    best_val_acc = 0.0\n",
    "    best_val_acc_at_epoch = 0\n",
    "    final_val_acc = 0.0\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        \n",
    "        for batch in train_iter:\n",
    "            text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(text, text_lengths)\n",
    "            loss = criterion(predictions, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        train_acc = accuracy_score(train_labels, train_preds)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_iter:\n",
    "                text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "                predictions = model(text, text_lengths)\n",
    "                loss = criterion(predictions, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                preds = torch.argmax(predictions, dim=1)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_acc = accuracy_score(val_labels, val_preds)\n",
    "        \n",
    "        # Track best validation accuracy (but don't stop early)\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_val_acc_at_epoch = epoch + 1\n",
    "        \n",
    "        final_val_acc = val_acc  # Store final epoch's validation accuracy\n",
    "        \n",
    "        # Optional: print progress every 10 epochs or at the end\n",
    "        if (epoch + 1) % 10 == 0 or (epoch + 1) == max_epochs:\n",
    "            print(f\"    Epoch {epoch+1}/{max_epochs}: Train Acc={train_acc*100:.2f}%, Val Acc={val_acc*100:.2f}%\")\n",
    "    \n",
    "    print(f\"    Final Val Acc: {final_val_acc*100:.2f}% | Best Val Acc: {best_val_acc*100:.2f}% (at epoch {best_val_acc_at_epoch}/{max_epochs})\")\n",
    "    return best_val_acc, best_val_acc_at_epoch, max_epochs\n",
    "\n",
    "step0_results = []\n",
    "for idx, ep_config in enumerate(step0_configs):\n",
    "    # Set fixed seed for reproducibility - ensures consistent batch ordering\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "    \n",
    "    val_acc, best_epoch, total_epochs = train_and_evaluate_with_epochs(\n",
    "        ep_config['config'],\n",
    "        ep_config['max_epochs'],\n",
    "        ep_config['patience'],\n",
    "        f\"Step 0 Config {idx+1}/{len(step0_configs)}\"\n",
    "    )\n",
    "    step0_results.append({\n",
    "        'num_epochs': ep_config['max_epochs'],\n",
    "        'val_acc': val_acc,\n",
    "        'best_epoch': best_epoch,\n",
    "        'total_epochs': total_epochs\n",
    "    })\n",
    "\n",
    "# Find best epoch configuration\n",
    "best_step0 = max(step0_results, key=lambda x: x['val_acc'])\n",
    "BEST_EPOCHS = best_step0['num_epochs']\n",
    "\n",
    "# Set appropriate MAX_EPOCHS and PATIENCE for subsequent steps\n",
    "# Use the best number of epochs with some buffer, and set a reasonable patience\n",
    "MAX_EPOCHS = BEST_EPOCHS\n",
    "PATIENCE = 7  # Default patience for early stopping in subsequent steps\n",
    "\n",
    "print(f\"\\n>>> Step 0 Results:\")\n",
    "print(f\"{'#':<4} {'Epochs':<8} {'Val Acc':<10} {'Best At Epoch':<15} {'Total Trained':<15}\")\n",
    "print(\"-\" * 60)\n",
    "for idx, result in enumerate(step0_results, 1):\n",
    "    print(f\"{idx:<4} {result['num_epochs']:<8} {result['val_acc']*100:<10.2f}% {result['best_epoch']:<15} {result['total_epochs']:<15}\")\n",
    "print(f\"\\n>>> Best from Step 0: Epochs={BEST_EPOCHS}, Val Acc={best_step0['val_acc']*100:.2f}%\")\n",
    "print(f\"    Best validation accuracy was achieved at epoch {best_step0['best_epoch']} out of {best_step0['total_epochs']}\")\n",
    "print(f\"\\n>>> Using MAX_EPOCHS={MAX_EPOCHS} and PATIENCE={PATIENCE} for subsequent steps (with early stopping)\")\n",
    "\n",
    "# Helper function to train and evaluate a model configuration\n",
    "def train_and_evaluate(config, config_name=\"config\"):\n",
    "    \"\"\"Train a model with given configuration and return validation accuracy\"\"\"\n",
    "    print(f\"\\n>>> Testing: {config_name}\")\n",
    "    print(f\"    LR={config['lr']}, Batch={config['batch_size']}, Hidden={config['hidden_dim']}, Opt={config['optimizer']}\")\n",
    "    \n",
    "    # Create iterators with specific batch size\n",
    "    train_iter = data.BucketIterator(\n",
    "        train_data,\n",
    "        batch_size=config['batch_size'],\n",
    "        sort_key=lambda x: len(x.text),\n",
    "        sort_within_batch=True,\n",
    "        shuffle=True,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    val_iter = data.BucketIterator(\n",
    "        validation_data,\n",
    "        batch_size=config['batch_size'],\n",
    "        sort_key=lambda x: len(x.text),\n",
    "        sort_within_batch=True,\n",
    "        shuffle=False,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Create model with specific hidden dimension\n",
    "    model = SimpleRNNClassifier(\n",
    "        vocab_size=fasttext_vocab_size,\n",
    "        embedding_dim=embedding_dim,\n",
    "        hidden_dim=config['hidden_dim'],\n",
    "        output_dim=num_classes,\n",
    "        n_layers=N_LAYERS,\n",
    "        dropout=0.0,  # Baseline: no dropout\n",
    "        padding_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "        pretrained_embeddings=pretrained_embeddings\n",
    "    ).to(device)\n",
    "    \n",
    "    # Select optimizer\n",
    "    if config['optimizer'] == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config['lr'])\n",
    "    elif config['optimizer'] == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=config['lr'])\n",
    "    elif config['optimizer'] == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=config['lr'])\n",
    "    elif config['optimizer'] == 'Adagrad':\n",
    "        optimizer = optim.Adagrad(model.parameters(), lr=config['lr'])\n",
    "    \n",
    "    # Training loop with early stopping\n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        \n",
    "        for batch in train_iter:\n",
    "            text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(text, text_lengths)\n",
    "            loss = criterion(predictions, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        train_acc = accuracy_score(train_labels, train_preds)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_iter:\n",
    "                text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "                predictions = model(text, text_lengths)\n",
    "                loss = criterion(predictions, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                preds = torch.argmax(predictions, dim=1)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_acc = accuracy_score(val_labels, val_preds)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(f\"    Early stopping at epoch {epoch+1}, best val acc: {best_val_acc*100:.2f}%\")\n",
    "                break\n",
    "    \n",
    "    print(f\"    Best Val Acc: {best_val_acc*100:.2f}% (stopped at epoch {epoch+1})\")\n",
    "    return best_val_acc, epoch + 1\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8590e566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 1: LEARNING RATE + BATCH SIZE TUNING\n",
      "================================================================================\n",
      "Testing ALL combinations of LR and Batch Size (they interact)\n",
      "Total combinations to test: 9\n",
      "Combinations:\n",
      "  1. LR=0.01, Batch=32\n",
      "  2. LR=0.01, Batch=64\n",
      "  3. LR=0.01, Batch=128\n",
      "  4. LR=0.001, Batch=32\n",
      "  5. LR=0.001, Batch=64\n",
      "  6. LR=0.001, Batch=128\n",
      "  7. LR=0.0001, Batch=32\n",
      "  8. LR=0.0001, Batch=64\n",
      "  9. LR=0.0001, Batch=128\n",
      "\n",
      ">>> Testing: Step 1 Config 1/9\n",
      "    LR=0.01, Batch=32, Hidden=256, Opt=Adam\n",
      "    Early stopping at epoch 21, best val acc: 50.00%\n",
      "    Best Val Acc: 50.00% (stopped at epoch 21)\n",
      "\n",
      ">>> Testing: Step 1 Config 2/9\n",
      "    LR=0.01, Batch=64, Hidden=256, Opt=Adam\n",
      "    Early stopping at epoch 30, best val acc: 41.28%\n",
      "    Best Val Acc: 41.28% (stopped at epoch 30)\n",
      "\n",
      ">>> Testing: Step 1 Config 3/9\n",
      "    LR=0.01, Batch=128, Hidden=256, Opt=Adam\n",
      "    Early stopping at epoch 31, best val acc: 37.61%\n",
      "    Best Val Acc: 37.61% (stopped at epoch 31)\n",
      "\n",
      ">>> Testing: Step 1 Config 4/9\n",
      "    LR=0.001, Batch=32, Hidden=256, Opt=Adam\n",
      "    Early stopping at epoch 11, best val acc: 73.67%\n",
      "    Best Val Acc: 73.67% (stopped at epoch 11)\n",
      "\n",
      ">>> Testing: Step 1 Config 5/9\n",
      "    LR=0.001, Batch=64, Hidden=256, Opt=Adam\n",
      "    Early stopping at epoch 15, best val acc: 78.90%\n",
      "    Best Val Acc: 78.90% (stopped at epoch 15)\n",
      "\n",
      ">>> Testing: Step 1 Config 6/9\n",
      "    LR=0.001, Batch=128, Hidden=256, Opt=Adam\n",
      "    Early stopping at epoch 19, best val acc: 69.91%\n",
      "    Best Val Acc: 69.91% (stopped at epoch 19)\n",
      "\n",
      ">>> Testing: Step 1 Config 7/9\n",
      "    LR=0.0001, Batch=32, Hidden=256, Opt=Adam\n",
      "    Early stopping at epoch 28, best val acc: 83.12%\n",
      "    Best Val Acc: 83.12% (stopped at epoch 28)\n",
      "\n",
      ">>> Testing: Step 1 Config 8/9\n",
      "    LR=0.0001, Batch=64, Hidden=256, Opt=Adam\n",
      "    Early stopping at epoch 37, best val acc: 82.84%\n",
      "    Best Val Acc: 82.84% (stopped at epoch 37)\n",
      "\n",
      ">>> Testing: Step 1 Config 9/9\n",
      "    LR=0.0001, Batch=128, Hidden=256, Opt=Adam\n",
      "    Early stopping at epoch 45, best val acc: 82.02%\n",
      "    Best Val Acc: 82.02% (stopped at epoch 45)\n",
      "\n",
      ">>> Step 1 Results:\n",
      "#    LR       Batch   Val Acc    Epochs \n",
      "----------------------------------------\n",
      "1    0.01     32      50.00     % 21     \n",
      "2    0.01     64      41.28     % 30     \n",
      "3    0.01     128     37.61     % 31     \n",
      "4    0.001    32      73.67     % 11     \n",
      "5    0.001    64      78.90     % 15     \n",
      "6    0.001    128     69.91     % 19     \n",
      "7    0.0001   32      83.12     % 28     \n",
      "8    0.0001   64      82.84     % 37     \n",
      "9    0.0001   128     82.02     % 45     \n",
      "\n",
      ">>> Best from Step 1: LR=0.0001, Batch=32, Val Acc=83.12%\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Step 1: Group 1 - Learning Rate + Batch Size (Test Together)\n",
    "# ============================================================================\n",
    "\n",
    "# Helper function to train and evaluate a model configuration (uses best MAX_EPOCHS and PATIENCE from Step 0)\n",
    "def train_and_evaluate(config, config_name=\"config\"):\n",
    "    \"\"\"Train a model with given configuration and return validation accuracy\"\"\"\n",
    "    print(f\"\\n>>> Testing: {config_name}\")\n",
    "    print(f\"    LR={config['lr']}, Batch={config['batch_size']}, Hidden={config['hidden_dim']}, Opt={config['optimizer']}\")\n",
    "    \n",
    "    # Create iterators with specific batch size\n",
    "    train_iter = data.BucketIterator(\n",
    "        train_data,\n",
    "        batch_size=config['batch_size'],\n",
    "        sort_key=lambda x: len(x.text),\n",
    "        sort_within_batch=True,\n",
    "        shuffle=True,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    val_iter = data.BucketIterator(\n",
    "        validation_data,\n",
    "        batch_size=config['batch_size'],\n",
    "        sort_key=lambda x: len(x.text),\n",
    "        sort_within_batch=True,\n",
    "        shuffle=False,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Create model with specific hidden dimension\n",
    "    model = SimpleRNNClassifier(\n",
    "        vocab_size=fasttext_vocab_size,\n",
    "        embedding_dim=embedding_dim,\n",
    "        hidden_dim=config['hidden_dim'],\n",
    "        output_dim=num_classes,\n",
    "        n_layers=N_LAYERS,\n",
    "        dropout=0.0,  # Baseline: no dropout\n",
    "        padding_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "        pretrained_embeddings=pretrained_embeddings\n",
    "    ).to(device)\n",
    "    \n",
    "    # Select optimizer\n",
    "    if config['optimizer'] == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config['lr'])\n",
    "    elif config['optimizer'] == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=config['lr'], momentum=0.9)\n",
    "    elif config['optimizer'] == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=config['lr'])\n",
    "    elif config['optimizer'] == 'Adagrad':\n",
    "        optimizer = optim.Adagrad(model.parameters(), lr=config['lr'])\n",
    "    \n",
    "    # Training loop with early stopping (using best MAX_EPOCHS and PATIENCE from Step 0)\n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        \n",
    "        for batch in train_iter:\n",
    "            text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(text, text_lengths)\n",
    "            loss = criterion(predictions, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        train_acc = accuracy_score(train_labels, train_preds)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_iter:\n",
    "                text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "                predictions = model(text, text_lengths)\n",
    "                loss = criterion(predictions, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                preds = torch.argmax(predictions, dim=1)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_acc = accuracy_score(val_labels, val_preds)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(f\"    Early stopping at epoch {epoch+1}, best val acc: {best_val_acc*100:.2f}%\")\n",
    "                break\n",
    "    \n",
    "    print(f\"    Best Val Acc: {best_val_acc*100:.2f}% (stopped at epoch {epoch+1})\")\n",
    "    return best_val_acc, epoch + 1\n",
    "    \n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 1: LEARNING RATE + BATCH SIZE TUNING\")\n",
    "print(\"=\"*80)\n",
    "print(\"Testing ALL combinations of LR and Batch Size (they interact)\")\n",
    "\n",
    "# Test all combinations of learning rates and batch sizes\n",
    "learning_rates = [0.01, 0.001, 0.0001]\n",
    "batch_sizes = [32, 64, 128]\n",
    "\n",
    "step1_configs = []\n",
    "for lr in learning_rates:\n",
    "    for batch_size in batch_sizes:\n",
    "        step1_configs.append({\n",
    "            'lr': lr,\n",
    "            'batch_size': batch_size,\n",
    "            'hidden_dim': 256,\n",
    "            'optimizer': 'Adam'\n",
    "        })\n",
    "\n",
    "print(f\"Total combinations to test: {len(step1_configs)}\")\n",
    "print(\"Combinations:\")\n",
    "for idx, config in enumerate(step1_configs, 1):\n",
    "    print(f\"  {idx}. LR={config['lr']}, Batch={config['batch_size']}\")\n",
    "\n",
    "step1_results = []\n",
    "for idx, config in enumerate(step1_configs):\n",
    "    # Set fixed seed for reproducibility\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "    \n",
    "    val_acc, epoch_stopped = train_and_evaluate(config, f\"Step 1 Config {idx+1}/{len(step1_configs)}\")\n",
    "    step1_results.append({\n",
    "        'config': config,\n",
    "        'val_acc': val_acc,\n",
    "        'epoch_stopped': epoch_stopped\n",
    "    })\n",
    "\n",
    "# Find best LR + Batch Size\n",
    "best_step1 = max(step1_results, key=lambda x: x['val_acc'])\n",
    "best_lr = best_step1['config']['lr']\n",
    "best_batch_size = best_step1['config']['batch_size']\n",
    "\n",
    "print(f\"\\n>>> Step 1 Results:\")\n",
    "print(f\"{'#':<4} {'LR':<8} {'Batch':<7} {'Val Acc':<10} {'Epochs':<7}\")\n",
    "print(\"-\" * 40)\n",
    "for idx, result in enumerate(step1_results):\n",
    "    c = result['config']\n",
    "    print(f\"{idx+1:<4} {c['lr']:<8} {c['batch_size']:<7} {result['val_acc']*100:<10.2f}% {result['epoch_stopped']:<7}\")\n",
    "print(f\"\\n>>> Best from Step 1: LR={best_lr}, Batch={best_batch_size}, Val Acc={best_step1['val_acc']*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f576e603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 2: OPTIMIZER\n",
      "================================================================================\n",
      "Using best LR=0.0001 and Batch=32 from Step 1\n",
      "\n",
      ">>> Testing: Step 2 Config 1/4\n",
      "    LR=0.0001, Batch=32, Hidden=256, Opt=Adam\n",
      "    Early stopping at epoch 28, best val acc: 83.12%\n",
      "    Best Val Acc: 83.12% (stopped at epoch 28)\n",
      "\n",
      ">>> Testing: Step 2 Config 2/4\n",
      "    LR=0.0001, Batch=32, Hidden=256, Opt=SGD\n",
      "    Early stopping at epoch 10, best val acc: 23.21%\n",
      "    Best Val Acc: 23.21% (stopped at epoch 10)\n",
      "\n",
      ">>> Testing: Step 2 Config 3/4\n",
      "    LR=0.0001, Batch=32, Hidden=256, Opt=RMSprop\n",
      "    Early stopping at epoch 26, best val acc: 83.12%\n",
      "    Best Val Acc: 83.12% (stopped at epoch 26)\n",
      "\n",
      ">>> Testing: Step 2 Config 4/4\n",
      "    LR=0.0001, Batch=32, Hidden=256, Opt=Adagrad\n",
      "    Early stopping at epoch 8, best val acc: 23.21%\n",
      "    Best Val Acc: 23.21% (stopped at epoch 8)\n",
      "\n",
      ">>> Step 2 Results:\n",
      "#    LR       Optimizer  Val Acc    Epochs \n",
      "---------------------------------------------\n",
      "1    0.0001   Adam       83.12     % 28     \n",
      "2    0.0001   SGD        23.21     % 10     \n",
      "3    0.0001   RMSprop    83.12     % 26     \n",
      "4    0.0001   Adagrad    23.21     % 8      \n",
      "\n",
      ">>> Best from Step 2: LR=0.0001, Optimizer=Adam, Val Acc=83.12%\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Step 2: Group 2 - Optimizer\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 2: OPTIMIZER\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Using best LR={best_lr} and Batch={best_batch_size} from Step 1\")\n",
    "\n",
    "step2_configs = [\n",
    "    {'lr': best_lr, 'batch_size': best_batch_size, 'hidden_dim': 256, 'optimizer': 'Adam'},\n",
    "    {'lr': best_lr, 'batch_size': best_batch_size, 'hidden_dim': 256, 'optimizer': 'SGD'},\n",
    "    {'lr': best_lr, 'batch_size': best_batch_size, 'hidden_dim': 256, 'optimizer': 'RMSprop'},\n",
    "    {'lr': best_lr, 'batch_size': best_batch_size, 'hidden_dim': 256, 'optimizer': 'Adagrad'},\n",
    "]\n",
    "\n",
    "step2_results = []\n",
    "for idx, config in enumerate(step2_configs):\n",
    "    # Set fixed seed for reproducibility\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "    \n",
    "    val_acc, epoch_stopped = train_and_evaluate(config, f\"Step 2 Config {idx+1}/{len(step2_configs)}\")\n",
    "    step2_results.append({\n",
    "        'config': config,\n",
    "        'val_acc': val_acc,\n",
    "        'epoch_stopped': epoch_stopped\n",
    "    })\n",
    "\n",
    "# Find best Optimizer (and potentially adjusted LR)\n",
    "best_step2 = max(step2_results, key=lambda x: x['val_acc'])\n",
    "best_optimizer = best_step2['config']['optimizer']\n",
    "final_lr = best_step2['config']['lr']  # May be different if SGD needed higher LR\n",
    "\n",
    "print(f\"\\n>>> Step 2 Results:\")\n",
    "print(f\"{'#':<4} {'LR':<8} {'Optimizer':<10} {'Val Acc':<10} {'Epochs':<7}\")\n",
    "print(\"-\" * 45)\n",
    "for idx, result in enumerate(step2_results):\n",
    "    c = result['config']\n",
    "    print(f\"{idx+1:<4} {c['lr']:<8} {c['optimizer']:<10} {result['val_acc']*100:<10.2f}% {result['epoch_stopped']:<7}\")\n",
    "print(f\"\\n>>> Best from Step 2: LR={final_lr}, Optimizer={best_optimizer}, Val Acc={best_step2['val_acc']*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c7091121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 3: HIDDEN DIMENSION TUNING\n",
      "================================================================================\n",
      "Using best LR=0.0001, Batch=32, Optimizer=Adam from Steps 1-2\n",
      "\n",
      ">>> Testing: Step 3 Config 1/3\n",
      "    LR=0.0001, Batch=32, Hidden=128, Opt=Adam\n",
      "    Early stopping at epoch 27, best val acc: 82.94%\n",
      "    Best Val Acc: 82.94% (stopped at epoch 27)\n",
      "\n",
      ">>> Testing: Step 3 Config 2/3\n",
      "    LR=0.0001, Batch=32, Hidden=256, Opt=Adam\n",
      "    Early stopping at epoch 28, best val acc: 83.12%\n",
      "    Best Val Acc: 83.12% (stopped at epoch 28)\n",
      "\n",
      ">>> Testing: Step 3 Config 3/3\n",
      "    LR=0.0001, Batch=32, Hidden=512, Opt=Adam\n",
      "    Early stopping at epoch 22, best val acc: 83.58%\n",
      "    Best Val Acc: 83.58% (stopped at epoch 22)\n",
      "\n",
      ">>> Step 3 Results:\n",
      "#    Hidden Dim   Val Acc    Epochs \n",
      "-----------------------------------\n",
      "1    128          82.94     % 27     \n",
      "2    256          83.12     % 28     \n",
      "3    512          83.58     % 22     \n",
      "\n",
      ">>> Best from Step 3: Hidden Dim=512, Val Acc=83.58%\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Step 3: Hidden Dimension (Test Independently)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 3: HIDDEN DIMENSION TUNING\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Using best LR={final_lr}, Batch={best_batch_size}, Optimizer={best_optimizer} from Steps 1-2\")\n",
    "\n",
    "step3_configs = [\n",
    "    {'lr': final_lr, 'batch_size': best_batch_size, 'hidden_dim': 128, 'optimizer': best_optimizer},\n",
    "    {'lr': final_lr, 'batch_size': best_batch_size, 'hidden_dim': 256, 'optimizer': best_optimizer},\n",
    "    {'lr': final_lr, 'batch_size': best_batch_size, 'hidden_dim': 512, 'optimizer': best_optimizer},\n",
    "]\n",
    "\n",
    "step3_results = []\n",
    "for idx, config in enumerate(step3_configs):\n",
    "    # Set fixed seed for reproducibility\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "    \n",
    "    val_acc, epoch_stopped = train_and_evaluate(config, f\"Step 3 Config {idx+1}/{len(step3_configs)}\")\n",
    "    step3_results.append({\n",
    "        'config': config,\n",
    "        'val_acc': val_acc,\n",
    "        'epoch_stopped': epoch_stopped\n",
    "    })\n",
    "\n",
    "# Find best Hidden Dimension\n",
    "best_step3 = max(step3_results, key=lambda x: x['val_acc'])\n",
    "best_hidden_dim = best_step3['config']['hidden_dim']\n",
    "\n",
    "print(f\"\\n>>> Step 3 Results:\")\n",
    "print(f\"{'#':<4} {'Hidden Dim':<12} {'Val Acc':<10} {'Epochs':<7}\")\n",
    "print(\"-\" * 35)\n",
    "for idx, result in enumerate(step3_results):\n",
    "    c = result['config']\n",
    "    print(f\"{idx+1:<4} {c['hidden_dim']:<12} {result['val_acc']*100:<10.2f}% {result['epoch_stopped']:<7}\")\n",
    "print(f\"\\n>>> Best from Step 3: Hidden Dim={best_hidden_dim}, Val Acc={best_step3['val_acc']*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9b97f2d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "HYPERPARAMETER TUNING COMPLETE - FINAL BEST CONFIGURATION\n",
      "================================================================================\n",
      "\n",
      ">>> Best Configuration Found:\n",
      "    Learning Rate: 0.0001\n",
      "    Batch Size: 32\n",
      "    Hidden Dimension: 512\n",
      "    Optimizer: Adam\n",
      "    Max Epochs: 500 (with early stopping, patience=7)\n",
      "    Best Validation Accuracy: 83.58%\n",
      "\n",
      "================================================================================\n",
      "SEQUENTIAL HYPERPARAMETER TUNING COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Final Best Configuration Summary\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"HYPERPARAMETER TUNING COMPLETE - FINAL BEST CONFIGURATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "final_best_config = {\n",
    "    'lr': final_lr,\n",
    "    'batch_size': best_batch_size,\n",
    "    'hidden_dim': best_hidden_dim,\n",
    "    'optimizer': best_optimizer,\n",
    "    'max_epochs': MAX_EPOCHS,\n",
    "    'patience': PATIENCE\n",
    "}\n",
    "\n",
    "print(f\"\\n>>> Best Configuration Found:\")\n",
    "print(f\"    Learning Rate: {final_best_config['lr']}\")\n",
    "print(f\"    Batch Size: {final_best_config['batch_size']}\")\n",
    "print(f\"    Hidden Dimension: {final_best_config['hidden_dim']}\")\n",
    "print(f\"    Optimizer: {final_best_config['optimizer']}\")\n",
    "print(f\"    Max Epochs: {final_best_config['max_epochs']} (with early stopping, patience={final_best_config['patience']})\")\n",
    "print(f\"    Best Validation Accuracy: {best_step3['val_acc']*100:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SEQUENTIAL HYPERPARAMETER TUNING COMPLETE\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb78d4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "WORD AGGREGATION METHOD COMPARISON\n",
      "================================================================================\n",
      "Using best hyperparameters from tuning:\n",
      "    LR=0.0001, Batch=32, Hidden=512, Optimizer=Adam\n",
      "    Max Epochs=500, Patience=7\n",
      "\n",
      "Testing 4 aggregation methods:\n",
      "  - last\n",
      "  - mean\n",
      "  - max\n",
      "  - attention\n",
      "\n",
      "================================================================================\n",
      "Testing Aggregation Method: LAST\n",
      "================================================================================\n",
      "\n",
      ">>> Training model with last aggregation...\n",
      "    Epoch 10: Train Acc=94.09%, Val Acc=78.90%\n",
      "    Epoch 20: Train Acc=97.78%, Val Acc=82.29%\n",
      "    Early stopping at epoch 22, best val acc: 83.58%\n",
      "\n",
      ">>> Results for last aggregation:\n",
      "    Validation Acc: 83.58%\n",
      "    Test Acc: 87.60%\n",
      "    Test F1: 0.8734\n",
      "    Test AUC-ROC: 0.9770\n",
      "\n",
      "================================================================================\n",
      "Testing Aggregation Method: MEAN\n",
      "================================================================================\n",
      "\n",
      ">>> Training model with mean aggregation...\n",
      "    Epoch 10: Train Acc=93.21%, Val Acc=79.63%\n",
      "    Epoch 20: Train Acc=95.94%, Val Acc=82.94%\n",
      "    Early stopping at epoch 28, best val acc: 85.05%\n",
      "\n",
      ">>> Results for mean aggregation:\n",
      "    Validation Acc: 85.05%\n",
      "    Test Acc: 86.60%\n",
      "    Test F1: 0.8624\n",
      "    Test AUC-ROC: 0.9657\n",
      "\n",
      "================================================================================\n",
      "Testing Aggregation Method: MAX\n",
      "================================================================================\n",
      "\n",
      ">>> Training model with max aggregation...\n",
      "    Early stopping at epoch 8, best val acc: 22.94%\n",
      "    Warning: Could not calculate AUC-ROC: Input contains NaN.\n",
      "\n",
      ">>> Results for max aggregation:\n",
      "    Validation Acc: 22.94%\n",
      "    Test Acc: 18.80%\n",
      "    Test F1: 0.0595\n",
      "    Test AUC-ROC: 0.0000\n",
      "\n",
      "================================================================================\n",
      "Testing Aggregation Method: ATTENTION\n",
      "================================================================================\n",
      "\n",
      ">>> Training model with attention aggregation...\n",
      "    Epoch 10: Train Acc=97.02%, Val Acc=83.49%\n",
      "    Epoch 20: Train Acc=99.54%, Val Acc=84.22%\n",
      "    Early stopping at epoch 28, best val acc: 85.41%\n",
      "\n",
      ">>> Results for attention aggregation:\n",
      "    Validation Acc: 85.41%\n",
      "    Test Acc: 88.00%\n",
      "    Test F1: 0.8788\n",
      "    Test AUC-ROC: 0.9621\n",
      "\n",
      "================================================================================\n",
      "AGGREGATION METHOD COMPARISON - RESULTS SUMMARY\n",
      "================================================================================\n",
      "\n",
      ">>> Results Summary:\n",
      "Method       Val Acc    Test Acc   Test F1    Test AUC  \n",
      "-------------------------------------------------------\n",
      "last         83.58     % 87.60     % 0.8734     0.9770    \n",
      "mean         85.05     % 86.60     % 0.8624     0.9657    \n",
      "max          22.94     % 18.80     % 0.0595     0.0000    \n",
      "attention    85.41     % 88.00     % 0.8788     0.9621    \n",
      "\n",
      ">>> Best Aggregation Method: ATTENTION\n",
      "    Validation Accuracy: 85.41%\n",
      "    Test Accuracy: 88.00%\n",
      "    Test F1 Score: 0.8788\n",
      "    Test AUC-ROC: 0.9621\n",
      "\n",
      "================================================================================\n",
      "AGGREGATION METHOD COMPARISON COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Word Aggregation Method Comparison\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"WORD AGGREGATION METHOD COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Using best hyperparameters from tuning:\")\n",
    "print(f\"    LR={final_lr}, Batch={best_batch_size}, Hidden={best_hidden_dim}, Optimizer={best_optimizer}\")\n",
    "print(f\"    Max Epochs={MAX_EPOCHS}, Patience={PATIENCE}\")\n",
    "\n",
    "# Extended RNN Classifier with multiple aggregation methods\n",
    "class RNN_Classifier_Aggregation(nn.Module):\n",
    "    \"\"\"\n",
    "    RNN for topic classification with multiple aggregation strategies.\n",
    "    Uses pretrained embeddings (learnable/updated during training).\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, \n",
    "                 n_layers=1, dropout=0.0, padding_idx=0, pretrained_embeddings=None,\n",
    "                 aggregation='last'):\n",
    "        super(RNN_Classifier_Aggregation, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.aggregation = aggregation  # 'last', 'mean', 'max', 'attention'\n",
    "        \n",
    "        # Embedding layer initialized with pretrained embeddings\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "            # Make embeddings learnable (updated during training)\n",
    "            self.embedding.weight.requires_grad = True\n",
    "        \n",
    "        # RNN layer\n",
    "        self.rnn = nn.RNN(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=n_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.0  # No dropout in baseline\n",
    "        )\n",
    "        \n",
    "        # Attention mechanism for aggregation (only created if needed)\n",
    "        if aggregation == 'attention':\n",
    "            self.attention = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        # Fully connected output layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        # text: [batch_size, seq_len]\n",
    "        # text_lengths: [batch_size]\n",
    "        \n",
    "        # Embed the input\n",
    "        embedded = self.embedding(text)  # [batch_size, seq_len, embedding_dim]\n",
    "        \n",
    "        # Get dimensions\n",
    "        batch_size = embedded.size(0)\n",
    "        seq_len = embedded.size(1)\n",
    "        \n",
    "        # Handle text_lengths\n",
    "        text_lengths_flat = text_lengths.flatten().cpu().long()\n",
    "        if len(text_lengths_flat) != batch_size:\n",
    "            raise ValueError(\n",
    "                f\"text_lengths size mismatch: got {len(text_lengths_flat)} elements, \"\n",
    "                f\"expected {batch_size}\"\n",
    "            )\n",
    "        \n",
    "        # Clamp lengths\n",
    "        text_lengths_clamped = torch.clamp(text_lengths_flat, min=1, max=seq_len)\n",
    "        \n",
    "        # Move to same device as text for mask operations later\n",
    "        # Keep CPU version for pack_padded_sequence (requires CPU)\n",
    "        # Create device version for mask operations later\n",
    "        text_lengths_clamped_device = text_lengths_clamped.to(text.device)\n",
    "        # Pack the padded sequences\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, text_lengths_clamped, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        \n",
    "        # Pass through RNN\n",
    "        packed_output, hidden = self.rnn(packed_embedded)\n",
    "        # hidden: [n_layers, batch_size, hidden_dim]\n",
    "        \n",
    "        # Aggregate word representations to sentence representation\n",
    "        if self.aggregation == 'last':\n",
    "            # Use the last hidden state from the last layer\n",
    "            sentence_repr = hidden[-1]  # [batch_size, hidden_dim]\n",
    "            \n",
    "        elif self.aggregation == 'mean':\n",
    "            # Mean pooling over all outputs (ignoring padding)\n",
    "            # Unpack the sequences first\n",
    "            output, output_lengths = nn.utils.rnn.pad_packed_sequence(\n",
    "                packed_output, batch_first=True\n",
    "            )\n",
    "            # output: [batch_size, seq_len, hidden_dim]\n",
    "            \n",
    "            # Create mask for padding\n",
    "            mask = torch.arange(seq_len, device=text.device).unsqueeze(0) < text_lengths_clamped_device.unsqueeze(1)\n",
    "            mask = mask.unsqueeze(2).float()  # [batch_size, seq_len, 1]\n",
    "            \n",
    "            # Apply mask and compute mean\n",
    "            masked_output = output * mask\n",
    "            sum_output = masked_output.sum(dim=1)  # [batch_size, hidden_dim]\n",
    "            sentence_repr = sum_output / text_lengths_clamped_device.unsqueeze(1).float()\n",
    "            \n",
    "        elif self.aggregation == 'max':\n",
    "            # Max pooling over all outputs\n",
    "            output, output_lengths = nn.utils.rnn.pad_packed_sequence(\n",
    "                packed_output, batch_first=True\n",
    "            )\n",
    "            # output: [batch_size, seq_len, hidden_dim]\n",
    "            \n",
    "            # Create mask for padding (set padding to -inf before max)\n",
    "            mask = torch.arange(seq_len, device=text.device).unsqueeze(0) < text_lengths_clamped_device.unsqueeze(1)\n",
    "            mask = mask.unsqueeze(2).float()  # [batch_size, seq_len, 1]\n",
    "            \n",
    "            masked_output = output * mask + (1 - mask) * float('-inf')\n",
    "            sentence_repr, _ = torch.max(masked_output, dim=1)\n",
    "            \n",
    "        elif self.aggregation == 'attention':\n",
    "            # Attention mechanism\n",
    "            output, output_lengths = nn.utils.rnn.pad_packed_sequence(\n",
    "                packed_output, batch_first=True\n",
    "            )\n",
    "            # output: [batch_size, seq_len, hidden_dim]\n",
    "            \n",
    "            # Compute attention scores\n",
    "            attn_scores = self.attention(output).squeeze(2)  # [batch_size, seq_len]\n",
    "            \n",
    "            # Mask padding positions\n",
    "            mask = torch.arange(seq_len, device=text.device).unsqueeze(0) < text_lengths_clamped_device.unsqueeze(1)\n",
    "            attn_scores = attn_scores.masked_fill(~mask, float('-inf'))\n",
    "            \n",
    "            # Apply softmax\n",
    "            attn_weights = torch.softmax(attn_scores, dim=1).unsqueeze(1)  # [batch_size, 1, seq_len]\n",
    "            \n",
    "            # Weighted sum\n",
    "            sentence_repr = torch.bmm(attn_weights, output).squeeze(1)  # [batch_size, hidden_dim]\n",
    "        \n",
    "        # Pass through fully connected layer\n",
    "        output = self.fc(sentence_repr)  # [batch_size, output_dim]\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Test different aggregation methods\n",
    "aggregation_methods = ['last', 'mean', 'max', 'attention']\n",
    "\n",
    "print(f\"\\nTesting {len(aggregation_methods)} aggregation methods:\")\n",
    "for method in aggregation_methods:\n",
    "    print(f\"  - {method}\")\n",
    "\n",
    "aggregation_results = []\n",
    "\n",
    "for agg_method in aggregation_methods:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Testing Aggregation Method: {agg_method.upper()}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Set fixed seed for reproducibility - ensures consistent batch ordering\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "    \n",
    "    # Create iterators with best batch size\n",
    "    train_iter = data.BucketIterator(\n",
    "        train_data,\n",
    "        batch_size=best_batch_size,\n",
    "        sort_key=lambda x: len(x.text),\n",
    "        sort_within_batch=True,\n",
    "        shuffle=True,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    val_iter = data.BucketIterator(\n",
    "        validation_data,\n",
    "        batch_size=best_batch_size,\n",
    "        sort_key=lambda x: len(x.text),\n",
    "        sort_within_batch=True,\n",
    "        shuffle=False,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    test_iter = data.BucketIterator(\n",
    "        test_data,\n",
    "        batch_size=best_batch_size,\n",
    "        sort_key=lambda x: len(x.text),\n",
    "        sort_within_batch=True,\n",
    "        shuffle=False,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Create model with best hyperparameters and specific aggregation method\n",
    "    model = RNN_Classifier_Aggregation(\n",
    "        vocab_size=fasttext_vocab_size,\n",
    "        embedding_dim=embedding_dim,\n",
    "        hidden_dim=best_hidden_dim,\n",
    "        output_dim=num_classes,\n",
    "        n_layers=N_LAYERS,\n",
    "        dropout=0.0,  # Baseline: no dropout\n",
    "        padding_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "        pretrained_embeddings=pretrained_embeddings,\n",
    "        aggregation=agg_method\n",
    "    ).to(device)\n",
    "    \n",
    "    # Select optimizer with best learning rate\n",
    "    if best_optimizer == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=final_lr)\n",
    "    elif best_optimizer == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=final_lr, momentum=0.9)\n",
    "    elif best_optimizer == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=final_lr)\n",
    "    elif best_optimizer == 'Adagrad':\n",
    "        optimizer = optim.Adagrad(model.parameters(), lr=final_lr)\n",
    "    \n",
    "    # Training loop with early stopping\n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    print(f\"\\n>>> Training model with {agg_method} aggregation...\")\n",
    "    \n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        \n",
    "        for batch in train_iter:\n",
    "            text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(text, text_lengths)\n",
    "            loss = criterion(predictions, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        train_acc = accuracy_score(train_labels, train_preds)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_iter:\n",
    "                text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "                predictions = model(text, text_lengths)\n",
    "                loss = criterion(predictions, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                preds = torch.argmax(predictions, dim=1)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_acc = accuracy_score(val_labels, val_preds)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            # Save best model for this aggregation method\n",
    "            torch.save(model.state_dict(), f'rnn_agg_{agg_method}_best.pt')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(f\"    Early stopping at epoch {epoch+1}, best val acc: {best_val_acc*100:.2f}%\")\n",
    "                break\n",
    "        \n",
    "        # Print progress every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"    Epoch {epoch+1}: Train Acc={train_acc*100:.2f}%, Val Acc={val_acc*100:.2f}%\")\n",
    "    \n",
    "    # Load best model and evaluate on test set\n",
    "    model.load_state_dict(torch.load(f'rnn_agg_{agg_method}_best.pt'))\n",
    "    model.eval()\n",
    "    \n",
    "    test_loss = 0\n",
    "    test_preds = []\n",
    "    test_labels = []\n",
    "    test_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_iter:\n",
    "            text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "            predictions = model(text, text_lengths)\n",
    "            loss = criterion(predictions, labels)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            probs = torch.softmax(predictions, dim=1)\n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            test_preds.extend(preds.cpu().numpy())\n",
    "            test_labels.extend(labels.cpu().numpy())\n",
    "            test_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    # Calculate test metrics\n",
    "    test_acc = accuracy_score(test_labels, test_preds)\n",
    "    test_f1 = f1_score(test_labels, test_preds, average='weighted')\n",
    "    test_loss_avg = test_loss / len(test_iter)\n",
    "    \n",
    "    # Calculate AUC-ROC\n",
    "    try:\n",
    "        test_probs_array = np.array(test_probs)\n",
    "        test_labels_bin = label_binarize(test_labels, classes=range(num_classes))\n",
    "        test_auc = roc_auc_score(test_labels_bin, test_probs_array, average='weighted', multi_class='ovr')\n",
    "    except Exception as e:\n",
    "        print(f\"    Warning: Could not calculate AUC-ROC: {e}\")\n",
    "        test_auc = 0.0\n",
    "    \n",
    "    aggregation_results.append({\n",
    "        'method': agg_method,\n",
    "        'val_acc': best_val_acc,\n",
    "        'test_acc': test_acc,\n",
    "        'test_f1': test_f1,\n",
    "        'test_auc': test_auc,\n",
    "        'test_loss': test_loss_avg\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n>>> Results for {agg_method} aggregation:\")\n",
    "    print(f\"    Validation Acc: {best_val_acc*100:.2f}%\")\n",
    "    print(f\"    Test Acc: {test_acc*100:.2f}%\")\n",
    "    print(f\"    Test F1: {test_f1:.4f}\")\n",
    "    print(f\"    Test AUC-ROC: {test_auc:.4f}\")\n",
    "\n",
    "# Print summary table\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"AGGREGATION METHOD COMPARISON - RESULTS SUMMARY\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "print(f\"\\n>>> Results Summary:\")\n",
    "print(f\"{'Method':<12} {'Val Acc':<10} {'Test Acc':<10} {'Test F1':<10} {'Test AUC':<10}\")\n",
    "print(\"-\" * 55)\n",
    "for result in aggregation_results:\n",
    "    print(f\"{result['method']:<12} {result['val_acc']*100:<10.2f}% {result['test_acc']*100:<10.2f}% \"\n",
    "          f\"{result['test_f1']:<10.4f} {result['test_auc']:<10.4f}\")\n",
    "\n",
    "# Find best aggregation method\n",
    "best_aggregation = max(aggregation_results, key=lambda x: x['val_acc'])\n",
    "\n",
    "print(f\"\\n>>> Best Aggregation Method: {best_aggregation['method'].upper()}\")\n",
    "print(f\"    Validation Accuracy: {best_aggregation['val_acc']*100:.2f}%\")\n",
    "print(f\"    Test Accuracy: {best_aggregation['test_acc']*100:.2f}%\")\n",
    "print(f\"    Test F1 Score: {best_aggregation['test_f1']:.4f}\")\n",
    "print(f\"    Test AUC-ROC: {best_aggregation['test_auc']:.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"AGGREGATION METHOD COMPARISON COMPLETE\")\n",
    "print(f\"{'='*80}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e77f7d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "REGULARIZATION STEP 0: BASELINE (NO REGULARIZATION)\n",
      "================================================================================\n",
      "Using best hyperparameters from tuning:\n",
      "    LR=0.0001, Batch=32, Hidden=512, Optimizer=Adam\n",
      "    Max Epochs=500, Patience=7\n",
      "    Best Aggregation Method: ATTENTION\n",
      "\n",
      "Baseline Configuration:\n",
      "    Dropout: 0.0\n",
      "    Gradient Clipping: 0.0\n",
      "    L1 Lambda: 0.0\n",
      "    L2 Lambda: 0.0\n",
      "\n",
      ">>> Training baseline model...\n",
      "    Epoch 10: Train Acc=96.91%, Val Acc=82.57%\n",
      "    Epoch 20: Train Acc=99.61%, Val Acc=84.40%\n",
      "    Early stopping at epoch 26, best val acc: 84.68%\n",
      "\n",
      ">>> Baseline Results:\n",
      "    Validation Acc: 84.68%\n",
      "    Test Acc: 87.40%\n",
      "    Test F1: 0.8719\n",
      "    Test AUC-ROC: 0.9638\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Regularization: Baseline (No Regularization)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"REGULARIZATION STEP 0: BASELINE (NO REGULARIZATION)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Using best hyperparameters from tuning:\")\n",
    "print(f\"    LR={final_lr}, Batch={best_batch_size}, Hidden={best_hidden_dim}, Optimizer={best_optimizer}\")\n",
    "print(f\"    Max Epochs={MAX_EPOCHS}, Patience={PATIENCE}\")\n",
    "print(f\"    Best Aggregation Method: {best_aggregation['method'].upper()}\")\n",
    "\n",
    "# Baseline configuration\n",
    "baseline_config = {\n",
    "    'dropout': 0.0,\n",
    "    'grad_clip': 0.0,\n",
    "    'l1_lambda': 0.0,\n",
    "    'l2_lambda': 0.0\n",
    "}\n",
    "\n",
    "print(f\"\\nBaseline Configuration:\")\n",
    "print(f\"    Dropout: {baseline_config['dropout']}\")\n",
    "print(f\"    Gradient Clipping: {baseline_config['grad_clip']}\")\n",
    "print(f\"    L1 Lambda: {baseline_config['l1_lambda']}\")\n",
    "print(f\"    L2 Lambda: {baseline_config['l2_lambda']}\")\n",
    "\n",
    "# Create iterators\n",
    "train_iter = data.BucketIterator(\n",
    "    train_data,\n",
    "    batch_size=best_batch_size,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    shuffle=True,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "val_iter = data.BucketIterator(\n",
    "    validation_data,\n",
    "    batch_size=best_batch_size,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    shuffle=False,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "test_iter = data.BucketIterator(\n",
    "    test_data,\n",
    "    batch_size=best_batch_size,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    shuffle=False,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Create model\n",
    "model = RNN_Classifier_Aggregation(\n",
    "    vocab_size=fasttext_vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_dim=best_hidden_dim,\n",
    "    output_dim=num_classes,\n",
    "    n_layers=N_LAYERS,\n",
    "    dropout=baseline_config['dropout'],\n",
    "    padding_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "    pretrained_embeddings=pretrained_embeddings,\n",
    "    aggregation=best_aggregation['method']\n",
    ").to(device)\n",
    "\n",
    "# Select optimizer\n",
    "if best_optimizer == 'Adam':\n",
    "    optimizer = optim.Adam(model.parameters(), lr=final_lr, weight_decay=baseline_config['l2_lambda'])\n",
    "elif best_optimizer == 'SGD':\n",
    "    optimizer = optim.SGD(model.parameters(), lr=final_lr, momentum=0.9, weight_decay=baseline_config['l2_lambda'])\n",
    "elif best_optimizer == 'RMSprop':\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=final_lr, weight_decay=baseline_config['l2_lambda'])\n",
    "elif best_optimizer == 'Adagrad':\n",
    "    optimizer = optim.Adagrad(model.parameters(), lr=final_lr, weight_decay=baseline_config['l2_lambda'])\n",
    "\n",
    "# Helper function for L1 regularization\n",
    "def compute_l1_loss(model, l1_lambda):\n",
    "    \"\"\"Compute L1 regularization loss\"\"\"\n",
    "    if l1_lambda > 0:\n",
    "        return l1_lambda * sum(p.abs().sum() for p in model.parameters() if p.requires_grad)\n",
    "    return 0.0\n",
    "\n",
    "# Training loop\n",
    "best_val_acc = 0.0\n",
    "patience_counter = 0\n",
    "\n",
    "print(f\"\\n>>> Training baseline model...\")\n",
    "\n",
    "for epoch in range(MAX_EPOCHS):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_preds = []\n",
    "    train_labels = []\n",
    "    \n",
    "    for batch in train_iter:\n",
    "        text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(text, text_lengths)\n",
    "        \n",
    "        loss = criterion(predictions, labels)\n",
    "        \n",
    "        # Add L1 regularization\n",
    "        if baseline_config['l1_lambda'] > 0:\n",
    "            loss = loss + compute_l1_loss(model, baseline_config['l1_lambda'])\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        if baseline_config['grad_clip'] > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), baseline_config['grad_clip'])\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        preds = torch.argmax(predictions, dim=1)\n",
    "        train_preds.extend(preds.cpu().numpy())\n",
    "        train_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    train_acc = accuracy_score(train_labels, train_preds)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_iter:\n",
    "            text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "            predictions = model(text, text_lengths)\n",
    "            loss = criterion(predictions, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    val_acc = accuracy_score(val_labels, val_preds)\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), 'rnn_reg_baseline_best.pt')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(f\"    Early stopping at epoch {epoch+1}, best val acc: {best_val_acc*100:.2f}%\")\n",
    "            break\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"    Epoch {epoch+1}: Train Acc={train_acc*100:.2f}%, Val Acc={val_acc*100:.2f}%\")\n",
    "\n",
    "# Evaluate on test set\n",
    "model.load_state_dict(torch.load('rnn_reg_baseline_best.pt'))\n",
    "model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "test_preds = []\n",
    "test_labels = []\n",
    "test_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_iter:\n",
    "        text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "        predictions = model(text, text_lengths)\n",
    "        loss = criterion(predictions, labels)\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        probs = torch.softmax(predictions, dim=1)\n",
    "        preds = torch.argmax(predictions, dim=1)\n",
    "        test_preds.extend(preds.cpu().numpy())\n",
    "        test_labels.extend(labels.cpu().numpy())\n",
    "        test_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "test_acc = accuracy_score(test_labels, test_preds)\n",
    "test_f1 = f1_score(test_labels, test_preds, average='weighted')\n",
    "test_loss_avg = test_loss / len(test_iter)\n",
    "\n",
    "try:\n",
    "    test_probs_array = np.array(test_probs)\n",
    "    test_labels_bin = label_binarize(test_labels, classes=range(num_classes))\n",
    "    test_auc = roc_auc_score(test_labels_bin, test_probs_array, average='weighted', multi_class='ovr')\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not calculate AUC-ROC: {e}\")\n",
    "    test_auc = 0.0\n",
    "\n",
    "baseline_results = {\n",
    "    'name': 'baseline',\n",
    "    'dropout': baseline_config['dropout'],\n",
    "    'grad_clip': baseline_config['grad_clip'],\n",
    "    'l1_lambda': baseline_config['l1_lambda'],\n",
    "    'l2_lambda': baseline_config['l2_lambda'],\n",
    "    'val_acc': best_val_acc,\n",
    "    'test_acc': test_acc,\n",
    "    'test_f1': test_f1,\n",
    "    'test_auc': test_auc\n",
    "}\n",
    "\n",
    "print(f\"\\n>>> Baseline Results:\")\n",
    "print(f\"    Validation Acc: {best_val_acc*100:.2f}%\")\n",
    "print(f\"    Test Acc: {test_acc*100:.2f}%\")\n",
    "print(f\"    Test F1: {test_f1:.4f}\")\n",
    "print(f\"    Test AUC-ROC: {test_auc:.4f}\")\n",
    "\n",
    "# Store for next steps\n",
    "best_grad_clip = baseline_config['grad_clip']  # Will be updated in Step 1\n",
    "best_dropout = baseline_config['dropout']  # Will be updated in Step 2\n",
    "best_l1_lambda = baseline_config['l1_lambda']  # Will be updated in Step 3\n",
    "best_l2_lambda = baseline_config['l2_lambda']  # Will be updated in Step 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1903f6fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "REGULARIZATION STEP 1: GRADIENT CLIPPING TUNING\n",
      "================================================================================\n",
      "Using baseline settings: dropout=0.0, L1=0.0, L2=0.0\n",
      "\n",
      "Testing gradient clipping values: [0.0, 1.0]\n",
      "\n",
      "================================================================================\n",
      "Testing: Gradient Clipping = 0.0\n",
      "================================================================================\n",
      "    Epoch 10: Train Acc=96.63%, Val Acc=81.74%\n",
      "    Epoch 20: Train Acc=97.18%, Val Acc=83.67%\n",
      "    Early stopping at epoch 27, best val acc: 83.67%\n",
      "    Result: Val Acc=83.67%\n",
      "\n",
      "================================================================================\n",
      "Testing: Gradient Clipping = 1.0\n",
      "================================================================================\n",
      "    Epoch 10: Train Acc=93.31%, Val Acc=81.47%\n",
      "    Epoch 20: Train Acc=98.88%, Val Acc=82.48%\n",
      "    Epoch 30: Train Acc=99.50%, Val Acc=83.58%\n",
      "    Early stopping at epoch 35, best val acc: 84.40%\n",
      "    Result: Val Acc=84.40%\n",
      "\n",
      ">>> Step 1 Results:\n",
      "Grad Clip    Val Acc   \n",
      "-------------------------\n",
      "0.0          83.67     %\n",
      "1.0          84.40     %\n",
      "\n",
      ">>> Best Gradient Clipping: 1.0, Val Acc=84.40%\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Regularization Step 1: Gradient Clipping Tuning\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"REGULARIZATION STEP 1: GRADIENT CLIPPING TUNING\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Using baseline settings: dropout={best_dropout}, L1={best_l1_lambda}, L2={best_l2_lambda}\")\n",
    "\n",
    "# Test different gradient clipping values\n",
    "grad_clip_options = [0.0, 1.0]  # 0.0 = no clipping, 1.0 = clip at 1.0\n",
    "\n",
    "print(f\"\\nTesting gradient clipping values: {grad_clip_options}\")\n",
    "\n",
    "step1_results = []\n",
    "\n",
    "for grad_clip in grad_clip_options:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Testing: Gradient Clipping = {grad_clip}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Create model with baseline settings\n",
    "    model = RNN_Classifier_Aggregation(\n",
    "        vocab_size=fasttext_vocab_size,\n",
    "        embedding_dim=embedding_dim,\n",
    "        hidden_dim=best_hidden_dim,\n",
    "        output_dim=num_classes,\n",
    "        n_layers=N_LAYERS,\n",
    "        dropout=best_dropout,\n",
    "        padding_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "        pretrained_embeddings=pretrained_embeddings,\n",
    "        aggregation=best_aggregation['method']\n",
    "    ).to(device)\n",
    "    \n",
    "    # Select optimizer\n",
    "    if best_optimizer == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=final_lr, weight_decay=best_l2_lambda)\n",
    "    elif best_optimizer == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=final_lr, momentum=0.9, weight_decay=best_l2_lambda)\n",
    "    elif best_optimizer == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=final_lr, weight_decay=best_l2_lambda)\n",
    "    elif best_optimizer == 'Adagrad':\n",
    "        optimizer = optim.Adagrad(model.parameters(), lr=final_lr, weight_decay=best_l2_lambda)\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        \n",
    "        for batch in train_iter:\n",
    "            text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(text, text_lengths)\n",
    "            \n",
    "            loss = criterion(predictions, labels)\n",
    "            \n",
    "            if best_l1_lambda > 0:\n",
    "                loss = loss + compute_l1_loss(model, best_l1_lambda)\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            if grad_clip > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        train_acc = accuracy_score(train_labels, train_preds)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_iter:\n",
    "                text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "                predictions = model(text, text_lengths)\n",
    "                loss = criterion(predictions, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                preds = torch.argmax(predictions, dim=1)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_acc = accuracy_score(val_labels, val_preds)\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), f'rnn_reg_step1_gradclip{grad_clip}_best.pt')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(f\"    Early stopping at epoch {epoch+1}, best val acc: {best_val_acc*100:.2f}%\")\n",
    "                break\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"    Epoch {epoch+1}: Train Acc={train_acc*100:.2f}%, Val Acc={val_acc*100:.2f}%\")\n",
    "    \n",
    "    step1_results.append({\n",
    "        'grad_clip': grad_clip,\n",
    "        'val_acc': best_val_acc\n",
    "    })\n",
    "    \n",
    "    print(f\"    Result: Val Acc={best_val_acc*100:.2f}%\")\n",
    "\n",
    "# Find best gradient clipping\n",
    "best_step1 = max(step1_results, key=lambda x: x['val_acc'])\n",
    "best_grad_clip = best_step1['grad_clip']\n",
    "\n",
    "print(f\"\\n>>> Step 1 Results:\")\n",
    "print(f\"{'Grad Clip':<12} {'Val Acc':<10}\")\n",
    "print(\"-\" * 25)\n",
    "for result in step1_results:\n",
    "    print(f\"{result['grad_clip']:<12} {result['val_acc']*100:<10.2f}%\")\n",
    "print(f\"\\n>>> Best Gradient Clipping: {best_grad_clip}, Val Acc={best_step1['val_acc']*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "20fccda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "REGULARIZATION STEP 2: DROPOUT TUNING\n",
      "================================================================================\n",
      "Using best from Step 1: grad_clip=1.0\n",
      "Using baseline settings: L1=0.0, L2=0.0\n",
      "\n",
      "Testing dropout values: [0.0, 0.3, 0.5, 0.7]\n",
      "\n",
      "================================================================================\n",
      "Testing: Dropout = 0.0\n",
      "================================================================================\n",
      "    Epoch 10: Train Acc=93.35%, Val Acc=82.39%\n",
      "    Epoch 20: Train Acc=98.81%, Val Acc=82.11%\n",
      "    Early stopping at epoch 21, best val acc: 84.68%\n",
      "    Result: Val Acc=84.68%\n",
      "\n",
      "================================================================================\n",
      "Testing: Dropout = 0.3\n",
      "================================================================================\n",
      "    Epoch 10: Train Acc=93.90%, Val Acc=80.28%\n",
      "    Early stopping at epoch 20, best val acc: 82.48%\n",
      "    Result: Val Acc=82.48%\n",
      "\n",
      "================================================================================\n",
      "Testing: Dropout = 0.5\n",
      "================================================================================\n",
      "    Epoch 10: Train Acc=94.29%, Val Acc=82.39%\n",
      "    Early stopping at epoch 18, best val acc: 83.58%\n",
      "    Result: Val Acc=83.58%\n",
      "\n",
      "================================================================================\n",
      "Testing: Dropout = 0.7\n",
      "================================================================================\n",
      "    Epoch 10: Train Acc=93.60%, Val Acc=81.56%\n",
      "    Epoch 20: Train Acc=98.67%, Val Acc=83.03%\n",
      "    Early stopping at epoch 24, best val acc: 83.39%\n",
      "    Result: Val Acc=83.39%\n",
      "\n",
      ">>> Step 2 Results:\n",
      "Dropout      Val Acc   \n",
      "-------------------------\n",
      "0.0          84.68     %\n",
      "0.3          82.48     %\n",
      "0.5          83.58     %\n",
      "0.7          83.39     %\n",
      "\n",
      ">>> Best Dropout: 0.0, Val Acc=84.68%\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Regularization Step 2: Dropout Tuning\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"REGULARIZATION STEP 2: DROPOUT TUNING\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Using best from Step 1: grad_clip={best_grad_clip}\")\n",
    "print(f\"Using baseline settings: L1={best_l1_lambda}, L2={best_l2_lambda}\")\n",
    "\n",
    "# Test different dropout values\n",
    "dropout_options = [0.0, 0.3, 0.5, 0.7]\n",
    "\n",
    "print(f\"\\nTesting dropout values: {dropout_options}\")\n",
    "\n",
    "step2_results = []\n",
    "\n",
    "for dropout_val in dropout_options:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Testing: Dropout = {dropout_val}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Create model with best grad_clip and current dropout\n",
    "    model = RNN_Classifier_Aggregation(\n",
    "        vocab_size=fasttext_vocab_size,\n",
    "        embedding_dim=embedding_dim,\n",
    "        hidden_dim=best_hidden_dim,\n",
    "        output_dim=num_classes,\n",
    "        n_layers=N_LAYERS,\n",
    "        dropout=dropout_val,\n",
    "        padding_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "        pretrained_embeddings=pretrained_embeddings,\n",
    "        aggregation=best_aggregation['method']\n",
    "    ).to(device)\n",
    "    \n",
    "    # Select optimizer\n",
    "    if best_optimizer == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=final_lr, weight_decay=best_l2_lambda)\n",
    "    elif best_optimizer == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=final_lr, momentum=0.9, weight_decay=best_l2_lambda)\n",
    "    elif best_optimizer == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=final_lr, weight_decay=best_l2_lambda)\n",
    "    elif best_optimizer == 'Adagrad':\n",
    "        optimizer = optim.Adagrad(model.parameters(), lr=final_lr, weight_decay=best_l2_lambda)\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        \n",
    "        for batch in train_iter:\n",
    "            text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(text, text_lengths)\n",
    "            \n",
    "            loss = criterion(predictions, labels)\n",
    "            \n",
    "            if best_l1_lambda > 0:\n",
    "                loss = loss + compute_l1_loss(model, best_l1_lambda)\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            if best_grad_clip > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), best_grad_clip)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        train_acc = accuracy_score(train_labels, train_preds)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_iter:\n",
    "                text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "                predictions = model(text, text_lengths)\n",
    "                loss = criterion(predictions, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                preds = torch.argmax(predictions, dim=1)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_acc = accuracy_score(val_labels, val_preds)\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), f'rnn_reg_step2_dropout{dropout_val}_best.pt')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(f\"    Early stopping at epoch {epoch+1}, best val acc: {best_val_acc*100:.2f}%\")\n",
    "                break\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"    Epoch {epoch+1}: Train Acc={train_acc*100:.2f}%, Val Acc={val_acc*100:.2f}%\")\n",
    "    \n",
    "    step2_results.append({\n",
    "        'dropout': dropout_val,\n",
    "        'val_acc': best_val_acc\n",
    "    })\n",
    "    \n",
    "    print(f\"    Result: Val Acc={best_val_acc*100:.2f}%\")\n",
    "\n",
    "# Find best dropout\n",
    "best_step2 = max(step2_results, key=lambda x: x['val_acc'])\n",
    "best_dropout = best_step2['dropout']\n",
    "\n",
    "print(f\"\\n>>> Step 2 Results:\")\n",
    "print(f\"{'Dropout':<12} {'Val Acc':<10}\")\n",
    "print(\"-\" * 25)\n",
    "for result in step2_results:\n",
    "    print(f\"{result['dropout']:<12} {result['val_acc']*100:<10.2f}%\")\n",
    "print(f\"\\n>>> Best Dropout: {best_dropout}, Val Acc={best_step2['val_acc']*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d0afecca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "REGULARIZATION STEP 3: L1 REGULARIZATION TUNING\n",
      "================================================================================\n",
      "Using best from Steps 1-2: grad_clip=1.0, dropout=0.0\n",
      "Using baseline setting: L2=0.0\n",
      "\n",
      "Testing L1 lambda values: [0.0, 1e-06, 1e-05, 0.0001]\n",
      "\n",
      "================================================================================\n",
      "Testing: L1 Lambda = 0.0\n",
      "================================================================================\n",
      "    Epoch 10: Train Acc=93.17%, Val Acc=80.92%\n",
      "    Epoch 20: Train Acc=98.53%, Val Acc=83.30%\n",
      "    Early stopping at epoch 24, best val acc: 84.31%\n",
      "    Result: Val Acc=84.31%\n",
      "\n",
      "================================================================================\n",
      "Testing: L1 Lambda = 1e-06\n",
      "================================================================================\n",
      "    Epoch 10: Train Acc=93.31%, Val Acc=79.63%\n",
      "    Epoch 20: Train Acc=98.40%, Val Acc=82.02%\n",
      "    Early stopping at epoch 21, best val acc: 82.20%\n",
      "    Result: Val Acc=82.20%\n",
      "\n",
      "================================================================================\n",
      "Testing: L1 Lambda = 1e-05\n",
      "================================================================================\n",
      "    Epoch 10: Train Acc=92.76%, Val Acc=83.67%\n",
      "    Epoch 20: Train Acc=97.23%, Val Acc=80.73%\n",
      "    Early stopping at epoch 28, best val acc: 84.31%\n",
      "    Result: Val Acc=84.31%\n",
      "\n",
      "================================================================================\n",
      "Testing: L1 Lambda = 0.0001\n",
      "================================================================================\n",
      "    Epoch 10: Train Acc=73.02%, Val Acc=70.55%\n",
      "    Epoch 20: Train Acc=85.33%, Val Acc=75.14%\n",
      "    Epoch 30: Train Acc=88.51%, Val Acc=79.54%\n",
      "    Early stopping at epoch 31, best val acc: 80.37%\n",
      "    Result: Val Acc=80.37%\n",
      "\n",
      ">>> Step 3 Results:\n",
      "L1 Lambda    Val Acc   \n",
      "-------------------------\n",
      "0e+00        84.31     %\n",
      "1e-06        82.20     %\n",
      "1e-05        84.31     %\n",
      "1e-04        80.37     %\n",
      "\n",
      ">>> Best L1 Lambda: 0.0, Val Acc=84.31%\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Regularization Step 3: L1 Regularization Tuning\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"REGULARIZATION STEP 3: L1 REGULARIZATION TUNING\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Using best from Steps 1-2: grad_clip={best_grad_clip}, dropout={best_dropout}\")\n",
    "print(f\"Using baseline setting: L2={best_l2_lambda}\")\n",
    "\n",
    "# Test different L1 lambda values\n",
    "l1_lambda_options = [0.0, 1e-6, 1e-5, 1e-4]\n",
    "\n",
    "print(f\"\\nTesting L1 lambda values: {l1_lambda_options}\")\n",
    "\n",
    "step3_results = []\n",
    "\n",
    "for l1_lambda in l1_lambda_options:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Testing: L1 Lambda = {l1_lambda}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Create model with best grad_clip, dropout, and current L1\n",
    "    model = RNN_Classifier_Aggregation(\n",
    "        vocab_size=fasttext_vocab_size,\n",
    "        embedding_dim=embedding_dim,\n",
    "        hidden_dim=best_hidden_dim,\n",
    "        output_dim=num_classes,\n",
    "        n_layers=N_LAYERS,\n",
    "        dropout=best_dropout,\n",
    "        padding_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "        pretrained_embeddings=pretrained_embeddings,\n",
    "        aggregation=best_aggregation['method']\n",
    "    ).to(device)\n",
    "    \n",
    "    # Select optimizer\n",
    "    if best_optimizer == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=final_lr, weight_decay=best_l2_lambda)\n",
    "    elif best_optimizer == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=final_lr, momentum=0.9, weight_decay=best_l2_lambda)\n",
    "    elif best_optimizer == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=final_lr, weight_decay=best_l2_lambda)\n",
    "    elif best_optimizer == 'Adagrad':\n",
    "        optimizer = optim.Adagrad(model.parameters(), lr=final_lr, weight_decay=best_l2_lambda)\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        \n",
    "        for batch in train_iter:\n",
    "            text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(text, text_lengths)\n",
    "            \n",
    "            loss = criterion(predictions, labels)\n",
    "            \n",
    "            if l1_lambda > 0:\n",
    "                loss = loss + compute_l1_loss(model, l1_lambda)\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            if best_grad_clip > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), best_grad_clip)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        train_acc = accuracy_score(train_labels, train_preds)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_iter:\n",
    "                text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "                predictions = model(text, text_lengths)\n",
    "                loss = criterion(predictions, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                preds = torch.argmax(predictions, dim=1)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_acc = accuracy_score(val_labels, val_preds)\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), f'rnn_reg_step3_l1{l1_lambda}_best.pt')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(f\"    Early stopping at epoch {epoch+1}, best val acc: {best_val_acc*100:.2f}%\")\n",
    "                break\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"    Epoch {epoch+1}: Train Acc={train_acc*100:.2f}%, Val Acc={val_acc*100:.2f}%\")\n",
    "    \n",
    "    step3_results.append({\n",
    "        'l1_lambda': l1_lambda,\n",
    "        'val_acc': best_val_acc\n",
    "    })\n",
    "    \n",
    "    print(f\"    Result: Val Acc={best_val_acc*100:.2f}%\")\n",
    "\n",
    "# Find best L1 lambda\n",
    "best_step3 = max(step3_results, key=lambda x: x['val_acc'])\n",
    "best_l1_lambda = best_step3['l1_lambda']\n",
    "\n",
    "print(f\"\\n>>> Step 3 Results:\")\n",
    "print(f\"{'L1 Lambda':<12} {'Val Acc':<10}\")\n",
    "print(\"-\" * 25)\n",
    "for result in step3_results:\n",
    "    print(f\"{result['l1_lambda']:<12.0e} {result['val_acc']*100:<10.2f}%\")\n",
    "print(f\"\\n>>> Best L1 Lambda: {best_l1_lambda}, Val Acc={best_step3['val_acc']*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "71abaf6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "REGULARIZATION STEP 4: L2 REGULARIZATION TUNING\n",
      "================================================================================\n",
      "Using best from Steps 1-3: grad_clip=1.0, dropout=0.0, L1=0.0\n",
      "\n",
      "Testing L2 lambda values: [0.0, 1e-05, 0.0001, 0.001]\n",
      "\n",
      "================================================================================\n",
      "Testing: L2 Lambda = 0.0\n",
      "================================================================================\n",
      "    Epoch 10: Train Acc=94.34%, Val Acc=82.75%\n",
      "    Epoch 20: Train Acc=99.06%, Val Acc=83.21%\n",
      "    Early stopping at epoch 23, best val acc: 84.04%\n",
      "    Result: Val Acc=84.04%\n",
      "\n",
      "================================================================================\n",
      "Testing: L2 Lambda = 1e-05\n",
      "================================================================================\n",
      "    Epoch 10: Train Acc=93.47%, Val Acc=80.28%\n",
      "    Epoch 20: Train Acc=98.85%, Val Acc=83.03%\n",
      "    Epoch 30: Train Acc=99.77%, Val Acc=83.12%\n",
      "    Epoch 40: Train Acc=99.79%, Val Acc=83.58%\n",
      "    Early stopping at epoch 46, best val acc: 83.85%\n",
      "    Result: Val Acc=83.85%\n",
      "\n",
      "================================================================================\n",
      "Testing: L2 Lambda = 0.0001\n",
      "================================================================================\n",
      "    Epoch 10: Train Acc=92.92%, Val Acc=79.17%\n",
      "    Epoch 20: Train Acc=98.23%, Val Acc=82.29%\n",
      "    Early stopping at epoch 25, best val acc: 83.67%\n",
      "    Result: Val Acc=83.67%\n",
      "\n",
      "================================================================================\n",
      "Testing: L2 Lambda = 0.001\n",
      "================================================================================\n",
      "    Epoch 10: Train Acc=85.49%, Val Acc=76.06%\n",
      "    Epoch 20: Train Acc=91.38%, Val Acc=80.55%\n",
      "    Early stopping at epoch 27, best val acc: 80.55%\n",
      "    Result: Val Acc=80.55%\n",
      "\n",
      ">>> Step 4 Results:\n",
      "L2 Lambda    Val Acc   \n",
      "-------------------------\n",
      "0e+00        84.04     %\n",
      "1e-05        83.85     %\n",
      "1e-04        83.67     %\n",
      "1e-03        80.55     %\n",
      "\n",
      ">>> Best L2 Lambda: 0.0, Val Acc=84.04%\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Regularization Step 4: L2 Regularization Tuning\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"REGULARIZATION STEP 4: L2 REGULARIZATION TUNING\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Using best from Steps 1-3: grad_clip={best_grad_clip}, dropout={best_dropout}, L1={best_l1_lambda}\")\n",
    "\n",
    "# Test different L2 lambda values (via weight_decay)\n",
    "l2_lambda_options = [0.0, 1e-5, 1e-4, 1e-3]\n",
    "\n",
    "print(f\"\\nTesting L2 lambda values: {l2_lambda_options}\")\n",
    "\n",
    "step4_results = []\n",
    "\n",
    "for l2_lambda in l2_lambda_options:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Testing: L2 Lambda = {l2_lambda}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Create model with best grad_clip, dropout, L1, and current L2\n",
    "    model = RNN_Classifier_Aggregation(\n",
    "        vocab_size=fasttext_vocab_size,\n",
    "        embedding_dim=embedding_dim,\n",
    "        hidden_dim=best_hidden_dim,\n",
    "        output_dim=num_classes,\n",
    "        n_layers=N_LAYERS,\n",
    "        dropout=best_dropout,\n",
    "        padding_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "        pretrained_embeddings=pretrained_embeddings,\n",
    "        aggregation=best_aggregation['method']\n",
    "    ).to(device)\n",
    "    \n",
    "    # Select optimizer with L2 regularization (weight_decay)\n",
    "    if best_optimizer == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=final_lr, weight_decay=l2_lambda)\n",
    "    elif best_optimizer == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=final_lr, momentum=0.9, weight_decay=l2_lambda)\n",
    "    elif best_optimizer == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=final_lr, weight_decay=l2_lambda)\n",
    "    elif best_optimizer == 'Adagrad':\n",
    "        optimizer = optim.Adagrad(model.parameters(), lr=final_lr, weight_decay=l2_lambda)\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        \n",
    "        for batch in train_iter:\n",
    "            text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(text, text_lengths)\n",
    "            \n",
    "            loss = criterion(predictions, labels)\n",
    "            \n",
    "            if best_l1_lambda > 0:\n",
    "                loss = loss + compute_l1_loss(model, best_l1_lambda)\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            if best_grad_clip > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), best_grad_clip)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        train_acc = accuracy_score(train_labels, train_preds)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_iter:\n",
    "                text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "                predictions = model(text, text_lengths)\n",
    "                loss = criterion(predictions, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                preds = torch.argmax(predictions, dim=1)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_acc = accuracy_score(val_labels, val_preds)\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), f'rnn_reg_step4_l2{l2_lambda}_best.pt')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(f\"    Early stopping at epoch {epoch+1}, best val acc: {best_val_acc*100:.2f}%\")\n",
    "                break\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"    Epoch {epoch+1}: Train Acc={train_acc*100:.2f}%, Val Acc={val_acc*100:.2f}%\")\n",
    "    \n",
    "    step4_results.append({\n",
    "        'l2_lambda': l2_lambda,\n",
    "        'val_acc': best_val_acc\n",
    "    })\n",
    "    \n",
    "    print(f\"    Result: Val Acc={best_val_acc*100:.2f}%\")\n",
    "\n",
    "# Find best L2 lambda\n",
    "best_step4 = max(step4_results, key=lambda x: x['val_acc'])\n",
    "best_l2_lambda = best_step4['l2_lambda']\n",
    "\n",
    "print(f\"\\n>>> Step 4 Results:\")\n",
    "print(f\"{'L2 Lambda':<12} {'Val Acc':<10}\")\n",
    "print(\"-\" * 25)\n",
    "for result in step4_results:\n",
    "    print(f\"{result['l2_lambda']:<12.0e} {result['val_acc']*100:<10.2f}%\")\n",
    "print(f\"\\n>>> Best L2 Lambda: {best_l2_lambda}, Val Acc={best_step4['val_acc']*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "48f3c33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "REGULARIZATION FINAL: ALL BEST SETTINGS COMBINED\n",
      "================================================================================\n",
      "Best settings from all steps:\n",
      "    Gradient Clipping: 1.0\n",
      "    Dropout: 0.0\n",
      "    L1 Lambda: 0.0\n",
      "    L2 Lambda: 0.0\n",
      "\n",
      ">>> Training final model with all best regularization settings...\n",
      "    Epoch 10: Train Acc=94.18%, Val Acc=81.01%\n",
      "    Epoch 20: Train Acc=98.83%, Val Acc=84.31%\n",
      "    Early stopping at epoch 27, best val acc: 84.31%\n",
      "\n",
      ">>> Final Combined Results:\n",
      "    Configuration:\n",
      "      - Gradient Clipping: 1.0\n",
      "      - Dropout: 0.0\n",
      "      - L1 Lambda: 0.0\n",
      "      - L2 Lambda: 0.0\n",
      "    Validation Acc: 84.31%\n",
      "    Test Acc: 87.60%\n",
      "    Test F1: 0.8744\n",
      "    Test AUC-ROC: 0.9733\n",
      "\n",
      ">>> Comparison with Baseline:\n",
      "    Baseline Test Acc: 87.40%\n",
      "    Final Regularized Test Acc: 87.60%\n",
      "    Improvement: +0.20% (+0.23% relative)\n",
      "\n",
      ">>> Plotting training curves for best configuration and regularization...\n",
      "    Saved training curves to 'best_config_training_curves.png'\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "REGULARIZATION TUNING COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Regularization Final: All Best Combined\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"REGULARIZATION FINAL: ALL BEST SETTINGS COMBINED\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Best settings from all steps:\")\n",
    "print(f\"    Gradient Clipping: {best_grad_clip}\")\n",
    "print(f\"    Dropout: {best_dropout}\")\n",
    "print(f\"    L1 Lambda: {best_l1_lambda}\")\n",
    "print(f\"    L2 Lambda: {best_l2_lambda}\")\n",
    "\n",
    "# Create model with all best settings\n",
    "model = RNN_Classifier_Aggregation(\n",
    "    vocab_size=fasttext_vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_dim=best_hidden_dim,\n",
    "    output_dim=num_classes,\n",
    "    n_layers=N_LAYERS,\n",
    "    dropout=best_dropout,\n",
    "    padding_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "    pretrained_embeddings=pretrained_embeddings,\n",
    "    aggregation=best_aggregation['method']\n",
    ").to(device)\n",
    "\n",
    "# Select optimizer with best L2 (weight_decay)\n",
    "if best_optimizer == 'Adam':\n",
    "    optimizer = optim.Adam(model.parameters(), lr=final_lr, weight_decay=best_l2_lambda)\n",
    "elif best_optimizer == 'SGD':\n",
    "    optimizer = optim.SGD(model.parameters(), lr=final_lr, momentum=0.9, weight_decay=best_l2_lambda)\n",
    "elif best_optimizer == 'RMSprop':\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=final_lr, weight_decay=best_l2_lambda)\n",
    "elif best_optimizer == 'Adagrad':\n",
    "    optimizer = optim.Adagrad(model.parameters(), lr=final_lr, weight_decay=best_l2_lambda)\n",
    "\n",
    "# Training loop\n",
    "best_val_acc = 0.0\n",
    "patience_counter = 0\n",
    "\n",
    "# Store training history for plotting\n",
    "train_losses = []\n",
    "val_accs = []\n",
    "\n",
    "print(f\"\\n>>> Training final model with all best regularization settings...\")\n",
    "\n",
    "for epoch in range(MAX_EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_preds = []\n",
    "    train_labels = []\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch in train_iter:\n",
    "        num_batches += 1\n",
    "        text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(text, text_lengths)\n",
    "        \n",
    "        loss = criterion(predictions, labels)\n",
    "        \n",
    "        if best_l1_lambda > 0:\n",
    "            loss = loss + compute_l1_loss(model, best_l1_lambda)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        if best_grad_clip > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), best_grad_clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        preds = torch.argmax(predictions, dim=1)\n",
    "        train_preds.extend(preds.cpu().numpy())\n",
    "        train_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    train_acc = accuracy_score(train_labels, train_preds)\n",
    "    # Calculate average training loss (train_loss is sum over all batches)\n",
    "    num_train_batches = len(train_labels) // train_iter.batch_size + (1 if len(train_labels) % train_iter.batch_size != 0 else 0)\n",
    "    train_loss_avg = train_loss / num_train_batches if num_train_batches > 0 else train_loss\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_iter:\n",
    "            text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "            predictions = model(text, text_lengths)\n",
    "            loss = criterion(predictions, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    val_acc = accuracy_score(val_labels, val_preds)\n",
    "    \n",
    "    # Store training history for plotting\n",
    "    train_losses.append(train_loss_avg)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), 'rnn_reg_final_best.pt')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(f\"    Early stopping at epoch {epoch+1}, best val acc: {best_val_acc*100:.2f}%\")\n",
    "            break\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"    Epoch {epoch+1}: Train Acc={train_acc*100:.2f}%, Val Acc={val_acc*100:.2f}%\")\n",
    "\n",
    "# Evaluate on test set\n",
    "model.load_state_dict(torch.load('rnn_reg_final_best.pt'))\n",
    "model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "test_preds = []\n",
    "test_labels = []\n",
    "test_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_iter:\n",
    "        text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "        predictions = model(text, text_lengths)\n",
    "        loss = criterion(predictions, labels)\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        probs = torch.softmax(predictions, dim=1)\n",
    "        preds = torch.argmax(predictions, dim=1)\n",
    "        test_preds.extend(preds.cpu().numpy())\n",
    "        test_labels.extend(labels.cpu().numpy())\n",
    "        test_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "test_acc = accuracy_score(test_labels, test_preds)\n",
    "test_f1 = f1_score(test_labels, test_preds, average='weighted')\n",
    "test_loss_avg = test_loss / len(test_iter)\n",
    "\n",
    "try:\n",
    "    test_probs_array = np.array(test_probs)\n",
    "    test_labels_bin = label_binarize(test_labels, classes=range(num_classes))\n",
    "    test_auc = roc_auc_score(test_labels_bin, test_probs_array, average='weighted', multi_class='ovr')\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not calculate AUC-ROC: {e}\")\n",
    "    test_auc = 0.0\n",
    "\n",
    "final_results = {\n",
    "    'name': 'final_combined',\n",
    "    'dropout': best_dropout,\n",
    "    'grad_clip': best_grad_clip,\n",
    "    'l1_lambda': best_l1_lambda,\n",
    "    'l2_lambda': best_l2_lambda,\n",
    "    'val_acc': best_val_acc,\n",
    "    'test_acc': test_acc,\n",
    "    'test_f1': test_f1,\n",
    "    'test_auc': test_auc\n",
    "}\n",
    "\n",
    "print(f\"\\n>>> Final Combined Results:\")\n",
    "print(f\"    Configuration:\")\n",
    "print(f\"      - Gradient Clipping: {best_grad_clip}\")\n",
    "print(f\"      - Dropout: {best_dropout}\")\n",
    "print(f\"      - L1 Lambda: {best_l1_lambda}\")\n",
    "print(f\"      - L2 Lambda: {best_l2_lambda}\")\n",
    "print(f\"    Validation Acc: {best_val_acc*100:.2f}%\")\n",
    "print(f\"    Test Acc: {test_acc*100:.2f}%\")\n",
    "print(f\"    Test F1: {test_f1:.4f}\")\n",
    "print(f\"    Test AUC-ROC: {test_auc:.4f}\")\n",
    "\n",
    "# Compare with baseline\n",
    "improvement = test_acc - baseline_results['test_acc']\n",
    "improvement_pct = (improvement / baseline_results['test_acc']) * 100 if baseline_results['test_acc'] > 0 else 0\n",
    "\n",
    "print(f\"\\n>>> Comparison with Baseline:\")\n",
    "print(f\"    Baseline Test Acc: {baseline_results['test_acc']*100:.2f}%\")\n",
    "print(f\"    Final Regularized Test Acc: {test_acc*100:.2f}%\")\n",
    "print(f\"    Improvement: {improvement*100:+.2f}% ({improvement_pct:+.2f}% relative)\")\n",
    "\n",
    "# Plot training curves for best configuration and regularization\n",
    "print(f\"\\n>>> Plotting training curves for best configuration and regularization...\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Training Loss vs Epochs\n",
    "epochs = range(1, len(train_losses) + 1)\n",
    "ax1.plot(epochs, train_losses, 'b-', label='Training Loss', linewidth=2, marker='o', markersize=4)\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Training Loss', fontsize=12)\n",
    "ax1.set_title('Training Loss Curve', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xlim(left=1)\n",
    "\n",
    "# Plot 2: Validation Accuracy vs Epochs\n",
    "ax2.plot(epochs, [acc*100 for acc in val_accs], 'r-', label='Validation Accuracy', linewidth=2, marker='s', markersize=4)\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Validation Accuracy (%)', fontsize=12)\n",
    "ax2.set_title('Validation Accuracy Curve', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xlim(left=1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('best_config_training_curves.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"    Saved training curves to 'best_config_training_curves.png'\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"REGULARIZATION TUNING COMPLETE\")\n",
    "print(f\"{'='*80}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbbd83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7634f4ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214decac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed = 42\n",
    "# np.random.seed(seed)\n",
    "# torch.manual_seed(seed)\n",
    "# if torch.cuda.is_available():\n",
    "#     torch.cuda.manual_seed(seed)\n",
    "\n",
    "# # Build vocabulary for labels\n",
    "# LABEL.build_vocab(train_data)\n",
    "# num_classes = len(LABEL.vocab)\n",
    "# print(f\"\\nNumber of classes: {num_classes}\")\n",
    "# print(f\"Classes: {LABEL.vocab.itos}\")\n",
    "\n",
    "# # Create iterators for batching\n",
    "# def create_iterators(train_data, validation_data, test_data, batch_size):\n",
    "#     train_iterator = data.BucketIterator(\n",
    "#         train_data,\n",
    "#         batch_size=batch_size,\n",
    "#         sort_key=lambda x: len(x.text),\n",
    "#         sort_within_batch=True,\n",
    "#         device=device\n",
    "#     )\n",
    "    \n",
    "#     val_iterator = data.BucketIterator(\n",
    "#         validation_data,\n",
    "#         batch_size=batch_size,\n",
    "#         sort_key=lambda x: len(x.text),\n",
    "#         sort_within_batch=True,\n",
    "#         device=device\n",
    "#     )\n",
    "    \n",
    "#     test_iterator = data.BucketIterator(\n",
    "#         test_data,\n",
    "#         batch_size=batch_size,\n",
    "#         sort_key=lambda x: len(x.text),\n",
    "#         sort_within_batch=True,\n",
    "#         device=device\n",
    "#     )\n",
    "    \n",
    "#     return train_iterator, val_iterator, test_iterator\n",
    "\n",
    "\n",
    "# class RNN_Classifier(nn.Module):\n",
    "#     \"\"\"\n",
    "#     Simple RNN for topic classification with multiple aggregation strategies\n",
    "#     \"\"\"\n",
    "#     def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, \n",
    "#                  n_layers=1, bidirectional=False, dropout=0.5, \n",
    "#                  padding_idx=0, pretrained_embeddings=None,\n",
    "#                  aggregation='last'):\n",
    "#         super(RNN_Classifier, self).__init__()\n",
    "        \n",
    "#         self.hidden_dim = hidden_dim\n",
    "#         self.n_layers = n_layers\n",
    "#         self.bidirectional = bidirectional\n",
    "#         self.aggregation = aggregation  # 'last', 'mean', 'max', 'attention'\n",
    "        \n",
    "#         # Embedding layer\n",
    "#         self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        \n",
    "#         # Initialize with pretrained embeddings\n",
    "#         if pretrained_embeddings is not None:\n",
    "#             self.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "        \n",
    "#         # Make embeddings learnable (updated during training)\n",
    "#         self.embedding.weight.requires_grad = True\n",
    "        \n",
    "#         # RNN layer\n",
    "#         self.rnn = nn.RNN(\n",
    "#             embedding_dim,\n",
    "#             hidden_dim,\n",
    "#             num_layers=n_layers,\n",
    "#             bidirectional=bidirectional,\n",
    "#             batch_first=True,\n",
    "#             dropout=dropout if n_layers > 1 else 0\n",
    "#         )\n",
    "        \n",
    "#         # Dropout layer\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "#         # Attention mechanism for aggregation\n",
    "#         if aggregation == 'attention':\n",
    "#             rnn_output_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
    "#             self.attention = nn.Linear(rnn_output_dim, 1)\n",
    "        \n",
    "#         # Fully connected output layer\n",
    "#         rnn_output_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
    "#         self.fc = nn.Linear(rnn_output_dim, output_dim)\n",
    "        \n",
    "#     def forward(self, text, text_lengths):\n",
    "#         # text: [batch_size, seq_len]\n",
    "#         # text_lengths: [batch_size]\n",
    "        \n",
    "#         # Embed the input\n",
    "#         embedded = self.dropout(self.embedding(text))\n",
    "#         # embedded: [batch_size, seq_len, embedding_dim]\n",
    "        \n",
    "#         # Pack the padded sequences\n",
    "#         packed_embedded = nn.utils.rnn.pack_padded_sequence(\n",
    "#             embedded, text_lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "#         )\n",
    "        \n",
    "#         # Pass through RNN\n",
    "#         packed_output, hidden = self.rnn(packed_embedded)\n",
    "#         # packed_output: packed sequence of [batch_size, seq_len, hidden_dim * num_directions]\n",
    "#         # hidden: [n_layers * num_directions, batch_size, hidden_dim]\n",
    "        \n",
    "#         # Unpack the sequences\n",
    "#         output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
    "#         # output: [batch_size, seq_len, hidden_dim * num_directions]\n",
    "        \n",
    "#         # Aggregate word representations to sentence representation\n",
    "#         if self.aggregation == 'last':\n",
    "#             # Use the last hidden state\n",
    "#             if self.bidirectional:\n",
    "#                 # Concatenate last states from forward and backward\n",
    "#                 hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "#             else:\n",
    "#                 hidden = hidden[-1,:,:]\n",
    "#             sentence_repr = hidden\n",
    "            \n",
    "#         elif self.aggregation == 'mean':\n",
    "#             # Mean pooling over all outputs (ignoring padding)\n",
    "#             # Create mask for padding\n",
    "#             batch_size, seq_len, hidden_size = output.size()\n",
    "#             mask = torch.arange(seq_len, device=device).unsqueeze(0) < text_lengths.unsqueeze(1)\n",
    "#             mask = mask.unsqueeze(2).float()  # [batch_size, seq_len, 1]\n",
    "            \n",
    "#             # Apply mask and compute mean\n",
    "#             masked_output = output * mask\n",
    "#             sum_output = masked_output.sum(dim=1)\n",
    "#             sentence_repr = sum_output / text_lengths.unsqueeze(1).float()\n",
    "            \n",
    "#         elif self.aggregation == 'max':\n",
    "#             # Max pooling over all outputs\n",
    "#             sentence_repr, _ = torch.max(output, dim=1)\n",
    "            \n",
    "#         elif self.aggregation == 'attention':\n",
    "#             # Attention mechanism\n",
    "#             # Compute attention scores\n",
    "#             attn_scores = self.attention(output).squeeze(2)  # [batch_size, seq_len]\n",
    "            \n",
    "#             # Mask padding positions\n",
    "#             mask = torch.arange(output.size(1), device=device).unsqueeze(0) < text_lengths.unsqueeze(1)\n",
    "#             attn_scores = attn_scores.masked_fill(~mask, float('-inf'))\n",
    "            \n",
    "#             # Apply softmax\n",
    "#             attn_weights = torch.softmax(attn_scores, dim=1).unsqueeze(1)  # [batch_size, 1, seq_len]\n",
    "            \n",
    "#             # Weighted sum\n",
    "#             sentence_repr = torch.bmm(attn_weights, output).squeeze(1)  # [batch_size, hidden_dim * num_directions]\n",
    "        \n",
    "#         # Apply dropout\n",
    "#         sentence_repr = self.dropout(sentence_repr)\n",
    "        \n",
    "#         # Pass through fully connected layer\n",
    "#         output = self.fc(sentence_repr)\n",
    "        \n",
    "#         return output\n",
    "\n",
    "\n",
    "# def count_parameters(model):\n",
    "#     \"\"\"Count trainable parameters\"\"\"\n",
    "#     return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "# def train_epoch(model, iterator, optimizer, criterion, device, l1_lambda=0.0, l2_lambda=0.0):\n",
    "#     \"\"\"Train for one epoch\"\"\"\n",
    "#     model.train()\n",
    "#     epoch_loss = 0\n",
    "#     all_preds = []\n",
    "#     all_labels = []\n",
    "    \n",
    "#     for batch in iterator:\n",
    "#         text, text_lengths = batch.text\n",
    "#         labels = batch.label\n",
    "        \n",
    "#         # Zero gradients\n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         # Forward pass\n",
    "#         predictions = model(text, text_lengths)\n",
    "        \n",
    "#         # Calculate loss\n",
    "#         loss = criterion(predictions, labels)\n",
    "        \n",
    "#         # Add L1 regularization\n",
    "#         if l1_lambda > 0:\n",
    "#             l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "#             loss = loss + l1_lambda * l1_norm\n",
    "        \n",
    "#         # Add L2 regularization (can also use weight_decay in optimizer)\n",
    "#         if l2_lambda > 0:\n",
    "#             l2_norm = sum(p.pow(2).sum() for p in model.parameters())\n",
    "#             loss = loss + l2_lambda * l2_norm\n",
    "        \n",
    "#         # Backward pass\n",
    "#         loss.backward()\n",
    "        \n",
    "#         # Gradient clipping to prevent exploding gradients\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "#         # Update weights\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         epoch_loss += loss.item()\n",
    "        \n",
    "#         # Store predictions and labels for metrics\n",
    "#         preds = torch.argmax(predictions, dim=1)\n",
    "#         all_preds.extend(preds.cpu().numpy())\n",
    "#         all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "#     # Calculate metrics\n",
    "#     accuracy = accuracy_score(all_labels, all_preds)\n",
    "#     f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "#     return epoch_loss / len(iterator), accuracy, f1\n",
    "\n",
    "\n",
    "# def evaluate(model, iterator, criterion, device, return_predictions=False):\n",
    "#     \"\"\"Evaluate the model\"\"\"\n",
    "#     model.eval()\n",
    "#     epoch_loss = 0\n",
    "#     all_preds = []\n",
    "#     all_labels = []\n",
    "#     all_probs = []\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for batch in iterator:\n",
    "#             text, text_lengths = batch.text\n",
    "#             labels = batch.label\n",
    "            \n",
    "#             # Forward pass\n",
    "#             predictions = model(text, text_lengths)\n",
    "            \n",
    "#             # Calculate loss\n",
    "#             loss = criterion(predictions, labels)\n",
    "#             epoch_loss += loss.item()\n",
    "            \n",
    "#             # Store predictions and labels\n",
    "#             probs = torch.softmax(predictions, dim=1)\n",
    "#             preds = torch.argmax(predictions, dim=1)\n",
    "            \n",
    "#             all_preds.extend(preds.cpu().numpy())\n",
    "#             all_labels.extend(labels.cpu().numpy())\n",
    "#             all_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "#     # Calculate metrics\n",
    "#     accuracy = accuracy_score(all_labels, all_preds)\n",
    "#     f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "#     # Calculate AUC-ROC (one-vs-rest for multiclass)\n",
    "#     try:\n",
    "#         all_probs_array = np.array(all_probs)\n",
    "#         all_labels_bin = label_binarize(all_labels, classes=range(num_classes))\n",
    "#         auc_roc = roc_auc_score(all_labels_bin, all_probs_array, average='weighted', multi_class='ovr')\n",
    "#     except:\n",
    "#         auc_roc = 0.0\n",
    "    \n",
    "#     if return_predictions:\n",
    "#         return epoch_loss / len(iterator), accuracy, f1, auc_roc, all_preds, all_labels\n",
    "    \n",
    "#     return epoch_loss / len(iterator), accuracy, f1, auc_roc\n",
    "\n",
    "\n",
    "# def train_model(model, train_iterator, val_iterator, optimizer, criterion, \n",
    "#                 n_epochs, device, patience=5, l1_lambda=0.0, l2_lambda=0.0,\n",
    "#                 save_path='best_model.pt'):\n",
    "#     \"\"\"\n",
    "#     Train the model with early stopping\n",
    "#     \"\"\"\n",
    "#     best_val_acc = 0\n",
    "#     patience_counter = 0\n",
    "    \n",
    "#     train_losses = []\n",
    "#     train_accs = []\n",
    "#     val_losses = []\n",
    "#     val_accs = []\n",
    "    \n",
    "#     print(f\"\\nStarting training for {n_epochs} epochs...\")\n",
    "#     print(f\"Device: {device}\")\n",
    "#     print(f\"Model parameters: {count_parameters(model):,}\")\n",
    "    \n",
    "#     for epoch in range(n_epochs):\n",
    "#         start_time = time.time()\n",
    "        \n",
    "#         # Train\n",
    "#         train_loss, train_acc, train_f1 = train_epoch(\n",
    "#             model, train_iterator, optimizer, criterion, device, l1_lambda, l2_lambda\n",
    "#         )\n",
    "        \n",
    "#         # Evaluate on validation set\n",
    "#         val_loss, val_acc, val_f1, val_auc = evaluate(model, val_iterator, criterion, device)\n",
    "        \n",
    "#         end_time = time.time()\n",
    "#         epoch_mins, epoch_secs = divmod(end_time - start_time, 60)\n",
    "        \n",
    "#         # Store metrics\n",
    "#         train_losses.append(train_loss)\n",
    "#         train_accs.append(train_acc)\n",
    "#         val_losses.append(val_loss)\n",
    "#         val_accs.append(val_acc)\n",
    "        \n",
    "#         print(f'Epoch: {epoch+1:02}/{n_epochs} | Time: {int(epoch_mins)}m {int(epoch_secs)}s')\n",
    "#         print(f'\\tTrain Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}% | Train F1: {train_f1:.4f}')\n",
    "#         print(f'\\tVal Loss: {val_loss:.4f} | Val Acc: {val_acc*100:.2f}% | Val F1: {val_f1:.4f} | Val AUC: {val_auc:.4f}')\n",
    "        \n",
    "#         # Early stopping\n",
    "#         if val_acc > best_val_acc:\n",
    "#             best_val_acc = val_acc\n",
    "#             patience_counter = 0\n",
    "#             # Save best model\n",
    "#             torch.save(model.state_dict(), save_path)\n",
    "#             print(f'\\t>>> New best model saved with Val Acc: {val_acc*100:.2f}%')\n",
    "#         else:\n",
    "#             patience_counter += 1\n",
    "#             print(f'\\t>>> No improvement. Patience: {patience_counter}/{patience}')\n",
    "            \n",
    "#             if patience_counter >= patience:\n",
    "#                 print(f'\\nEarly stopping triggered after epoch {epoch+1}')\n",
    "#                 break\n",
    "    \n",
    "#     return {\n",
    "#         'train_losses': train_losses,\n",
    "#         'train_accs': train_accs,\n",
    "#         'val_losses': val_losses,\n",
    "#         'val_accs': val_accs,\n",
    "#         'best_val_acc': best_val_acc\n",
    "#     }\n",
    "\n",
    "\n",
    "# def evaluate_per_topic(model, iterator, device):\n",
    "#     \"\"\"Evaluate model performance per topic category\"\"\"\n",
    "#     model.eval()\n",
    "    \n",
    "#     topic_correct = defaultdict(int)\n",
    "#     topic_total = defaultdict(int)\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for batch in iterator:\n",
    "#             text, text_lengths = batch.text\n",
    "#             labels = batch.label\n",
    "            \n",
    "#             # Forward pass\n",
    "#             predictions = model(text, text_lengths)\n",
    "#             preds = torch.argmax(predictions, dim=1)\n",
    "            \n",
    "#             # Count per topic\n",
    "#             for pred, label in zip(preds.cpu().numpy(), labels.cpu().numpy()):\n",
    "#                 topic_name = LABEL.vocab.itos[label]\n",
    "#                 topic_total[topic_name] += 1\n",
    "#                 if pred == label:\n",
    "#                     topic_correct[topic_name] += 1\n",
    "    \n",
    "#     # Calculate accuracy per topic\n",
    "#     topic_accuracies = {}\n",
    "#     for topic in sorted(topic_total.keys()):\n",
    "#         acc = topic_correct[topic] / topic_total[topic] if topic_total[topic] > 0 else 0\n",
    "#         topic_accuracies[topic] = acc\n",
    "#         print(f'{topic}: {topic_correct[topic]}/{topic_total[topic]} = {acc*100:.2f}%')\n",
    "    \n",
    "#     return topic_accuracies\n",
    "\n",
    "\n",
    "# def plot_training_curves(history, save_prefix='rnn'):\n",
    "#     \"\"\"Plot training and validation curves\"\"\"\n",
    "#     fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "#     # Loss curve\n",
    "#     axes[0].plot(history['train_losses'], label='Train Loss', marker='o')\n",
    "#     axes[0].plot(history['val_losses'], label='Val Loss', marker='s')\n",
    "#     axes[0].set_xlabel('Epoch')\n",
    "#     axes[0].set_ylabel('Loss')\n",
    "#     axes[0].set_title('Training and Validation Loss')\n",
    "#     axes[0].legend()\n",
    "#     axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "#     # Accuracy curve\n",
    "#     axes[1].plot([acc*100 for acc in history['train_accs']], label='Train Acc', marker='o')\n",
    "#     axes[1].plot([acc*100 for acc in history['val_accs']], label='Val Acc', marker='s')\n",
    "#     axes[1].set_xlabel('Epoch')\n",
    "#     axes[1].set_ylabel('Accuracy (%)')\n",
    "#     axes[1].set_title('Training and Validation Accuracy')\n",
    "#     axes[1].legend()\n",
    "#     axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(f'{save_prefix}_training_curves.png', dpi=200)\n",
    "#     plt.close()\n",
    "#     print(f'Saved training curves to {save_prefix}_training_curves.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e62050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"\\n\" + \"=\"*80)\n",
    "# print(\"PART 2: RNN MODEL TRAINING\")\n",
    "# print(\"=\"*80)\n",
    "\n",
    "# # Get pretrained embeddings from Part 1\n",
    "# pretrained_embeddings = fatter_embedding.weight.data.clone()\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Hyperparameters for baseline\n",
    "# BATCH_SIZE = 64\n",
    "# HIDDEN_DIM = 256\n",
    "# N_LAYERS = 1\n",
    "# DROPOUT = 0.5\n",
    "# N_EPOCHS = 50\n",
    "# LEARNING_RATE = 0.001\n",
    "# PATIENCE = 10\n",
    "\n",
    "# # Create data iterators\n",
    "# train_iterator, val_iterator, test_iterator = create_iterators(\n",
    "#     train_data, validation_data, test_data, BATCH_SIZE\n",
    "# )\n",
    "\n",
    "# # Initialize baseline model\n",
    "# baseline_model = RNN_Classifier(\n",
    "#     vocab_size=len(TEXT.vocab),\n",
    "#     embedding_dim=embedding_dim,\n",
    "#     hidden_dim=HIDDEN_DIM,\n",
    "#     output_dim=num_classes,\n",
    "#     n_layers=N_LAYERS,\n",
    "#     bidirectional=False,\n",
    "#     dropout=DROPOUT,\n",
    "#     padding_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "#     pretrained_embeddings=pretrained_embeddings,\n",
    "#     aggregation='last'\n",
    "# ).to(device)\n",
    "\n",
    "# # Loss function and optimizer\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(baseline_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# print(f\"\\n>>> Training Baseline RNN Model\")\n",
    "# print(f\"Configuration: Hidden={HIDDEN_DIM}, Layers={N_LAYERS}, Dropout={DROPOUT}, LR={LEARNING_RATE}, Batch={BATCH_SIZE}\")\n",
    "\n",
    "# # Train baseline model\n",
    "# baseline_history = train_model(\n",
    "#     baseline_model, train_iterator, val_iterator, optimizer, criterion,\n",
    "#     n_epochs=N_EPOCHS, device=device, patience=PATIENCE,\n",
    "#     save_path='rnn_baseline_best.pt'\n",
    "# )\n",
    "\n",
    "# # Load best model and evaluate on test set\n",
    "# baseline_model.load_state_dict(torch.load('rnn_baseline_best.pt'))\n",
    "# test_loss, test_acc, test_f1, test_auc = evaluate(baseline_model, test_iterator, criterion, device)\n",
    "\n",
    "# print(f\"\\n>>> Baseline Model Test Results:\")\n",
    "# print(f\"Test Loss: {test_loss:.4f}\")\n",
    "# print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n",
    "# print(f\"Test F1 Score: {test_f1:.4f}\")\n",
    "# print(f\"Test AUC-ROC: {test_auc:.4f}\")\n",
    "\n",
    "# # Topic-wise accuracy\n",
    "# print(f\"\\n>>> Topic-wise Accuracy (Baseline):\")\n",
    "# baseline_topic_acc = evaluate_per_topic(baseline_model, test_iterator, device)\n",
    "\n",
    "# # Plot training curves\n",
    "# plot_training_curves(baseline_history, save_prefix='rnn_baseline')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
