{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05ac734e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ.setdefault('TORCH_COMPILE_DISABLE', '1')\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "import torch\n",
    "\n",
    "# Method 2: Patch torch._dynamo.disable decorator after import\n",
    "try:\n",
    "    import torch._dynamo\n",
    "    # Patch the disable function to ignore the 'wrapping' parameter\n",
    "    if hasattr(torch._dynamo, 'disable'):\n",
    "        def patched_disable(fn=None, *args, **kwargs):\n",
    "            # Remove problematic 'wrapping' parameter if present\n",
    "            if 'wrapping' in kwargs:\n",
    "                kwargs.pop('wrapping')\n",
    "            if fn is None:\n",
    "                # Decorator usage: @disable\n",
    "                return lambda f: f\n",
    "            # Function usage: disable(fn) or disable(fn, **kwargs)\n",
    "            # Simply return the function unwrapped to avoid recursion\n",
    "            # The original disable was causing issues, so we bypass it entirely\n",
    "            return fn\n",
    "        torch._dynamo.disable = patched_disable\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not patch torch._dynamo: {e}\")\n",
    "    pass  # If patching fails, continue anyway\n",
    "\n",
    "import random, string\n",
    "\n",
    "from torchtext import data , datasets\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "\n",
    "os.environ['GENSIM_DATA_DIR'] = os.path.join(os.getcwd(), 'gensim-data')\n",
    "\n",
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.fasttext import load_facebook_model\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import time, copy\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44e533a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Prepping Data...\n",
      "[+] Test set formed!\n",
      "[+] Train and Validation sets formed!\n",
      "[+] Data prepped successfully!\n",
      "[*] Retrieving pretrained word embeddings...\n",
      "[*] Loading fasttext model...\n",
      "[+] Model loaded!\n",
      "[*] Forming embedding matrix...\n",
      "[+] Embedding matrix formed!\n",
      "[+] Embeddings retrieved successfully!\n",
      "\n",
      "Label distribution in training set:\n",
      "- ABBR: 69 samples (1.58%)\n",
      "- DESC: 930 samples (21.32%)\n",
      "- ENTY: 1000 samples (22.93%)\n",
      "- HUM: 978 samples (22.42%)\n",
      "- LOC: 668 samples (15.31%)\n",
      "- NUM: 717 samples (16.44%)\n",
      "Total samples: 4362, Sum of percentages: 100.00%\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "train_data, validation_data, test_data, LABEL, TEXT, pretrained_embed = data_prep(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c176434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86a01357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of classes: 6\n",
      "Classes: ['ENTY', 'HUM', 'DESC', 'NUM', 'LOC', 'ABBR']\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary for labels\n",
    "LABEL.build_vocab(train_data)\n",
    "num_classes = len(LABEL.vocab)\n",
    "print(f\"\\nNumber of classes: {num_classes}\")\n",
    "print(f\"Classes: {LABEL.vocab.itos}\")\n",
    "\n",
    "# Get pretrained embeddings from Part 1 (frozen embeddings)\n",
    "pretrained_embeddings = pretrained_embed.weight.data\n",
    "\n",
    "# Get embedding dimension and vocab size from the fasttext embedding layer\n",
    "embedding_dim = pretrained_embed.weight.shape[1]\n",
    "embedding_vocab_size = pretrained_embed.weight.shape[0]  # Vocab size from saved embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2597781b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT Vocab Size: 8102\n"
     ]
    }
   ],
   "source": [
    "print(f'TEXT Vocab Size: {len(TEXT.vocab.stoi)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aad6c8e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PART 3.4: TARGETED IMPROVEMENT FOR WEAK TOPICS\n",
      "================================================================================\n",
      "\n",
      "Strategies:\n",
      "  1. Data Augmentation for imbalanced classes (especially ABBR)\n",
      "  2. Positional Embeddings in attention layer\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PART 3.4: Targeted Improvement for Weak Topics\n",
    "# Strategy: Data Augmentation, Positional Embeddings\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 3.4: TARGETED IMPROVEMENT FOR WEAK TOPICS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nStrategies:\")\n",
    "print(\"  1. Data Augmentation for imbalanced classes (especially ABBR)\")\n",
    "print(\"  2. Positional Embeddings in attention layer\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Import required libraries for augmentation\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "try:\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c2db80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Step 2: Implementing Data Augmentation Functions...\n",
      "    ✓ Data augmentation functions ready\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Step 2: Data Augmentation Functions for Imbalanced Classes\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n>>> Step 2: Implementing Data Augmentation Functions...\")\n",
    "\n",
    "def get_synonyms(word):\n",
    "    \"\"\"Get synonyms for a word using WordNet\"\"\"\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonym = lemma.name().replace('_', ' ').lower()\n",
    "            if synonym != word and synonym.isalpha():\n",
    "                synonyms.add(synonym)\n",
    "    return list(synonyms)\n",
    "\n",
    "def synonym_replacement(tokens, n=1):\n",
    "    \"\"\"Replace n random words with their synonyms\"\"\"\n",
    "    new_tokens = tokens.copy()\n",
    "    words_to_replace = [i for i, word in enumerate(tokens) if word.isalpha() and len(word) > 2]\n",
    "    \n",
    "    if len(words_to_replace) == 0:\n",
    "        return tokens\n",
    "    \n",
    "    num_replacements = min(n, len(words_to_replace))\n",
    "    indices_to_replace = random.sample(words_to_replace, num_replacements)\n",
    "    \n",
    "    for idx in indices_to_replace:\n",
    "        synonyms = get_synonyms(tokens[idx])\n",
    "        if synonyms:\n",
    "            new_tokens[idx] = random.choice(synonyms)\n",
    "    \n",
    "    return new_tokens\n",
    "\n",
    "def random_insertion(tokens, n=1):\n",
    "    \"\"\"Randomly insert synonyms of n words\"\"\"\n",
    "    new_tokens = tokens.copy()\n",
    "    \n",
    "    for _ in range(n):\n",
    "        if len(new_tokens) == 0:\n",
    "            break\n",
    "        word = random.choice(new_tokens)\n",
    "        synonyms = get_synonyms(word)\n",
    "        if synonyms:\n",
    "            synonym = random.choice(synonyms)\n",
    "            insert_pos = random.randint(0, len(new_tokens))\n",
    "            new_tokens.insert(insert_pos, synonym)\n",
    "    \n",
    "    return new_tokens\n",
    "\n",
    "def random_deletion(tokens, p=0.1):\n",
    "    \"\"\"Randomly delete words with probability p\"\"\"\n",
    "    if len(tokens) == 1:\n",
    "        return tokens\n",
    "    \n",
    "    new_tokens = []\n",
    "    for token in tokens:\n",
    "        if random.random() > p:\n",
    "            new_tokens.append(token)\n",
    "    \n",
    "    if len(new_tokens) == 0:\n",
    "        return tokens[:1]\n",
    "    \n",
    "    return new_tokens\n",
    "\n",
    "def random_swap(tokens, n=1):\n",
    "    \"\"\"Randomly swap n pairs of words\"\"\"\n",
    "    new_tokens = tokens.copy()\n",
    "    \n",
    "    for _ in range(n):\n",
    "        if len(new_tokens) < 2:\n",
    "            break\n",
    "        idx1, idx2 = random.sample(range(len(new_tokens)), 2)\n",
    "        new_tokens[idx1], new_tokens[idx2] = new_tokens[idx2], new_tokens[idx1]\n",
    "    \n",
    "    return new_tokens\n",
    "\n",
    "def augment_text(text, augmentation_techniques=['synonym', 'insertion', 'deletion', 'swap'], \n",
    "                 num_augmentations=3):\n",
    "    \"\"\"Apply data augmentation to text\"\"\"\n",
    "    augmented_texts = []\n",
    "    \n",
    "    for _ in range(num_augmentations):\n",
    "        aug_text = text.copy()\n",
    "        technique = random.choice(augmentation_techniques)\n",
    "        \n",
    "        if technique == 'synonym' and len(aug_text) > 0:\n",
    "            aug_text = synonym_replacement(aug_text, n=random.randint(1, 2))\n",
    "        elif technique == 'insertion' and len(aug_text) > 0:\n",
    "            aug_text = random_insertion(aug_text, n=random.randint(1, 2))\n",
    "        elif technique == 'deletion' and len(aug_text) > 1:\n",
    "            aug_text = random_deletion(aug_text, p=0.1)\n",
    "        elif technique == 'swap' and len(aug_text) > 1:\n",
    "            aug_text = random_swap(aug_text, n=1)\n",
    "        \n",
    "        augmented_texts.append(aug_text)\n",
    "    \n",
    "    return augmented_texts\n",
    "\n",
    "print(\"    ✓ Data augmentation functions ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f8ddcf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Step 3: Applying Data Augmentation for Imbalanced Classes...\n",
      "\n",
      "Original label distribution:\n",
      "  ABBR: 69 samples (1.58%)\n",
      "  DESC: 930 samples (21.32%)\n",
      "  ENTY: 1000 samples (22.93%)\n",
      "  HUM: 978 samples (22.42%)\n",
      "  LOC: 668 samples (15.31%)\n",
      "  NUM: 717 samples (16.44%)\n",
      "\n",
      "  Augmenting ENTY: 1000 -> 1300 samples\n",
      "    Generating 300 additional samples...\n",
      "    ✓ Generated 300 augmented samples\n",
      "\n",
      "  Augmenting HUM: 978 -> 1200 samples\n",
      "    Generating 222 additional samples...\n",
      "    ✓ Generated 222 augmented samples\n",
      "\n",
      "  Augmenting LOC: 668 -> 800 samples\n",
      "    Generating 132 additional samples...\n",
      "    ✓ Generated 132 augmented samples\n",
      "\n",
      "  Augmenting ABBR: 69 -> 900 samples\n",
      "    Generating 831 additional samples...\n",
      "    ✓ Generated 831 augmented samples\n",
      "\n",
      "  Augmenting NUM: 717 -> 850 samples\n",
      "    Generating 133 additional samples...\n",
      "    ✓ Generated 133 augmented samples\n",
      "\n",
      "Augmented label distribution:\n",
      "  ABBR: 900 samples (15.05%)\n",
      "  DESC: 930 samples (15.55%)\n",
      "  ENTY: 1300 samples (21.74%)\n",
      "  HUM: 1200 samples (20.07%)\n",
      "  LOC: 800 samples (13.38%)\n",
      "  NUM: 850 samples (14.21%)\n",
      "\n",
      "  Total samples: 4362 -> 5980\n",
      "  ✓ Data augmentation complete\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# Step 3: Apply Data Augmentation for Imbalanced Classes\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n>>> Step 3: Applying Data Augmentation for Imbalanced Classes...\")\n",
    "\n",
    "# Count current label distribution\n",
    "label_counts_p34 = Counter([ex.label for ex in train_data.examples])\n",
    "print(f\"\\nOriginal label distribution:\")\n",
    "for label, count in sorted(label_counts_p34.items()):\n",
    "    print(f\"  {label}: {count} samples ({count/len(train_data.examples)*100:.2f}%)\")\n",
    "\n",
    "# Augmentation targets (boost weaker topics more aggressively)\n",
    "target_counts_p34 = {\n",
    "    'ABBR': 900,   # heavy boost (~13x) to improve weakest class\n",
    "    'DESC': 930,   # keep strong class unchanged\n",
    "    'ENTY': 1300,  # moderate boost (~1.3x)\n",
    "    'HUM': 1200,   # boost (~1.23x)\n",
    "    'LOC': 800,    # modest boost (~1.2x)\n",
    "    'NUM': 850     # modest boost (~1.2x)\n",
    "}\n",
    "\n",
    "# Create augmented examples\n",
    "augmented_examples = list(train_data.examples)  # Start with all original examples\n",
    "\n",
    "for label in label_counts_p34.keys():\n",
    "    current_count = label_counts_p34[label]\n",
    "    target_count = target_counts_p34[label]\n",
    "    \n",
    "    if current_count < target_count:\n",
    "        label_examples = [ex for ex in train_data.examples if ex.label == label]\n",
    "        num_augmentations_needed = target_count - current_count\n",
    "        \n",
    "        print(f\"\\n  Augmenting {label}: {current_count} -> {target_count} samples\")\n",
    "        print(f\"    Generating {num_augmentations_needed} additional samples...\")\n",
    "        \n",
    "        augmented_count = 0\n",
    "        while augmented_count < num_augmentations_needed:\n",
    "            original_ex = random.choice(label_examples)\n",
    "            aug_texts = augment_text(original_ex.text, num_augmentations=1)\n",
    "            \n",
    "            for aug_text in aug_texts:\n",
    "                if augmented_count >= num_augmentations_needed:\n",
    "                    break\n",
    "                \n",
    "                new_ex = data.Example.fromlist([aug_text, label], \n",
    "                                               fields=[('text', TEXT), ('label', LABEL)])\n",
    "                augmented_examples.append(new_ex)\n",
    "                augmented_count += 1\n",
    "        \n",
    "        print(f\"    ✓ Generated {augmented_count} augmented samples\")\n",
    "\n",
    "# Create augmented dataset with proper field structure\n",
    "augmented_train_data = data.Dataset(augmented_examples, fields=[('text', TEXT), ('label', LABEL)])\n",
    "\n",
    "# Verify augmented distribution\n",
    "new_label_counts = Counter([ex.label for ex in augmented_examples])\n",
    "print(f\"\\nAugmented label distribution:\")\n",
    "for label, count in sorted(new_label_counts.items()):\n",
    "    print(f\"  {label}: {count} samples ({count/len(augmented_examples)*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\n  Total samples: {len(train_data.examples)} -> {len(augmented_examples)}\")\n",
    "print(f\"  ✓ Data augmentation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ec4bc0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# augmented_train_data.examples[0].label\n",
    "[(ex.text, ex.label) for ex in augmented_train_data if ex.label not in ['ABBR','DESC','ENTY','HUM','LOC','NUM']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce04616c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(None,\n",
       "            {'ENTY': 0, 'HUM': 1, 'DESC': 2, 'NUM': 3, 'LOC': 4, 'ABBR': 5})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABEL.vocab.stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9afaf5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "abbr_aug_ex = [ex for ex in augmented_train_data if ex.label == \"ABBR\"]\n",
    "abbr_ex = [ex for ex in train_data if ex.label == \"ABBR\"]\n",
    "\n",
    "count = 0\n",
    "# for ex in abbr_aug_ex:\n",
    "#     if ex not in abbr_ex:\n",
    "#         print(ex.text)\n",
    "#         print(ex.label)\n",
    "#         count += 1\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "858eb2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label distribution in training set:\n",
      "- ABBR: 900 samples (15.05%)\n",
      "- DESC: 930 samples (15.55%)\n",
      "- ENTY: 1300 samples (21.74%)\n",
      "- HUM: 1200 samples (20.07%)\n",
      "- LOC: 800 samples (13.38%)\n",
      "- NUM: 850 samples (14.21%)\n"
     ]
    }
   ],
   "source": [
    "# Count how many samples per label in the train set\n",
    "label_counts_p34 = Counter([ex.label for ex in augmented_train_data.examples])\n",
    "total_examples_p34 = len(augmented_train_data)\n",
    "\n",
    "print(\"\\nLabel distribution in training set:\")\n",
    "for label, count in sorted(label_counts_p34.items()):\n",
    "    percentage = (count / total_examples_p34) * 100\n",
    "    print(f\"- {label}: {count} samples ({percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2926e83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PART 3.4: SAMPLING STRATEGIES VS DATA AUGMENTATION\n",
      "================================================================================\n",
      "Comparing text augmentation, weighted sampling, and their combination\n",
      "across the Simple RNN baseline (Part 2 best config) and the RNN + BERT hybrid.\n",
      "================================================================================\n",
      ">>> Simple RNN ready (mean aggregation baseline)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PART 3.5: Sampling Strategies vs Data Augmentation\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 3.4: SAMPLING STRATEGIES VS DATA AUGMENTATION\")\n",
    "print(\"=\"*80)\n",
    "print(\"Comparing text augmentation, weighted sampling, and their combination\")\n",
    "print(\"across the Simple RNN baseline (Part 2 best config) and the RNN + BERT hybrid.\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Extended RNN Classifier with multiple aggregation methods\n",
    "class RNN_Classifier_Aggregation(nn.Module):\n",
    "    \"\"\"\n",
    "    RNN for topic classification with multiple aggregation strategies.\n",
    "    Uses pretrained embeddings (learnable/updated during training).\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, \n",
    "                 n_layers=1, dropout=0.0, padding_idx=0, pretrained_embeddings=None,\n",
    "                 aggregation='mean'):\n",
    "        super(RNN_Classifier_Aggregation, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.aggregation = aggregation  # 'last', 'mean', 'max'\n",
    "        \n",
    "        # Embedding layer initialized with pretrained embeddings\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "            self.embedding.weight.requires_grad = True\n",
    "        \n",
    "        # RNN layer\n",
    "        self.rnn = nn.RNN(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=n_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.0  # No dropout in baseline\n",
    "        )\n",
    "                \n",
    "        # Fully connected output layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, text, text_lengths):        \n",
    "        # Embed the input\n",
    "        embedded = self.embedding(text)  # [batch_size, seq_len, embedding_dim]\n",
    "        \n",
    "        # Get dimensions\n",
    "        batch_size = embedded.size(0)\n",
    "        seq_len = embedded.size(1)\n",
    "        \n",
    "        # Handle text_lengths\n",
    "        text_lengths_flat = text_lengths.flatten().cpu().long()\n",
    "        if len(text_lengths_flat) != batch_size:\n",
    "            raise ValueError(\n",
    "                f\"text_lengths size mismatch: got {len(text_lengths_flat)} elements, \"\n",
    "                f\"expected {batch_size}\"\n",
    "            )\n",
    "        \n",
    "        # Clamp lengths\n",
    "        text_lengths_clamped = torch.clamp(text_lengths_flat, min=1, max=seq_len)\n",
    "        \n",
    "        text_lengths_clamped_device = text_lengths_clamped.to(text.device)\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, text_lengths_clamped, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        \n",
    "        # Pass through RNN\n",
    "        packed_output, hidden = self.rnn(packed_embedded)\n",
    "        \n",
    "        # Aggregate word representations to sentence representation\n",
    "        if self.aggregation == 'last':\n",
    "            sentence_repr = hidden[-1]  # [batch_size, hidden_dim]\n",
    "            \n",
    "        elif self.aggregation == 'mean':\n",
    "            output, output_lengths = nn.utils.rnn.pad_packed_sequence(\n",
    "                packed_output, batch_first=True\n",
    "            )\n",
    "            # output: [batch_size, seq_len, hidden_dim]\n",
    "            \n",
    "            # Create mask for padding\n",
    "            mask = torch.arange(seq_len, device=text.device).unsqueeze(0) < text_lengths_clamped_device.unsqueeze(1)\n",
    "            mask = mask.unsqueeze(2).float()  # [batch_size, seq_len, 1]\n",
    "            \n",
    "            # Apply mask and compute mean\n",
    "            masked_output = output * mask\n",
    "            sum_output = masked_output.sum(dim=1)  # [batch_size, hidden_dim]\n",
    "            sentence_repr = sum_output / text_lengths_clamped_device.unsqueeze(1).float()\n",
    "            \n",
    "        elif self.aggregation == 'max':\n",
    "            # Max pooling over all outputs\n",
    "            output, output_lengths = nn.utils.rnn.pad_packed_sequence(\n",
    "                packed_output, batch_first=True\n",
    "            )\n",
    "            # output: [batch_size, seq_len, hidden_dim]\n",
    "            \n",
    "            mask = torch.arange(seq_len, device=text.device).unsqueeze(0) < text_lengths_clamped_device.unsqueeze(1)\n",
    "            mask = mask.unsqueeze(2).float()  # [batch_size, seq_len, 1]\n",
    "            \n",
    "            masked_output = output * mask + (1 - mask) * float('-inf')\n",
    "            sentence_repr, _ = torch.max(masked_output, dim=1)\n",
    "        \n",
    "        # Pass through fully connected layer\n",
    "        output = self.fc(sentence_repr)  # [batch_size, output_dim]\n",
    "        \n",
    "        return output\n",
    "\n",
    "print(\">>> Simple RNN ready (mean aggregation baseline)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02842886",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jacob Aaron Rossman\\Documents\\GitHub Repo\\NTU-NLP-Assignment\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7904aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Transformers library available\n",
      ">>> RNNBertClassifier ready (Part 3.3 best model)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Hybrid Model: RNN + BERT with Attention (Part 3.3 best model)\n",
    "# ============================================================================\n",
    "\n",
    "try:\n",
    "    from transformers import BertModel, BertTokenizer\n",
    "    BERT_AVAILABLE = True\n",
    "    print(\">>> Transformers library available\")\n",
    "except ImportError:\n",
    "    BERT_AVAILABLE = False\n",
    "    print(\">>> Warning: transformers library not found. Install `transformers` to run BERT experiments.\")\n",
    "\n",
    "\n",
    "class RNNBertClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    RNN with Pretrained BERT embeddings\n",
    "    Uses BERT to get contextualized embeddings, then passes through BiLSTM with attention\n",
    "    \"\"\"\n",
    "    def __init__(self, output_dim, hidden_dim=256, n_layers=2, dropout=0.5,\n",
    "                 bert_model_name='distilbert-base-uncased', freeze_bert=False):\n",
    "        super(RNNBertClassifier, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.freeze_bert = freeze_bert\n",
    "        \n",
    "        # Load pretrained BERT model and tokenizer\n",
    "        self.bert_tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "        self.bert_model = BertModel.from_pretrained(bert_model_name)\n",
    "        \n",
    "        # Freeze BERT parameters if specified\n",
    "        if freeze_bert:\n",
    "            for param in self.bert_model.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # BERT output dimension (768 for bert-base-uncased)\n",
    "        bert_output_dim = self.bert_model.config.hidden_size\n",
    "        \n",
    "        # Bidirectional LSTM to process BERT embeddings\n",
    "        self.bilstm = nn.LSTM(\n",
    "            bert_output_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=n_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout if n_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Additive Attention Mechanism (Bahdanau-style)\n",
    "        self.attention_linear1 = nn.Linear(hidden_dim * 2, hidden_dim)  # *2 for bidirectional\n",
    "        self.attention_linear2 = nn.Linear(hidden_dim, 1)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Fully connected output layer\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        \n",
    "    def forward(self, text, text_lengths, text_vocab=None):        \n",
    "        batch_size = text.size(0)\n",
    "        seq_len = text.size(1)\n",
    "        device = text.device\n",
    "        \n",
    "        # Convert token indices back to text strings using vocab\n",
    "        text_list = []\n",
    "        for i in range(batch_size):\n",
    "            actual_len = text_lengths[i].item() if isinstance(text_lengths[i], torch.Tensor) else text_lengths[i]\n",
    "            tokens = []\n",
    "            for j in range(min(actual_len, seq_len)):\n",
    "                token_idx = text[i, j].item()\n",
    "                if text_vocab is not None and token_idx < len(text_vocab):\n",
    "                    token = text_vocab[token_idx]\n",
    "                    # Skip special tokens\n",
    "                    if token not in ['<pad>', '<unk>', '<sos>', '<eos>']:\n",
    "                        tokens.append(token)\n",
    "                else:\n",
    "                    # Fallback if vocab not provided\n",
    "                    tokens.append(str(token_idx))\n",
    "            # Join tokens to form sentence\n",
    "            sentence = \" \".join(tokens)\n",
    "            text_list.append(sentence)\n",
    "        \n",
    "        # Tokenize with BERT\n",
    "        encoded = self.bert_tokenizer(\n",
    "            text_list,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors='pt'\n",
    "        ).to(device)\n",
    "        \n",
    "        # Get BERT embeddings\n",
    "        with torch.set_grad_enabled(not self.freeze_bert):\n",
    "            bert_outputs = self.bert_model(**encoded)\n",
    "            bert_embeddings = bert_outputs.last_hidden_state  # [batch_size, seq_len, 768]\n",
    "        \n",
    "        # Get actual sequence lengths from BERT tokenizer\n",
    "        bert_lengths = encoded['attention_mask'].sum(dim=1).cpu()\n",
    "\n",
    "\n",
    "        # Get BERT embeddings\n",
    "        with torch.set_grad_enabled(not self.freeze_bert):\n",
    "            bert_outputs = self.bert_model(**encoded)\n",
    "            bert_embeddings = bert_outputs.last_hidden_state  # [batch_size, seq_len, 768]\n",
    "\n",
    "        # Align lengths\n",
    "        bert_lengths = encoded['attention_mask'].sum(dim=1)\n",
    "        max_len = bert_embeddings.size(1)\n",
    "        bert_lengths = bert_lengths.clamp(max=max_len).cpu()\n",
    "\n",
    "        # Pack safely\n",
    "        packed_bert = nn.utils.rnn.pack_padded_sequence(\n",
    "        bert_embeddings, bert_lengths, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "\n",
    "        \n",
    "        # Pack sequences for efficient RNN processing\n",
    "        packed_bert = nn.utils.rnn.pack_padded_sequence(\n",
    "            bert_embeddings, bert_lengths, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        \n",
    "        # Pass through bidirectional LSTM\n",
    "        packed_output, (hidden, cell) = self.bilstm(packed_bert)\n",
    "        \n",
    "        # Unpack sequences\n",
    "        bilstm_output, output_lengths = nn.utils.rnn.pad_packed_sequence(\n",
    "            packed_output, batch_first=True\n",
    "        )\n",
    "        # bilstm_output: [batch_size, seq_len, hidden_dim * 2]\n",
    "        \n",
    "        # Apply Attention Mechanism\n",
    "        attention_scores = self.attention_linear1(bilstm_output)  # [batch_size, seq_len, hidden_dim]\n",
    "        attention_scores = self.tanh(attention_scores)\n",
    "        attention_scores = self.attention_linear2(attention_scores).squeeze(2)  # [batch_size, seq_len]\n",
    "        \n",
    "        # Mask padding positions\n",
    "        batch_size_attn, seq_len_attn = bilstm_output.size(0), bilstm_output.size(1)\n",
    "        mask = torch.arange(seq_len_attn, device=device).unsqueeze(0) < bert_lengths.unsqueeze(1).to(device)\n",
    "        attention_scores = attention_scores.masked_fill(~mask, float('-inf'))\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        attention_weights = torch.softmax(attention_scores, dim=1).unsqueeze(2)  # [batch_size, seq_len, 1]\n",
    "        \n",
    "        # Compute weighted sum\n",
    "        context_vector = torch.sum(attention_weights * bilstm_output, dim=1)  # [batch_size, hidden_dim * 2]\n",
    "        \n",
    "        # Apply dropout\n",
    "        context_vector = self.dropout(context_vector)\n",
    "        \n",
    "        # Pass through fully connected layer\n",
    "        output = self.fc(context_vector)  # [batch_size, output_dim]\n",
    "        \n",
    "        return output\n",
    "\n",
    "print(\">>> RNNBertClassifier ready (Part 3.3 best model)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dcda8508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================\n",
    "# Helper: Topic-wise evaluation\n",
    "# =========================================================================\n",
    "\n",
    "def evaluate_per_topic_p35(model, iterator, device, text_vocab=None):\n",
    "    \"\"\"Evaluate accuracy per topic on the provided iterator.\"\"\"\n",
    "    model.eval()\n",
    "    topic_correct = defaultdict(int)\n",
    "    topic_total = defaultdict(int)\n",
    "    idx_to_label = LABEL.vocab.itos\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text, text_lengths, labels = process_batch(batch, debug=False)\n",
    "            if text_vocab is not None:\n",
    "                predictions = model(text, text_lengths, text_vocab=text_vocab)\n",
    "            else:\n",
    "                predictions = model(text, text_lengths)\n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            for pred, label in zip(preds.cpu().numpy(), labels.cpu().numpy()):\n",
    "                topic_name = idx_to_label[label]\n",
    "                topic_total[topic_name] += 1\n",
    "                if pred == label:\n",
    "                    topic_correct[topic_name] += 1\n",
    "\n",
    "    topic_metrics = {}\n",
    "    for topic in sorted(topic_total.keys()):\n",
    "        total = topic_total[topic]\n",
    "        correct = topic_correct[topic]\n",
    "        accuracy = correct / total if total > 0 else 0.0\n",
    "        topic_metrics[topic] = {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"correct\": correct,\n",
    "            \"total\": total,\n",
    "        }\n",
    "    return topic_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0220a7b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Preparing dataset variants for Part 3.4 experiments...\n",
      "  - Original train: 4362 samples\n",
      "      ABBR: 69 (1.58%)\n",
      "      DESC: 930 (21.32%)\n",
      "      ENTY: 1000 (22.93%)\n",
      "      HUM: 978 (22.42%)\n",
      "      LOC: 668 (15.31%)\n",
      "      NUM: 717 (16.44%)\n",
      "  - Augmented train: 5980 samples\n",
      "      ABBR: 900 (15.05%)\n",
      "      DESC: 930 (15.55%)\n",
      "      ENTY: 1300 (21.74%)\n",
      "      HUM: 1200 (20.07%)\n",
      "      LOC: 800 (13.38%)\n",
      "      NUM: 850 (14.21%)\n",
      "  - Weighted-sampled train: 4362 samples\n",
      "      ABBR: 849 (19.46%)\n",
      "      DESC: 581 (13.32%)\n",
      "      ENTY: 862 (19.76%)\n",
      "      HUM: 704 (16.14%)\n",
      "      LOC: 670 (15.36%)\n",
      "      NUM: 696 (15.96%)\n",
      "  - Augmented + weighted train: 5980 samples\n",
      "      ABBR: 1057 (17.68%)\n",
      "      DESC: 813 (13.60%)\n",
      "      ENTY: 1213 (20.28%)\n",
      "      HUM: 922 (15.42%)\n",
      "      LOC: 944 (15.79%)\n",
      "      NUM: 1031 (17.24%)\n",
      "\n",
      ">>> Dataset variants ready. Criterion initialised for upcoming runs.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Dataset Variants & Utilities for Experiments\n",
    "# ============================================================================\n",
    "\n",
    "def describe_dataset(name, dataset):\n",
    "    counts = Counter(ex.label for ex in dataset.examples)\n",
    "    total = len(dataset.examples)\n",
    "    print(f\"  - {name}: {total} samples\")\n",
    "    for label, count in sorted(counts.items()):\n",
    "        print(f\"      {label}: {count} ({count/total*100:.2f}%)\")\n",
    "    return counts\n",
    "\n",
    "\n",
    "# Topic-wise accuracy from latest weighted-sampler evaluation (used to boost weak classes)\n",
    "P35_TOPIC_ACCURACY = {\n",
    "    \"ABBR\": 0.7778,\n",
    "    \"DESC\": 0.9855,\n",
    "    \"ENTY\": 0.7128,\n",
    "    \"HUM\": 0.8769,\n",
    "    \"LOC\": 0.8889,\n",
    "    \"NUM\": 0.8584,\n",
    "}\n",
    "# Convert to difficulty scores (higher when accuracy is lower)\n",
    "P35_TOPIC_DIFFICULTY = {label: max(0.0, 1.0 - acc) for label, acc in P35_TOPIC_ACCURACY.items()}\n",
    "# Global multiplier for difficulty adjustment; tweak to emphasise weak topics more/less\n",
    "P35_DIFFICULTY_SCALE = 2.0\n",
    "\n",
    "\n",
    "def create_weighted_dataset(source_dataset, target_size=None, seed=SEED, difficulty_scale=P35_DIFFICULTY_SCALE):\n",
    "    \"\"\"Mimic WeightedRandomSampler by sampling examples according to class weights, with extra boosts for weak topics.\"\"\"\n",
    "    rng = random.Random(seed)\n",
    "    counts = Counter(ex.label for ex in source_dataset.examples)\n",
    "    total = sum(counts.values())\n",
    "    base_class_weights = {label: total / count for label, count in counts.items()}\n",
    "\n",
    "    class_boosts = {\n",
    "        label: 1.0 + difficulty_scale * P35_TOPIC_DIFFICULTY.get(label, 0.0)\n",
    "        for label in counts.keys()\n",
    "    }\n",
    "\n",
    "    weights = [base_class_weights[ex.label] * class_boosts.get(ex.label, 1.0) for ex in source_dataset.examples]\n",
    "    sample_size = target_size or len(source_dataset.examples)\n",
    "\n",
    "    sampled_examples = rng.choices(source_dataset.examples, weights=weights, k=sample_size)\n",
    "    fields = [('text', TEXT), ('label', LABEL)]\n",
    "    return data.Dataset(sampled_examples, fields=fields)\n",
    "\n",
    "\n",
    "print(\"\\n>>> Preparing dataset variants for Part 3.4 experiments...\")\n",
    "base_counts = describe_dataset(\"Original train\", train_data)\n",
    "aug_counts = describe_dataset(\"Augmented train\", augmented_train_data)\n",
    "\n",
    "weighted_train_data = create_weighted_dataset(train_data)\n",
    "weighted_counts = describe_dataset(\"Weighted-sampled train\", weighted_train_data)\n",
    "\n",
    "augmented_weighted_train_data = create_weighted_dataset(augmented_train_data)\n",
    "aug_weighted_counts = describe_dataset(\"Augmented + weighted train\", augmented_weighted_train_data)\n",
    "\n",
    "p35_datasets = {\n",
    "    \"original\": train_data,\n",
    "    \"augmented\": augmented_train_data,\n",
    "    \"weighted\": weighted_train_data,\n",
    "    \"augmented_weighted\": augmented_weighted_train_data,\n",
    "}\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "p35_results = {\n",
    "    \"simple_rnn_baseline\": {},\n",
    "    \"rnn_bert\": {}\n",
    "}\n",
    "\n",
    "print(\"\\n>>> Dataset variants ready. Criterion initialised for upcoming runs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bda6694d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Simple RNN (mean pooling) experiment runner\n",
    "# ============================================================================\n",
    "\n",
    "def reset_random_seeds(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def build_iterator(dataset, batch_size, shuffle):\n",
    "    return data.BucketIterator(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        sort_key=lambda x: len(x.text),\n",
    "        sort_within_batch=True,\n",
    "        shuffle=shuffle,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "RNN_BASE_HYPERPARM = hyperparam_prep()\n",
    "RNN_BASE_HYPERPARM['HIDDEN_DIM'] *= 2\n",
    "RNN_BASE_HYPERPARM['N_LAYERS'] = 1\n",
    "RNN_BASE_HYPERPARM['SAVE_MODEL'] = True\n",
    "\n",
    "\n",
    "def run_simple_rnn_experiment(dataset_key, description, save_suffix):\n",
    "    if dataset_key not in p35_datasets:\n",
    "        raise ValueError(f\"Unknown dataset key: {dataset_key}\")\n",
    "\n",
    "    reset_random_seeds(SEED)\n",
    "    train_dataset = p35_datasets[dataset_key]\n",
    "\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(f\"Running Simple RNN (mean pooling) experiment: {description}\")\n",
    "    print(\"-\"*80)\n",
    "\n",
    "    train_iter = build_iterator(train_dataset, RNN_BASE_HYPERPARM['BATCH_SIZE'], shuffle=True)\n",
    "    val_iter = build_iterator(validation_data, RNN_BASE_HYPERPARM['BATCH_SIZE'], shuffle=False)\n",
    "    test_iter = build_iterator(test_data, RNN_BASE_HYPERPARM['BATCH_SIZE'], shuffle=False)\n",
    "\n",
    "    model = RNN_Classifier_Aggregation(\n",
    "        vocab_size=len(TEXT.vocab),\n",
    "        embedding_dim=embedding_dim,\n",
    "        hidden_dim=RNN_BASE_HYPERPARM['HIDDEN_DIM'],\n",
    "        output_dim=num_classes,\n",
    "        n_layers=RNN_BASE_HYPERPARM['N_LAYERS'],\n",
    "        dropout=RNN_BASE_HYPERPARM['DROPOUT'],\n",
    "        padding_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "        pretrained_embeddings=pretrained_embeddings,\n",
    "        aggregation=RNN_BASE_HYPERPARM['AGGREGATOR'],\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Select optimizer with best learning rate\n",
    "    if RNN_BASE_HYPERPARM['OPTIMIZER'] == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=RNN_BASE_HYPERPARM['LEARNING_RATE'],\n",
    "                                        weight_decay=RNN_BASE_HYPERPARM['L2_LAMBDA'])\n",
    "    elif RNN_BASE_HYPERPARM['OPTIMIZER'] == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=RNN_BASE_HYPERPARM['LEARNING_RATE'], momentum=0.9,\n",
    "                                        weight_decay=RNN_BASE_HYPERPARM['L2_LAMBDA'])\n",
    "    elif RNN_BASE_HYPERPARM['OPTIMIZER'] == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=RNN_BASE_HYPERPARM['LEARNING_RATE'],\n",
    "                                        weight_decay=RNN_BASE_HYPERPARM['L2_LAMBDA'])\n",
    "    elif RNN_BASE_HYPERPARM['OPTIMIZER'] == 'Adagrad':\n",
    "        optimizer = optim.Adagrad(model.parameters(), lr=RNN_BASE_HYPERPARM['LEARNING_RATE'],\n",
    "                                        weight_decay=RNN_BASE_HYPERPARM['L2_LAMBDA'])\n",
    "\n",
    "    model, history = train_model_with_history(\n",
    "        model,\n",
    "        train_iter,\n",
    "        val_iter,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        RNN_BASE_HYPERPARM['N_EPOCHS'],\n",
    "        device,\n",
    "        num_classes,\n",
    "        RNN_BASE_HYPERPARM['L1_LAMBDA'],\n",
    "        patience=RNN_BASE_HYPERPARM['PATIENCE'],\n",
    "        model_name=f\"Simple RNN ({description})\",\n",
    "    )\n",
    "\n",
    "    test_loss, test_acc, test_f1, test_auc = evaluate_model(\n",
    "        model,\n",
    "        test_iter,\n",
    "        criterion,\n",
    "        device,\n",
    "        f\"Simple RNN ({description})\",\n",
    "        num_classes,\n",
    "    )\n",
    "\n",
    "    topic_metrics = evaluate_per_topic_p35(model, test_iter, device)\n",
    "\n",
    "    model_path = f\"weights/part35_simple_rnn_{save_suffix}.pt\"\n",
    "    if RNN_BASE_HYPERPARM['SAVE_MODEL']:\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    p35_results[\"simple_rnn_baseline\"][save_suffix] = {\n",
    "        \"description\": description,\n",
    "        \"dataset_key\": dataset_key,\n",
    "        \"history\": history,\n",
    "        \"test_metrics\": {\n",
    "            \"loss\": test_loss,\n",
    "            \"accuracy\": test_acc,\n",
    "            \"f1\": test_f1,\n",
    "            \"auc\": test_auc,\n",
    "        },\n",
    "        \"topic_metrics\": topic_metrics,\n",
    "        \"model_path\": model_path if RNN_BASE_HYPERPARM['SAVE_MODEL'] else None,\n",
    "        \"model\": model,\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"history\": history,\n",
    "        \"topic_metrics\": topic_metrics,\n",
    "        \"test_metrics\": {\n",
    "            \"loss\": test_loss,\n",
    "            \"accuracy\": test_acc,\n",
    "            \"f1\": test_f1,\n",
    "            \"auc\": test_auc,\n",
    "        },\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8bff8e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Executing Simple RNN experiments...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Running Simple RNN (mean pooling) experiment: Text Augmentation\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      ">>> Training Simple RNN (Text Augmentation)\n",
      "    Parameters: 2,850,446\n",
      "    Max epochs: 100, Patience: 10\n",
      "Epoch: 01/100 | Time: 0m 1s\n",
      "\tTrain Loss: 1.5556 | Train Acc: 37.42%\n",
      "\tVal Loss: 1.2733 | Val Acc: 44.04% | Val F1: 0.3522 | Val AUC: 0.8234\n",
      "Epoch: 02/100 | Time: 0m 0s\n",
      "\tTrain Loss: 1.0554 | Train Acc: 59.82%\n",
      "\tVal Loss: 1.0492 | Val Acc: 56.88% | Val F1: 0.5445 | Val AUC: 0.8734\n",
      "Epoch: 03/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.7579 | Train Acc: 76.40%\n",
      "\tVal Loss: 0.8201 | Val Acc: 71.19% | Val F1: 0.7267 | Val AUC: 0.9248\n",
      "Epoch: 04/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.5398 | Train Acc: 85.50%\n",
      "\tVal Loss: 0.7163 | Val Acc: 76.24% | Val F1: 0.7732 | Val AUC: 0.9406\n",
      "Epoch: 05/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.4751 | Train Acc: 87.76%\n",
      "\tVal Loss: 0.6613 | Val Acc: 80.46% | Val F1: 0.8099 | Val AUC: 0.9485\n",
      "Epoch: 06/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.3454 | Train Acc: 91.91%\n",
      "\tVal Loss: 0.6652 | Val Acc: 81.19% | Val F1: 0.8156 | Val AUC: 0.9495\n",
      "Epoch: 07/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.3046 | Train Acc: 93.18%\n",
      "\tVal Loss: 0.6610 | Val Acc: 80.92% | Val F1: 0.8194 | Val AUC: 0.9523\n",
      "Epoch: 08/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.2689 | Train Acc: 93.96%\n",
      "\tVal Loss: 0.6404 | Val Acc: 83.58% | Val F1: 0.8383 | Val AUC: 0.9560\n",
      "Epoch: 09/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.2346 | Train Acc: 94.62%\n",
      "\tVal Loss: 0.7852 | Val Acc: 79.54% | Val F1: 0.8041 | Val AUC: 0.9516\n",
      "Epoch: 10/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.2087 | Train Acc: 95.37%\n",
      "\tVal Loss: 0.8519 | Val Acc: 77.06% | Val F1: 0.7871 | Val AUC: 0.9388\n",
      "Epoch: 11/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.1791 | Train Acc: 96.12%\n",
      "\tVal Loss: 0.6852 | Val Acc: 81.56% | Val F1: 0.8206 | Val AUC: 0.9515\n",
      "Epoch: 12/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.1449 | Train Acc: 96.81%\n",
      "\tVal Loss: 0.6370 | Val Acc: 83.94% | Val F1: 0.8408 | Val AUC: 0.9573\n",
      "Epoch: 13/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.1153 | Train Acc: 97.79%\n",
      "\tVal Loss: 0.6345 | Val Acc: 84.40% | Val F1: 0.8460 | Val AUC: 0.9582\n",
      "Epoch: 14/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.1651 | Train Acc: 95.90%\n",
      "\tVal Loss: 1.3109 | Val Acc: 76.79% | Val F1: 0.7706 | Val AUC: 0.9249\n",
      "Epoch: 15/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.2372 | Train Acc: 94.11%\n",
      "\tVal Loss: 0.6593 | Val Acc: 81.28% | Val F1: 0.8142 | Val AUC: 0.9534\n",
      "Epoch: 16/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.1021 | Train Acc: 98.09%\n",
      "\tVal Loss: 0.6662 | Val Acc: 82.48% | Val F1: 0.8262 | Val AUC: 0.9587\n",
      "Epoch: 17/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0783 | Train Acc: 98.39%\n",
      "\tVal Loss: 0.6407 | Val Acc: 83.21% | Val F1: 0.8334 | Val AUC: 0.9602\n",
      "Epoch: 18/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0632 | Train Acc: 98.86%\n",
      "\tVal Loss: 0.6438 | Val Acc: 82.57% | Val F1: 0.8293 | Val AUC: 0.9614\n",
      "Epoch: 19/100 | Time: 0m 1s\n",
      "\tTrain Loss: 0.0558 | Train Acc: 99.05%\n",
      "\tVal Loss: 0.6570 | Val Acc: 83.12% | Val F1: 0.8314 | Val AUC: 0.9606\n",
      "Epoch: 20/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0500 | Train Acc: 98.96%\n",
      "\tVal Loss: 0.7059 | Val Acc: 82.39% | Val F1: 0.8256 | Val AUC: 0.9583\n",
      "Epoch: 21/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0547 | Train Acc: 98.70%\n",
      "\tVal Loss: 0.9250 | Val Acc: 82.02% | Val F1: 0.8244 | Val AUC: 0.9535\n",
      "Epoch: 22/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.1106 | Train Acc: 97.51%\n",
      "\tVal Loss: 0.7338 | Val Acc: 82.66% | Val F1: 0.8293 | Val AUC: 0.9566\n",
      "Epoch: 23/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0576 | Train Acc: 98.78%\n",
      "\tVal Loss: 0.7141 | Val Acc: 83.03% | Val F1: 0.8330 | Val AUC: 0.9575\n",
      "\t>>> Early stopping at epoch 23, best val acc: 84.40%\n",
      "\n",
      ">>> Training completed! Best validation accuracy: 84.40%\n",
      "    Best validation F1: 0.8460\n",
      "    Best validation AUC-ROC: 0.9582\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Running Simple RNN (mean pooling) experiment: Weighted Sampling\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      ">>> Training Simple RNN (Weighted Sampling)\n",
      "    Parameters: 2,850,446\n",
      "    Max epochs: 100, Patience: 10\n",
      "Epoch: 01/100 | Time: 0m 0s\n",
      "\tTrain Loss: 1.7030 | Train Acc: 28.34%\n",
      "\tVal Loss: 1.6006 | Val Acc: 31.19% | Val F1: 0.2627 | Val AUC: 0.7244\n",
      "Epoch: 02/100 | Time: 0m 0s\n",
      "\tTrain Loss: 1.2449 | Train Acc: 52.09%\n",
      "\tVal Loss: 1.2641 | Val Acc: 47.61% | Val F1: 0.4473 | Val AUC: 0.8262\n",
      "Epoch: 03/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.9088 | Train Acc: 66.83%\n",
      "\tVal Loss: 1.0611 | Val Acc: 62.29% | Val F1: 0.6244 | Val AUC: 0.8728\n",
      "Epoch: 04/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.6742 | Train Acc: 78.84%\n",
      "\tVal Loss: 0.8953 | Val Acc: 69.17% | Val F1: 0.7147 | Val AUC: 0.9144\n",
      "Epoch: 05/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.5844 | Train Acc: 83.61%\n",
      "\tVal Loss: 0.8513 | Val Acc: 74.31% | Val F1: 0.7513 | Val AUC: 0.9230\n",
      "Epoch: 06/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.3898 | Train Acc: 90.78%\n",
      "\tVal Loss: 0.7775 | Val Acc: 75.14% | Val F1: 0.7616 | Val AUC: 0.9318\n",
      "Epoch: 07/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.3725 | Train Acc: 90.35%\n",
      "\tVal Loss: 0.7853 | Val Acc: 75.50% | Val F1: 0.7666 | Val AUC: 0.9332\n",
      "Epoch: 08/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.2904 | Train Acc: 93.35%\n",
      "\tVal Loss: 0.7454 | Val Acc: 77.43% | Val F1: 0.7812 | Val AUC: 0.9380\n",
      "Epoch: 09/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.2521 | Train Acc: 94.70%\n",
      "\tVal Loss: 0.7787 | Val Acc: 76.88% | Val F1: 0.7754 | Val AUC: 0.9371\n",
      "Epoch: 10/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.2590 | Train Acc: 93.90%\n",
      "\tVal Loss: 0.8643 | Val Acc: 77.80% | Val F1: 0.7830 | Val AUC: 0.9322\n",
      "Epoch: 11/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.1955 | Train Acc: 96.49%\n",
      "\tVal Loss: 0.7887 | Val Acc: 78.53% | Val F1: 0.7913 | Val AUC: 0.9368\n",
      "Epoch: 12/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.1548 | Train Acc: 97.48%\n",
      "\tVal Loss: 0.7715 | Val Acc: 79.27% | Val F1: 0.7948 | Val AUC: 0.9399\n",
      "Epoch: 13/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.1366 | Train Acc: 97.75%\n",
      "\tVal Loss: 0.8070 | Val Acc: 78.17% | Val F1: 0.7840 | Val AUC: 0.9385\n",
      "Epoch: 14/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.1174 | Train Acc: 98.07%\n",
      "\tVal Loss: 0.7937 | Val Acc: 80.55% | Val F1: 0.8073 | Val AUC: 0.9409\n",
      "Epoch: 15/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.1336 | Train Acc: 97.18%\n",
      "\tVal Loss: 0.9033 | Val Acc: 77.43% | Val F1: 0.7747 | Val AUC: 0.9353\n",
      "Epoch: 16/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.1085 | Train Acc: 98.17%\n",
      "\tVal Loss: 0.9373 | Val Acc: 78.90% | Val F1: 0.7931 | Val AUC: 0.9368\n",
      "Epoch: 17/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.1952 | Train Acc: 95.83%\n",
      "\tVal Loss: 0.9407 | Val Acc: 76.79% | Val F1: 0.7731 | Val AUC: 0.9347\n",
      "Epoch: 18/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.1207 | Train Acc: 97.64%\n",
      "\tVal Loss: 0.8167 | Val Acc: 80.28% | Val F1: 0.8031 | Val AUC: 0.9421\n",
      "Epoch: 19/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0731 | Train Acc: 98.88%\n",
      "\tVal Loss: 0.9162 | Val Acc: 79.08% | Val F1: 0.7941 | Val AUC: 0.9405\n",
      "Epoch: 20/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0869 | Train Acc: 98.44%\n",
      "\tVal Loss: 0.9089 | Val Acc: 78.90% | Val F1: 0.7910 | Val AUC: 0.9391\n",
      "Epoch: 21/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0728 | Train Acc: 98.76%\n",
      "\tVal Loss: 0.8694 | Val Acc: 79.17% | Val F1: 0.7934 | Val AUC: 0.9408\n",
      "Epoch: 22/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0510 | Train Acc: 99.36%\n",
      "\tVal Loss: 0.8856 | Val Acc: 79.17% | Val F1: 0.7935 | Val AUC: 0.9404\n",
      "Epoch: 23/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.1082 | Train Acc: 97.96%\n",
      "\tVal Loss: 0.9350 | Val Acc: 78.62% | Val F1: 0.7918 | Val AUC: 0.9376\n",
      "Epoch: 24/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0553 | Train Acc: 99.15%\n",
      "\tVal Loss: 0.9186 | Val Acc: 80.18% | Val F1: 0.8034 | Val AUC: 0.9378\n",
      "\t>>> Early stopping at epoch 24, best val acc: 80.55%\n",
      "\n",
      ">>> Training completed! Best validation accuracy: 80.55%\n",
      "    Best validation F1: 0.8073\n",
      "    Best validation AUC-ROC: 0.9409\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Running Simple RNN (mean pooling) experiment: Augmentation + Weighted Sampling\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      ">>> Training Simple RNN (Augmentation + Weighted Sampling)\n",
      "    Parameters: 2,850,446\n",
      "    Max epochs: 100, Patience: 10\n",
      "Epoch: 01/100 | Time: 0m 0s\n",
      "\tTrain Loss: 1.5783 | Train Acc: 34.60%\n",
      "\tVal Loss: 1.3182 | Val Acc: 45.41% | Val F1: 0.4302 | Val AUC: 0.8088\n",
      "Epoch: 02/100 | Time: 0m 0s\n",
      "\tTrain Loss: 1.0116 | Train Acc: 64.13%\n",
      "\tVal Loss: 1.0398 | Val Acc: 62.66% | Val F1: 0.6740 | Val AUC: 0.8870\n",
      "Epoch: 03/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.6938 | Train Acc: 79.48%\n",
      "\tVal Loss: 1.0158 | Val Acc: 69.36% | Val F1: 0.7058 | Val AUC: 0.9112\n",
      "Epoch: 04/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.4633 | Train Acc: 88.14%\n",
      "\tVal Loss: 0.7529 | Val Acc: 74.04% | Val F1: 0.7644 | Val AUC: 0.9398\n",
      "Epoch: 05/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.3413 | Train Acc: 92.01%\n",
      "\tVal Loss: 0.7332 | Val Acc: 77.80% | Val F1: 0.7808 | Val AUC: 0.9436\n",
      "Epoch: 06/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.2682 | Train Acc: 94.65%\n",
      "\tVal Loss: 0.7024 | Val Acc: 78.07% | Val F1: 0.7909 | Val AUC: 0.9477\n",
      "Epoch: 07/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.2422 | Train Acc: 94.87%\n",
      "\tVal Loss: 0.6986 | Val Acc: 81.28% | Val F1: 0.8185 | Val AUC: 0.9497\n",
      "Epoch: 08/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.1810 | Train Acc: 96.91%\n",
      "\tVal Loss: 0.7010 | Val Acc: 82.20% | Val F1: 0.8234 | Val AUC: 0.9522\n",
      "Epoch: 09/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.2499 | Train Acc: 94.13%\n",
      "\tVal Loss: 0.6800 | Val Acc: 82.02% | Val F1: 0.8229 | Val AUC: 0.9535\n",
      "Epoch: 10/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.1455 | Train Acc: 97.56%\n",
      "\tVal Loss: 0.7182 | Val Acc: 79.82% | Val F1: 0.8066 | Val AUC: 0.9494\n",
      "Epoch: 11/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.1076 | Train Acc: 98.33%\n",
      "\tVal Loss: 0.9680 | Val Acc: 77.71% | Val F1: 0.7964 | Val AUC: 0.9408\n",
      "Epoch: 12/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.1921 | Train Acc: 95.27%\n",
      "\tVal Loss: 0.7185 | Val Acc: 80.37% | Val F1: 0.8060 | Val AUC: 0.9543\n",
      "Epoch: 13/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0763 | Train Acc: 98.68%\n",
      "\tVal Loss: 0.7163 | Val Acc: 82.66% | Val F1: 0.8277 | Val AUC: 0.9549\n",
      "Epoch: 14/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0587 | Train Acc: 98.96%\n",
      "\tVal Loss: 0.7297 | Val Acc: 83.12% | Val F1: 0.8333 | Val AUC: 0.9554\n",
      "Epoch: 15/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0585 | Train Acc: 98.83%\n",
      "\tVal Loss: 0.8785 | Val Acc: 80.09% | Val F1: 0.8043 | Val AUC: 0.9493\n",
      "Epoch: 16/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0724 | Train Acc: 98.65%\n",
      "\tVal Loss: 0.8386 | Val Acc: 80.09% | Val F1: 0.8096 | Val AUC: 0.9513\n",
      "Epoch: 17/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0808 | Train Acc: 98.14%\n",
      "\tVal Loss: 0.8624 | Val Acc: 81.93% | Val F1: 0.8248 | Val AUC: 0.9484\n",
      "Epoch: 18/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0366 | Train Acc: 99.43%\n",
      "\tVal Loss: 0.8217 | Val Acc: 82.20% | Val F1: 0.8258 | Val AUC: 0.9535\n",
      "Epoch: 19/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0285 | Train Acc: 99.58%\n",
      "\tVal Loss: 0.8335 | Val Acc: 82.20% | Val F1: 0.8278 | Val AUC: 0.9537\n",
      "Epoch: 20/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0288 | Train Acc: 99.55%\n",
      "\tVal Loss: 0.8974 | Val Acc: 80.64% | Val F1: 0.8114 | Val AUC: 0.9510\n",
      "Epoch: 21/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.2046 | Train Acc: 94.78%\n",
      "\tVal Loss: 0.9024 | Val Acc: 78.62% | Val F1: 0.8041 | Val AUC: 0.9527\n",
      "Epoch: 22/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0351 | Train Acc: 99.46%\n",
      "\tVal Loss: 0.8122 | Val Acc: 81.10% | Val F1: 0.8152 | Val AUC: 0.9548\n",
      "Epoch: 23/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0242 | Train Acc: 99.67%\n",
      "\tVal Loss: 0.8465 | Val Acc: 80.18% | Val F1: 0.8166 | Val AUC: 0.9561\n",
      "Epoch: 24/100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.0207 | Train Acc: 99.82%\n",
      "\tVal Loss: 0.8922 | Val Acc: 78.62% | Val F1: 0.8054 | Val AUC: 0.9553\n",
      "\t>>> Early stopping at epoch 24, best val acc: 83.12%\n",
      "\n",
      ">>> Training completed! Best validation accuracy: 83.12%\n",
      "    Best validation F1: 0.8333\n",
      "    Best validation AUC-ROC: 0.9554\n",
      "\n",
      ">>> Simple RNN experiments queued. Run the cells to execute training if needed.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Run Simple RNN + Attention experiments\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n>>> Executing Simple RNN experiments...\")\n",
    "\n",
    "rnn_results_text_aug = run_simple_rnn_experiment(\n",
    "    dataset_key=\"augmented\",\n",
    "    description=\"Text Augmentation\",\n",
    "    save_suffix=\"text_aug\",\n",
    ")\n",
    "\n",
    "rnn_results_weighted = run_simple_rnn_experiment(\n",
    "    dataset_key=\"weighted\",\n",
    "    description=\"Weighted Sampling\",\n",
    "    save_suffix=\"weighted_sampler\",\n",
    ")\n",
    "\n",
    "rnn_results_aug_weighted = run_simple_rnn_experiment(\n",
    "    dataset_key=\"augmented_weighted\",\n",
    "    description=\"Augmentation + Weighted Sampling\",\n",
    "    save_suffix=\"text_aug_weighted\",\n",
    ")\n",
    "\n",
    "print(\"\\n>>> Simple RNN experiments queued. Run the cells to execute training if needed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "426bd88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Topic-wise accuracy for Simple RNN variants\n",
      "\n",
      "Simple RNN (Text Augmentation)\n",
      "------------------------------\n",
      "Topic      Accuracy %   Correct    Total     \n",
      "---------------------------------------------\n",
      "ABBR       77.78        7          9         \n",
      "DESC       86.23        119        138       \n",
      "ENTY       67.02        63         94        \n",
      "HUM        87.69        57         65        \n",
      "LOC        87.65        71         81        \n",
      "NUM        86.73        98         113       \n",
      "---------------------------------------------\n",
      "Topic      0.83         415        500       \n",
      "\n",
      "Simple RNN (Weighted Sampling)\n",
      "------------------------------\n",
      "Topic      Accuracy %   Correct    Total     \n",
      "---------------------------------------------\n",
      "ABBR       77.78        7          9         \n",
      "DESC       96.38        133        138       \n",
      "ENTY       71.28        67         94        \n",
      "HUM        80.00        52         65        \n",
      "LOC        90.12        73         81        \n",
      "NUM        85.84        97         113       \n",
      "---------------------------------------------\n",
      "Topic      0.86         429        500       \n",
      "\n",
      "Simple RNN (Augmentation + Weighted Sampling)\n",
      "---------------------------------------------\n",
      "Topic      Accuracy %   Correct    Total     \n",
      "---------------------------------------------\n",
      "ABBR       100.00       9          9         \n",
      "DESC       42.03        58         138       \n",
      "ENTY       67.02        63         94        \n",
      "HUM        80.00        52         65        \n",
      "LOC        87.65        71         81        \n",
      "NUM        79.65        90         113       \n",
      "---------------------------------------------\n",
      "Topic      0.69         343        500       \n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Topic-wise accuracy summary for Simple RNN experiments\n",
    "# ============================================================================\n",
    "\n",
    "def display_topic_metrics(title, metrics_dict):\n",
    "    print(f\"\\n{title}\")\n",
    "    print(\"-\" * len(title))\n",
    "    header = f\"{'Topic':<10} {'Accuracy %':<12} {'Correct':<10} {'Total':<10}\"\n",
    "    print(header)\n",
    "    print(\"-\" * len(header))\n",
    "    for topic in sorted(metrics_dict.keys()):\n",
    "        stats = metrics_dict[topic]\n",
    "        acc_pct = stats['accuracy'] * 100\n",
    "        print(f\"{topic:<10} {acc_pct:<12.2f} {stats['correct']:<10} {stats['total']:<10}\")\n",
    "    print(\"-\" * len(header))\n",
    "    \n",
    "    total_cor = sum([metrics_dict[topic]['correct'] for topic in metrics_dict])\n",
    "    total_sam = sum([metrics_dict[topic]['total'] for topic in metrics_dict])\n",
    "    total_acc = total_cor / total_sam \n",
    "    print(f\"{'Topic':<10} {total_acc:<12.2f} {total_cor:<10} {total_sam:<10}\")\n",
    "\n",
    "print(\"\\n>>> Topic-wise accuracy for Simple RNN variants\")\n",
    "for run_key, info in p35_results[\"simple_rnn_baseline\"].items():\n",
    "    topic_metrics = info.get(\"topic_metrics\")\n",
    "    if not topic_metrics:\n",
    "        continue\n",
    "    title = f\"Simple RNN ({info['description']})\"\n",
    "    display_topic_metrics(title, topic_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e096065b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# RNN + BERT experiment runner\n",
    "# ============================================================================\n",
    "BERT_HYPERPARAM = hyperparam_prep()\n",
    "BERT_HYPERPARAM['LEARNING_RATE'] = 2e-5\n",
    "BERT_HYPERPARAM['OTHER_LR'] = BERT_HYPERPARAM['LEARNING_RATE'] * 10\n",
    "BERT_HYPERPARAM['N_EPOCHS'] //= 2\n",
    "BERT_HYPERPARAM['MODEL_NAME'] = 'bert-base-uncased'\n",
    "BERT_HYPERPARAM['FREEZE'] = False\n",
    "BERT_HYPERPARAM['SAVE_MODEL'] = True\n",
    "\n",
    "\n",
    "def run_rnn_bert_experiment(dataset_key, description, save_suffix, TEXT, freeze_bert=BERT_HYPERPARAM['FREEZE']):\n",
    "    if not BERT_AVAILABLE:\n",
    "        raise RuntimeError(\"Transformers library is unavailable; cannot run RNN+BERT experiments.\")\n",
    "    if dataset_key not in p35_datasets:\n",
    "        raise ValueError(f\"Unknown dataset key: {dataset_key}\")\n",
    "\n",
    "    reset_random_seeds(SEED)\n",
    "    train_dataset = p35_datasets[dataset_key]\n",
    "\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(f\"Running RNN + BERT experiment: {description}\")\n",
    "    print(\"-\"*80)\n",
    "\n",
    "    train_iter = build_iterator(train_dataset, BERT_HYPERPARAM['BATCH_SIZE'], shuffle=True)\n",
    "    val_iter = build_iterator(validation_data, BERT_HYPERPARAM['BATCH_SIZE'], shuffle=False)\n",
    "    test_iter = build_iterator(test_data, BERT_HYPERPARAM['BATCH_SIZE'], shuffle=False)\n",
    "\n",
    "    model = RNNBertClassifier(\n",
    "        output_dim=num_classes,\n",
    "        hidden_dim=BERT_HYPERPARAM['HIDDEN_DIM'],\n",
    "        n_layers=BERT_HYPERPARAM['N_LAYERS'],\n",
    "        dropout=BERT_HYPERPARAM['DROPOUT'],\n",
    "        bert_model_name=BERT_HYPERPARAM['MODEL_NAME'],\n",
    "        freeze_bert=freeze_bert,\n",
    "    ).to(device)\n",
    "\n",
    "    bert_params = [p for p in model.bert_model.parameters() if p.requires_grad]\n",
    "    other_params = [p for n, p in model.named_parameters() if 'bert_model' not in n]\n",
    "\n",
    "    optimizer_grouped_parameters = []\n",
    "    if bert_params:\n",
    "        optimizer_grouped_parameters.append({'params': bert_params, 'lr': BERT_HYPERPARAM['LEARNING_RATE']})\n",
    "    if other_params:\n",
    "        optimizer_grouped_parameters.append({'params': other_params, 'lr': BERT_HYPERPARAM['OTHER_LR']})\n",
    "\n",
    "    # Select optimizer with best learning rate\n",
    "    if BERT_HYPERPARAM['OPTIMIZER'] == 'Adam':\n",
    "        optimizer = optim.Adam(optimizer_grouped_parameters, weight_decay=BERT_HYPERPARAM['L2_LAMBDA'])\n",
    "    elif BERT_HYPERPARAM['OPTIMIZER'] == 'SGD':\n",
    "        optimizer = optim.SGD(optimizer_grouped_parameters, momentum=0.9, weight_decay=BERT_HYPERPARAM['L2_LAMBDA'])\n",
    "    elif BERT_HYPERPARAM['OPTIMIZER'] == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(optimizer_grouped_parameters, weight_decay=BERT_HYPERPARAM['L2_LAMBDA'])\n",
    "    elif BERT_HYPERPARAM['OPTIMIZER'] == 'Adagrad':\n",
    "        optimizer = optim.Adagrad(optimizer_grouped_parameters, weight_decay=BERT_HYPERPARAM['L2_LAMBDA'])\n",
    "\n",
    "    model, history = train_model_with_history_bert(\n",
    "        model,\n",
    "        train_iter,\n",
    "        val_iter,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        BERT_HYPERPARAM['N_EPOCHS'],\n",
    "        device,\n",
    "        num_classes,\n",
    "        BERT_HYPERPARAM['L1_LAMBDA'],\n",
    "        patience=BERT_HYPERPARAM['PATIENCE'],\n",
    "        model_name=f\"RNN+BERT ({description})\",\n",
    "        text_vocab=TEXT.vocab.itos,\n",
    "    )\n",
    "\n",
    "    test_loss, test_acc, test_f1, test_auc = evaluate_model_bert(\n",
    "        model,\n",
    "        test_iter,\n",
    "        criterion,\n",
    "        device,\n",
    "        f\"RNN+BERT ({description})\",\n",
    "        num_classes,\n",
    "        text_vocab=TEXT.vocab.itos,\n",
    "    )\n",
    "\n",
    "    topic_metrics = evaluate_per_topic_p35(model, test_iter, device, TEXT.vocab.itos)\n",
    "\n",
    "    model_path = f\"weights/part35_rnn_bert_{save_suffix}.pt\"\n",
    "    if BERT_HYPERPARAM['SAVE_MODEL']:\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    p35_results[\"rnn_bert\"][save_suffix] = {\n",
    "        \"description\": description,\n",
    "        \"dataset_key\": dataset_key,\n",
    "        \"history\": history,\n",
    "        \"test_metrics\": {\n",
    "            \"loss\": test_loss,\n",
    "            \"accuracy\": test_acc,\n",
    "            \"f1\": test_f1,\n",
    "            \"auc\": test_auc,\n",
    "        },\n",
    "        \"topic_metrics\": topic_metrics,\n",
    "        \"model_path\": model_path if BERT_HYPERPARAM['SAVE_MODEL'] else None,\n",
    "        \"freeze_bert\": freeze_bert,\n",
    "        \"model\": model,\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"history\": history,\n",
    "        \"topic_metrics\": topic_metrics,\n",
    "        \"test_metrics\": {\n",
    "            \"loss\": test_loss,\n",
    "            \"accuracy\": test_acc,\n",
    "            \"f1\": test_f1,\n",
    "            \"auc\": test_auc,\n",
    "        },\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c9059c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Executing RNN + BERT experiments...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Running RNN + BERT experiment: Text Augmentation\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      ">>> Training RNN+BERT (Text Augmentation)\n",
      "    Parameters: 113,295,111\n",
      "    Max epochs: 50, Patience: 10\n",
      "Epoch: 01/50 | Time: 0m 19s\n",
      "\tTrain Loss: 0.7184 | Train Acc: 74.15%\n",
      "\tVal Loss: 0.4994 | Val Acc: 86.33% | Val F1: 0.8724 | Val AUC: 0.9749\n",
      "Epoch: 02/50 | Time: 0m 20s\n",
      "\tTrain Loss: 0.1680 | Train Acc: 95.30%\n",
      "\tVal Loss: 0.4237 | Val Acc: 89.08% | Val F1: 0.8952 | Val AUC: 0.9839\n",
      "Epoch: 03/50 | Time: 0m 19s\n",
      "\tTrain Loss: 0.1031 | Train Acc: 97.44%\n",
      "\tVal Loss: 0.7240 | Val Acc: 86.88% | Val F1: 0.8804 | Val AUC: 0.9655\n",
      "Epoch: 04/50 | Time: 0m 19s\n",
      "\tTrain Loss: 0.0555 | Train Acc: 98.75%\n",
      "\tVal Loss: 0.7127 | Val Acc: 88.53% | Val F1: 0.8943 | Val AUC: 0.9772\n",
      "Epoch: 05/50 | Time: 0m 19s\n",
      "\tTrain Loss: 0.0393 | Train Acc: 99.11%\n",
      "\tVal Loss: 0.6487 | Val Acc: 88.81% | Val F1: 0.8971 | Val AUC: 0.9760\n",
      "Epoch: 06/50 | Time: 0m 20s\n",
      "\tTrain Loss: 0.0222 | Train Acc: 99.52%\n",
      "\tVal Loss: 0.8417 | Val Acc: 88.17% | Val F1: 0.8916 | Val AUC: 0.9732\n",
      "Epoch: 07/50 | Time: 0m 18s\n",
      "\tTrain Loss: 0.0173 | Train Acc: 99.60%\n",
      "\tVal Loss: 0.8212 | Val Acc: 88.17% | Val F1: 0.8896 | Val AUC: 0.9728\n",
      "Epoch: 08/50 | Time: 0m 19s\n",
      "\tTrain Loss: 0.0116 | Train Acc: 99.73%\n",
      "\tVal Loss: 0.9979 | Val Acc: 87.06% | Val F1: 0.8811 | Val AUC: 0.9729\n",
      "Epoch: 09/50 | Time: 0m 21s\n",
      "\tTrain Loss: 0.0162 | Train Acc: 99.70%\n",
      "\tVal Loss: 0.8393 | Val Acc: 88.35% | Val F1: 0.8921 | Val AUC: 0.9771\n",
      "Epoch: 10/50 | Time: 0m 22s\n",
      "\tTrain Loss: 0.0169 | Train Acc: 99.67%\n",
      "\tVal Loss: 0.9558 | Val Acc: 87.61% | Val F1: 0.8870 | Val AUC: 0.9718\n",
      "Epoch: 11/50 | Time: 0m 22s\n",
      "\tTrain Loss: 0.0061 | Train Acc: 99.87%\n",
      "\tVal Loss: 0.8356 | Val Acc: 88.72% | Val F1: 0.8964 | Val AUC: 0.9739\n",
      "Epoch: 12/50 | Time: 0m 21s\n",
      "\tTrain Loss: 0.0123 | Train Acc: 99.75%\n",
      "\tVal Loss: 1.0066 | Val Acc: 88.44% | Val F1: 0.8946 | Val AUC: 0.9716\n",
      "\t>>> Early stopping at epoch 12, best val acc: 89.08%\n",
      "\n",
      ">>> Training completed! Best validation accuracy: 89.08%\n",
      "    Best validation F1: 0.8952\n",
      "    Best validation AUC-ROC: 0.9839\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Running RNN + BERT experiment: Weighted Sampling\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      ">>> Training RNN+BERT (Weighted Sampling)\n",
      "    Parameters: 113,295,111\n",
      "    Max epochs: 50, Patience: 10\n",
      "Epoch: 01/50 | Time: 0m 16s\n",
      "\tTrain Loss: 0.8443 | Train Acc: 70.75%\n",
      "\tVal Loss: 0.5176 | Val Acc: 83.85% | Val F1: 0.8465 | Val AUC: 0.9679\n",
      "Epoch: 02/50 | Time: 0m 16s\n",
      "\tTrain Loss: 0.1584 | Train Acc: 95.71%\n",
      "\tVal Loss: 0.5424 | Val Acc: 88.72% | Val F1: 0.8898 | Val AUC: 0.9722\n",
      "Epoch: 03/50 | Time: 0m 16s\n",
      "\tTrain Loss: 0.0680 | Train Acc: 98.44%\n",
      "\tVal Loss: 0.5320 | Val Acc: 90.28% | Val F1: 0.9028 | Val AUC: 0.9782\n",
      "Epoch: 04/50 | Time: 0m 16s\n",
      "\tTrain Loss: 0.0355 | Train Acc: 98.99%\n",
      "\tVal Loss: 0.7156 | Val Acc: 88.72% | Val F1: 0.8874 | Val AUC: 0.9697\n",
      "Epoch: 05/50 | Time: 0m 16s\n",
      "\tTrain Loss: 0.0193 | Train Acc: 99.45%\n",
      "\tVal Loss: 0.7042 | Val Acc: 89.36% | Val F1: 0.8930 | Val AUC: 0.9741\n",
      "Epoch: 06/50 | Time: 0m 16s\n",
      "\tTrain Loss: 0.0169 | Train Acc: 99.66%\n",
      "\tVal Loss: 0.8089 | Val Acc: 89.63% | Val F1: 0.8971 | Val AUC: 0.9750\n",
      "Epoch: 07/50 | Time: 0m 15s\n",
      "\tTrain Loss: 0.0112 | Train Acc: 99.75%\n",
      "\tVal Loss: 0.9849 | Val Acc: 86.97% | Val F1: 0.8792 | Val AUC: 0.9682\n",
      "Epoch: 08/50 | Time: 0m 16s\n",
      "\tTrain Loss: 0.0073 | Train Acc: 99.91%\n",
      "\tVal Loss: 1.0264 | Val Acc: 86.24% | Val F1: 0.8693 | Val AUC: 0.9685\n",
      "Epoch: 09/50 | Time: 0m 15s\n",
      "\tTrain Loss: 0.0170 | Train Acc: 99.66%\n",
      "\tVal Loss: 0.8983 | Val Acc: 89.17% | Val F1: 0.8948 | Val AUC: 0.9726\n",
      "Epoch: 10/50 | Time: 0m 15s\n",
      "\tTrain Loss: 0.0121 | Train Acc: 99.68%\n",
      "\tVal Loss: 1.0296 | Val Acc: 88.53% | Val F1: 0.8892 | Val AUC: 0.9693\n",
      "Epoch: 11/50 | Time: 0m 15s\n",
      "\tTrain Loss: 0.0063 | Train Acc: 99.91%\n",
      "\tVal Loss: 1.1189 | Val Acc: 87.98% | Val F1: 0.8830 | Val AUC: 0.9697\n",
      "Epoch: 12/50 | Time: 0m 16s\n",
      "\tTrain Loss: 0.0117 | Train Acc: 99.72%\n",
      "\tVal Loss: 0.9787 | Val Acc: 88.99% | Val F1: 0.8906 | Val AUC: 0.9724\n",
      "Epoch: 13/50 | Time: 0m 16s\n",
      "\tTrain Loss: 0.0222 | Train Acc: 99.63%\n",
      "\tVal Loss: 1.0592 | Val Acc: 88.26% | Val F1: 0.8871 | Val AUC: 0.9534\n",
      "\t>>> Early stopping at epoch 13, best val acc: 90.28%\n",
      "\n",
      ">>> Training completed! Best validation accuracy: 90.28%\n",
      "    Best validation F1: 0.9028\n",
      "    Best validation AUC-ROC: 0.9782\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Running RNN + BERT experiment: Augmentation + Weighted Sampling\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      ">>> Training RNN+BERT (Augmentation + Weighted Sampling)\n",
      "    Parameters: 113,295,111\n",
      "    Max epochs: 50, Patience: 10\n",
      "Epoch: 01/50 | Time: 0m 20s\n",
      "\tTrain Loss: 0.6622 | Train Acc: 77.49%\n",
      "\tVal Loss: 0.4877 | Val Acc: 87.80% | Val F1: 0.8817 | Val AUC: 0.9754\n",
      "Epoch: 02/50 | Time: 0m 20s\n",
      "\tTrain Loss: 0.1358 | Train Acc: 96.62%\n",
      "\tVal Loss: 0.5366 | Val Acc: 88.44% | Val F1: 0.8903 | Val AUC: 0.9779\n",
      "Epoch: 03/50 | Time: 0m 21s\n",
      "\tTrain Loss: 0.0544 | Train Acc: 98.75%\n",
      "\tVal Loss: 0.7787 | Val Acc: 86.61% | Val F1: 0.8771 | Val AUC: 0.9760\n",
      "Epoch: 04/50 | Time: 0m 20s\n",
      "\tTrain Loss: 0.0372 | Train Acc: 99.25%\n",
      "\tVal Loss: 0.7228 | Val Acc: 87.80% | Val F1: 0.8868 | Val AUC: 0.9743\n",
      "Epoch: 05/50 | Time: 0m 19s\n",
      "\tTrain Loss: 0.0172 | Train Acc: 99.67%\n",
      "\tVal Loss: 1.0961 | Val Acc: 86.51% | Val F1: 0.8777 | Val AUC: 0.9621\n",
      "Epoch: 06/50 | Time: 0m 20s\n",
      "\tTrain Loss: 0.0114 | Train Acc: 99.78%\n",
      "\tVal Loss: 0.8958 | Val Acc: 88.07% | Val F1: 0.8889 | Val AUC: 0.9756\n",
      "Epoch: 07/50 | Time: 0m 19s\n",
      "\tTrain Loss: 0.0121 | Train Acc: 99.72%\n",
      "\tVal Loss: 0.8868 | Val Acc: 88.99% | Val F1: 0.8978 | Val AUC: 0.9713\n",
      "Epoch: 08/50 | Time: 0m 19s\n",
      "\tTrain Loss: 0.0140 | Train Acc: 99.73%\n",
      "\tVal Loss: 1.2963 | Val Acc: 86.24% | Val F1: 0.8775 | Val AUC: 0.9688\n",
      "Epoch: 09/50 | Time: 0m 19s\n",
      "\tTrain Loss: 0.0181 | Train Acc: 99.58%\n",
      "\tVal Loss: 0.9976 | Val Acc: 86.51% | Val F1: 0.8752 | Val AUC: 0.9741\n",
      "Epoch: 10/50 | Time: 0m 19s\n",
      "\tTrain Loss: 0.0117 | Train Acc: 99.82%\n",
      "\tVal Loss: 1.0883 | Val Acc: 86.97% | Val F1: 0.8773 | Val AUC: 0.9671\n",
      "Epoch: 11/50 | Time: 0m 20s\n",
      "\tTrain Loss: 0.0114 | Train Acc: 99.78%\n",
      "\tVal Loss: 1.0083 | Val Acc: 87.89% | Val F1: 0.8886 | Val AUC: 0.9708\n",
      "Epoch: 12/50 | Time: 0m 19s\n",
      "\tTrain Loss: 0.0080 | Train Acc: 99.82%\n",
      "\tVal Loss: 1.0211 | Val Acc: 88.26% | Val F1: 0.8870 | Val AUC: 0.9692\n",
      "Epoch: 13/50 | Time: 0m 20s\n",
      "\tTrain Loss: 0.0157 | Train Acc: 99.75%\n",
      "\tVal Loss: 1.1332 | Val Acc: 87.25% | Val F1: 0.8840 | Val AUC: 0.9677\n",
      "Epoch: 14/50 | Time: 0m 21s\n",
      "\tTrain Loss: 0.0123 | Train Acc: 99.73%\n",
      "\tVal Loss: 0.9707 | Val Acc: 88.44% | Val F1: 0.8929 | Val AUC: 0.9755\n",
      "Epoch: 15/50 | Time: 0m 21s\n",
      "\tTrain Loss: 0.0160 | Train Acc: 99.65%\n",
      "\tVal Loss: 0.9454 | Val Acc: 87.34% | Val F1: 0.8834 | Val AUC: 0.9614\n",
      "Epoch: 16/50 | Time: 0m 20s\n",
      "\tTrain Loss: 0.0199 | Train Acc: 99.70%\n",
      "\tVal Loss: 1.0797 | Val Acc: 87.43% | Val F1: 0.8864 | Val AUC: 0.9657\n",
      "Epoch: 17/50 | Time: 0m 21s\n",
      "\tTrain Loss: 0.0189 | Train Acc: 99.75%\n",
      "\tVal Loss: 0.8684 | Val Acc: 88.53% | Val F1: 0.8925 | Val AUC: 0.9766\n",
      "\t>>> Early stopping at epoch 17, best val acc: 88.99%\n",
      "\n",
      ">>> Training completed! Best validation accuracy: 88.99%\n",
      "    Best validation F1: 0.8978\n",
      "    Best validation AUC-ROC: 0.9713\n",
      "\n",
      ">>> RNN + BERT experiments queued. Run the cells to execute training if needed.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Run RNN + BERT experiments\n",
    "# ============================================================================\n",
    "\n",
    "if BERT_AVAILABLE:\n",
    "    print(\"\\n>>> Executing RNN + BERT experiments...\")\n",
    "\n",
    "    bert_results_text_aug = run_rnn_bert_experiment(\n",
    "        dataset_key=\"augmented\",\n",
    "        description=\"Text Augmentation\",\n",
    "        save_suffix=\"text_aug\",\n",
    "        TEXT=TEXT,\n",
    "    )\n",
    "\n",
    "    bert_results_weighted = run_rnn_bert_experiment(\n",
    "        dataset_key=\"weighted\",\n",
    "        description=\"Weighted Sampling\",\n",
    "        save_suffix=\"weighted_sampler\",\n",
    "        TEXT=TEXT,\n",
    "    )\n",
    "\n",
    "    bert_results_aug_weighted = run_rnn_bert_experiment(\n",
    "        dataset_key=\"augmented_weighted\",\n",
    "        description=\"Augmentation + Weighted Sampling\",\n",
    "        save_suffix=\"text_aug_weighted\",\n",
    "        TEXT=TEXT,\n",
    "    )\n",
    "\n",
    "    print(\"\\n>>> RNN + BERT experiments queued. Run the cells to execute training if needed.\")\n",
    "else:\n",
    "    print(\"\\n>>> Skipping RNN + BERT experiments (transformers library unavailable).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d1ce0d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Topic-wise accuracy for RNN + BERT variants\n",
      "\n",
      "RNN + BERT (Text Augmentation)\n",
      "------------------------------\n",
      "Topic      Accuracy %   Correct    Total     \n",
      "---------------------------------------------\n",
      "ABBR       88.89        8          9         \n",
      "DESC       34.06        47         138       \n",
      "ENTY       86.17        81         94        \n",
      "HUM        95.38        62         65        \n",
      "LOC        98.77        80         81        \n",
      "NUM        94.69        107        113       \n",
      "---------------------------------------------\n",
      "Topic      0.77         385        500       \n",
      "\n",
      "RNN + BERT (Weighted Sampling)\n",
      "------------------------------\n",
      "Topic      Accuracy %   Correct    Total     \n",
      "---------------------------------------------\n",
      "ABBR       77.78        7          9         \n",
      "DESC       83.33        115        138       \n",
      "ENTY       86.17        81         94        \n",
      "HUM        92.31        60         65        \n",
      "LOC        97.53        79         81        \n",
      "NUM        93.81        106        113       \n",
      "---------------------------------------------\n",
      "Topic      0.90         448        500       \n",
      "\n",
      "RNN + BERT (Augmentation + Weighted Sampling)\n",
      "---------------------------------------------\n",
      "Topic      Accuracy %   Correct    Total     \n",
      "---------------------------------------------\n",
      "ABBR       88.89        8          9         \n",
      "DESC       40.58        56         138       \n",
      "ENTY       78.72        74         94        \n",
      "HUM        93.85        61         65        \n",
      "LOC        98.77        80         81        \n",
      "NUM        92.92        105        113       \n",
      "---------------------------------------------\n",
      "Topic      0.77         384        500       \n"
     ]
    }
   ],
   "source": [
    "# =========================================================================\n",
    "# Topic-wise accuracy summary for RNN + BERT experiments\n",
    "# =========================================================================\n",
    "\n",
    "if p35_results[\"rnn_bert\"]:\n",
    "    print(\"\\n>>> Topic-wise accuracy for RNN + BERT variants\")\n",
    "    for key, info in p35_results[\"rnn_bert\"].items():\n",
    "        topic_metrics = info.get(\"topic_metrics\")\n",
    "        if not topic_metrics:\n",
    "            continue\n",
    "        title = f\"RNN + BERT ({info['description']})\"\n",
    "        display_topic_metrics(title, topic_metrics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f9b5ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PART 3.4: TEXT AUGMENTATION VS WEIGHTED SAMPLING\n",
      "================================================================================\n",
      "\n",
      ">>> Dataset Variants Used:\n",
      "  - original: 4362 samples\n",
      "  - augmented: 5980 samples\n",
      "  - weighted: 4362 samples\n",
      "  - augmented_weighted: 5980 samples\n",
      "\n",
      ">>> Simple RNN (mean pooling) Experiments:\n",
      "  Text Augmentation [text_aug]\n",
      "    - Test Accuracy: 83.00%\n",
      "    - Test F1: 0.8382\n",
      "    - Test AUC: 0.9586\n",
      "    - Test Loss: 0.6631\n",
      "    - Weakest Topic: ENTY (67.02%)\n",
      "    - Model saved to: weights/part35_simple_rnn_text_aug.pt\n",
      "  Weighted Sampling [weighted_sampler]\n",
      "    - Test Accuracy: 85.80%\n",
      "    - Test F1: 0.8570\n",
      "    - Test AUC: 0.9565\n",
      "    - Test Loss: 0.5815\n",
      "    - Weakest Topic: ENTY (71.28%)\n",
      "    - Model saved to: weights/part35_simple_rnn_weighted_sampler.pt\n",
      "  Augmentation + Weighted Sampling [text_aug_weighted]\n",
      "    - Test Accuracy: 68.60%\n",
      "    - Test F1: 0.7295\n",
      "    - Test AUC: 0.9464\n",
      "    - Test Loss: 0.7742\n",
      "    - Weakest Topic: DESC (42.03%)\n",
      "    - Model saved to: weights/part35_simple_rnn_text_aug_weighted.pt\n",
      "\n",
      ">>> RNN + BERT Experiments:\n",
      "  Text Augmentation [text_aug]\n",
      "    - Test Accuracy: 77.00%\n",
      "    - Test F1: 0.8045\n",
      "    - Test AUC: 0.9689\n",
      "    - Test Loss: 1.9090\n",
      "    - Weakest Topic: DESC (34.06%)\n",
      "    - Model saved to: weights/part35_rnn_bert_text_aug.pt\n",
      "  Weighted Sampling [weighted_sampler]\n",
      "    - Test Accuracy: 89.60%\n",
      "    - Test F1: 0.9059\n",
      "    - Test AUC: 0.9662\n",
      "    - Test Loss: 0.5863\n",
      "    - Weakest Topic: ABBR (77.78%)\n",
      "    - Model saved to: weights/part35_rnn_bert_weighted_sampler.pt\n",
      "  Augmentation + Weighted Sampling [text_aug_weighted]\n",
      "    - Test Accuracy: 76.80%\n",
      "    - Test F1: 0.8061\n",
      "    - Test AUC: 0.9504\n",
      "    - Test Loss: 1.7366\n",
      "    - Weakest Topic: DESC (40.58%)\n",
      "    - Model saved to: weights/part35_rnn_bert_text_aug_weighted.pt\n",
      "\n",
      "================================================================================\n",
      "PART 3.4 SETUP COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PART 3.4 SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 3.4: TEXT AUGMENTATION VS WEIGHTED SAMPLING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n>>> Dataset Variants Used:\")\n",
    "for key in [\"original\", \"augmented\", \"weighted\", \"augmented_weighted\"]:\n",
    "    dataset = p35_datasets[key]\n",
    "    print(f\"  - {key}: {len(dataset.examples)} samples\")\n",
    "\n",
    "print(\"\\n>>> Simple RNN (mean pooling) Experiments:\")\n",
    "if p35_results[\"simple_rnn_baseline\"]:\n",
    "    for key, info in p35_results[\"simple_rnn_baseline\"].items():\n",
    "        metrics = info.get(\"test_metrics\", {})\n",
    "        print(f\"  {info['description']} [{key}]\")\n",
    "        if metrics:\n",
    "            print(f\"    - Test Accuracy: {metrics.get('accuracy', 0)*100:.2f}%\")\n",
    "            print(f\"    - Test F1: {metrics.get('f1', 0):.4f}\")\n",
    "            print(f\"    - Test AUC: {metrics.get('auc', 0):.4f}\")\n",
    "            print(f\"    - Test Loss: {metrics.get('loss', 0):.4f}\")\n",
    "        topic_metrics = info.get(\"topic_metrics\")\n",
    "        if topic_metrics:\n",
    "            weakest = min(topic_metrics.items(), key=lambda kv: kv[1]['accuracy'])\n",
    "            print(f\"    - Weakest Topic: {weakest[0]} ({weakest[1]['accuracy']*100:.2f}%)\")\n",
    "        print(f\"    - Model saved to: {info['model_path']}\")\n",
    "else:\n",
    "    print(\"  - Pending (run the experiment cells above)\")\n",
    "\n",
    "print(\"\\n>>> RNN + BERT Experiments:\")\n",
    "if p35_results[\"rnn_bert\"]:\n",
    "    for key, info in p35_results[\"rnn_bert\"].items():\n",
    "        metrics = info.get(\"test_metrics\", {})\n",
    "        print(f\"  {info['description']} [{key}]\")\n",
    "        if metrics:\n",
    "            print(f\"    - Test Accuracy: {metrics.get('accuracy', 0)*100:.2f}%\")\n",
    "            print(f\"    - Test F1: {metrics.get('f1', 0):.4f}\")\n",
    "            print(f\"    - Test AUC: {metrics.get('auc', 0):.4f}\")\n",
    "            print(f\"    - Test Loss: {metrics.get('loss', 0):.4f}\")\n",
    "        topic_metrics = info.get(\"topic_metrics\")\n",
    "        if topic_metrics:\n",
    "            weakest = min(topic_metrics.items(), key=lambda kv: kv[1]['accuracy'])\n",
    "            print(f\"    - Weakest Topic: {weakest[0]} ({weakest[1]['accuracy']*100:.2f}%)\")\n",
    "        print(f\"    - Model saved to: {info['model_path']}\")\n",
    "else:\n",
    "    print(\"  - Pending (run the experiment cells above)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 3.4 SETUP COMPLETE\")\n",
    "print(\"=\"*80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
