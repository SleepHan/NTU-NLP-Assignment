{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05ac734e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ.setdefault('TORCH_COMPILE_DISABLE', '1')\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "import torch\n",
    "\n",
    "# Method 2: Patch torch._dynamo.disable decorator after import\n",
    "try:\n",
    "    import torch._dynamo\n",
    "    # Patch the disable function to ignore the 'wrapping' parameter\n",
    "    if hasattr(torch._dynamo, 'disable'):\n",
    "        def patched_disable(fn=None, *args, **kwargs):\n",
    "            # Remove problematic 'wrapping' parameter if present\n",
    "            if 'wrapping' in kwargs:\n",
    "                kwargs.pop('wrapping')\n",
    "            if fn is None:\n",
    "                # Decorator usage: @disable\n",
    "                return lambda f: f\n",
    "            # Function usage: disable(fn) or disable(fn, **kwargs)\n",
    "            # Simply return the function unwrapped to avoid recursion\n",
    "            # The original disable was causing issues, so we bypass it entirely\n",
    "            return fn\n",
    "        torch._dynamo.disable = patched_disable\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not patch torch._dynamo: {e}\")\n",
    "    pass  # If patching fails, continue anyway\n",
    "\n",
    "import random, string\n",
    "\n",
    "from torchtext import data , datasets\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "\n",
    "os.environ['GENSIM_DATA_DIR'] = os.path.join(os.getcwd(), 'gensim-data')\n",
    "\n",
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.fasttext import load_facebook_model\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import time, copy\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44e533a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Prepping Data...\n",
      "[+] Test set formed!\n",
      "[+] Train and Validation sets formed!\n",
      "[+] Data prepped successfully!\n",
      "[*] Retrieving pretrained word embeddings...\n",
      "[*] Loading fasttext model...\n",
      "[+] Model loaded!\n",
      "[*] Forming embedding matrix...\n",
      "[+] Embedding matrix formed!\n",
      "[+] Embeddings retrieved successfully!\n",
      "\n",
      "Label distribution in training set:\n",
      "- ABBR: 69 samples (1.58%)\n",
      "- DESC: 930 samples (21.32%)\n",
      "- ENTY: 1000 samples (22.93%)\n",
      "- HUM: 978 samples (22.42%)\n",
      "- LOC: 668 samples (15.31%)\n",
      "- NUM: 717 samples (16.44%)\n",
      "Total samples: 4362, Sum of percentages: 100.00%\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "train_data, validation_data, test_data, LABEL, TEXT, pretrained_embed = data_prep(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c176434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86a01357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of classes: 6\n",
      "Classes: ['ENTY', 'HUM', 'DESC', 'NUM', 'LOC', 'ABBR']\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary for labels\n",
    "LABEL.build_vocab(train_data)\n",
    "num_classes = len(LABEL.vocab)\n",
    "print(f\"\\nNumber of classes: {num_classes}\")\n",
    "print(f\"Classes: {LABEL.vocab.itos}\")\n",
    "\n",
    "# Get pretrained embeddings from Part 1 (frozen embeddings)\n",
    "# TODO: Check if this step is redundant\n",
    "pretrained_embeddings = pretrained_embed.weight.data\n",
    "\n",
    "# Get embedding dimension and vocab size from the fasttext embedding layer\n",
    "embedding_dim = pretrained_embed.weight.shape[1]\n",
    "embedding_vocab_size = pretrained_embed.weight.shape[0]  # Vocab size from saved embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aad6c8e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PART 3.4: TARGETED IMPROVEMENT FOR WEAK TOPICS\n",
      "================================================================================\n",
      "\n",
      "Strategies:\n",
      "  1. Data Augmentation for imbalanced classes (especially ABBR)\n",
      "  2. Positional Embeddings in attention layer\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PART 3.4: Targeted Improvement for Weak Topics\n",
    "# Strategy: Data Augmentation, Positional Embeddings\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 3.4: TARGETED IMPROVEMENT FOR WEAK TOPICS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nStrategies:\")\n",
    "print(\"  1. Data Augmentation for imbalanced classes (especially ABBR)\")\n",
    "print(\"  2. Positional Embeddings in attention layer\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Import required libraries for augmentation\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "try:\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53414583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FastText model\n",
    "# print(\"\\n    Loading FastText model...\")\n",
    "# fatter_fasttext_bin = load_facebook_model('crawl-300d-2M-subword/crawl-300d-2M-subword.bin')\n",
    "# print(\"    ✓ FastText model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c93d87c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ============================================================================\n",
    "# # Step 1: Prepare Enhanced Embeddings with <mask> Token\n",
    "# # ============================================================================\n",
    "\n",
    "# print(\"\\n>>> Step 1: Preparing Enhanced Embeddings with <mask> Token...\")\n",
    "\n",
    "# #TODO: Redundant --> Can switch back to using TEXT, LABEL, train_data\n",
    "# TEXT_P34 = data.Field()\n",
    "# LABEL_P34 = data.LabelField()\n",
    "\n",
    "# train_data_p34 = data.Dataset(train_data.examples, train_data.fields)\n",
    "# TEXT_P34 = copy.deepcopy(TEXT)\n",
    "# LABEL_P34 = copy.deepcopy(LABEL)\n",
    "\n",
    "\n",
    "# #TODO: Add loading of fasttext back here\n",
    "\n",
    "# pretrained_embeddings_p34 = None\n",
    "# embedding_dim_p34 = fatter_fasttext_bin.wv.vector_size\n",
    "\n",
    "# # Build embedding matrix with <mask> token\n",
    "# num_tokens_p34 = len(TEXT_P34.vocab.stoi)\n",
    "# emb_matrix_p34 = np.zeros((num_tokens_p34, embedding_dim_p34), dtype=np.float32)\n",
    "\n",
    "# pad_tok = TEXT_P34.pad_token\n",
    "# unk_tok = TEXT_P34.unk_token\n",
    "# pad_index = TEXT_P34.vocab.stoi[pad_tok]\n",
    "# unk_index = TEXT_P34.vocab.stoi[unk_tok]\n",
    "\n",
    "# known_vecs_p34 = []\n",
    "# mask_candidates = []\n",
    "\n",
    "# print(\"    Processing vocabulary tokens...\")\n",
    "# for idx, token in enumerate(TEXT_P34.vocab.itos):\n",
    "#     # if token in {pad_tok, unk_tok, mask_token}:\n",
    "#     if token in {pad_tok, unk_tok}:\n",
    "#         continue\n",
    "    \n",
    "#     try:\n",
    "#         vec = fatter_fasttext_bin.wv[token]\n",
    "#         emb_matrix_p34[idx] = vec\n",
    "#         known_vecs_p34.append(vec)\n",
    "#         if len(mask_candidates) < 1000:\n",
    "#             mask_candidates.append(vec)\n",
    "#     except KeyError:\n",
    "#         try:\n",
    "#             vec = fatter_fasttext_bin.wv[token.lower()]\n",
    "#             emb_matrix_p34[idx] = vec\n",
    "#             known_vecs_p34.append(vec)\n",
    "#             if len(mask_candidates) < 1000:\n",
    "#                 mask_candidates.append(vec)\n",
    "#         except KeyError:\n",
    "#             emb_matrix_p34[idx] = np.random.uniform(-0.05, 0.05, embedding_dim_p34).astype(np.float32)\n",
    "\n",
    "# # Initialize special tokens\n",
    "# if len(known_vecs_p34) > 0:\n",
    "#     unk_mean_p34 = np.mean(known_vecs_p34, axis=0)\n",
    "#     emb_matrix_p34[unk_index] = unk_mean_p34\n",
    "#     print(f\"    ✓ <unk> initialized as mean of {len(known_vecs_p34)} vectors\")\n",
    "\n",
    "# # if len(mask_candidates) > 0:\n",
    "# #     mask_mean_p34 = np.mean(mask_candidates, axis=0)\n",
    "# #     emb_matrix_p34[mask_index] = mask_mean_p34\n",
    "# #     print(f\"    ✓ <mask> initialized as mean of {len(mask_candidates)} vectors\")\n",
    "# # else:\n",
    "# #     emb_matrix_p34[mask_index] = emb_matrix_p34[unk_index]\n",
    "\n",
    "# emb_matrix_p34[pad_index] = np.zeros(embedding_dim_p34, dtype=np.float32)\n",
    "\n",
    "# # Create and save embedding layer\n",
    "# p34_embedding = torch.nn.Embedding(\n",
    "#     num_tokens_p34, \n",
    "#     embedding_dim_p34, \n",
    "#     padding_idx=pad_index\n",
    "# )\n",
    "# p34_embedding.weight.data.copy_(torch.from_numpy(emb_matrix_p34))\n",
    "# p34_embedding.weight.requires_grad = True\n",
    "\n",
    "# torch.save(p34_embedding, 'embedding_weights_part34_enhanced.pt')\n",
    "# pretrained_embeddings_p34 = p34_embedding.weight.data.clone()\n",
    "\n",
    "# print(f\"    ✓ Embeddings saved (shape: {pretrained_embeddings_p34.shape})\")\n",
    "# print(f\"    ✓ Ready for Part 3.4 training\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c2db80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Step 2: Implementing Data Augmentation Functions...\n",
      "    ✓ Data augmentation functions ready\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Step 2: Data Augmentation Functions for Imbalanced Classes\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n>>> Step 2: Implementing Data Augmentation Functions...\")\n",
    "\n",
    "def get_synonyms(word):\n",
    "    \"\"\"Get synonyms for a word using WordNet\"\"\"\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonym = lemma.name().replace('_', ' ').lower()\n",
    "            if synonym != word and synonym.isalpha():\n",
    "                synonyms.add(synonym)\n",
    "    return list(synonyms)\n",
    "\n",
    "def synonym_replacement(tokens, n=1):\n",
    "    \"\"\"Replace n random words with their synonyms\"\"\"\n",
    "    new_tokens = tokens.copy()\n",
    "    words_to_replace = [i for i, word in enumerate(tokens) if word.isalpha() and len(word) > 2]\n",
    "    \n",
    "    if len(words_to_replace) == 0:\n",
    "        return tokens\n",
    "    \n",
    "    num_replacements = min(n, len(words_to_replace))\n",
    "    indices_to_replace = random.sample(words_to_replace, num_replacements)\n",
    "    \n",
    "    for idx in indices_to_replace:\n",
    "        synonyms = get_synonyms(tokens[idx])\n",
    "        if synonyms:\n",
    "            new_tokens[idx] = random.choice(synonyms)\n",
    "    \n",
    "    return new_tokens\n",
    "\n",
    "def random_insertion(tokens, n=1):\n",
    "    \"\"\"Randomly insert synonyms of n words\"\"\"\n",
    "    new_tokens = tokens.copy()\n",
    "    \n",
    "    for _ in range(n):\n",
    "        if len(new_tokens) == 0:\n",
    "            break\n",
    "        word = random.choice(new_tokens)\n",
    "        synonyms = get_synonyms(word)\n",
    "        if synonyms:\n",
    "            synonym = random.choice(synonyms)\n",
    "            insert_pos = random.randint(0, len(new_tokens))\n",
    "            new_tokens.insert(insert_pos, synonym)\n",
    "    \n",
    "    return new_tokens\n",
    "\n",
    "def random_deletion(tokens, p=0.1):\n",
    "    \"\"\"Randomly delete words with probability p\"\"\"\n",
    "    if len(tokens) == 1:\n",
    "        return tokens\n",
    "    \n",
    "    new_tokens = []\n",
    "    for token in tokens:\n",
    "        if random.random() > p:\n",
    "            new_tokens.append(token)\n",
    "    \n",
    "    if len(new_tokens) == 0:\n",
    "        return tokens[:1]\n",
    "    \n",
    "    return new_tokens\n",
    "\n",
    "def random_swap(tokens, n=1):\n",
    "    \"\"\"Randomly swap n pairs of words\"\"\"\n",
    "    new_tokens = tokens.copy()\n",
    "    \n",
    "    for _ in range(n):\n",
    "        if len(new_tokens) < 2:\n",
    "            break\n",
    "        idx1, idx2 = random.sample(range(len(new_tokens)), 2)\n",
    "        new_tokens[idx1], new_tokens[idx2] = new_tokens[idx2], new_tokens[idx1]\n",
    "    \n",
    "    return new_tokens\n",
    "\n",
    "def augment_text(text, augmentation_techniques=['synonym', 'insertion', 'deletion', 'swap'], \n",
    "                 num_augmentations=3):\n",
    "    \"\"\"Apply data augmentation to text\"\"\"\n",
    "    augmented_texts = []\n",
    "    \n",
    "    for _ in range(num_augmentations):\n",
    "        aug_text = text.copy()\n",
    "        technique = random.choice(augmentation_techniques)\n",
    "        \n",
    "        if technique == 'synonym' and len(aug_text) > 0:\n",
    "            aug_text = synonym_replacement(aug_text, n=random.randint(1, 2))\n",
    "        elif technique == 'insertion' and len(aug_text) > 0:\n",
    "            aug_text = random_insertion(aug_text, n=random.randint(1, 2))\n",
    "        elif technique == 'deletion' and len(aug_text) > 1:\n",
    "            aug_text = random_deletion(aug_text, p=0.1)\n",
    "        elif technique == 'swap' and len(aug_text) > 1:\n",
    "            aug_text = random_swap(aug_text, n=1)\n",
    "        \n",
    "        augmented_texts.append(aug_text)\n",
    "    \n",
    "    return augmented_texts\n",
    "\n",
    "print(\"    ✓ Data augmentation functions ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f8ddcf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Step 3: Applying Data Augmentation for Imbalanced Classes...\n",
      "\n",
      "Original label distribution:\n",
      "  ABBR: 69 samples (1.58%)\n",
      "  DESC: 930 samples (21.32%)\n",
      "  ENTY: 1000 samples (22.93%)\n",
      "  HUM: 978 samples (22.42%)\n",
      "  LOC: 668 samples (15.31%)\n",
      "  NUM: 717 samples (16.44%)\n",
      "\n",
      "  Augmenting ABBR: 69 -> 500 samples\n",
      "    Generating 431 additional samples...\n",
      "    ✓ Generated 431 augmented samples\n",
      "\n",
      "Augmented label distribution:\n",
      "  ABBR: 500 samples (10.43%)\n",
      "  DESC: 930 samples (19.40%)\n",
      "  ENTY: 1000 samples (20.86%)\n",
      "  HUM: 978 samples (20.40%)\n",
      "  LOC: 668 samples (13.94%)\n",
      "  NUM: 717 samples (14.96%)\n",
      "\n",
      "  Total samples: 4362 -> 4793\n",
      "  ✓ Data augmentation complete\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# Step 3: Apply Data Augmentation for Imbalanced Classes\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n>>> Step 3: Applying Data Augmentation for Imbalanced Classes...\")\n",
    "\n",
    "# Count current label distribution\n",
    "label_counts_p34 = Counter([ex.label for ex in train_data.examples])\n",
    "print(f\"\\nOriginal label distribution:\")\n",
    "for label, count in sorted(label_counts_p34.items()):\n",
    "    print(f\"  {label}: {count} samples ({count/len(train_data.examples)*100:.2f}%)\")\n",
    "\n",
    "# Target counts: Augment ABBR significantly, keep others similar\n",
    "target_counts_p34 = {\n",
    "    'ABBR': 500,   # Augment from 69 to 500 (~7x)\n",
    "    'DESC': 930,   # Keep original\n",
    "    'ENTY': 1000,  # Keep original\n",
    "    'HUM': 978,    # Keep original\n",
    "    'LOC': 668,    # Keep original\n",
    "    'NUM': 717     # Keep original\n",
    "}\n",
    "\n",
    "# Create augmented examples\n",
    "augmented_examples = list(train_data.examples)  # Start with all original examples\n",
    "\n",
    "for label in label_counts_p34.keys():\n",
    "    current_count = label_counts_p34[label]\n",
    "    target_count = target_counts_p34[label]\n",
    "    \n",
    "    if current_count < target_count:\n",
    "        label_examples = [ex for ex in train_data.examples if ex.label == label]\n",
    "        num_augmentations_needed = target_count - current_count\n",
    "        \n",
    "        print(f\"\\n  Augmenting {label}: {current_count} -> {target_count} samples\")\n",
    "        print(f\"    Generating {num_augmentations_needed} additional samples...\")\n",
    "        \n",
    "        augmented_count = 0\n",
    "        while augmented_count < num_augmentations_needed:\n",
    "            original_ex = random.choice(label_examples)\n",
    "            aug_texts = augment_text(original_ex.text, num_augmentations=1)\n",
    "            \n",
    "            for aug_text in aug_texts:\n",
    "                if augmented_count >= num_augmentations_needed:\n",
    "                    break\n",
    "                \n",
    "                new_ex = data.Example.fromlist([aug_text, label], \n",
    "                                               fields=[('text', TEXT), ('label', LABEL)])\n",
    "                augmented_examples.append(new_ex)\n",
    "                augmented_count += 1\n",
    "        \n",
    "        print(f\"    ✓ Generated {augmented_count} augmented samples\")\n",
    "\n",
    "# Create augmented dataset with proper field structure\n",
    "augmented_train_data = data.Dataset(augmented_examples, fields=[('text', TEXT), ('label', LABEL)])\n",
    "\n",
    "# Verify augmented distribution\n",
    "new_label_counts = Counter([ex.label for ex in augmented_examples])\n",
    "print(f\"\\nAugmented label distribution:\")\n",
    "for label, count in sorted(new_label_counts.items()):\n",
    "    print(f\"  {label}: {count} samples ({count/len(augmented_examples)*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\n  Total samples: {len(train_data.examples)} -> {len(augmented_examples)}\")\n",
    "print(f\"  ✓ Data augmentation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ec4bc0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# augmented_train_data.examples[0].label\n",
    "[(ex.text, ex.label) for ex in augmented_train_data if ex.label not in ['ABBR','DESC','ENTY','HUM','LOC','NUM']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce04616c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(None,\n",
       "            {'ENTY': 0, 'HUM': 1, 'DESC': 2, 'NUM': 3, 'LOC': 4, 'ABBR': 5})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABEL.vocab.stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9afaf5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What', 'coif', 'snafu', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NAFTA', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NASA', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'acronym', 'office', 'for', 'the', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'of', 'the', 'company', 'name', '`', 'General', 'Motors', \"'\", '?']\n",
      "ABBR\n",
      "['CNN', 'is', 'an', 'acronym', 'for', 'what', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviated', 'form', 'of', 'the', 'home', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['cwt', 'does', 'the', 'abbreviation', 'What', '.', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviated', 'form', 'of', 'the', 'internal', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'is', 'DTMF', '?']\n",
      "ABBR\n",
      "['What', 'is', 'IOC', 'an', 'abbreviation', 'of', '?']\n",
      "ABBR\n",
      "['What', 'does', 'mean', 'word', 'LASER', 'the', '?']\n",
      "ABBR\n",
      "['What', 'does', 'INRI', 'stand', 'when', 'used', 'on', 'Jesus', \"'\", 'cross', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'IOC', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'fin', 'the', 'number', '`', '`', 'total', '5', \"''\", 'stand', 'for', 'on', 'FUBU', 'clothing', '?']\n",
      "ABBR\n",
      "['When', 'reading', 'classified', 'ads', ',', 'what', 'does', 'EENTY', ':', 'other', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'Manufacturer', 'Original', 'Equipment', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'SVHS', '?']\n",
      "ABBR\n",
      "['What', 'mean', 'LOL', 'does', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'of', 'the', 'company', 'name', '`', 'General', 'Motors', \"'\", '?']\n",
      "ABBR\n",
      "['?', 'does', 'the', 'abbreviation', 'OAS', 'stand', 'for', 'What']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'expression', 'for', 'the', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'for', 'NASDAQ', 'stand', 'does', '?']\n",
      "ABBR\n",
      "['What', 'is', 'RCD', '?']\n",
      "ABBR\n",
      "['What', 'energy', 'VCR', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'RCA', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'abbreviation', 'IOC', 'stall', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'a', '`', '`', 'USB', \"''\", 'larboard', 'on', 'a', 'computer', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'of', 'the', 'National', 'Bureau', 'dresser', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', '?', 'LMDS', 'is']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'correct', 'way', 'to', 'abbreviate', 'cc', '.', 'at', 'the', 'bottom', 'of', 'set', 'a', 'business', 'letter', '?']\n",
      "ABBR\n",
      "['What', 'does', 'IQ', '?', 'for', 'stand']\n",
      "ABBR\n",
      "['What', 'does', 'IBM', 'support', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NAFTA', 'fare', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', '?', 'abbreviation', 'of', 'General', 'Motors', 'the']\n",
      "ABBR\n",
      "['What', 'is', 'AFS', '?']\n",
      "ABBR\n",
      "['What', 'is', 'HTML', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'scheme', 'acronym', 'for', 'the', 'rating', 'system', 'for', 'air', 'conditioner', 'efficiency', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'acronym', 'CPR', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'name', 'the', 'company', 'of', '`', 'General', 'Motors', \"'\", '?']\n",
      "ABBR\n",
      "['What', 'does', 'SIDS', 'stand', '?', 'for']\n",
      "ABBR\n",
      "['cpr', 'What', 'does', 'CPR', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['do', 'the', 'letters', 'ZIP', 'stand', 'in', 'the', 'phrase', '`', '`', 'ZIP', 'code', \"''\", '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'of', 'the', 'International', 'Olympic', 'Committee']\n",
      "ABBR\n",
      "['What', 'does', 'HIV', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'T.S.', 'stand', 'for', 'in', 'T.S.', 'Eliot', \"'s\", 'name', '?']\n",
      "ABBR\n",
      "['What', 'does', 'gangrene', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NAFTA', 'stand', 'nafta', 'for', 'nafta', '?']\n",
      "ABBR\n",
      "['DEET', 'is', 'What', '?']\n",
      "ABBR\n",
      "['NASA', 'does', 'What', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'of', 'the', '`', 'General', 'Motors', \"'\", '?']\n",
      "ABBR\n",
      "['does', 'What', 'pH', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'SHIELD', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NAFTA', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'INRI', 'stand', 'for', 'when', 'used', 'on', 'Jesus', \"'\", 'cross', '?']\n",
      "ABBR\n",
      "['What', 'does', 'abbreviation', 'the', 'AIDS', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'INRI', 'stand', 'for', 'when', 'used', 'on', 'christ', \"'\", 'cross', '?']\n",
      "ABBR\n",
      "['What', 'RCD', 'is', '?']\n",
      "ABBR\n",
      "['What', 'is', 'a', '`', '`', 'USB', 'estimator', \"''\", 'port', 'on', 'a', 'computer', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'abbreviation', 'AIDS', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'snafu', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'number', '`', '`', '5', \"''\", 'for', 'stand', 'on', 'FUBU', 'clothing', '?']\n",
      "ABBR\n",
      "['What', 'for', 'snafu', 'stand', 'does', '?']\n",
      "ABBR\n",
      "['What', 'is', 'dsl', '?']\n",
      "ABBR\n",
      "['CPR', 'is', 'the', 'abbreviation', 'for', 'what', '?']\n",
      "ABBR\n",
      "['What', 'does', '`', '`', 'B.Y.O.B.', \"''\", 'mean', '?']\n",
      "ABBR\n",
      "['What', 'does', '`', 'PSI', \"'\", 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', '?', 'T.S.', 'stand', 'for', 'in', 'T.S.', 'Eliot', \"'s\", 'name', 'the']\n",
      "ABBR\n",
      "['What', 'is', 'IOC', 'an', 'abbreviation', 'of', '?']\n",
      "ABBR\n",
      "['What', 'is', 'RCD', '?']\n",
      "ABBR\n",
      "['for', 'does', 'BMW', 'stand', 'What', '?']\n",
      "ABBR\n",
      "['What', 'does', 'HIV', 'stand', '?', 'for']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviated', 'term', 'used', 'the', 'for', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'does', 'BMW', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'E', 'stand', 'for', 'in', 'the', 'equation', 'E', '=', 'mc2', '?']\n",
      "ABBR\n",
      "['does', 'What', 'BTU', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'does', 'mean', '`', 'B.Y.O.B.', \"''\", '`', '?']\n",
      "ABBR\n",
      "['What', 'is', 'AFS', '?']\n",
      "ABBR\n",
      "['?', 'is', 'DEET', 'What']\n",
      "ABBR\n",
      "['What', 'BUD', 'does', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'mean', 'the', 'word', 'LASER', 'does', '?']\n",
      "ABBR\n",
      "['mean', 'does', 'the', 'word', 'LASER', 'What', '?']\n",
      "ABBR\n",
      "['What', 'does', 'SIDS', 'stand', 'sids', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'drive', 'abbreviation', 'of', 'General', 'Motors', '?']\n",
      "ABBR\n",
      "['What', '?', 'DTMF', 'is']\n",
      "ABBR\n",
      "['?', 'is', 'AFS', 'What']\n",
      "ABBR\n",
      "['?', 'is', 'the', 'abbreviation', 'of', 'the', 'International', 'Olympic', 'Committee', 'What']\n",
      "ABBR\n",
      "['of', 'is', 'the', 'abbreviated', 'form', 'What', 'the', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'mean', 'NECROSIS', 'does', '?']\n",
      "ABBR\n",
      "['rack', 'What', 'does', 'CPR', 'stand', 'for', 'cpr', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', '`', 'in', 'c', \"''\", 'stand', 'for', '`', 'the', 'equation', 'E', '=', 'mc2', '?']\n",
      "ABBR\n",
      "['When', 'reading', 'classified', 'ads', ',', 'what', 'does', 'EENTY', ':', 'other', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'an', 'IOC', 'is', 'abbreviation', 'of', '?']\n",
      "ABBR\n",
      "['What', 'is', 'bph', '?']\n",
      "ABBR\n",
      "['What', 'does', 'HIV', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'bud', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'DTMF', '?']\n",
      "ABBR\n",
      "['What', 'is', 'RCD', '?']\n",
      "ABBR\n",
      "['LMDS', 'is', 'What', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'T.S.', '?', 'for', 'in', 'T.S.', 'Eliot', \"'s\", 'name', 'stand']\n",
      "ABBR\n",
      "['What', \"'s\", 'the', 'abbreviation', 'for', 'trinitrotoluene', '?']\n",
      "ABBR\n",
      "['What', 'INRI', 'stand', 'for', 'when', 'used', 'Jesus', \"'\", 'cross', '?']\n",
      "ABBR\n",
      "['What', 'answer', '`', '`', 'B.Y.O.B.', \"''\", 'think', '?']\n",
      "ABBR\n",
      "['is', 'DTMF', '?']\n",
      "ABBR\n",
      "['What', 'is', '?', 'abbreviated', 'expression', 'for', 'the', 'National', 'Bureau', 'of', 'Investigation', 'the']\n",
      "ABBR\n",
      "['CPR', 'is', 'the', 'abbreviation', 'for', 'what', '?']\n",
      "ABBR\n",
      "['When', 'reading', 'classified', 'ads', ',', 'what', 'EENTY', 'does', ':', 'other', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', '?']\n",
      "ABBR\n",
      "['What', 'does', 'BMW', 'stand', '?', 'for']\n",
      "ABBR\n",
      "['is', 'the', 'abbreviated', 'form', 'of', 'the', 'National', 'Bureau', 'Investigation']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'correct', 'way', 'to', 'abbreviate', 'letter', '.', 'at', 'the', 'bottom', 'of', 'a', 'business', 'cc', '?']\n",
      "ABBR\n",
      "['?', 'does', 'CPR', 'stand', 'for', 'What']\n",
      "ABBR\n",
      "['What', 'does', 'LOL', 'mean', '?', 'practice']\n",
      "ABBR\n",
      "['What', \"'s\", 'the', 'abbreviation', '?', 'trinitrotoluene', 'for']\n",
      "ABBR\n",
      "['What', 'does', 'Ms.', ',', 'Miss', 'stand', 'and', 'Mrs.', ',', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', '?', 'stand', 'for', 'IQ']\n",
      "ABBR\n",
      "['live', 'What', 'is', 'DSL', '?']\n",
      "ABBR\n",
      "['When', 'reading', 'classified', 'ads', ',', 'what', 'does', 'advertizing', 'EENTY', ':', 'other', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'word', 'LASER', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'manage', 'the', 'abbreviation', 'cwt', '.', '?']\n",
      "ABBR\n",
      "['What', 'is', 'SVHS']\n",
      "ABBR\n",
      "['What', 'is', 'Mikhail', 'Gorbachev', \"'s\", 'midriff', 'initial', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'full', 'name', 'of', 'the', 'PLO', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NECROSIS', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'committee', 'olympian', 'of', 'the', 'International', 'Olympic', 'Committee', '?']\n",
      "ABBR\n",
      "['What', 'do', 'the', 'letters', 'speed', 'stand', 'for', 'in', 'the', 'phrase', '`', '`', 'ZIP', 'code', \"''\", '?']\n",
      "ABBR\n",
      "['What', 'personify', 'is', 'IOC', 'an', 'abbreviation', 'of', '?']\n",
      "ABBR\n",
      "['What', 'does', 'INRI', 'stand', 'for', 'when', 'used', 'on', 'deliverer', \"'\", 'baffle', '?']\n",
      "ABBR\n",
      "['What', 'is', 'a', '`', '`', 'USB', \"''\", 'port', 'on', 'a', 'computer', '?']\n",
      "ABBR\n",
      "['What', 'is', 'HDLC', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', '`', '`', 'c', \"''\", 'stand', 'for', 'in', 'the', 'equation', 'E', '=', '?']\n",
      "ABBR\n",
      "['What', 'is', 'p.m.', 'an', 'abbreviation', 'for', ',', 'as', 'in', '5', 'p.m.', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'abbreviation', 'OAS', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'correct', 'way', 'to', 'abbreviate', 'cc', '.', 'of', 'the', 'bottom', 'at', 'a', 'business', 'letter', '?']\n",
      "ABBR\n",
      "['CNN', 'does', 'What', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'DSL', '?']\n",
      "ABBR\n",
      "['company', 'is', 'the', 'abbreviation', 'of', 'the', 'What', 'name', '`', 'General', 'Motors', \"'\", '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'term', 'used', 'for', 'the', 'National', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'manage', 'SHIELD', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'AFS', '?']\n",
      "ABBR\n",
      "['What', 'does', 'LOL', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviated', 'term', 'condition', 'used', 'for', 'the', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'is', 'DEET', '?']\n",
      "ABBR\n",
      "['What', 'does', 'standpoint', 'CPR', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'of', 'form', 'abbreviated', 'the', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'rating', 'for', 'the', 'acronym', 'system', 'for', 'air', 'conditioner', 'efficiency', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'abbreviation', 'cwt', '.', '?']\n",
      "ABBR\n",
      "['What', 'does', 'IOC', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'abbreviation', 'attention', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'SVHS', 'represent', '?']\n",
      "ABBR\n",
      "['?', 'does', 'RCA', 'stand', 'for', 'What']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'abbreviation', 'AIDS', '?', 'for', 'stand']\n",
      "ABBR\n",
      "['What', 'coiffe', 'CPR', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'SIDS', 'stand', 'for', '?', 'bear']\n",
      "ABBR\n",
      "['What', 'HTML', 'is', '?']\n",
      "ABBR\n",
      "['What', 'does', 'gangrene', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'does', 'LOL', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', '`', '`', 'c', \"''\", 'e', 'stand', 'for', 'in', 'the', 'equation', 'E', '=', 'mc2', '?']\n",
      "ABBR\n",
      "['What', 'does', 'IOC', 'endure', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'dsl', '?']\n",
      "ABBR\n",
      "['What', 'do', 'the', 'letters', 'D.C.', 'stand', 'for', 'in', 'Washington', ',', 'D.C.', '?']\n",
      "ABBR\n",
      "['What', 'does', 'LOL', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'IBM', 'does', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'IQ', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'DSL', '?']\n",
      "ABBR\n",
      "['for', 'does', 'SHIELD', 'stand', 'What', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviated', 'expression', 'for', 'the', 'National', 'Bureau', 'of', 'Investigation', 'construction', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'of', 'the', 'International', 'Olympic', 'Committee', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviated', 'expression', 'for', 'the', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['does', 'the', 'channel', 'ESPN', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', \"'s\", 'the', 'abbreviation', 'for', 'trinitrotoluene', '?']\n",
      "ABBR\n",
      "['What', 'is', 'acronym', 'for', 'the', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'is', 'HTML', '?']\n",
      "ABBR\n",
      "['What', 'does', 'HIV', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'of', 'the', 'international', 'Olympic', 'Committee', '?']\n",
      "ABBR\n",
      "['What', 'does', 'CNN', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'IQ', 'bear', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'HIV', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'p.m.', 'an', 'abbreviation', 'for', ',', 'as', 'in', '5', 'p.m.', '?']\n",
      "ABBR\n",
      "['What', 'cause', 'G.M.T.', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'assist', 'the', 'abbreviation', 'AIDS', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'DSL', '?']\n",
      "ABBR\n",
      "['What', 'is', 'LMDS', '?']\n",
      "ABBR\n",
      "['What', 'is', 'p.m.', 'an', 'abbreviation', 'for', ',', 'as', 'in', '5', 'p.m.', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'of', 'the', 'International', 'Olympic', 'Committee', '?']\n",
      "ABBR\n",
      "['What', 'does', 'VCR', 'stand', '?']\n",
      "ABBR\n",
      "['What', 'does', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NAFTA', '?', 'for', 'stand']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'full', 'name', 'of', 'the', 'PLO', '?']\n",
      "ABBR\n",
      "['What', 'is', 'DTMF', '?']\n",
      "ABBR\n",
      "['What', 'does', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'T.S.', 'stand', 'for', 'in', 'T.S.', 'Eliot', \"'s\", 'name', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'correct', 'way', 'to', 'abbreviate', 'cc', '.', 'bottom', 'of', 'a', 'business', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'abbreviation', 'AIDS', 'viewpoint', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'IBM', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NAFTA', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['is', 'the', 'acronym', 'for', 'the', 'National', 'Bureau', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'does', 'fend', 'NASA', 'answer', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'exercise', 'the', 'abbreviation', 'OAS', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'IBM', 'rack', 'for', '?']\n",
      "ABBR\n",
      "['What', 'do', 'the', 'D.C.', 'letters', 'stand', 'for', 'in', 'Washington', ',', 'D.C.', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'of', 'the', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'does', 'for', 'stand', 'SIDS', '?']\n",
      "ABBR\n",
      "['quintal', 'What', 'does', 'the', 'abbreviation', 'cwt', '.', '?']\n",
      "ABBR\n",
      "['What', 'does', 'sphacelus', 'NECROSIS', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'coif', '`', '`', 'B.Y.O.B.', \"''\", 'meanspirited', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviated', 'form', 'of', 'the', 'National', 'Bureau', 'of', 'Investigation', 'follow', '?']\n",
      "ABBR\n",
      "['cpr', 'is', 'the', 'abbreviation', 'for', 'what', '?']\n",
      "ABBR\n",
      "['the', 'does', 'What', 'acronym', 'CPR', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'behave', 'the', 'acronym', 'cpr', 'mean', '?']\n",
      "ABBR\n",
      "['does', 'What', 'NASA', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['for', 'is', 'the', 'abbreviation', 'CPR', 'what', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'ESPN', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'Ms.', ',', 'coif', 'Miss', ',', 'and', 'Mrs.', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'RCD', '?']\n",
      "ABBR\n",
      "['What', 'does', '`', 'PSI', \"'\", 'stand', '?', 'for']\n",
      "ABBR\n",
      "['What', 'does', 'VCR', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'word', 'laser', 'mean', '?']\n",
      "ABBR\n",
      "['?', 'is', 'AFS', 'What']\n",
      "ABBR\n",
      "[\"'s\", 'the', 'abbreviation', 'for', 'trinitrotoluene', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'abbreviation', '?', '.', 'cwt']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviated', 'term', 'used', 'for', 'National', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'does', 'INRI', 'stand', 'for', 'when', 'used', \"'\", 'cross', '?']\n",
      "ABBR\n",
      "['is', 'an', 'acronym', 'for', 'what', '?']\n",
      "ABBR\n",
      "['What', 'is', 'IOC', 'an', 'abbreviation', 'of', '?']\n",
      "ABBR\n",
      "['What', 'does', 'nasdaq', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviated', 'expression', 'for', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'does', 'CNN', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', \"''\", 'the', '`', '`', 'c', 'does', 'stand', 'for', 'in', 'the', 'equation', 'E', '=', 'mc2', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviated', 'expression', 'for', 'the', 'National', 'Bureau', 'of', 'Investigation', '?', 'formula']\n",
      "ABBR\n",
      "['What', 'does', 'VCR', 'stand', 'for', '?', 'vcr']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviated', 'term', 'used', 'for', 'the', 'National', 'Bureau', 'of', '?']\n",
      "ABBR\n",
      "['What', \"'s\", 'the', 'abbreviation', 'for', 'trinitrotoluene', '?']\n",
      "ABBR\n",
      "['What', 'is', 'IOC', 'an', 'abbreviation', 'of', '?']\n",
      "ABBR\n",
      "['does', 'What', 'pH', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'VCR', 'stand', 'for']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'abbreviation', 'IOC', 'support', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'pH', 'stand', 'for', 'ph', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', '?', 'General', 'Motors', 'of']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'of', 'the', 'companionship', 'name', '`', 'General', 'Motors', \"'\", '?']\n",
      "ABBR\n",
      "['an', 'is', 'CNN', 'acronym', 'for', 'what', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'acronym', 'the', 'rating', 'system', 'for', 'air', 'conditioner', 'efficiency']\n",
      "ABBR\n",
      "['What', 'does', 'IQ', 'stand', 'for', 'make', '?']\n",
      "ABBR\n",
      "['does', 'NASDAQ', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'bud', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'make', 'IQ', 'bear', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'abbreviation', 'AIDS', 'resist', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', '`', 'PSI', \"'\", '?', 'for', 'stand']\n",
      "ABBR\n",
      "['What', 'is', 'BPH', '?']\n",
      "ABBR\n",
      "['What', 'does', 'CNN', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['does', 'BUD', 'stand', '?']\n",
      "ABBR\n",
      "['What', 'is', 'HDLC', '?']\n",
      "ABBR\n",
      "['What', 'do', 'stand', 'letters', 'D.C.', 'the', 'for', 'in', 'Washington', ',', 'D.C.', '?']\n",
      "ABBR\n",
      "['What', 'does', 'for', 'stand', 'IOC', '?']\n",
      "ABBR\n",
      "['What', 'is', 'HDLC', '?']\n",
      "ABBR\n",
      "['What', 'BPH', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'E', 'stand', 'for', 'in', 'the', 'equation', 'E', '=', 'mc2', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'abbreviation', '.', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'of', 'the', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'do', 'the', 'letters', 'D.C.', 'stand', 'for', 'in', 'Washington', ',', 'D.C.', '?']\n",
      "ABBR\n",
      "['What', 'is', 'acronym', 'for', 'the', 'rating', 'for', 'air', 'conditioner', 'efficiency', '?']\n",
      "ABBR\n",
      "['What', 'subject', 'is', 'the', 'acronym', 'for', 'the', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'does', 'S.O.S.', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'of', 'ecumenical', 'Motors', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'number', '`', '`', '5', \"''\", 'stand', 'for', 'on', 'FUBU', 'clothing', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviated', 'for', 'the', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'number', '`', '`', '5', \"''\", 'stand', 'for', 'on', 'FUBU', 'raiment', '?']\n",
      "ABBR\n",
      "['What', 'does', 'Ms.', ',', 'Miss', ',', 'and', 'Mrs.', 'ms', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'do', 'the', 'letters', 'ZIP', 'stomach', 'for', 'in', 'the', 'phrase', '`', '`', 'ZIP', 'code', \"''\", '?']\n",
      "ABBR\n",
      "['What', 'does', 'INRI', 'stand', 'for', 'used', 'on', 'Jesus', \"'\", 'cross']\n",
      "ABBR\n",
      "['What', 'does', 'the', '`', '`', 'c', \"''\", 'stand', 'for', 'in', 'the', 'equation', 'E', '=', 'mc2', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'abbreviation', 'cwt', '.', '?']\n",
      "ABBR\n",
      "['What', 'does', 'S.O.S.', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'acronym', 'for', 'the', 'National', 'Bureau', '?', 'Investigation', 'of']\n",
      "ABBR\n",
      "['What', 'does', 'LOL', 'mean']\n",
      "ABBR\n",
      "['What', 'do', 'the', 'letters', 'ZIP', 'stand', 'for', 'in', 'the', 'phrase', '`', '`', 'ZIP', 'code', 'inscribe', \"''\", '?']\n",
      "ABBR\n",
      "['What', 'does', 'RCA', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', ',', ',', 'Miss', 'Ms.', 'and', 'Mrs.', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'of', 'the', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'does', '`', 'B.Y.O.B.', \"''\", 'mean', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'abbreviation', 'OAS', 'stand', '?']\n",
      "ABBR\n",
      "['What', 'does', 'HIV', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'IQ', 'rack', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'BUD', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviated', 'construction', 'for', 'the', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'does', 'SIDS', 'resist', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'word', 'LASER', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'channel', 'ESPN', 'stand', 'for']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'of', 'General', 'Motors', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NECROSIS', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'is', 'p.m.', 'an', 'abbreviation', 'for', ',', 'as', 'in', '5', 'p.m.', '?']\n",
      "ABBR\n",
      "['What', 'is', 'a', '`', '`', 'USB', \"''\", 'interface', 'on', 'a', 'computer', '?']\n",
      "ABBR\n",
      "['What', 'does', 'pH', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'snafu', 'support', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['CNN', 'what', 'an', 'acronym', 'for', 'is', '?']\n",
      "ABBR\n",
      "['What', 'way', 'the', 'correct', 'is', 'to', 'abbreviate', 'cc', '.', 'at', 'the', 'bottom', 'of', 'a', 'business', 'letter', '?']\n",
      "ABBR\n",
      "['What', 'does', 'BTU', 'btu', 'hateful', 'mean', '?']\n",
      "ABBR\n",
      "['When', 'reading', 'classified', 'ads', ',', 'what', 'does', 'EENTY', ':', 'other', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'stand', 'G.M.T.', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'Ms.', ',', 'Miss', ',', 'and', 'Mrs.', 'for']\n",
      "ABBR\n",
      "['What', 'do', 'the', 'letters', 'D.C.', 'stand', 'for', 'in', 'Washington', 'D.C.', '?']\n",
      "ABBR\n",
      "['What', 'is', 'a', '`', '`', 'USB', \"''\", 'port', 'on', 'a', 'calculator', '?']\n",
      "ABBR\n",
      "['What', 'does', 'BUD', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'INRI', 'stand', 'for', 'when', 'used', 'cross', 'Jesus', \"'\", 'on', '?']\n",
      "ABBR\n",
      "['midsection', 'What', 'is', 'Mikhail', 'Gorbachev', \"'s\", 'middle', 'initial', '?']\n",
      "ABBR\n",
      "['What', 'is', 'a', '`', '`', 'USB', \"''\", 'port', 'on', 'a', 'estimator', '?']\n",
      "ABBR\n",
      "['What', 'stand', 'CNN', 'does', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'LOL', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NAFTA', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'AFS', '?']\n",
      "ABBR\n",
      "['What', 'is', 'DSL', '?', 'dsl']\n",
      "ABBR\n",
      "['What', 'home', 'is', 'the', 'abbreviated', 'term', 'used', 'for', 'the', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'arrange', 'the', 'acronym', 'CPR', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'oas', 'abbreviation', 'OAS', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'VCR', 'for', '?']\n",
      "ABBR\n",
      "['What', 'do', 'the', 'letter', 'ZIP', 'brook', 'for', 'in', 'the', 'phrase', '`', '`', 'ZIP', 'code', \"''\", '?']\n",
      "ABBR\n",
      "['What', 'does', '`', '`', 'B.Y.O.B.', \"''\", 'mean', '?']\n",
      "ABBR\n",
      "['What', 'does', 'BUD', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'IOC', 'an', 'of', 'abbreviation', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviated', 'expression', 'for', 'the', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'is', 'DEET', '?', 'live']\n",
      "ABBR\n",
      "['What', 'do', 'the', 'letters', 'D.C.', 'stand', 'for', 'in', 'washington', ',', 'D.C.', '?']\n",
      "ABBR\n",
      "['What', 'does', 'CPR', 'tolerate', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'of', 'the', 'National', 'Investigation', 'of', 'Bureau', '?']\n",
      "ABBR\n",
      "['What', 'do', 'the', 'letters', 'ZIP', 'stand', 'for', 'in', 'the', '`', '`', 'phrase', 'ZIP', 'code', \"''\", '?']\n",
      "ABBR\n",
      "['abbreviation', 'is', 'p.m.', 'an', 'What', 'for', ',', 'as', 'in', '5', 'p.m.', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'T.S.', 'standpoint', 'for', 'in', 'T.S.', 'Eliot', \"'s\", 'name', '?']\n",
      "ABBR\n",
      "['What', 'is', 'DTMF', '?']\n",
      "ABBR\n",
      "['What', 'does', 'CNN', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviated', 'form', 'of', '?', 'National', 'Bureau', 'of', 'Investigation', 'the']\n",
      "ABBR\n",
      "['What', 'is', 'IOC', 'an', 'abbreviation', 'of', 'personify', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NASDAQ', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'p.m.', 'an', 'abbreviation', 'for', ',', 'as', 'in', '5', '?']\n",
      "ABBR\n",
      "['What', 'is', 'p.m.', 'an', 'abbreviation', 'for', ',', 'as', 'in', 'represent', '5', 'p.m.', '?']\n",
      "ABBR\n",
      "['for', 'does', 'CNN', 'stand', 'What', '?']\n",
      "ABBR\n",
      "['What', 'doe', 'BMW', 'resist', 'for', '?']\n",
      "ABBR\n",
      "['What', 'do', 'the', 'letters', 'ZIP', 'stand', 'for', 'in', 'the', 'inward', 'phrase', 'inwards', '`', '`', 'ZIP', 'code', \"''\", '?']\n",
      "ABBR\n",
      "['ph', 'What', 'abide', 'does', 'pH', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['for', 'does', 'the', 'E', 'stand', 'What', 'in', 'the', 'equation', 'E', '=', 'mc2', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviated', 'form', 'of', 'the', 'National', 'Bureau', 'of', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'of', 'the', 'International', 'Olympic', 'Committee', '?']\n",
      "ABBR\n",
      "['What', 'does', 'G.M.T.', 'for', '?']\n",
      "ABBR\n",
      "['?', 'is', 'HDLC', 'What']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviated', 'contract', 'term', 'used', 'for', 'the', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'T.S.', 'stand', 'for', 'in', 'T.S.', 'Eliot', \"'s\", 'name', '?']\n",
      "ABBR\n",
      "['What', 'is', 'Mikhail', 'Gorbachev', \"'s\", 'middle', 'initial', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'acronym', 'for', 'the', 'rating', 'for', 'air', 'conditioner', 'efficiency', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'right', 'way', 'to', 'abbreviate', 'cc', '.', 'at', 'the', 'bottom', 'of', 'a', 'business', 'letter', '?']\n",
      "ABBR\n",
      "['What', 'S.O.S.', 'does', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'RCD', '?']\n",
      "ABBR\n",
      "['What', 'does', 'RCA', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'IQ', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'CPR', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'CNN', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'correct', 'way', 'to', 'abbreviate', 'cc', '.', 'at', 'the', 'bottom', 'of', 'a', 'business', 'letter', '?']\n",
      "ABBR\n",
      "['CNN', 'is', 'an', 'acronym', 'for', 'what', '?']\n",
      "ABBR\n",
      "['What', \"'s\", 'the', 'abbreviation', 'for', 'trinitrotoluene', '?']\n",
      "ABBR\n",
      "['?', 'is', 'the', 'abbreviation', 'of', 'the', 'company', 'name', '`', 'General', 'Motors', \"'\", 'What']\n",
      "ABBR\n",
      "['What', 'does', 'NASDAQ', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', '`', 'PSI', \"'\", 'stand', 'for', '?']\n",
      "ABBR\n",
      "['CNN', 'is', 'an', 'acronym', 'for', 'what', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NASDAQ', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'abbreviation', 'IOC', 'for', '?']\n",
      "ABBR\n",
      "['What', 'do', 'the', 'letters', 'ZIP', 'stand', 'for', 'in', 'the', 'phrase', '`', '`', 'ZIP', 'code', \"''\", '?']\n",
      "ABBR\n",
      "['What', 'does', '?', 'stand', 'for', 'IOC']\n",
      "ABBR\n",
      "['What', 'is', 'HTML', '?']\n",
      "ABBR\n",
      "['BUD', 'stand', 'for']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'number', '`', '`', '5', \"''\", 'stand', 'for', 'on', 'FUBU', 'clothing', '?']\n",
      "ABBR\n",
      "['What', 'cause', 'the', 'channel', 'ESPN', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'Original', 'Equipment', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'full', 'name', 'of', 'the', 'PLO', '?']\n",
      "ABBR\n",
      "['What', 'is', 'Investigation', 'abbreviated', 'form', 'of', 'the', 'National', 'Bureau', 'of', 'the', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviated', 'descriptor', 'of', 'the', 'interior', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['accompany', 'What', 'is', 'follow', 'HDLC', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'of', 'the', 'company', 'name', 'General', 'Motors', \"'\", '?']\n",
      "ABBR\n",
      "['What', 'is', 'SVHS', '?']\n",
      "ABBR\n",
      "['What', 'does', 'coif', 'BMW', 'stand', 'for', '?', 'do']\n",
      "ABBR\n",
      "['What', 'does', 'pH', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'word', 'the', 'LASER', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'full', 'name', 'of', 'the', 'PLO', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviated', 'form', 'of', 'the', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'does', 'CPR', 'stand', 'for', 'wear', '?', 'endure']\n",
      "ABBR\n",
      "['What', 'does', 'snafu', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'Bureau', 'for', 'the', 'National', 'acronym', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'does', 'LOL', 'think', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NECROSIS', 'seth', 'set', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'is', 'Mikhail', 'Gorbachev', 'initial', 'middle', \"'s\", '?']\n",
      "ABBR\n",
      "['What', 'does', 'CNN', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'IQ', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'acronym', 'CPR', 'mean', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'stand', 'IOC', 'abbreviation', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'AFS', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NAFTA', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['hiv', 'What', 'does', 'HIV', 'stand', 'hiv', 'for', '?']\n",
      "ABBR\n",
      "['is', '?']\n",
      "ABBR\n",
      "['What', 'does', 'NAFTA', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', '`', 'PSI', \"'\", 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'computer', '`', '`', 'USB', \"''\", 'port', 'on', 'a', 'a', '?']\n",
      "ABBR\n",
      "['?', 'is', 'BPH', 'What']\n",
      "ABBR\n",
      "['What', '?', 'RCD', 'is']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'of', 'the', 'company', 'name', '`', 'General', 'Motors', \"'\", '?']\n",
      "ABBR\n",
      "['What', 'is', 'DTMF', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'abbreviation', 'OAS', 'stand', 'for', 'suffice', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'number', '`', '`', 'for', \"''\", 'stand', '5', 'on', 'FUBU', 'clothing', '?']\n",
      "ABBR\n",
      "['What', 'does', 'snafu', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'be', 'is', 'IOC', 'an', 'abbreviation', 'of', '?']\n",
      "ABBR\n",
      "['What', 'is', 'BPH', '?']\n",
      "ABBR\n",
      "['What', 'is', 'Manufacturer', 'abbreviation', 'for', 'Original', 'Equipment', 'the', '?']\n",
      "ABBR\n",
      "['When', 'reading', 'classified', 'ads', ',', 'what', 'does', 'EENTY', ':', 'other', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'SIDS', 'stand', 'sids', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'IBM', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'correct', 'way', 'to', 'abbreviate', 'cc', '.', 'at', 'the', 'bottom', 'of', 'business', 'a', 'letter', '?']\n",
      "ABBR\n",
      "['What', 'does', 'CPR', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'acronym', 'for', 'the', 'National', 'Bureau', 'of', 'Investigation', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'abbreviation', 'IOC', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviated', 'face', 'for', 'the', 'National', 'Bureau', 'of', 'investigating', '?']\n",
      "ABBR\n",
      "['What', 'BMW', 'stand', '?']\n",
      "ABBR\n",
      "['What', 'does', '`', '`', 'B.Y.O.B.', \"''\", 'mean', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'E', 'stand', 'for', 'in', 'the', 'equation', 'E', '=', 'mc2', '?']\n",
      "ABBR\n",
      "['What', 'does', 'G.M.T.', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'Ms.', ',', 'Miss', ',', 'and', 'Mrs.', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'decline', 'way', 'to', 'abbreviate', 'cc', '.', 'at', 'the', 'bottom', 'of', 'a', 'business', 'missive', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'abbreviation', 'OAS', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', '?', 'HDLC', 'is']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'wide', 'name', 'of', 'the', 'PLO', '?']\n",
      "ABBR\n",
      "['What', 'does', 'for', 'stand', 'HIV', '?']\n",
      "ABBR\n",
      "['What', 'is', 'the', 'abbreviation', 'of', 'the', 'company', 'cite', '`', 'cosmopolitan', 'Motors', \"'\", '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'channel', 'groove', 'ESPN', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'SIDS', 'stand', 'for', '?']\n",
      "ABBR\n",
      "['What', 'does', 'the', 'abbreviation', 'centner', '.', '?']\n",
      "ABBR\n",
      "['for', 'do', 'the', 'letters', 'D.C.', 'stand', 'What', 'in', 'Washington', ',', 'D.C.', '?']\n",
      "ABBR\n",
      "431\n"
     ]
    }
   ],
   "source": [
    "abbr_aug_ex = [ex for ex in augmented_train_data if ex.label == \"ABBR\"]\n",
    "abbr_ex = [ex for ex in train_data if ex.label == \"ABBR\"]\n",
    "\n",
    "count = 0\n",
    "for ex in abbr_aug_ex:\n",
    "    if ex not in abbr_ex:\n",
    "        print(ex.text)\n",
    "        print(ex.label)\n",
    "        count += 1\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "858eb2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label distribution in training set:\n",
      "- ABBR: 500 samples (10.43%)\n",
      "- DESC: 930 samples (19.40%)\n",
      "- ENTY: 1000 samples (20.86%)\n",
      "- HUM: 978 samples (20.40%)\n",
      "- LOC: 668 samples (13.94%)\n",
      "- NUM: 717 samples (14.96%)\n"
     ]
    }
   ],
   "source": [
    "# Count how many samples per label in the train set\n",
    "label_counts_p34 = Counter([ex.label for ex in augmented_train_data.examples])\n",
    "total_examples_p34 = len(augmented_train_data)\n",
    "\n",
    "print(\"\\nLabel distribution in training set:\")\n",
    "for label, count in sorted(label_counts_p34.items()):\n",
    "    percentage = (count / total_examples_p34) * 100\n",
    "    print(f\"- {label}: {count} samples ({percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2926e83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Step 5: Creating RNN Model with Positional Embeddings...\n",
      "    ✓ RNNWithPositionalAttentionClassifier model created\n",
      "    Features:\n",
      "      - Word embeddings (FastText pretrained)\n",
      "      - Positional embeddings (sinusoidal pattern)\n",
      "      - Bidirectional LSTM\n",
      "      - Attention mechanism with positional awareness\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Step 5: RNN Model with Positional Embeddings in Attention Layer\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n>>> Step 5: Creating RNN Model with Positional Embeddings...\")\n",
    "\n",
    "class RNNWithPositionalAttentionClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    RNN with Attention and Positional Embeddings\n",
    "    Adds positional information to help the model understand word order\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, \n",
    "                 n_layers=2, dropout=0.5, padding_idx=0, pretrained_embeddings=None,\n",
    "                 attention_dim=None, max_seq_length=100, use_positional_emb=True):\n",
    "        super(RNNWithPositionalAttentionClassifier, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.attention_dim = attention_dim if attention_dim else hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.use_positional_emb = use_positional_emb\n",
    "        \n",
    "        # Word Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "            self.embedding.weight.requires_grad = True\n",
    "        \n",
    "        # Positional Embedding layer (learnable)\n",
    "        if self.use_positional_emb:\n",
    "            self.positional_embedding = nn.Embedding(max_seq_length, embedding_dim)\n",
    "            # Initialize with sinusoidal pattern\n",
    "            self._init_positional_embeddings()\n",
    "        \n",
    "        # Bidirectional LSTM layer\n",
    "        self.rnn = nn.LSTM(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=n_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout if n_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Attention Mechanism with positional awareness\n",
    "        self.attention_linear1 = nn.Linear(hidden_dim * 2, self.attention_dim)\n",
    "        self.attention_linear2 = nn.Linear(self.attention_dim, 1)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Fully connected output layer\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        \n",
    "    def _init_positional_embeddings(self):\n",
    "        \"\"\"Initialize positional embeddings with sinusoidal pattern\"\"\"\n",
    "        pe = torch.zeros(self.max_seq_length, self.embedding_dim)\n",
    "        position = torch.arange(0, self.max_seq_length).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, self.embedding_dim, 2).float() * \n",
    "                            -(np.log(10000.0) / self.embedding_dim))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Set as parameter data (no gradient for positional embeddings initially, but can be learned)\n",
    "        with torch.no_grad():\n",
    "            self.positional_embedding.weight.data = pe\n",
    "    \n",
    "    def forward(self, text, text_lengths):\n",
    "        # text: [batch_size, seq_len]\n",
    "        batch_size, seq_len = text.size()\n",
    "        \n",
    "        # Word Embeddings\n",
    "        word_embeddings = self.embedding(text)  # [batch_size, seq_len, embedding_dim]\n",
    "        \n",
    "        # Positional Embeddings\n",
    "        if self.use_positional_emb:\n",
    "            positions = torch.arange(0, seq_len, device=text.device).unsqueeze(0).expand(batch_size, -1)\n",
    "            positions = torch.clamp(positions, 0, self.max_seq_length - 1)\n",
    "            pos_embeddings = self.positional_embedding(positions)  # [batch_size, seq_len, embedding_dim]\n",
    "            \n",
    "            # Combine word and positional embeddings\n",
    "            embedded = word_embeddings + pos_embeddings\n",
    "        else:\n",
    "            embedded = word_embeddings\n",
    "        \n",
    "        # Pack sequences for efficient RNN processing\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, text_lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        \n",
    "        # Pass through bidirectional LSTM\n",
    "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
    "        \n",
    "        # Unpack the sequences\n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(\n",
    "            packed_output, batch_first=True\n",
    "        )\n",
    "        # output: [batch_size, seq_len, hidden_dim * 2]\n",
    "        \n",
    "        # Apply Attention Mechanism with positional information\n",
    "        attention_scores = self.attention_linear1(output)  # [batch_size, seq_len, attention_dim]\n",
    "        attention_scores = self.tanh(attention_scores)\n",
    "        attention_scores = self.attention_linear2(attention_scores).squeeze(2)  # [batch_size, seq_len]\n",
    "        \n",
    "        # Mask padding positions\n",
    "        mask = torch.arange(seq_len, device=text.device).unsqueeze(0) < text_lengths.unsqueeze(1)\n",
    "        attention_scores = attention_scores.masked_fill(~mask, float('-inf'))\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        attention_weights = torch.softmax(attention_scores, dim=1).unsqueeze(2)  # [batch_size, seq_len, 1]\n",
    "        \n",
    "        # Compute weighted sum of RNN outputs\n",
    "        context_vector = torch.sum(attention_weights * output, dim=1)  # [batch_size, hidden_dim * 2]\n",
    "        \n",
    "        # Apply dropout\n",
    "        context_vector = self.dropout(context_vector)\n",
    "        \n",
    "        # Pass through fully connected layer\n",
    "        output = self.fc(context_vector)  # [batch_size, output_dim]\n",
    "        \n",
    "        return output\n",
    "\n",
    "print(\"    ✓ RNNWithPositionalAttentionClassifier model created\")\n",
    "print(\"    Features:\")\n",
    "print(\"      - Word embeddings (FastText pretrained)\")\n",
    "print(\"      - Positional embeddings (sinusoidal pattern)\")\n",
    "print(\"      - Bidirectional LSTM\")\n",
    "print(\"      - Attention mechanism with positional awareness\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7904aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Step 6: Creating Iterators...\n",
      "    ✓ Training iterator: 4793 samples\n",
      "    ✓ Validation iterator: 1090 samples\n",
      "    ✓ Test iterator: 500 samples\n",
      "    ✓ Batch size: 64\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Step 6: Create Iterators\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n>>> Step 6: Creating Iterators...\")\n",
    "\n",
    "P34_BATCH_SIZE = 64\n",
    "\n",
    "# Create iterators\n",
    "train_iter_p34 = data.BucketIterator(\n",
    "    augmented_train_data,\n",
    "    batch_size=P34_BATCH_SIZE,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    shuffle=True,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Validation and test iterators\n",
    "val_iter_p34 = data.BucketIterator(\n",
    "    validation_data,\n",
    "    batch_size=P34_BATCH_SIZE,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    shuffle=False,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "test_iter_p34 = data.BucketIterator(\n",
    "    test_data,\n",
    "    batch_size=P34_BATCH_SIZE,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    shuffle=False,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"    ✓ Training iterator: {len(augmented_train_data)} samples\")\n",
    "print(f\"    ✓ Validation iterator: {len(validation_data)} samples\")\n",
    "print(f\"    ✓ Test iterator: {len(test_data)} samples\")\n",
    "print(f\"    ✓ Batch size: {P34_BATCH_SIZE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0220a7b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Step 7: Training Part 3.4 Enhanced Model...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     17\u001b[39m P34_NUM_CLASSES = \u001b[38;5;28mlen\u001b[39m(LABEL.vocab)\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Create model\u001b[39;00m\n\u001b[32m     20\u001b[39m p34_model = \u001b[43mRNNWithPositionalAttentionClassifier\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mTEXT\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstoi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mP34_EMBEDDING_DIM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mP34_HIDDEN_DIM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mP34_NUM_CLASSES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_layers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mP34_N_LAYERS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mP34_DROPOUT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTEXT\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstoi\u001b[49m\u001b[43m[\u001b[49m\u001b[43mTEXT\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpad_token\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpretrained_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mP34_ATTENTION_DIM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mP34_MAX_SEQ_LENGTH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_positional_emb\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m>>> Part 3.4 Model Configuration:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     35\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m    Embedding Dim: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mP34_EMBEDDING_DIM\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hiast\\Desktop\\School\\NTU-NLP-Assignment\\nlpEnv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1355\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1352\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1353\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1355\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hiast\\Desktop\\School\\NTU-NLP-Assignment\\nlpEnv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:915\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    914\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m915\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    918\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    919\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    920\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    925\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    926\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hiast\\Desktop\\School\\NTU-NLP-Assignment\\nlpEnv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:942\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    938\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    939\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    940\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    943\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    945\u001b[39m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hiast\\Desktop\\School\\NTU-NLP-Assignment\\nlpEnv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1341\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1334\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1335\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1336\u001b[39m             device,\n\u001b[32m   1337\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1338\u001b[39m             non_blocking,\n\u001b[32m   1339\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1340\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1341\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1342\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1343\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1344\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1345\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1346\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1347\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Step 7: Train Part 3.4 Model\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n>>> Step 7: Training Part 3.4 Enhanced Model...\")\n",
    "\n",
    "# Model hyperparameters\n",
    "P34_EMBEDDING_DIM = embedding_dim\n",
    "P34_HIDDEN_DIM = 256\n",
    "P34_N_LAYERS = 2\n",
    "P34_DROPOUT = 0.5\n",
    "P34_ATTENTION_DIM = 256\n",
    "P34_MAX_SEQ_LENGTH = 100\n",
    "P34_LEARNING_RATE = 0.001\n",
    "P34_N_EPOCHS = 50\n",
    "P34_PATIENCE = 10\n",
    "P34_NUM_CLASSES = len(LABEL.vocab)\n",
    "\n",
    "# Create model\n",
    "p34_model = RNNWithPositionalAttentionClassifier(\n",
    "    vocab_size=len(TEXT.vocab.stoi),\n",
    "    embedding_dim=P34_EMBEDDING_DIM,\n",
    "    hidden_dim=P34_HIDDEN_DIM,\n",
    "    output_dim=P34_NUM_CLASSES,\n",
    "    n_layers=P34_N_LAYERS,\n",
    "    dropout=P34_DROPOUT,\n",
    "    padding_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "    pretrained_embeddings=pretrained_embeddings,\n",
    "    attention_dim=P34_ATTENTION_DIM,\n",
    "    max_seq_length=P34_MAX_SEQ_LENGTH,\n",
    "    use_positional_emb=True\n",
    ").to(device)\n",
    "\n",
    "print(f\"\\n>>> Part 3.4 Model Configuration:\")\n",
    "print(f\"    Embedding Dim: {P34_EMBEDDING_DIM}\")\n",
    "print(f\"    Hidden Dim: {P34_HIDDEN_DIM}\")\n",
    "print(f\"    Layers: {P34_N_LAYERS}\")\n",
    "print(f\"    Dropout: {P34_DROPOUT}\")\n",
    "print(f\"    Attention Dim: {P34_ATTENTION_DIM}\")\n",
    "print(f\"    Max Seq Length: {P34_MAX_SEQ_LENGTH}\")\n",
    "print(f\"    Positional Embeddings: Enabled\")\n",
    "print(f\"    Parameters: {sum(p.numel() for p in p34_model.parameters()):,}\")\n",
    "\n",
    "# Loss function and optimizer\n",
    "p34_criterion = nn.CrossEntropyLoss()\n",
    "p34_optimizer = optim.Adam(p34_model.parameters(), lr=P34_LEARNING_RATE)\n",
    "\n",
    "# Training function (using existing train_model_with_history if available, otherwise create one)\n",
    "def process_batch_p34(batch):\n",
    "    \"\"\"Process batch for Part 3.4\"\"\"\n",
    "    text, text_lengths = batch.text\n",
    "    labels = batch.label\n",
    "    return text, text_lengths, labels\n",
    "\n",
    "# Train the model\n",
    "print(\"\\n>>> Starting training...\")\n",
    "p34_model.train()\n",
    "best_val_acc = 0.0\n",
    "best_val_f1 = 0.0\n",
    "best_val_auc = 0.0\n",
    "patience_counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "train_losses_p34 = []\n",
    "val_losses_p34 = []\n",
    "val_accs_p34 = []\n",
    "val_f1s_p34 = []\n",
    "val_aucs_p34 = []\n",
    "\n",
    "for epoch in range(P34_N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Training phase\n",
    "    p34_model.train()\n",
    "    train_loss = 0\n",
    "    train_preds = []\n",
    "    train_labels = []\n",
    "    \n",
    "    for batch in train_iter_p34:\n",
    "        text, text_lengths, labels = process_batch_p34(batch)\n",
    "        p34_optimizer.zero_grad()\n",
    "        predictions = p34_model(text_lengths, text)\n",
    "        loss = p34_criterion(predictions, labels)\n",
    "        loss.backward()\n",
    "        p34_optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        preds = torch.argmax(predictions, dim=1)\n",
    "        train_preds.extend(preds.cpu().numpy())\n",
    "        train_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_iter_p34)\n",
    "    train_acc = accuracy_score(train_labels, train_preds)\n",
    "    \n",
    "    # Validation phase\n",
    "    p34_model.eval()\n",
    "    val_loss = 0\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "    val_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_iter_p34:\n",
    "            text, text_lengths, labels = process_batch_p34(batch)\n",
    "            predictions = p34_model(text, text_lengths)\n",
    "            loss = p34_criterion(predictions, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            probs = torch.softmax(predictions, dim=1)\n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "            val_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_iter_p34)\n",
    "    val_acc = accuracy_score(val_labels, val_preds)\n",
    "    val_f1 = f1_score(val_labels, val_preds, average='weighted')\n",
    "    \n",
    "    # Calculate AUC-ROC\n",
    "    try:\n",
    "        val_probs_array = np.array(val_probs)\n",
    "        val_labels_bin = label_binarize(val_labels, classes=range(P34_NUM_CLASSES))\n",
    "        val_auc = roc_auc_score(val_labels_bin, val_probs_array, average='weighted', multi_class='ovr')\n",
    "    except:\n",
    "        val_auc = 0.0\n",
    "    \n",
    "    # Store history\n",
    "    train_losses_p34.append(avg_train_loss)\n",
    "    val_losses_p34.append(avg_val_loss)\n",
    "    val_accs_p34.append(val_acc)\n",
    "    val_f1s_p34.append(val_f1)\n",
    "    val_aucs_p34.append(val_auc)\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_val_f1 = val_f1\n",
    "        best_val_auc = val_auc\n",
    "        patience_counter = 0\n",
    "        best_model_state = p34_model.state_dict().copy()\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = divmod(end_time - start_time, 60)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02}/{P34_N_EPOCHS} | Time: {int(epoch_mins)}m {int(epoch_secs)}s')\n",
    "    print(f'\\tTrain Loss: {avg_train_loss:.4f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\tVal Loss: {avg_val_loss:.4f} | Val Acc: {val_acc*100:.2f}% | Val F1: {val_f1:.4f} | Val AUC: {val_auc:.4f}')\n",
    "    \n",
    "    if patience_counter >= P34_PATIENCE:\n",
    "        print(f'\\t>>> Early stopping at epoch {epoch+1}, best val acc: {best_val_acc*100:.2f}%')\n",
    "        break\n",
    "\n",
    "# Load best model state\n",
    "if best_model_state is not None:\n",
    "    p34_model.load_state_dict(best_model_state)\n",
    "\n",
    "# Save model\n",
    "torch.save(p34_model.state_dict(), 'weights/part34_enhanced_best.pt')\n",
    "print(f\"\\n>>> Part 3.4 model saved to 'weights/part34_enhanced_best.pt'\")\n",
    "\n",
    "p34_history = {\n",
    "    'train_losses': train_losses_p34,\n",
    "    'val_losses': val_losses_p34,\n",
    "    'val_accs': val_accs_p34,\n",
    "    'val_f1s': val_f1s_p34,\n",
    "    'val_aucs': val_aucs_p34,\n",
    "    'best_val_acc': best_val_acc,\n",
    "    'best_val_f1': best_val_f1,\n",
    "    'best_val_auc': best_val_auc\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda6694d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Step 8: Evaluate Part 3.4 Model on Test Set\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n>>> Step 8: Evaluating Part 3.4 Model on Test Set...\")\n",
    "\n",
    "p34_model.eval()\n",
    "test_loss_p34 = 0\n",
    "test_preds_p34 = []\n",
    "test_labels_p34 = []\n",
    "test_probs_p34 = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_iter_p34:\n",
    "        text, text_lengths, labels = process_batch_p34(batch)\n",
    "        predictions = p34_model(text, text_lengths)\n",
    "        loss = p34_criterion(predictions, labels)\n",
    "        test_loss_p34 += loss.item()\n",
    "        \n",
    "        probs = torch.softmax(predictions, dim=1)\n",
    "        preds = torch.argmax(predictions, dim=1)\n",
    "        test_preds_p34.extend(preds.cpu().numpy())\n",
    "        test_labels_p34.extend(labels.cpu().numpy())\n",
    "        test_probs_p34.extend(probs.cpu().numpy())\n",
    "\n",
    "test_loss_p34 = test_loss_p34 / len(test_iter_p34)\n",
    "test_acc_p34 = accuracy_score(test_labels_p34, test_preds_p34)\n",
    "test_f1_p34 = f1_score(test_labels_p34, test_preds_p34, average='weighted')\n",
    "\n",
    "try:\n",
    "    test_probs_array_p34 = np.array(test_probs_p34)\n",
    "    test_labels_bin_p34 = label_binarize(test_labels_p34, classes=range(P34_NUM_CLASSES))\n",
    "    test_auc_p34 = roc_auc_score(test_labels_bin_p34, test_probs_array_p34, average='weighted', multi_class='ovr')\n",
    "except:\n",
    "    test_auc_p34 = 0.0\n",
    "\n",
    "print(f\"\\n>>> Part 3.4 Test Set Results:\")\n",
    "print(f\"    Test Loss: {test_loss_p34:.4f}\")\n",
    "print(f\"    Test Accuracy: {test_acc_p34*100:.2f}%\")\n",
    "print(f\"    Test F1 Score: {test_f1_p34:.4f}\")\n",
    "print(f\"    Test AUC-ROC: {test_auc_p34:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bff8e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Step 9: Topic-wise Accuracy Evaluation and Comparison\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n>>> Step 9: Topic-wise Accuracy Evaluation...\")\n",
    "\n",
    "def evaluate_per_topic_p34(model, iterator, device):\n",
    "    \"\"\"Evaluate model performance per topic category\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    topic_correct = defaultdict(int)\n",
    "    topic_total = defaultdict(int)\n",
    "    \n",
    "    idx_to_label = LABEL.vocab.itos\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text, text_lengths, labels = process_batch_p34(batch)\n",
    "            predictions = model(text, text_lengths)\n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            \n",
    "            for pred, label in zip(preds.cpu().numpy(), labels.cpu().numpy()):\n",
    "                topic_name = idx_to_label[label]\n",
    "                topic_total[topic_name] += 1\n",
    "                if pred == label:\n",
    "                    topic_correct[topic_name] += 1\n",
    "    \n",
    "    # Calculate accuracy per topic\n",
    "    topic_accuracies = {}\n",
    "    for topic in sorted(topic_total.keys()):\n",
    "        acc = topic_correct[topic] / topic_total[topic] if topic_total[topic] > 0 else 0\n",
    "        topic_accuracies[topic] = acc\n",
    "        print(f'  {topic}: {topic_correct[topic]}/{topic_total[topic]} = {acc*100:.2f}%')\n",
    "    \n",
    "    return topic_accuracies\n",
    "\n",
    "print(\"\\n>>> Part 3.4 Enhanced Model - Topic-wise Accuracy:\")\n",
    "p34_topic_acc = evaluate_per_topic_p34(p34_model, test_iter_p34, device)\n",
    "\n",
    "# Note: To compare with Part 2(e), you would need to load and evaluate the baseline model\n",
    "# For now, we'll display Part 3.4 results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 3.4 TOPIC-WISE ACCURACY SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Topic':<10} {'Accuracy':<12} {'Correct':<10} {'Total':<10}\")\n",
    "print(\"-\" * 50)\n",
    "for topic in sorted(p34_topic_acc.keys()):\n",
    "    # Get counts from evaluation\n",
    "    # (We'd need to modify evaluate_per_topic_p34 to return counts, or re-evaluate)\n",
    "    acc = p34_topic_acc[topic]\n",
    "    print(f\"{topic:<10} {acc*100:<12.2f}%\")\n",
    "\n",
    "print(\"\\n>>> Note: Compare these results with Part 2(e) baseline for improvement analysis\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e096065b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Step 10: Plot Training Curves for Part 3.4\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n>>> Step 10: Plotting Training Curves...\")\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Training Loss Curve\n",
    "ax1.plot(p34_history['train_losses'], label='Part 3.4 Train Loss', marker='o', linewidth=2, color='blue')\n",
    "ax1.plot(p34_history['val_losses'], label='Part 3.4 Val Loss', marker='s', linewidth=2, color='red')\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Loss', fontsize=12)\n",
    "ax1.set_title('Training Loss Curve (Part 3.4)', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Validation Accuracy Curve\n",
    "ax2.plot([acc*100 for acc in p34_history['val_accs']], label='Part 3.4 Val Acc', marker='o', linewidth=2, color='purple')\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Validation Accuracy (%)', fontsize=12)\n",
    "ax2.set_title('Validation Accuracy Curve (Part 3.4)', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('part3_4_training_curves.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"    Saved training curves to 'part3_4_training_curves.png'\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n>>> Training curves plotted and saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9059c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PART 3.4 SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 3.4: TARGETED IMPROVEMENT FOR WEAK TOPICS - SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n>>> Strategies Implemented:\")\n",
    "print(\"  1. Data Augmentation for Imbalanced Classes\")\n",
    "print(\"     - ABBR: 69 -> 500 samples (~7x augmentation)\")\n",
    "print(\"     - Used synonym replacement, random insertion, deletion, and swap\")\n",
    "print(\"     - Other classes maintained original size\")\n",
    "print(\"\\n  2. Positional Embeddings in Attention Layer\")\n",
    "print(\"     - Sinusoidal positional embeddings added to word embeddings\")\n",
    "print(\"     - Helps model understand word order and sequence structure\")\n",
    "print(\"     - Integrated into attention mechanism\")\n",
    "\n",
    "print(\"\\n>>> Model Configuration:\")\n",
    "print(f\"  - Embedding Dim: {P34_EMBEDDING_DIM}\")\n",
    "print(f\"  - Hidden Dim: {P34_HIDDEN_DIM}\")\n",
    "print(f\"  - Layers: {P34_N_LAYERS} (bidirectional LSTM)\")\n",
    "print(f\"  - Dropout: {P34_DROPOUT}\")\n",
    "print(f\"  - Attention Dim: {P34_ATTENTION_DIM}\")\n",
    "print(f\"  - Max Seq Length: {P34_MAX_SEQ_LENGTH}\")\n",
    "print(f\"  - Learning Rate: {P34_LEARNING_RATE}\")\n",
    "print(f\"  - Batch Size: {P34_BATCH_SIZE}\")\n",
    "\n",
    "print(\"\\n>>> Results:\")\n",
    "print(f\"  - Best Val Accuracy: {p34_history['best_val_acc']*100:.2f}%\")\n",
    "print(f\"  - Best Val F1: {p34_history['best_val_f1']:.4f}\")\n",
    "print(f\"  - Best Val AUC: {p34_history['best_val_auc']:.4f}\")\n",
    "print(f\"  - Test Accuracy: {test_acc_p34*100:.2f}%\")\n",
    "print(f\"  - Test F1: {test_f1_p34:.4f}\")\n",
    "print(f\"  - Test AUC: {test_auc_p34:.4f}\")\n",
    "\n",
    "print(\"\\n>>> Topic-wise Accuracy (Test Set):\")\n",
    "for topic in sorted(p34_topic_acc.keys()):\n",
    "    print(f\"  - {topic}: {p34_topic_acc[topic]*100:.2f}%\")\n",
    "\n",
    "print(\"\\n>>> Expected Improvements:\")\n",
    "print(\"  - ABBR accuracy should improve significantly due to:\")\n",
    "print(\"    * 7x data augmentation (69 -> 500 samples)\")\n",
    "print(\"    * Positional embeddings helping with question structure\")\n",
    "print(\"\\n  - Other topics should maintain or slightly improve performance\")\n",
    "\n",
    "print(\"\\n>>> Files Saved:\")\n",
    "print(\"  - Model: weights/part34_enhanced_best.pt\")\n",
    "print(\"  - Embeddings: embedding_weights_part34_enhanced.pt\")\n",
    "print(\"  - Training Curves: part3_4_training_curves.png\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 3.4 COMPLETE\")\n",
    "print(\"=\"*80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
